issueId,body
324403942,"feature description universal language model fine-tuning for text classification presented a novel method to fine tune a pre-trained universal language model to a particular classification task which achieved beyond state-of-the art reduction in error rate on multiple benchmark text classification tasks the fine tuning requires very few examples to achieve very good results here is an excerpt of the abstract which provides a good tl;dr of the paper duh):inductive transfer learning has greatly impactedcomputer vision but existing approachesin nlp still require task-specificmodifications and training from scratch.we propose universal language modelfine-tuning ulmfit an effective transferlearning method that can be applied toany task in nlp and introduce techniquesthat are key for fine-tuning a languagemodel our method significantly outperformsthe state-of-the-art on six text classificationtasks reducing the error by on the majority of datasets furthermore,with only labeled examples it matches the performance of training fromscratch on more data we opensourceour pretrained models and codei propose that spacy adds their pre-trained models and a simple way to fine tune to a new task as a core feature of the library please describe the feature which area of the library is it related to what specific solution would you like could the feature be a custom component or spacy plugin so we will tag it as project idea so other users can take it on.this seems like a core feature of spacy greatly increasing its industrial potential i would argue to make it a first class citizen if authors and licensing of this work permits that"
306216625,provide a general summary of your changes in the title descriptionpip v has mercilessly broken anything that runs import pip see and for lolz can reproduce for spacy x by doing in a virtual environment): pip install upgrade pippip install spacy==.python m spacy and observe this error: traceback most recent call last file users/orionmontoya/.pyenv/versions/../python.framework/versions/./lib/python./runpy.py line in run_module_as_main mod_name mod_spec code get_module_details(mod_name error file users/orionmontoya/.pyenv/versions/../python.framework/versions/./lib/python./runpy.py line in get_module_details return get_module_details(pkg_main_name error file users/orionmontoya/.pyenv/versions/../python.framework/versions/./lib/python./runpy.py line in get_module_details import__(pkg_name file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/spacy/__init__.py line in module from deprecated import resolve_model_name file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/spacy/deprecated.py line in module from cli import download file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/spacy/cli/__init__.py line in module from download import download file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/spacy/cli/download.py line in module from link import link_package file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/spacy/cli/link.py line in module import pip file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/pip/__init__.py line in module from pip.vcs import git mercurial subversion bazaar noqa file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/pip/vcs/mercurial.py line in module from pip.download import path_to_url file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/pip/download.py line in module from pip._vendor import requests six file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/pip/_vendor/requests/__init__.py line in module from import packages file users/orionmontoya/spacy-demo-venv/lib/python./site-packages/pip/_vendor/requests/packages.py line in module sys.modules pip._vendor.requests.packages mod sys.modules pip._vendor mod keyerror pip._vendor.urllib.contrib pip is imported only to check is_package when downloading models and spacy x has a better implementation using setuptools pkg_resources since i have several projects still dependent on spacy x i would rather not do pips workaround on pinning to pip so i backported what was in x.it seems to me that is a showstopper that merits a patch release types of changebugfix backport of x code and test).when i tried to run the tests locally in my venv i got importerror no module named spacy.gold and im not sure why didnt have this issue on prior prs but it runs fine in travis checklist before you submit the pr go over this checklist and make sure you cantick off all the boxes x x i have submitted the spacy contributor agreement x i ran the tests and all new and existing tests passed x my changes dont require a change to the documentation or if they do ive added all required information
298978549,update best way to run the conll experiments is: git clone b developcd spacymake./dist/spacy.pex ud-train help the conference for natural language learning conll shared task is a great standard for evaluating parsing algorithms unlike previous parsing evaluations conll is end-to-end from raw text to dependencies across many languages while we missed the evaluation id like to participate in to participate in conll we would need to adapt tokenizers for to match ud tokenization more closely add pipeline component for statistical lemmatization to improve lemmatizer coverage across languages add pipeline component to predict morphological tags support joint segmentation and tagging or parsing for languages like chinese.all of these are great goals regardless of the competition however its a lot of work especially the tokenization which really needs speakers of the various languages.even if we dont get everything done in time to participate in the official evaluation it will be a great step for spacy to publish accuracy figures using the official evaluation software and methodology this will allow direct comparison against other systems and make quality control across languages much easier.what would be really awesome is if we got a few people working on this together so we could participate as team spacy ideally wed have people taking ownership of some of the main languages e.g french spanish german chinese japanese etc its much easier to work on a specific language that youre well familiar with the official evaluation will consider all language equally but im okay with having low accuracy on like ancient greek or dothraki.the official testing period will run april to june however we can get started right away by working with the conll data.to get started ive made a quick script to run an experiment which ive been testing on the english data you can run it by building the feature/better-gold branch and running the examples/training/conllu.py script like so: python examples/training/conllu.py en data/ud-treebanks-conll/ud_english/en-ud-train.conllu data/ud-treebanks-conll/ud_english/en-ud-train.txt data/ud-treebanks-conll/ud_english/en-ud-dev.conllu data/ud-treebanks-conll/ud_english/en-ud-dev.txt tmp/dev.conllu this will write you an output file tmp/dev.conllu after each training epoch which you can pass into the official conll evaluation scorer scores currently suck as there are various things to tweak and fix but at least the evaluation runs
296338036,related issues were currently in the process of rewriting the match loop fixing long-standing issues and making it easier to extend the matcher and phrasematcher the community contributions by gregdubbin and savkov have already made a big difference we cant wait to get it all ready and shipped.this issue discusses some of the planned new features and additions to the match patterns api including matching by custom extension attributes token regular expressions set membership and rich comparison for numeric values new features custom extension attributesspacy v introduced custom extension attributes on the doc span and token custom attributes make it easier to attach arbitrary data to the built-in objects and let users take advantage of spacys data structures and the doc object as the single source of truth however not being able to match on custom attributes was quite limiting see the new patterns spec will allow an space on token patterns which can map to a dictionary keyed by the attribute names: pythontoken.set_extension(is_fruit getter=lambda token token.text in apple banana))pattern lemma have is_fruit true}} matcher.add(having_fruit none pattern) both regular attribute extensions with a default value and property extensions with a getter will be supported and can be combined for more exact matches. pythonpattern is_fruit true fruit_color red fruit_rating rich comparison for numeric valuestoken patterns already allow specifying a length the tokens character length however matching tokens of between five and ten characters previously required adding copies of the exact same pattern introducing unnecessary overhead numeric attributes can now also specify a dictionary with the predicate e.g or mapped to the value for example: pythonpattern ent_type org length exact lengthpattern ent_type org length length with predicate the second pattern above will match a token with the entity type org thats or more characters long combined with custom attributes this allows very powerful queries combining both linguistic features and numeric data: python match a token based on custom numeric attributespattern fruit_rating fruit_weight match a verb with sentiment_score and one token on each sidepattern pos verb sentiment_score defining predicates and values as a dictionary instead of a single string like allows us to avoid string parsing and lets spacy handle custom attributes without requiring the user to specify their types upfront while we know the type of the built-in length attribute spacy has no way of knowing whether the value of a custom attribute should be interpreted as less than or the heart emoticon set membershipthis is another feature that has been requested before and will now be much easier to implement similar to the predicate mapping for numeric values token attributes can now also be defined as dictionaries the keys in or not_in can be used to indicate set membership and non-membership. pythonpattern lemma in like love lower in apples bananas }} the above pattern will match a token with the lemma like or love followed by a token whose lowercase form is either apples or bananas for example loving apples or likes bananas lists can be used for all non-boolean values including custom attributes: python verb or conjunction followed by custom is_fruit tokenpattern pos in verb conj cconj is_fruit true fruit_color not_in red yellow set membership of numeric custom attributespattern is_customer true customer_year in combination of predicates and and non-membershippattern custom_count not_in regular expressionsusing regular expressions within token patterns is already possible via custom binary flags see however this has some inconvenient limitations including the patterns not being json-serializable if the solution is to add binary flags spacy might as well take care of that the following example is based on the work by savkov see pythonpattern orth regex uu (\\.?|nited ss (\\.?|tates lower president} regex as an operator instead of a top-level property that only matches on the tokens text allows defining rules for any string value including custom attributes: python match tokens with fine-grained pos tags starting with vpattern tag regex v match custom attribute values with regular expressionspattern country regex uu (\\.?|nited ss (\\.?|tates new operatorstl;dr the new patterns spec will allow two ways of defining properties attribute values for exact matches and dictionaries using operators for more fine-grained matches. python property value exact match property operator value match with operators} the following operators can be used within dictionaries describing attribute values operator value type description example int float attribute value is equal greater or equal smaller or equal greater or smaller length in any attribute value is member of a list lemma in like love not_in any attribute value is not member of a list pos not_in noun propn regex unicode attribute value matches regular expression tag regex v api improvements and bug fixessee honnibals comments in and the feature/better-faster-matcher branch for more details and implementation examples other fixes x remove hard-coded length limit of on the phrasematcher fix pickling of phrasematcher x matcher.pipe should yield matches instead of doc objects support deleting rules in phrasematcher
272034397,please provide a summary in the title and describe your issue here.is this a bug or feature request if a bug include all the steps that led to the issue.if youre looking for help with your code consider posting a question on stackoverflow instead your environment include details of your environment if youre using spacy you can also type python m spacy info markdown and copy-paste the result here info about spacy python version platform linux-..--generic-x_-with-debian-stretch-sid spacy version models eni just updated to v not sure what changed but the exact same pipeline of documents called in the standard nlp spacy.load(en nlp(ustring way is now x slower
270571251,fasttext is a nice tool and their algorithm makes a lot of sense for word vector learning for many languages its really not sufficient to have one id per whitespace-delimited word in languages such as french russian arabic hindi etc word formation is compositional stem affixes fasttext includes character ngram features to handle this.spacy should read in not only fasttexts word vector table but also its character ngram weights this will allow the vectors to be derived for novel words on the fly
233681321,were very excited to finally publish the first alpha pre-release of spacy v its still an early release and obviously not intended for production use you might come across a notimplementederror see the release notes for the implementation details that are still missing this thread is intended for general discussion feedback and all questions related to v if you come across more complex bugs feel free to open a separate issue quickstart overview spacy v alpha release notes whats new in v overview of new features backwards compatibility and a guide on migrating from v.x spacy everything you need to know the most important concepts and features explained in simple terms and with examples and illustrations spacy v alpha docs the most important new features new neural network models for english mb and multi-language ner mb plus gpu support via chainer cupy strings mapped to hash values instead of integer ids this means they will always match even across models improved saving and loading consistent serialization api across objects plus pickle support built-in displacy visualizers with jupyter notebook support improved language data with support for lazy loading and multi-language models alpha tokenization for norwegian bokml japanese danish and polish lookup-based lemmatization for english german french spanish italian hungarian portuguese and swedish revised api for matcher and language processing pipelines trainable document vectors and contextual similarity via convolutional neural networks various bug fixes and almost completely re-written documentation installationspacy v..-alpha is available on pip as spacy-nightly if you want to test the new version we recommend setting up a clean environment first to install the new model youll have to download it with its full name using the direct flag. bashpip install spacy-nightlypython m spacy download en_core_web_sm-..-alpha direct englishpython m spacy download xx_ent_wiki_sm-..-alpha direct multi-language ner pythonimport spacynlp spacy.load(en_core_web_sm) pythonimport en_core_web_smnlp en_core_web_sm.load() alpha models for german french and spanish are coming soon now on to the fun part stickers!! stickers just got our first delivery of spacy stickers and want to to share them with you theres only one small favour wed like to ask the part were currently behind on are the tests this includes our test suite as well as in-depth testing of the new features and usage examples so heres the idea find something thats currently not covered in the test suite and doesnt require the models and write a test for it for example language-specific tokenization tests alternatively find examples from the docs that havent been added to the tests yet and add them plus points if the examples dont actually work this means youve either discovered a bug in spacy or a bug in the docs submit a pr with your test to the develop branch if the test covers a bug and currently fails mark it with pytest.mark.xfail for more info see the test suite docs once your pull request is accepted send us your address via email (mailto:contact@explosion.ai or private message on gitter and well mail you stickers.if you cant find anything dont have time or cant be bothered thats fine too posting your feedback on spacy v here counts as well to be honest we really just want to mail out stickers
180419826,honnibal congrats on the release milestone!im posting to see what sort of community exists amonst spacy users for more robust date and time parsing for many nl-based applications date and time parsing is tremendously useful but is a difficult task for a statistical parser to provide consistent results from application to application there are so many arbitrary dates ranges and holidays that can confound a purely statistical date time parser there is potential within spacys existing features matcher and phrasematcher classes to build out a rule-based parser similar to that of stanfords sutime see sutime in action or wit.ais open source duckling sutime parses text using token-oriented regex patterns to map to semantic objects representing dates duration ranges etc these semantic objects can be combined to create higher-order date and time annotations based on additional rules duckling takes a more functional approach written in clojure and uses primitive rule based functions that can be composed into higher order functions ultimately returning potential date and time annotations i believe it further uses a naive bayes classifier to pick out the likeliest annotation as duckling can also parse out any sort of rule-based annotation currency etc thereby resolving conflicts.i am curious if there is a market for creating a similar parser built within the spacy ecosystem and under the same license of course some requirements would be as follows create a dsl to represent rules and patterns recognize what usable features in spacy can be extended and what features must be implemented method to combine rules in to higher order representations object composition tree structure function composition documentation tutorials on rule creation source code etc integrate parser into spacys pipeline make it fast in the spirit of spacyif this is not the right forum for this discussion please let me know is spacys gitter up a running id be happy to maintain this projects development but it would be a significant undertaking and a bit beyond my level of expertise.cheers!carl
172799139,im having the exact same problem as maybe dns got messed up again?thanks
160110302,the displacy page at spacy.io/demos/displacy now returns a it was a very useful reference for working with dependency trees in spacy i hope it will be coming back
