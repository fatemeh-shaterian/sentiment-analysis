issueId,body
342139923,this form is for bug reports and feature requests only!if youre looking for help check stack overflow and the troubleshooting guide the matter is security related please disclose it privately via this a bug report or feature request uncomment only one leave it on its own line:>/kind bug kind feature what happened :in the process of running kubeadm init kubernetes attempted to pull images required for setting up a kubernetes cluster and then failed to pull all of the images required. what you expected to happen :i expected a new cluster to be created with my vm as the master node. anything else we need to know? : environment kubernetes version use kubectl version cloud provider or hardware configuration os e.g from etc/os-release ubuntu lts kernel e.g uname a install tools apt others: screenshots ! image syslog entries_! image
340203045,is this a bug report or feature request? :/kind bug what happened :in ubuntu dns queries are being served via systemd-resolved daemon which is listening on and serves queries only from the only line in etc/resolv.conf is nameserver and this file is managed by systemd.once any pod with hostport notation appears on the host kubelet fires the iptables rule in postrouting chain of nat table a postrouting s o lo m comment comment snat for localhost access to hostports j masqueradesee commit lines details.therefore all queries to goes not from but from interface with default route due to masquerading and systemd-resolved rejects all of these requests with systemd-resolved got packet on unexpected ip range refusing. how to reproduce it as minimally and precisely as possible) :just install kubernetes with cni on ubuntu with kubeadm kubespray chef-kubernetes whatever and run any pod with hostport .voila you just lost dns resolving. environment kubernetes version use kubectl version v cloud provider or hardware configuration digital ocean but reproduceable on any configuration os e.g from etc/os-release ubuntu lts kernel e.g uname a linux kube generic ubuntu smp wed may utc x x x gnu/linux install tools chef-kubernetes but reproduceable with any install tool others cni cni plugins cni daemon weave/sig network
336552550,kind bug what happened :when getting nodes or pods using the command kubectl get pods kubectl get pods the following output appears: no resources found.error from server notacceptable unknown get nodes) no resources found.error from server notacceptable unknown get pods) what you expected to happen :when running the command kubectl get pods/nodes with kubectl version pods/nodes appear. how to reproduce it as minimally and precisely as possible run kubectl get pods using kubectl anything else we need to know? : environment kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:ebfdfcddfdacbececceefe gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:befcbadbffdcbcda gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration aws os e.g from etc/os-release kernel: linux default-ef-bf--ae-bbad generic ubuntu smp wed sep utc x x x gnu/linux install tools: curl lo
333689159,kind bug what happened :installed ks using kubeadm and added the admin config as described when asking for a kubectl cluster-info dump i get error missing apiversion or kind and cannot assign it no kind is registered for the type core.nodelist what you expected to happen :to get a cluster info dump how to reproduce it as minimally and precisely as possible) :with the following kubeadm.conf apiversion kubeadm.ks.io/valphakind masterconfigurationapi advertiseaddress networking servicesubnet fd:::/kubernetesversion beta.cloudprovider externalfeaturegates coredns falsecrisocket var/run/containerd/containerd.sock run sudo kubeadm init config=kubeadm.conf mkdir p home/.kube sudo cp i etc/kubernetes/admin.conf home/.kube/config sudo chown id u):$(id g home/.kube/config kubectl cluster-info kubectl cluster-info dump anything else we need to know? :running containerd environment kubernetes version use kubectl version ):client version version.info{major minor gitversion:v..-beta gitcommit:becfcfebaedcce gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v..-beta gitcommit:becfcfebaedcce gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration:brightbox os e.g from etc/os-release):ubuntu lts kernel e.g uname a ):linux srv-ir generic ubuntu smp wed may utc x x x gnu/linux install tools:kubeadm version version.info{major minor gitversion:v..-beta gitcommit:becfcfebaedcce gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd others:containerd github.com/containerd/containerd v..-rc cbefeaebbafeabecfaa
325889581,kind bug what happened :brand new hypriotos install on a raspberrypi model b installed kubeadm kubectl and kubelet per kubelet seg faults with kubeletunexpected fault address xcbafatal error fault signal sigsegv segmentation violation code=x addr=xcba pc=xcba goroutine running locked to thread :runtime.throw(xaae x usr/local/go/src/runtime/panic.go x fp=xe sp=xec pc=xefaruntime.sigpanic usr/local/go/src/runtime/signal_unix.go xcc fp=xebc sp=xe pc=xcks.io/kubernetes/vendor/github.com/appc/spec/schema/types.semver.empty workspace/anago-v..-beta..+bbadda/src/ks.io/kubernetes/_output/dockerized/go/src/ks.io/kubernetes/vendor/github.com/appc/spec/schema/types/semver.go:ks.io/kubernetes/vendor/github.com/appc/spec/schema/types.newsemver(xad xb xafbcf xb xc workspace/anago-v..-beta..+bbadda/src/ks.io/kubernetes/_output/dockerized/go/src/ks.io/kubernetes/vendor/github.com/appc/spec/schema/types/semver.go x fp=xf sp=xec pc=xcdgoroutine chan receive :ks.io/kubernetes/vendor/github.com/golang/glog.(*loggingt).flushdaemon(xf workspace/anago-v..-beta..+bbadda/src/ks.io/kubernetes/_output/dockerized/go/src/ks.io/kubernetes/vendor/github.com/golang/glog/glog.go xcreated by ks.io/kubernetes/vendor/github.com/golang/glog.init workspace/anago-v..-beta..+bbadda/src/ks.io/kubernetes/_output/dockerized/go/src/ks.io/kubernetes/vendor/github.com/golang/glog/glog.go xagoroutine syscall :os/signal.signal_recv(xbdc usr/local/go/src/runtime/sigqueue.go xos/signal.loop usr/local/go/src/os/signal/signal_unix.go xcreated by os/signal.init usr/local/go/src/os/signal/signal_unix.go x what you expected to happen : kubelet not to seg fault how to reproduce it as minimally and precisely as possible) :i believe any new install on armv hosts should experience this seg fault. anything else we need to know? :downgrading to kubelet resolves this issue: hypriotos/armv root@node in aptitude install kubelet=..-the following packages will be downgraded kubelet packages upgraded newly installed downgraded to remove and not upgraded.need to get mb of archives after unpacking kb will be freed.get kubernetes-xenial/main armhf kubelet armhf mb fetched mb in s kb/s)dpkg warning downgrading kubelet from to reading database files and directories currently installed.)preparing to unpack kubelet_..-_armhf.deb unpacking kubelet over setting up kubelet current status upgradable.hypriotos/armv root@node in kubelet versioni feature_gate.go feature gates map }w cni.go unable to update cni config no networks found in etc/cni/net.dw hostport_manager.go the binary conntrack is not installed this can cause failures in network connection cleanup.i server.go version v..i feature_gate.go feature gates map }i plugins.go no cloud provider specified.w server.go standalone mode no api cliente machine.go failed to get cache information for node open sys/devices/system/cpu/cpu/cache no such file or directoryw server.go no api server defined no events will be sent to api server.i server.go cgroups-per-qos enabled but cgroup-root was not specified defaulting to i container_manager_linux.go container manager verified user specified cgroup-root exists i container_manager_linux.go creating container manager object based on node config runtimecgroupsname systemcgroupsname kubeletcgroupsname containerruntime:docker cgroupsperqos:true cgrouproot cgroupdriver:cgroupfs kubeletrootdir:/var/lib/kubelet protectkerneldefaults:false nodeallocatableconfig:{kubereservedcgroupname systemreservedcgroupname enforcenodeallocatable:map pods kubereserved:map systemreserved:map hardevictionthresholds: {signal:memory.available operator:lessthan value:{quantity:mi percentage graceperiod:s minreclaim:
323892489,is this a bug report or feature request? :/kind feature/sig aws anything else we need to know? :aws nlbs support static ip addresses by associating an eip per az an already allocated eip can be supplied with the subnetmapping user can then supply eip allocation ids in the service definition of kubernetes by that the service will be available via those static ips
316927434,this form is for bug reports and feature requests only!if youre looking for help check stack overflow and the troubleshooting guide this may be security issue please disclose it privately via this a bug report or feature request? :/kind feature what you expected to happen : ec tags are useful for identifying types of compute resources for billing purposes identifying ownership within your organization segregating workloads and more node labels are used for similar purposes so it feels natural to expose ec tags or a subset of them perhaps based on a certain key prefix to kubernetes via node labels would love a discussion of how this behavior should be exposed environment cloud provider or hardware configuration aws os e.g from etc/os-release any sig aws
314393574,is this a bug report or feature request kind bug what happened liveness/readiness probes are failing so frequently and the failures are also inconsistent some of the pods in the same deployment are getting stuck in the crashloopbackoff back-off restarting failed containererror syncing pod skipping failed to startcontainer for web with crashloopbackoff back-off ms restarting failed container=web pod=web-controller--qtfg_micro-services(afb-a-e-aa-bce)liveness probe failed get dial tcp getsockopt connection refusedfailed to start container with id deeaadbaabdbaccebdbfeed with error rpc error code desc failed to create symbolic link var/log/pods/afb-a-e-aa-bce/web_.log to the container log file var/lib/docker/containers/deeaadbaabdbaccebdbfeed/deeaadbaabdbaccebdbfeed-json.log for container deeaadbaabdbaccebdbfeed symlink var/lib/docker/containers/deeaadbaabdbaccebdbfeed/deeaadbaabdbaccebdbfeed-json.log var/log/pods/afb-a-e-aa-bce/web_.log file exists and few more errors from other pods! error syncing pod skipping failed to createpodsandbox for work--zkf_micro-services(dd-f-e-aa-bce with createpodsandboxerror createpodsandbox for pod work--zkf_micro-services(dd-f-e-aa-bce failed rpc error code desc networkplugin kubenet failed to set up pod work--zkf_micro-services network error adding container to network failed to connect vethdaac to bridge cbr exchange full here is how my livenessprobe config livenessprobe httpget path port scheme http initialdelayseconds timeoutseconds periodseconds successthreshold failurethreshold what you expected to happen :health checks to pass if the app is running how to reproduce it as minimally and precisely as possible) : anything else we need to know? :its a nodejs app and its listening on port and its also exposed in dockerfile. environment kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:fefbcbcffdbedfcbfbebacd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:cccfbbceaafaff gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration aws os e.g from etc/os-release ubuntu lts kernel e.g uname a linux ip ks smp fri jul utc x gnu/linux install tools kops others
313269685,is this a bug report or feature request? :/kind bug what happened spec.backofflimit for a job does not work on kubernetes what you expected to happen : .spec.backofflimit can limit the number of time a pod is restarted when running inside a job how to reproduce it as minimally and precisely as possible) :use this resource file: apiversion batch/vkind jobmetadata name errorspec backofflimit template metadata name job spec restartpolicy never containers name job image ubuntu args sh c sleep false if the job was created in kubernetes it will soon fail: ....status conditions lastprobetime t::z lasttransitiontime t::z message job has reach the specified backoff limit reason backofflimitexceeded status true type failed failed starttime t::z... while creating the job in kubernetes it will be restarted infinitely: ...status active failed starttime t::z... anything else we need to know? : environment kubernetes version use kubectl version cloud provider or hardware configuration aws os e.g from etc/os-release ubuntu kernel e.g uname a linux install tools kubeadm others
312887074,kind feature/sig node what happened :a program started leaking tcp memory which filled up the nodes tcp stack memory the network performance on the node degraded and connections to pods running on the node either times out or will hang for a long time.nodes dmesg had lines mentioning tcp out of memory consider tuning tcp_mem further reading and investigation reveals that this could happen when tcp stack runs out of memory pages allocated by kernel or when there are lot of orphaned/open sockets.tcp stack limits max bash cat proc/sys/net/ipv/tcp_mem min pressure max usage when issue happened mem cat proc/net/sockstatsockets used tcp inuse orphan tw alloc mem udp inuse mem udplite inuse raw inuse frag inuse memory kubelet posts node status as ready. what you expected to happen :kubelet should say node is not ready.it would be great if kubelet could track the tcp_mem stats also along with cpu/ram/disk as network is also an important factor if tcp_mem limit is hit for some reason the node is not usable notifying the user that node has some issue can help debugging and further identifying the cause. how to reproduce it as minimally and precisely as possible create a gke cluster label a node to carry out tests bash kubectl label node node-name node=leak-test create an nginx deployment with loadbalancer with can serve a large file bash kubectl create f check if you can download the large file bash curl o large-file ip>/large-file create a deployment that can fill up the tcp stack memory bash kubectl create f ssh into the node and observe cat proc/sys/net/ipv/tcp_mem and cat proc/net/sockstat and scale the deployment until the current mem exceeds the limit try downloading the large file again it will either become very slow or will not happen at all anything else we need to know? :this is more of a feature request for kubelet rather than a bug tcp mem can get filled if the node is running a lot of tcp heavy workloads need not necessarily be a leak since kubelet is ultimately responsible for reporting nodes health network should also be a parameter. environment kubernetes version use kubectl version v..-gke cloud provider or hardware configuration gke node n-standard os e.g from etc/os-release container-optimized os kernel e.g uname a install tools others
311470925,kind feature/sig api-machinery/sig architecture/sig authim currently setting up a docker-image registry for multiple clusters with multiple namespaces in order to make this work for every namespace in every cluster i have to create a secret for each single namespace.i have no idea about the implications but from a users perspective it would be handy to have something like global secrets available that work across namespaces.im sorry if this was already asked i couldnt find anything related
304085891,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide request :/kind feature what happened :kubernetes uses golang/glog extensively for logging while this is reasonable enough for the kubernetes binaries sizeable parts of the codebase are re-used as client libraries eg ks.io/client-go this presents several problems for users of the libraries glog registers a bunch of flags in an init function and offers no programmatic way to configure its behaviour it can be surprising to users of the ks libraries that they have to call flag.parse and this is easy to get wrong eg coredns/coredns glogs default behaviour is to log to a file on disk a user of a library wouldnt typically expect it to write files without explicitly configuring it to do so worse if glog cannot create a file it calls os.exit this can be very harmful and since its common to run containerised binaries with a read-only root filesystem can be easy to trigger glog doesnt perform any management of the files it writes so without something like logrotate running especially problematic in a container the log files just accumulate.i think we need to find a way to fix this the use of glog makes working with the ks client libraries very difficult a recent twitter discussion on this topic shows this is a widely-held concern
303911453,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request? :bug what happened :kubelet is periodically going into an error state and causing errors with our storage layer ceph shared filesystem upon cleaning out the orphaned pod directory things eventually right themselves workaround rmdir var/lib/kubelet/pods/*/volumes/*rook/* what you expected to happen :kubelet should intelligently deal with orphaned pods cleaning a stale directory manually should not be required. how to reproduce it as minimally and precisely as possible) :using rook this isnt a rook problem as far as i can tell but this is how were reproducing):kubectl create f rook-operator.yamlkubectl create f rook-cluster.yamlkubectl create f rook-filesystem.yamlmount/write to the shared filesystem and monitor var/log/messages for the following: kubelet e kubelet_volumes.go orphaned pod fa-b-e-aa-ecdaaa found but volume paths are still present on disk there were a total of errors similar to this turn up verbosity to see them. anything else we need to know? :this looks identical to the following but for a different plugin. environment kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:dfceeadbedbb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:dfceeadbedbb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration:bare-metal private cloud os e.g from etc/os-release):red hat enterprise linux server release maipo kernel e.g uname a ):linux el.elrepo.x smp sat feb est x x x gnu/linux install tools:kubeadm
302458351,cvss:./av:n/ac:l/pr:l/ui:n/s:u/c:h/i:h/a:h vulnerability allows containers using subpath volume mounts with any volume type including non-privileged pods subject to file permissions to access files/directories outside of the volume including the hosts filesystem.thanks to maxim ivanov for reporting this problem. vulnerable versions kubernetes x-..x kubernetes kubernetes kubernetes vulnerable configurations clusters that allow untrusted users to control pod spec content and prevent host filesystem access via hostpath volumes or other volume types using podsecuritypolicy or custom admission plugins clusters that make use of subpath volume mounts with untrusted containers or containers that can be compromised vulnerability impact: a specially crafted pod spec combined with malicious container behavior can allow read/write access to arbitrary files outside volumes specified in the pod including the hosts filesystem this can be accomplished with any volume type including emptydir and can be accomplished with a non-privileged pod subject to file permissions). mitigations prior to upgrading: prevent untrusted users from creating pods and pod-creating objects like deployments replicasets etc or disable all volume types with podsecuritypolicy note that this prevents use of service account tokens in pods and requires use of automountserviceaccounttoken false versions fixed in v by fixed in v by fixed in v by fixed in master by included in v..-beta will be in v..) action required: in addition to upgrading podsecuritypolicy objects designed to limit container permissions must completely disable hostpath volumes as the allowedhostpaths feature does not restrict symlink creation and traversal future enhancements tracked in issue are required to limit hostpath use to read only volumes or exact path matches before a podsecuritypolicy can effectively restrict hostpath usage to a given subpath. known issues status and availability of fixes for regressions in subpath volume mount handling are tracked in
298766557,is this a bug report or feature request? :/kind feature what happened :every container gets environmentvariables per service exposing a lot of information and there is no way to turn it off. what you expected to happen :this docker deprecated link feature compatibility option to be an opt-in or at least opt-out option. how to reproduce it as minimally and precisely as possible) :create a new container in a cluster where there are existing services check the env you should see something like env grep kuberneteskubernetes_service_port=kubernetes_port=tcp://...:kubernetes_port__tcp_addr=...kubernetes_port__tcp_port=kubernetes_port__tcp_proto=tcpkubernetes_port__tcp=tcp://...:kubernetes_service_port_https=kubernetes_service_host one ingress-controller env grep ingress_nginx_ext wc l anything else we need to know? :the code doing this is and some docs at things like this should be exposed using eg the downwards api instead kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:dfceeadbedbb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:bbeebebcebbaaae gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration on-prem os e.g from etc/os-release ubuntu kernel e.g uname a install tools helm others
297085184,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request uncomment only one leave it on its own line kind bug kind feature what happened :cant delete a statefulsets and i get an errorerror error when stopping workers.yml no kind getoptions is registered for version apps/v sh kubectl auth can-i get statefulsetsyes kubectl auth can-i create statefulsetsyes kubectl auth can-i delete statefulsetsyes what you expected to happen :statefulsets will be delete how to reproduce it as minimally and precisely as possible) :create a statefulsets and try to delete it shapiversion apps/vkind statefulsetmetadata name workerspec servicename experience replicas selector matchlabels app experience template metadata labels app experience spec containers name ssh image anything else we need to know? :it works with a cluster-admin role environment kubernetes version use kubectl version ):client version version.info{major minor gitversion:v gitcommit:bcefacdccddfcefdec gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:bbeebebcebbaaae gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration:bare metal os e.g from etc/os-release):name=ubuntuversion lts xenial xerus)id=ubuntuid_like=debianpretty_name=ubuntu ltsversion_id=.home_url kernel e.g uname a ):linux generic ubuntu smp tue jan utc x x x gnu/linux install tools:gpus daemonsets others
292084257,the idea here is that folks should not have to use full privileged containers to be able to tweak small things like maximum file descriptors nofile or maximum number of bytes of memory that may be locked into ram memlock today they write extra scripts that get baked into their containers see example which is extra hassle and unnecessary as docker already supports specifying ulimits see so let use use the requests/limits mechanism to let the end users specify what they need in their container definitions right now since docker supports passing these directly let us experiment this capability on just docker fixes release-notealpha feature to support specifying resource limits works only on docker runtime
290630201,people want the ability to specify specific pods within a different namespace in networkpolicies.i propose we change the documentation/definition of networkpolicypeer so that instead of saying exactly one of its fields must be specified we say that there are valid kinds of networkpolicypeer if only podselector is set then the peer selects the indicated pods in the same namespace as the networkpolicy if only namespaceselector is set then the peer selects all pods in the indicated namespaces new if both podselector and namespaceselector are set then the peer selects the indicated pods in the indicated namespaces and in particular if podselector is set and namespaceselector is then the peer selects the indicated pods in all namespaces if only ipblock is set then the peer selects the indicated ip addresses.as before it would be invalid to have a networkpolicypeer in which all fields were unset or in which both ipblock and any other field were set.this shouldnt create any compatibility issues no old policies would be interpreted differently under the new rules because the new rules only describe the interpretation of policies that would be invalid under the old rules and likewise policies using the new features would not be interpreted incorrectly by old plugins theyd just be rejected as invalid./sig network/kind feature
288173884,"hi all,i use systemd to start kubelet,but log show these error: jan centos kubelet e container_manager_linux.go containermanager fail to get rootfs information unable to find data for container but i still can create the pod successfully.anyone can help?the version is sig apps"
286293887,is this a bug report or feature request? :/kind bug what happened :one of endpoint objects was missing for a service that had matching pod running the endpoint returned after hm. what you expected to happen :for the endpoint to stay with us and be there how to reproduce it as minimally and precisely as possible) :i have no idea i even dont know why it was fixed. anything else we need to know? :let me show some details.were running a service that uses providers to watch for endpoints changes there for one service i got the following event: time=--t::z level=info msg=endpoints delete event mongo-- the deployment was running just pod so this had to be the last one.i ran the following check to see whats going on kubectl n placeable-qa get svc/mongo o yamlapiversion vkind service name mongo namespace placeable-qa...spec clusterip ports port protocol tcp targetport selector app mongo sessionaffinity none type clusteripstatus loadbalancer so the service is defined and has selector app=mongo now deploy kubectl n placeable-qa get deploy/mongo o yamlapiversion extensions/vbetakind deploymentmetadata name mongo namespace placeable-qa resourceversion spec progressdeadlineseconds replicas revisionhistorylimit selector matchlabels app mongo--...status availablereplicas conditions lasttransitiontime t::z lastupdatetime t::z message deployment has minimum availability reason minimumreplicasavailable status true type available lasttransitiontime t::z lastupdatetime t::z message replicaset mongo---ff has successfully progressed reason newreplicasetavailable status true type progressing observedgeneration readyreplicas replicas updatedreplicas and pods kubectl n placeable-qa get po l app=mongo name ready status restarts agemongo---ff-snph running h yet for endpoints kubectl n placeable-qa get ep mongo--error from server notfound endpoints mongo not found surprisingly the endpoint came back after hm this is the log from our service: time=--t::z level=info msg=endpoints delete event mongo-- ... time=--t::z level=info msg=new endpoints created event mongo-- environment kubernetes version use kubectl version kubectl versionclient version version.info{major minor gitversion:v gitcommit:ccecadeacedaabf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:befedabfbdaaafaac gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration:aws x.xlarge os e.g from etc/os-release): name=ubuntuversion lts xenial xerus)id=ubuntuid_like=debianpretty_name=ubuntu ltsversion_id=.home_url kernel e.g uname a ):linux central-ansible generic ubuntu smp mon dec utc x x x gnu/linux install tools kubespray custom
283138924,kind bugin a kops created cluster running kubernetes nodes sometimes start but fail to connect to the masters i have tracked the issue down to this difference:failed:i aws.go building aws cloudprovideri aws.go zone not specified in configuration file querying aws metadata servicee tags.go tag kubernetescluster nor kubernetes.io/cluster not found kubernetes may behave unexpectedly.w tags.go aws cloud no clusterid filtering applied for shared resources do not run multiple clusters in this az.i server.go successfully initialized cloud provider aws from the config file successful:i aws.go building aws cloudprovideri aws.go zone not specified in configuration file querying aws metadata servicei tags.go aws cloud filtering on clusterid kube.test.inti server.go successfully initialized cloud provider aws from the config file so in the failing case the aws metadata either doesnt respond or the response is missing critical information im not sure how to find out which this query isnt retried during kubelets general retry connecting to the api loop so the kubelet will keep running but it will never retrieve the information it needs to install and configure calico and join the network
282869012,is this a bug report or feature request? :/kind feature what happened :i tried to reference a tls certificate key with the secretname variable of an ingress resource. what you expected to happen :one can use and this is quite common a single ingress controller for ingress of multiple namespaces.unfortunately the ingress resource may only reference a tls certificate which resides in the own namespace in the secretname variable especially when using wildcard certificates or just a common hostname for multiple services in different namespaces this means the certificate secret needs to be duplicated in all namespaces which use it.apart from untidy duplication of things this gives depending on your rbac policies the namespace admin access to the private key.enabling the ingress controller to reference a certificate which is located in another namespace i.e one only containing certificates would allow for easier management of this shared piece of data and even protect it more. how to reproduce it as minimally and precisely as possible spec tls hosts myhost.example.com secretname anothernamespace/wildcard-for-example-com-tls rules host myhost.example.com http paths path backend servicename myservice serviceport referencing a secret inside i.e anothernamespace simply does not work... anything else we need to know? :i opened this issue as towards the ingress-nginx project before. environment kubernetes version use kubectl version cloud provider or hardware configuration aws os e.g from etc/os-release coreos container linux
282741848,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request feature request/kind feature what happened :as part of a development workflow i intentionally killed a container in a pod with restartpolicy always the plan was to do this repeatedly as a quick way to restart the container and clear old state and in minikube to load image changes).the container went into a crash-loop backoff making this anything but a quick option. what you expected to happen i expected there so be some configuration allowing me to disable or at least tune the timing of the crashloopbackoff. how to reproduce it as minimally and precisely as possible) :create a pod with restartpolicy always and intentionally exit a container repeatedly. anything else we need to know? :i see that the backoff timing parameters are hard-coded constants here might reasonably expect these to be configurable at least at the kubelet level say by a setting like these that would be sufficient for my use-case local development with fast restarts and presumably useful as an advanced configuration setting for production workloads.a more aggressive change would allow tuning per-pod.there are other options for my target workflow put the pod in a deployment or similar kubectl delete the pod let kubernetes schedule another work with the new pod however this is much slower than a container restart without backoff and ironically causes more kubelet load than the backoff avoids it also relies on using kubectl/the kubernetes api to do the restart as opposed to just exiting the container run the server process as a secondary process in the container rather than the primary process this means the server can be started/stopped without container backoff but is trickier to implement and doesnt offer the same isolation guarantees as exiting the container and starting fresh it also means i probably cant use the same image i deploy to production because i probably dont want this extra restart-support stuff floating around in the production image). environment kubernetes version use kubectl version v cloud provider or hardware configuration minikube with virtualbox driver on osx
281112616,kind bug what happened :scenario you run an etcd cluster with etcd-operator you drain a node with etcd-operator on it.you get a message like kubectl drain nodepods not managed by replicationcontroller replicaset job daemonset or statefulset use force to override example-etcd-cluster- this error message is not useful given that operators are popular and recommended. what you expected to happen :no error message. how to reproduce it as minimally and precisely as possible prereq have a cluster install coreos operator as follows:mkdir coreoscd coreosgit clone etcd-operator/example/rbac/create_role.shkubectl create f example/deployment.yamlkubectl get customresourcedefinitions make an etcd clusterkubectl create f example/example-etcd-cluster.yamlkubectl get pods l etcd_cluster l etcd_cluster=example-etcd-clusterkubectl get etcdcluster example-etcd-cluster o json jq status.members wait until some guys are readywatch kubectl get etcdcluster example-etcd-cluster o json jq status.members find a node that has an etcd pod on it call it node in example node=gke-cluster--default-pool-abf-mtt kubectl drain nodenode gke-cluster--default-pool-abf-mtt already cordonederror pods with local storage use delete-local-data to override example-etcd-cluster pods not managed by replicationcontroller replicaset job daemonset or statefulset use force to override example-etcd-cluster fun kube-proxy-gke-cluster--default-pool-abf-mtt daemonset-managed pods use ignore-daemonsets to ignore fluentd-gcp-v..-bcjm the offending message is pods not managed by replicationcontroller replicaset job daemonset or statefulset use force to override) . anything else we need to know? : environment kubernetes version use kubectl version client version version.info{major minor gitversion:v gitcommit:dadaefecedaad gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v..-gke gitcommit:adbbabebcaefbeb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration gke os e.g from etc/os-release kernel e.g uname a install tools others
279926443,"is this a bug report or feature request? :/kind bug what happened :dns lookup is sometimes taking seconds. what you expected to happen :no delays in dns. how to reproduce it as minimally and precisely as possible create a cluster in aws using kops with cni networking: kops create cluster node-count zones eu-west-a,eu-west-b,eu-west-c master-zones eu-west-a,eu-west-b,eu-west-c dns-zone kube.example.com node-size t.medium master-size t.medium topology private networking cni cloud-labels env=staging name cni plugin: kubectl apply f version base tr d n run this script in any pod with that has curl: var=while true do res curl o dev/null s w time_namelookup}\\n var=$((var if res then now=$(date t echo var slow res now break fidone anything else we need to know i am encountering this issue in both staging and production clusters but for some reason staging cluster is having a lot more s delays delays happen both for external services google.com or internal such as service.namespace happens on both and version of kubernetes but did not encounter these issues in though the setup was a bit different no cni back then have not tested with without cni yet. environment kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:bdaeafafccfd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:bebdebffadaecbecb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration: aws os e.g from etc/os-release): pretty_name=ubuntu lts kernel e.g uname a ): linux ingress-nginx--sm ks smp tue may utc x x x gnu/linux similar issues closed but seems to be exactly the same has some comments matching this issue but is taking the direction of fixing kube-dns up/down scaling problem and is not about the intermittent failures./sig network"
270877357,kind featuresimple question here would it be possible to get resource usage for a given namespace using kubectl something like the following would be great kubectl get resources n namespace> and corresponding output should show total i.e among all running pods cpu and memory usage for the given namespace even better would be to show the min max and average resource usage so we can take into account potential job cronjob and so on of the given namespace assuming a monitoring system has been setup influxb queries could make the trick i guess still i think it might be a nice feature to be integrated within ks.as an example setting up a proper resources compartmentalization for the company im working with would be great it is kind of difficult also how to know if whether or not we can apply resource quotas on the kube-system namespace and if so which quota should we apply?im aware about the feature request however i still believe these two are different.@kubernetes/sig-cli-feature-requests
268081708,when a pod is being restarted the time it last restarted is very relevant if it was hours ago i dont care we should consider some information being presented about how recent the restart is.for example real world example kubectl get podsname ready status restarts ageprometheus running h i dont know when the time of the last restart was in this case it was hours ago so the restarts actually doesnt matter the age is h but thats not very useful trying to find a middle ground would be useful
263475425,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request uncomment only one leave it on its own line kind bug kind feature what happened :kubelet/kubernetes does not work with swap enabled on linux machines.i have found this original issue pr and last change which enabled it by default kubernetes does not know how to handle memory eviction when swap is enabled it should find a way how to do that but not asking to get rid of swap.please follow kernel.org chapter swap management for example the casual reader may think that with a sufficient amount of memory swap is unnecessary but this brings us to the second reason a significant number of the pages referenced by a process early in its life may only be used for initialisation and then never used again it is better to swap out those pages and create more disk buffers than leave them resident and unused.in case of running a lot of node/java applications i have seen always a lot of pages are swapped just because they arent used anymore. what you expected to happen :kubelet/kubernetes should work with swap enabled i believe instead of disabling swap and giving users no choices kubernetes should support more use cases and various workloads some of them can be an applications which might rely on caches.i am not sure how kubernetes decided what to kill with memory eviction but considering that linux has this capability maybe it should align with how linux does that would suggest to rollback the change for failing when swap is enabled and revisit how the memory eviction works currently in kubernetes swap can be important for some workloads how to reproduce it as minimally and precisely as possible) :run kubernetes/kublet with default settings on linux box anything else we need to know? : environment kubernetes version use kubectl version cloud provider or hardware configuration os e.g from etc/os-release kernel e.g uname a install tools others:/sig nodecc mtaufen vishh derekwaynecarr dims
261916032,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request? :/kind bug what happened : kubectl fails to validate a simple namespace yml: ymlapiversion vkind namespacemetadata name foo same problem if i use json which is how i first noticed it trying to restore from a get ns ojson export what you expected to happen : kubectl to validate and pass it on to the cluster how to reproduce it as minimally and precisely as possible create foo.yml with the content above kubectl apply f foo.yml or create if you prefer with kubectl it fails validation kubectl apply f foo.yml with kubectl it works anything else we need to know? : environment kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:eacacaceaacdfeba gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:dadaefecedaad gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} for the working client version version.info{major minor gitversion:v gitcommit:daccbbbeafabddd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:dadaefecedaad gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration :running on minikube to replicate os e.g from etc/os-release):from minikube cat etc/os-releasename=buildrootversion=.id=buildrootversion_id=.pretty_name=buildroot kernel e.g uname a uname alinux minikube smp tue jul utc x gnu/linux install tools:minikube others:summary: /tmp cat foo.ymlapiversion vkind namespacemetadata name foo/tmp kubectl apply f foo.ymlnamespace foo configured/tmp kubectl apply f foo.ymlerror error validating foo.yml error validating data unknown object type schema.groupversionkind{group version:v kind:namespace if you choose to ignore these errors turn validation off with validate=false
261426202,forked from discussion in and were raised that the validated docker version ce has already been eold before kuberentes v is release although some users may get longer support terms from commercially supported edition or vendor-provided version cos image on gcp rhel etc this is not ideal for users who rely purely on docker community edition.on the other hand with the current kubernetes release cycle i think chasing the month docker support window is futile even if we had validated docker ce with kubernetes itd still be eold a month after is out instead we should shift the focus from docker engine version to docker api version as euank suggested background with each new docker engine version docker publishes a new api version see the lookup table right now kubelet/dockershim always uses the latest docker api version that the engine supports but in fact each engine supports multiple api versions.for example with docker the latest supported api version is console docker versionserver version api version minimum version go version go git commit cba built wed aug os/arch linux/amd experimental false the following two requests using two different api versions both work against the engine. get v./containers/jsonget v./containers/json proposed we should allow users who want to upgrade their docker ce version for security patches not features an option to do so on the other hand its still valuable to publish the docker version that was used in the validation test publish the min and max_docker_api_version validated against the current kubernetes version set the max_docker_api_version to use in the docker client in kubelet/dockershim kubelet will use the the minimum of max_docker_api_version newest_api_version_supported_by_the_engine to talk to the docker engine add a flag in kubelet/dockershim so that users can override the max_docker_api_version if they desire publish the docker engine versions validated with each os distro in some form this information is still valuable since efforts have been put in each docker validation to discover document and work around bugs for users who want to upgrade their docker ce version for security patches they are free to do so because kubelet/dockershim will simply use the validated max_docker_api_version to communicate with docker of course usersd need to bear the risk of hitting bugs in the new docker versions but this is well-known and they can always reach out to the docker community to get the bugs fixed for users who want to try or test newer docker api version they can restart kubelet and override the setting with max-docker-api-version caveats kubernetes may get more user issues for docker engine versions that were not really validated against the release through ee test i think its a reasonable tradeoff and most issues should be redirected to the docker community since kubelet uses a fixed api version.note the current docker version client configuration has a bug need to re-vender to include the fix cc kubernetes/sig-node-proposals dchen derekwaynecarr
259137095,is this a bug report or feature request feature request/kind feature what happened when you write a deployment yaml file everything must be specified statically in cases where something is not specified at time of writing the only choice is using a rd-party tool and use yaml file as a template which will later be processed by the tool to replace some markers with actual values this is not efficient. what you expected to happen deployment yaml file format should support some kind of marker which specifies values which will be provided at the time of deployment via command line arguments e.g kubectl create f myfile.ymml var=data var=gcr.io/image or feed values from environment variables. how to reproduce it as minimally and precisely as possible) :example just a suggestion actual implementation could be different): apiversion apps/vbetakind deploymentmetadata name nginx-deploymentspec replicas template metadata labels app nginx spec containers name nginx image image_url}} then run kubectl create f file.yaml image_url=nginx.. anything else we need to know this will make life easier for devops. environment kubernetes version use kubectl version latest cloud provider or hardware configuration os e.g from etc/os-release os x kernel e.g uname a darwin install tools others
259010628,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request feature request kind feature what happened :as far we do not support sharing gpu to multiple containers one gpu can only be assigned to one container at a time but we do have some requirements on achieving this is it feasible that we manage gpu just like cpu or memory? what you expected to happen :sharing gpu to multiple containers just like cpu and memory
258857064,kind bug what happened :deployed a new cluster v with kubeadm and tried to access dashboard via kubectl proxy i tested two links but only one works: broken you expected to happen :should work because with kubernetes it works without any problem. how to reproduce it as minimally and precisely as possible) :deploy new cluster with kubeadm centos and calico network provider anything else we need to know? : environment kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:daccbbbeafabddd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:bcefacdccddfcefdec gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration : on prem os e.g from etc/os-release): centos kernel e.g uname a ): linux ks-play.customer-virt.eu el.x smp tue sep utc x x x gnu/linux install tools: kubeadm already discussed the problem at the dashboard repository and it seems to be a problem with kube proxy
256863792,what this pr does why we need it :adding a way to replace configmap from-file as suggested in issue which issue this pr fixes fixes closes special notes for your reviewer :it works the same way create configmap from-file works please let me know if you suggest a better way to reduce code redundancy by moving the common part in create to a common file since its my first time i preferred not to do aggressive changes unless suggested by someone who knows the system better than me. release-notenone
256391882,seriously i need it to draw some kind of truth table or something about whats allowed.why do we have so many restrictions? console kc logs f nsonobuoy lsonobuoy-plugin=ee error only one of follow f or selector l is allowedsee kubectl logs h for help and examples kc logs nsonobuoy lsonobuoy-plugin=ee error from server badrequest a container name must be specified for pod sonobuoy-ee-job-cabef choose one of ee sonobuoy-worker kc logs nsonobuoy lsonobuoy-plugin=ee c eeerror a container cannot be specified when using a selector l)see kubectl logs h for help and examples ok this one is fixed at head and this works just fine so it really strike me as strange that kubectl wont do it for me. console kc logs nsonobuoy f kc get pod oname nsonobuoy lsonobuoy-plugin=ee c ee /kind feature@kubernetes/sig-cli-feature-requests
256319608,assumptionsthe external attack surface of most kubernetes clusters is typically limited to the api server ssh to the nodes and any exposed service however there are at least two conditions where a malicious user can gain access inside the cluster insert malicious code into a container that gets run by an authorized user as a part of a deployment or helm chart e.g a working image that also has a backdoor/shell callback embedded an externally exposed container application is vulnerable to remote code execution local/remote file inclusion command injection or arbitrary url fetching e.g apache struts cve shellshock vulnerable wordpress plugin or jinja template injection.should either of these conditions occur the result is a malicious user/program would have the equivalent of a shell inside a container/pod on the cluster it is at this point that many cluster installations by default provide multiple avenues for privilege escalation inside kubernetes full cluster credential compromise root access to the underlying node and leakage of privileged cloud account keys/secrets/tokens--providing access to other cloud resources in that account outside of the cluster.i have evaluated eight commonly used cluster installers across the major public cloud providers so far and found a common set of post-exploit attack patterns that summarize themselves into the set of issues noted below not all cluster installations are vulnerable to all issues listed below but each one is present in at least one cluster by default. i recognize that the bulk of these findings have been identified already as isolated issues but i believe that there is merit to gathering them in one place to be able to see their combined effects purposeto raise the security awareness of the cluster installers/admins/operators of the kinds of potentially significant downstream attacks so that they may take appropriate security hardening steps to mitigate them and to aid the cluster installation tool projects in making positive changes to the security posture of clusters built using those tools by default post-container compromise issues default namespace tokens have full privileges risk high attackfrom a shell inside a container/pod after installing a copy of kubectl into the container sensitive items have been truncated): shellbash curl slo s chmod x kubectl mv kubectl usr/local/binbash cat var/run/secrets/kubernetes.io/serviceaccount/tokeneyjhbgcioijsuziniisi...bash kubectl get secrets all-namespaces o yamlapiversion vitems apiversion v data ca.crt lstlscrudjtibdrvjusuzjqfurstlstckjsumre resultif the auto-mounted token has the equivalent of full cluster permissions this is considered credential exposure that allows for privilege escalation in most cases this may allow running privileged containers that mount the underlying host filesystem read/write to add ssh keys and run docker/runc/rkt commands becoming root on the host extraction of secrets configmaps pod environment variables and more which may include api keys user/password pairs cloud account keys etc remediationa small number of installers do not enforce rbac by default on the latest versions you may need to customize the default options in your installation tool to enable rbac authorization if it doesnt already here is what a similar container/pod looks like when hitting an api server with rbac enabled using the default namespace serviceaccount token: shellbash bash kubectl get pods all-namespaceserror from server forbidden user system:serviceaccount:default:default cannot list pods at the cluster scope unknown user system:serviceaccount:default:default get pods unprotected kubernetes dashboard and other kube-system add-ons risk high attackfrom a shell inside a container/pod using curl : shellbash curl sk doctype html html ng-app=kubernetesdashboard head meta charset=utf title ng-controller=kdtitle as ctrl ng-bind=$ctrl.title()>
256275987,kind featurei want to be able to create loadbalancer services and use awss network load balancer along with how aws lbs are currently configured it would be most convenient to have an annotation to specify an nlb environment cloud provider aws
254767745,"this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request? :/kind bug what happened :pods stuck on terminating for a long time what you expected to happen :pods get terminated how to reproduce it as minimally and precisely as possible run a deployment delete it pods are still terminating anything else we need to know? :kubernetes pods stuck as terminating for a few hours after getting deleted.logs:kubectl describe pod my-pod--rhc name my-pod--rhcnamespace container--productionnode ip----.ec.internal/...start time fri sep labels pod-template-hash release=stable run=my-pod-annotations kubernetes.io/created-by={kind:serializedreference,apiversion:v,reference:{kind:replicaset,namespace:container--production,name:my-pod--,uid:c prometheus.io/scrape=truestatus terminating expires fri sep termination grace period sip:created by replicaset/my-pod--controlled by replicaset/my-pod--init containers ensure-network container id docker://guid image xxxxx image id docker-pullable://repo/ensure-network@sha:guid port none state terminated exit code started mon jan finished mon jan ready true restart count environment none mounts var/run/secrets/kubernetes.io/serviceaccount from default-token-xxxxx ro)containers container container id docker://container-id-guid image xxxxx image id docker-pullable://repo/container-@sha:guid port none state terminated exit code started mon jan finished mon jan ready false restart count limits cpu m memory g requests cpu m memory g environment xxxx mounts var/run/secrets/kubernetes.io/serviceaccount from default-token-xxxxx ro container container id docker://container-id-guid image alpine image id docker-pullable://alpine@sha:alpine-container-id port none command x state terminated exit code started mon jan finished mon jan ready false restart count limits cpu m memory m requests cpu m memory m environment none mounts var/run/secrets/kubernetes.io/serviceaccount from default-token-xxxxx ro container container id docker://container-id-guid image xxxxx image id docker-pullable://repo/container-@sha:guid port none state terminated exit code started mon jan finished mon jan ready false restart count limits cpu m memory m requests cpu m memory m readiness exec nc zv localhost delay=s timeout=s period=s success failure environment xxxx mounts var/run/secrets/kubernetes.io/serviceaccount from default-token-xxxxx ro container container id docker://container-id-guid image xxxx image id docker-pullable://repo/container-@sha:guid port tcp state terminated exit code started mon jan finished mon jan ready false restart count limits cpu m memory m requests cpu m memory m readiness http-get delay=s timeout=s period=s success failure environment xxxx mounts app/config/external from volume ro data/volume from volume ro var/run/secrets/kubernetes.io/serviceaccount from default-token-xxxxx ro)conditions type status initialized true ready false podscheduled truevolumes volume type secret a volume populated by a secret secretname volume optional false volume type configmap a volume populated by a configmap name external optional false default-token-xxxxx type secret a volume populated by a secret secretname default-token-xxxxx optional falseqos class burstablenode-selectors none> sudo journalctl u kubelet grep my-pod ... sep ip kubelet time=--t::z level=info msg=releasing address using workloadid workload=my-pod--rhcsep ip kubelet time=--t::z level=info msg=releasing all ips with handle my-pod--rhcsep ip kubelet time=--t::z level=warning msg=asked to release address but it doesnt exist ignoring workload=my-pod--rhc workloadid=my-pod--rhcsep ip kubelet time=--t::z level=info msg=teardown processing complete workload=my-pod--rhc endpoint=sep ip kubelet i kubelet.go syncloop delete api):my-pod-(bcfecd-f-e-ba-aac) sudo journalctl u docker grep docker-id-for-my-pod sep ip dockerd time=--t::.z level=error msg=handler for post v./containers/docker-id-for-my-pod/stop returned error container docker-id-for-my-pod is already stoppedsep ip dockerd time=--t::.z level=error msg=handler for post v./containers/docker-id-for-my-pod/stop returned error container docker-id-for-my-pod is already stopped environment kubernetes version use kubectl version ):client version version.info{major minor gitversion:v gitcommit:cfeeadbdabcfa gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:facdbcfafadcfada gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd cloud provider or hardware configuration :aws os e.g from etc/os-release):name=centos linuxversion core)id=centosid_like=rhel fedoraversion_id=pretty_name=centos linux core)ansi_color=;cpe_name=cpe:/o:centos:centos:home_url kernel e.g uname a ):linux ip el.x smp tue feb utc x x x gnu/linux install tools:kops others:docker version build d@kubernetes/sig-aws kubernetes/sig-scheduling"
248879582,kind bug what happened :i wanted to mount a configmap and a secret directly as a file and didnt want to mount it as a full directory so i used subpath to do so: yaml volumemounts name my-config mountpath usr/src/app/config/config.json subpath config.json name my-secret mountpath usr/src/app/secret/secret.json subpath secret.json volumes name my-config configmap name my-config name my-secret secret secretname my-secret when the pod is created it mounts the configmap and secret correctly however if i change them the updates are not projected into the currently running pods new pods get the updated file according to the documentation changes to a configmap should be automatically propagated to running containers that mount them.however if i dont use subpath and instead mount the configmap and secret as a directory: yaml volumemounts name my-config mountpath usr/src/app/config name my-secret mountpath usr/src/app/secret volumes name my-config configmap name my-config name my-secret secret secretname my-secret then the files are updated inside the container when the underlying configmap and secret are updated and everything works as expected. anything else we need to know? :in both cases the files are being updated on the host vm kelseyhightower and i tried to debug this and the only conclusion we could come up with is that subpath is using a different method to mount the files i think it is using symlinks and these either arent or cant be updated for whatever reason. action :the behavior that files mounted with subpath dont get updated needs to be documented or it needs to be fixed so that subpath mounts are updated when the underlying configmap or secret changes. environment gke kubernetes version use kubectl version cloud provider or hardware configuration gke n-standard os e.g from etc/os-release container-optimized os kernel e.g uname a
247794821,the main reason for having this is that watch.until can exit before the timeout is reached there are several issues about it in kubernetes repo already linked below it is caused by the watcher being closed prematurely the usual causes are api timeout or etcd timeout for watch this is destined to cause a lot of test flakes and watch.until already had to be removed from some tests also i think this was an issue with kubectl rollout status timeout so not just tests.the idea behind this pr is to inspect all the events being processed and keeping track of last resourceversion of its objects if the watch fails it will restart the watch from the last resourceversion.this is all wrapped in object retrywatcher implementing watch.interface so it looks like a regular watcher and the reader wont even notice those restarts on background.this pr is not fixing watch.until itself but only providing you with a watcher wrapper to make it behave properly the reason why i havent plumbed it into watch.until itself is that it would require changing its arguments which is a fairly big change i guess but still its broken now it would need to accept watchfunc instead of a watcher otherwise the change would be just in using retrywatcher internally in watch.until and fixing the code that calls it please let me know your opinion here about fixing that as well doing this in watch.until would be probably best it would fix all the related issues like note that this fixes most of the problems but not all there can be a situation when between the background restarts the lastresourceversion is not in etcd cache anymore causing it to fail i guess the only other path forward would be to use informers but thats kind of heavy and would make the user experience worse example usage goinitialresourceversion watch.until(*time.minute watch.withretry(initialresourceversion func(rv string watch.interface watcher err c.corev().secrets(secret.namespace).watch(metav.singleobject(metav.objectmeta{name secret.name resourceversion rv if err nil t.fatalf(failed to create watcher on secret v err return watcher})) related issues and prs about broken watch.until free to link more.)ccing few people from previous issues that might be interested:@smarterclayton mfojtik kargakis soltysh janetkuo liggitt wojtek-t release note : release-notefix premature timeouts/failures in commands that wait using watch split tasks to go in smaller prs move ks.io/apimachinery/pkg/watch.until to ks.io/client-go/tools/watch.untilwithoutretry introduce untilwithsync introduce until with retry individual prs to switch kubectl and other places to either untilwithsync or until
246103764,thanks for sending a pull request here are some tips for you if this is your first time read our contributor guidelines and developer guide if you want faster pr reviews read how follow the instructions for writing a release note this pr does why we need it this pr allows kubernetes in an azure context to use a vnet which is not in the same resource group as kubernetes.we need this because currently azure cloud provider driver assumes that it should have a vnet for himself but if there is one thing that should be shared amongst azure resources its a vnet cause well things might want to talk to each other in a private network dont you think i guess this should we backported down to branch. which issue this pr fixes optional in fixes issue number fixes issue_number format will close that issue when pr gets merged fixes release note steps to write your release note use the release-note labels to set the release note state if you have access enter your extended release note in the below block leaving it blank means using the pr title as the release note if no release note is required just write none .--> release-noteazure allow vnet to be in a separate resource group. @kubernetes/sig-azure@kubernetes/sig-azure-pr-reviews
244862246,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request? :/kind feature what happened :we run kuberentes on aws we are trying out loadbalancer service type.we use terraform heavily for managing infrastructure on aws as a result we have several shared security groups with perfect rules one of them is supposed to be attached to elbs so our own ips partners etc are whitelisted right now it seems there isnt a way for kubernetes to use the security group as is there are several alternatives pass my existing security group as service.beta.kubernetes.io/aws-load-balancer-extra-security-groups annotation however kubernetes still tries to create one first or use a global security group if i dont provide any ip range kubernetes will whitelist this seems insecure provide my own ip ranges as either annotation service.beta.kubernetes.io/load-balancer-source-ranges or service.spec.loadbalancersourceranges then kubernetes will create a security group for this elb with the given ips this means i need to duplicate my ips in all microservices provide my own ip ranges as either annotation service.beta.kubernetes.io/load-balancer-source-ranges or service.spec.loadbalancersourceranges and use a global security group then kubernetes will modify the global sg with the given ips this seems problematic when lots of microservices share one sg but some want different ips whitelisted.i want to entertain the idea below allow users to sepcify another security group to kube-controller-manager or service object which kubernetes will take it and attach it to elbs as is if such security group is provided skip other ip whitelisting steps. what you expected to happen :i can give kubernetes existing security group(s kuberentes just attaches the security group(s to the managed elbs without modifying the rules. how to reproduce it as minimally and precisely as possible) : anything else we need to know? :it may already be possible to achieve this and im not aware of or there are other concerns so that kubernetes didnt choose to implement this feature. environment kubernetes version use kubectl version v cloud provider or hardware configuration aws os e.g from etc/os-release kernel e.g uname a install tools others
244695261,what happened :id like to have a simple command to check for pods that are currently not ready what you expected to happen :i can see a couple of options there is some magic flag i am not aware of having a flag for kubectl get to filter the output using go/jsonpath distinguish between pod phase running&ready and running flag to filter on ready status how to get that currently : kubectl get pods all-namespaces o json jq r items select(.status.phase running or status.conditions select(.type ready and state false length metadata.namespace metadata.name
242624808,"what this pr does why we need it :this pull request aims to solve the problem of users not able to set custom cipher suites in the api server.several users have requested this given that some default ciphers are vulnerable.there is a discussion in of how to implement this the options are setting a fixed list of ciphers but users will have different requirements so a fixed list would be problematic letting the user set them by parameter this requires adding a new parameter that could be pretty long with the list of all the ciphers.i implemented the second option if the ciphers are not passed by parameter the go default ones will be used same behavior as now). which issue this pr fixes fixes special notes for your reviewer :the ciphers in go tls config are constants and the ones passed by parameters are a comma-separated list i needed to create the type ciphersuitesflag to support that conversion/mapping because i couldnt find any way to do this type of reflection in go.if you think there is another way to implement this let me know.if you want to test it out this is a ciphers combination i tested without the weak ones: tls_rsa_with_aes__cbc_sha,tls_ecdhe_ecdsa_with_aes__cbc_sha,tls_ecdhe_rsa_with_aes__cbc_sha,tls_ecdhe_rsa_with_aes__gcm_sha,tls_ecdhe_ecdsa_with_aes__gcm_sha,tls_ecdhe_rsa_with_aes__gcm_sha,tls_ecdhe_ecdsa_with_aes__gcm_sha,tls_ecdhe_rsa_with_chacha_poly,tls_ecdhe_ecdsa_with_chacha_poly,tls_rsa_with_aes__cbc_sha,tls_rsa_with_aes__cbc_sha,tls_rsa_with_aes__gcm_sha,tls_rsa_with_aes__gcm_sha,tls_ecdhe_ecdsa_with_aes__cbc_sha,tls_ecdhe_ecdsa_with_aes__cbc_sha,tls_ecdhe_rsa_with_aes__cbc_sha,tls_ecdhe_rsa_with_aes__cbc_sha if this is merged i will implement the same for the kubelet. release note : release-notekube-apiserver and kubelet now support customizing tls ciphers via a tls-cipher-suites flag"
241562263,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request? :/kind featurewe would like a way to dynamically generate host paths when mounting volumes the subpath feature creates directories on demand but the names assigned to those directories are static.supporting the downward api variables would provide a good way to share storage and avoid collisions.centralized log storage is one use case.for example volumemounts mountpath var/log/mysql name logs subpathfrom fieldref fieldpath metadata.name volumes name logs hostpath path mnt/log-repo another option env name name valuefrom fieldref fieldpath metadata.name volumemounts mountpath var/log/mysql name logs subpath name volumes name logs hostpath path mnt/log-repo over time the host storage would look something like this and the containers would not need to change any of their logging logic. /mnt/log-repo/mysql--pt/mnt/log-repo/mysql--plsf/mnt/log-repo/mysql--ns/mnt/log-repo/mysql--smx/mnt/log-repo/mysql--kgg/mnt/log-repo/mysql--rqsr
241380361,is this a bug report or feature request? :/kind bug what happened :operator is running an ha master setup with a lb in front kubelet attempts to update node status but tryupdatenodestatus wedges based on the goroutine dump the wedge happens when it attempts to get the latest state of the node from the master operator observed minute intervals between attempts to update node status when kubelet could not contact master assume this is when the lb ultimately closes the connection the impact is that node controller then marked node as lost and workload was evicted. what you expected to happen :expected the kubelet to timeout client-side.right now no kubelet->master communication has a timeout.ideally the kubelet master communication would have a timeout based on the configuration of the node-status-update-frequency so that no single attempt to update status wedges future attempts. how to reproduce it as minimally and precisely as possible) :see above
239925763,bug report i think?) what happened :i ran the following steps on ubuntu sudo apt-get update sudo apt-get upgrade sudo su kubeadm reset kubeadm init token redacted apiserver-advertise-address pod-network-cidr exit mkdir p home/.kube sudo cp i etc/kubernetes/admin.conf home/.kube/config sudo chown id u):$(id g home/.kube/config kubectl get nodes upon doing this i receive: unable to connect to the server x certificate signed by unknown authority possibly because of crypto/rsa verification error while trying to verify candidate authority certificate kubernetes) ive tried uninstalling kubectl kubeadm and kubelet a couple of times even with purge and no matter what i do it kubeadm doesnt generate a working admin.conf however i run the following: curl cacert etc/kubernetes/pki/ca.crt cert etc/kubernetes/pki/apiserver-kubelet-client.crt key etc/kubernetes/pki/apiserver-kubelet-client.key get paths api api/v apis apis apis/apiextensions.ks.io apis/apiextensions.ks.io/vbeta apis/apiregistration.ks.io apis/apiregistration.ks.io/vbeta apis/apps apis/apps/vbeta apis/authentication.ks.io apis/authentication.ks.io/v apis/authentication.ks.io/vbeta apis/authorization.ks.io apis/authorization.ks.io/v apis/authorization.ks.io/vbeta apis/autoscaling apis/autoscaling/v apis/batch apis/batch/v apis/certificates.ks.io apis/certificates.ks.io/vbeta apis/extensions apis/extensions/vbeta apis/networking.ks.io apis/networking.ks.io/v apis/policy apis/policy/vbeta apis/rbac.authorization.ks.io apis/rbac.authorization.ks.io/valpha apis/rbac.authorization.ks.io/vbeta apis/settings.ks.io apis/settings.ks.io/valpha apis/storage.ks.io apis/storage.ks.io/v apis/storage.ks.io/vbeta healthz healthz/autoregister-completion healthz/ping healthz/poststarthook/apiservice-registration-controller healthz/poststarthook/apiservice-status-available-controller healthz/poststarthook/bootstrap-controller healthz/poststarthook/ca-registration healthz/poststarthook/extensions/third-party-resources healthz/poststarthook/generic-apiserver-start-informers healthz/poststarthook/kube-apiserver-autoregistration healthz/poststarthook/rbac/bootstrap-roles healthz/poststarthook/start-apiextensions-controllers healthz/poststarthook/start-apiextensions-informers healthz/poststarthook/start-kube-aggregator-informers healthz/poststarthook/start-kube-apiserver-informers logs metrics swagger-...json swagger-...pb-v swagger-...pb-v.gz swagger.json swaggerapi ui ui version what you expected to happen :after initializing the master via kubeadm init i expected to be able to use kubectl to install a network plugin since it x s i cannot do that. environment kubernetes version use kubectl version os e.g from etc/os-release ubuntu lts kernel e.g uname a linux radium-control generic ubuntu smp mon jun utc x x x gnu/linux
239085454,this form is for bug reports and feature requests only if youre looking for help check stack overflow and the troubleshooting guide this a bug report or feature request? :probably a feature request/kind feature anything else we need to know? :i have not found a way to inject global environment variables either cluster-wide or per-namespace the most common use cases i have seen are indicate what environment or location we are in e.g environment=dev or environment=prod sure i could set something on every deployment and daemonset and manual pod and but that is overhead and is messy could use configmap or secret same issue the cluster administrator should be able to say define the following environment vars that are injected into every container in the cluster or at best limited by namespace enabling features e.g i might want apm enabled on many of my app containers but only when run in a kube cluster as opposed to just running the image on my laptop or perhaps only in certain environments cluster manager should be able to set a global env var of enable_apm=true that then causes any container that consumes it to know i must enable apm now. environment kubernetes version use kubectl version cloud provider or hardware configuration aws os e.g from etc/os-release coreos kernel e.g uname a coreos-r install tools direct others:if it is there and i missed it apologies
238589775,thanks for sending a pull request here are some tips for you if this is your first time read our contributor guidelines and developer guide if you want faster pr reviews read how follow the instructions for writing a release note this pr does why we need it implements the backoff policy and failed pod limit defined in issue this pr fixes fixes fixes special notes for your reviewer :this is a wip pr i updated the api batchv.jobspec in order to prepare the backoff policy implementation in the jobcontroller. release note : release-noteadd backoff policy and failed pod limit for a job
237629147,kubernetes currently supports port forwarding only for tcp ports however pod/service might expose udp port too
237156126,the following features are recommended===============================podspec v corehostmount boolean host mount requested for this pod use the hosts mount namespace optional default to false
235977274,it would be nice to support manual runs for cronjobs today iirc a cronjob misses its schedule for whatever reason and we wont start it until the next time its scheduled to run at the very least this could be a kubectl generator eventually we may want to support it as part of the cj api.@stevekuznetsov since you were asking about it today@kubernetes/sig-apps-feature-requests
234918289,bug report kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:dfdfcafaebefcbaae gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:feeceabfddefceefdc gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release coreos kernel e.g uname a linux coreos smp tue may utc x intel(r xeon(r cpu e v ghz genuineintel gnu/linux what happened : kubectl apply ignores changes in spec.initcontainers field the spec.initcontainers field is also mirrored into alpha and beta annotations i think the annotations from the previous run overwrite the changes specified in spec.initcontainers field if i apply my changes as annotations it works also there is no feature-gates option in kube-apiserver to turn it off annotations for initcontainers. what you expected to happen : spec.initcontainers changes should trump over annotations how to reproduce it as minimally and precisely as possible):create a deployment with spec.initcontainer field populated in pod template apply the deployment make changes spec.initcontainer field and apply again kubectl says deployment is configured but your changes are not applied
234677562,i couldnt find any discussion of this after searching for cron timezone cronjob timezone or scheduledjob timezone.the cronjob spec makes reference to that page suggests that cron would respect the timezone for a given user the controller manager runs in a single time zone under a single user so i cant use different time zones for each job i have jobs that run based on the schedule of external entities that observe daylight savings time so if i define that cronjob in utc i will be forced to update that job from time to time generally not something one remembers to do after just losing an hour of sleep).i see two options for how this support might work in kubernetes add some new field to the cronjobspec like timezone americas/chicago use an extended cron syntax that includes timezone e.g europe/stockholm
234323169,when try to connect external server from pod getting could not resolve host error intermittent from all nodes we are able to connect to the host and get response not seeing intermittent issue but from pods seeing intermittent error as below can anyone tell what could be reason bash curl myhost:curl empty reply from serverbash curl myhost:curl empty reply from serverbash-.$curl myhost.com:curl could not resolve host myhost.combash curl myhost:curl empty reply from server i tried most of the option mentioned in this ticket but does not helped scale up kube-dns nodes to rebooted nodesstill does not fixed this issue./sig area/dns@kubernetes/sig-cluster ops-misckubectl version: kubectl versionclient version version.info{major minor gitversion:v gitcommit:efccbeaeffcabdfaeeee gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:efccbeaeffcabdfaeeee gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment name=amazon linux amiversion=.id=amznid_like=rhel fedoraversion_id=.pretty_name=amazon linux ami ansi_color=;cpe_name=cpe:/o:amazon:linux:.:gahome_url happend kube dns intermittent does not resolve host name from pods but always works fine from nodes expectation pods always should resolve external servers like nodes
231672984,what this pr does why we need it :implements proposal adds address flag to port-forward command that allows listening on addresses other then localhost so that port-forward can ie be opened to consumers other then residing in local host like running in docker or different machine/vm which issue this pr fixes fixes fixes fixes release note : allows selecting non-localhost addresses to listen on with port-forward
231157705,gcloud /gke has an efficient way of getting credentials from somewhere by using the get-credentials command that automatically merges the content of a remote cluster in a local kubeconfig file we want to have something similar in ks a first approach would be having two kubeconfig files locally and being able to merge the content of one into the other without needing to do manual copy/paste we could have a command that looks like kubectl config merge source target where the target file defaults to kube/config and source would be some kubeconfig file that the user wants to have its entries users contexts and clusters copied to the target file if there is no name conflict i.e the entries in source dont exist in target the output is quite straightforward however if there is some conflict we need to decide on how to approach do anything and inform the user rename them in target copy only those with no conflict have this options as parameters ignore rename abort-if-conflict etc)i suggest having for now for the sake of simplicity doing anything and informing the user the others can me made in follow-up issues
229432719,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):--- is this a bug report or feature request choose one): kubernetes version use kubectl version ): kubectl versionclient version version.info{major minor gitversion:v gitcommit:ebacbaccadcacff gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:ebacbaccadcacff gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release pretty_name=container linux by coreos ladybug kernel e.g uname a coreos install tools custom ansible others kube dns related images gcr.io/google_containers/kubedns-amd and gcr.io/google_containers/kube-dnsmasq-amd:.. what happened :java.net.unknownhostexception dynamodb.us-east-.amazonaws.com what you expected to happen :receive a response to the name lookup request. how to reproduce it as minimally and precisely as possible):this is the kicker we are not able to reproduce this issue on purpose however we experience this in our production cluster times a week. anything else we need to know :in the past months or so we had experienced a handful of events where dns was failing for most/all of our production pods and the event would last for minutes during this time the kube-dns service was healthy with available endpoints at all times we increased our kube-dns pod count to in node production clusters this level of provisioning alleviated the dns issues that were taking down our production services however we still experience at least weekly smaller events ranging from second to seconds which affect a small subset of pods during these events pods on different nodes across the cluster experience a burst of dns failures which have a much smaller end user impact we enabled query logging in dnsmasq as we were not sure whether the queries made it from the client pod to one of the kube-dns pods or not what was interesting is that during the dns events where query logging was enabled none of the name lookup requests that resulted in an exception were received by dnsmasq at this point my colleague noticed these errors coming from dnsmasq-metrics error logging before flag.parse w server.go error getting metrics from dnsmasq read udp i/o timeoutthat error as near as i can tell is basically a name resolution error from dnsmasq-metrics as its trying to query the dnsmasq container in the same pod to get dnsmasqs internal metrics similar to running dig short chaos txt cachesize.bind .all of our dns events are happening at the exact same time that or more dnsmasq-metrics container is throwing those errors we thought we might be possibly exceeding the default connection limit that dnsmasq has but we do not see any logs indicating that if we did we would expect to see these log messages dnsmasq maximum number of concurrent dns queries reached max based off of conversations with other cluster operators and users in slack i know that other users are experiencing these same problems im hoping that this issue can be used to centralize our efforts and determine if dnsmasq refusing connections is the problem or a symptom of something else
228410049,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help no what keywords did you search in kubernetes issues before filing this one aws loadbalancer elb healthcheck--- is this a bug report or feature request choose one feature request kubernetes version use kubectl version ):client version version.info{major minor gitversion:v gitcommit:efccbeaeffcabdfaeeee gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:feeceabfddefceefdc gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration :aws os e.g from etc/os-release):name=container linux by coreosid=coreosversion=..version_id=..build_id=---pretty_name=container linux by coreos ladybug kernel e.g uname a ):linux ip----.us-west-.compute.internal coreos smp wed apr utc x intel(r xeon(r cpu e v ghz genuineintel gnu/linux install tools :coreos kubelet-wrapper others :n/a what happened :aws elb health check target seems to be tcp only backend pod is serving in ssl and elb health check uses tcp pod container will report error logs http tls handshake error from ip>:
228277962,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see but feels more like a bug... what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):aws ebs--- is this a bug report or feature request choose one):more of a bug report kubernetes version use kubectl version ):client version version.info{major minor gitversion:v gitcommit:efccbeaeffcabdfaeeee gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:cebaabceecddba gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration amazon ec os e.g from etc/os-release kernel e.g uname a admin@ip uname alinux ip ks smp mon jan utc x gnu/linuxadmin@ip cat etc/os-release pretty_name=debian gnu/linux jessie)name=debian gnu/linuxversion_id=version jessie)id=debianhome_url install tools kops version git-f others : what happened :i am new to kubernetes and try to learn hands-on by setting up a wordpress with this tutorial kubernetes cluster runs on amazon ec set up via kops i ended up with a little cluster master nodes then i started with the tutorial but i use amazon ebs volumes for persistent storage.in the first attempt i created the ebs volumes manually and then wondered why kubernetes created new ones until i learned about storageclasses and dynamic perstistent volumes indeed i seem to have a default storageclass using aws-ebs surely set up by kops).the tutorial consists of two pods one for a mysql database and one for wordpress the first pod mysql starts fine.but the second wordpress pod ends up in the state crashloopbackoff using kubectl describe i found this:events firstseen lastseen count from subobjectpath type reason message s s default-scheduler warning failedscheduling schedulerpredicates failed due to persistentvolumeclaim is not bound wp-pv-claim which is unexpected schedulerpredicates failed due to persistentvolumeclaim is not bound wp-pv-claim which is unexpected schedulerpredicates failed due to persistentvolumeclaim is not bound wp-pv-claim which is unexpected s s default-scheduler normal scheduled successfully assigned wordpress--hgnh to ip----.eu-west-.compute.internal s s controller-manager warning failedmount failed to attach volume pvc-bae-c-e-b-baef on node ip----.eu-west-.compute.internal with error attaching ebs volume vol-bbfadbfba to instance i-daa incorrectstate vol-bbfadbfba is not available status code request id:apparently it fails to mount the automatically provisioned volume to the node but i checked the node via the aws console and the volume is already attached. what you expected to happen :to use an existing pv or create a new one and attach it to the pod how to reproduce it as minimally and precisely as possible install ks cluster using kops on aws follow the wordpress example linked above except using aws ebs instead of gce/local disk as dynamic pvs are available creating own ones beforehand seems to be unnecessary) anything else we need to know
227842064,what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):error syncing pod no such container--- is this a bug report or feature request choose one bug report kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:bbaccaabeccbfe gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:efccbeaeffcabdfaeeee gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release ubuntu lts kernel e.g uname a generic x install tools kops beta others : what happened :a pod was restarted a few times it was killed by the kernel due to running out of memory after the last restart the pod appears stuck in an error state kubectl n master get pods dhrubacol-leaf--name ready status restarts agedhrubacol-leaf error h kubectl describe shows a no such container error firstseen lastseen count from subobjectpath type reason message h s kubelet ip----.us-west-.compute.internal warning failedsync error syncing pod skipping rpc error code desc error no such container bedfcbbbebbbcccfddaabadeddf kubectl logs show the logs from the last iteration of the pod the one that finished hour ago).interestingly the pod is actually running kubectl exec lets me enter the pod. what you expected to happen :i expected the pod to be restarted. how to reproduce it as minimally and precisely as possible):does not appear reproducible this has happened twice to us so far deleting the pod by hand fixed the problem. anything else we need to know :note the following in the kubelet logs more details below):at the pod dies the container id starts with bedfcb ...at the pod dies again the container id starts with af ...at getpodcontainerstatuses stats failing but the container id is from a previous iteration of the pod the one that died at not the one that died at may ip kubelet i kubelet.go syncloop pleg dhrubacol-leaf--_master(dadd--e-bc-edc event pleg.podlifecycleevent{id:dadd--e-bc-edc type:containerdied data:bedfcbbbebbbcccfddaabadeddf}may ip kubelet i kubelet.go syncloop pleg dhrubacol-leaf--_master(dadd--e-bc-edc event pleg.podlifecycleevent{id:dadd--e-bc-edc type:containerdied data:afdcdcfaecbfefbffaf}may ip kubelet i kuberuntime_manager.go checking backoff for container leafagg in pod dhrubacol-leaf--_master(dadd--e-bc-edc)may ip kubelet e kuberuntime_manager.go getpodcontainerstatuses for pod dhrubacol-leaf--_master(dadd--e-bc-edc failed rpc error code desc error no such container bedfcbbbebbbcccfddaabadeddfmay ip kubelet e generic.go pleg ignoring events for pod dhrubacol-leaf--/master rpc error code desc error no such container bedfcbbbebbbcccfddaabadeddfmay ip kubelet e kuberuntime_manager.go getpodcontainerstatuses for pod dhrubacol-leaf--_master(dadd--e-bc-edc failed rpc error code desc error no such container bedfcbbbebbbcccfddaabadeddfmay ip kubelet e generic.go pleg ignoring events for pod dhrubacol-leaf--/master rpc error code desc error no such container bedfcbbbebbbcccfddaabadeddfmay ip kubelet e kuberuntime_manager.go getpodcontainerstatuses for pod dhrubacol-leaf--_master(dadd--e-bc-edc failed rpc error code desc error no such container bedfcbbbebbbcccfddaabadeddf (and then the getpodcontainerstatuses failed messages spam the logs
227816000,ks just landed and broke a lot of our systems the issue shows as name ready status restarts age test-ceph-init-mon-hxh mkdir var/lib/kubelet/pods/cfe-bb-e-ad-faecebe/volumes/kubernetes.io~configmap/resolv-conf/resolv.conf not a directory mthe issue is we have subpath set to a file in a configmap to map it into the right place in our container but the new code assumes subpath is a directory and always does a mkdir over it this causes things to fail.the pr in question is
226598989,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help no what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there pleg notready kubelet--- is this a bug report or feature request bug kubernetes version use kubectl version environment cloud provider or hardware configuration coreos on aws os e.g from etc/os-release):coreos kernel e.g uname a coreos install tools others : what happened :i have a worker cluster two and sometimes all three nodes keep dropping into notready with the following messages in journalctl u kubelet : may ip----.ec.internal kubelet i kubelet_node_status.go recording nodenotready event message for node ip----.ec.internalmay ip----.ec.internal kubelet i kubelet_node_status.go node became not ready type:ready status:false lastheartbeattime utc lasttransitiontime utc reason:kubeletnotready message:pleg is not healthy pleg was last seen active m.s ago threshold is ms}may ip----.ec.internal kubelet i kubelet_node_status.go recording nodenotready event message for node ip----.ec.internalmay ip----.ec.internal kubelet i kubelet_node_status.go node became not ready type:ready status:false lastheartbeattime utc lasttransitiontime utc reason:kubeletnotready message:pleg is not healthy pleg was last seen active m.s ago threshold is ms}may ip----.ec.internal kubelet i kubelet_node_status.go recording nodenotready event message for node ip----.ec.internalmay ip----.ec.internal kubelet i kubelet_node_status.go node became not ready type:ready status:false lastheartbeattime utc lasttransitiontime utc reason:kubeletnotready message:pleg is not healthy pleg was last seen active m.s ago threshold is ms}may ip----.ec.internal kubelet i kubelet_node_status.go recording nodenotready event message for node ip----.ec.internalmay ip----.ec.internal kubelet i kubelet_node_status.go node became not ready type:ready status:false lastheartbeattime utc lasttransitiontime utc reason:kubeletnotready message:pleg is not healthy pleg was last seen active m.s ago threshold is ms}may ip----.ec.internal kubelet i kubelet_node_status.go recording nodenotready event message for node ip----.ec.internalmay ip----.ec.internal kubelet i kubelet_node_status.go node became not ready type:ready status:false lastheartbeattime utc lasttransitiontime utc reason:kubeletnotready message:pleg is not healthy pleg was last seen active m.s ago threshold is ms} docker daemon is fine local docker ps docker images etc all work and respond immediately using weave networking installed via kubectl apply f you expected to happen :nodes to be ready. how to reproduce it as minimally and precisely as possible):wish i knew how! anything else we need to know :all of the nodes workers and masters on same private subnet with nat gateway to internet workers in security group that allows unlimited access all ports from masters security group masters allow all ports from same subnet proxy is running on workers apiserver controller-manager scheduler on masters kubectl logs and kubectl exec always hang even when run from the master itself or from outside
226499417,poddisruptionbudget is immutable at this moment it would be a nice improvement to allow changes to it at the least to the minavailable field not sure whether this counts as a bug or a feature request after creating a poddisruptionbudget with the following manifest. yamlapiversion policy/vbetakind poddisruptionbudgetmetadata name elasticsearch-master labels app elasticsearch role masterspec selector matchlabels app elasticsearch role master minavailable when changing the spec to lets say minavailable and re-running kubectl apply it returns the following error: the poddisruptionbudget elasticsearch-master is invalid spec forbidden updates to poddisruptionbudget spec are forbidden. kubernetes version : client version version.info{major minor gitversion:v gitcommit:efccbeaeffcabdfaeeee gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:ffbeaabebdc gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment google container engine with cos image os e.g from etc/os-release): build_id=..name=container-optimized osgoogle_crash_id=lakituversion_id=bug_report_url os from googleversion=google_metrics_product_id=home_url kernel e.g uname a ): linux gke-production-europe-we-auto-scaling-daaf-zzzj smp fri feb pst x intel(r xeon(r cpu ghz genuineintel gnu/linux what happened :changing the value of spec.minavailable in a poddisruptionbudget failed due to updates being forbidden. what you expected to happen :at least spec.minavailable being allowed to change because its often used in conjunction with a statefulset which is likely to grow over time by incrementing the number of replicas at which time you also want to grow minavailable in the poddisruptionbudget
222820416,"is this a request for help no what keywords did you search in kubernetes issues before filing this one initcontainers--- is this a bug report or feature request choose one bug report kubernetes version client version version.info{major minor gitversion:v gitcommit:fffbebdfffaaddcdebef gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:bbaccaabeccbfe gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration gke what happened :initial template bb.yaml : apiversion extensions/vbetakind deploymentmetadata name bb-deploymentspec replicas template metadata labels name bb spec initcontainers name bb-liquibase image eu.gcr.io/project/bb-liquibase:master imagepullpolicy always containers name cloudsql-proxy image gcr.io/cloudsql-docker/gce-proxy name bb image eu.gcr.io/project/bb:master imagepullpolicy always after doing kubectl n dev apply f bb.yaml kubectl n dev describe deployment bb-deploymentname bb-deploymentnamespace devcreationtimestamp wed apr labels name=bbannotations deployment.kubernetes.io/revision kubectl.kubernetes.io/last-applied-configuration={apiversion:extensions/vbeta,kind:deployment,metadata:{annotations:{},name:bb-deployment,namespace:dev},spec:{replicas:,te...selector name=bbreplicas desired updated total available unavailablestrategytype rollingupdateminreadyseconds rollingupdatestrategy max unavailable max surgepod template labels name=bb init containers bb-liquibase image eu.gcr.io/porject/bb-liquibase:master containers cloudsql-proxy image gcr.io/cloudsql-docker/gce-proxy bb image eu.gcr.io/tripcreatorcom/bb:master conditions type status reason available true minimumreplicasavailableoldreplicasets bb-deployment replicas created)newreplicaset none>events firstseen lastseen count from subobjectpath type reason message s s deployment-controller normal scalingreplicaset scaled up replica set bb-deployment to now lets update both the initcontainer and container image tag so that bb.yaml would look like this: apiversion extensions/vbetakind deploymentmetadata name bb-deploymentspec replicas template metadata labels name bb spec initcontainers name bb-liquibase image eu.gcr.io/tripcreatorcom/bb-liquibase:master--efeb imagepullpolicy always containers name cloudsql-proxy image gcr.io/cloudsql-docker/gce-proxy name bb image eu.gcr.io/tripcreatorcom/bb:master--efeb imagepullpolicy always and lets check our deployment kubectl n dev describe deployment bb-deploymentname bb-deploymentnamespace devcreationtimestamp wed apr labels name=bbannotations deployment.kubernetes.io/revision kubectl.kubernetes.io/last-applied-configuration={apiversion:extensions/vbeta,kind:deployment,metadata:{annotations:{},name:bb-deployment,namespace:dev},spec:{replicas:,te...selector name=bbreplicas desired updated total available unavailablestrategytype rollingupdateminreadyseconds rollingupdatestrategy max unavailable max surgepod template labels name=bb init containers bb-liquibase image eu.gcr.io/project/bb-liquibase:master containers cloudsql-proxy image gcr.io/cloudsql-docker/gce-proxy bb image eu.gcr.io/project/bb:master--efeb conditions type status reason available true minimumreplicasavailableoldreplicasets bb-deployment replicas created)newreplicaset none>events firstseen lastseen count from subobjectpath type reason message m m deployment-controller normal scalingreplicaset scaled up replica set bb-deployment to m m deployment-controller normal scalingreplicaset scaled up replica set bb-deployment to m m deployment-controller normal scalingreplicaset scaled down replica set bb-deployment to as you can see image for bb has been updated to master--efeb but for initcontainer bb-liquibase the image is still at master- according to docs changes to the init container spec are limited to the container image field or does this imply that image tag changes are ignored? what you expected to happen : initcontainer bb-liquibase tag should be master--efeb"
222695266,kubernetes version v.. environment arm cavium thunder x ubuntu lts generic what happened :init kubernetes with kubeadm init kubernetes-version=v pod-network-cidr=.../ than tried kubectl taint nodes all node-role.kubernetes.io/master- and got this the connection to the server localhost was refused did you specify the right host or port? or this kubectl apply f connection to the server localhost was refused did you specify the right host or port? or kubectl versionclient version version.info{major minor gitversion:v gitcommit:bbaccaabeccbfe gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/arm}the connection to the server localhost was refused did you specify the right host or port
220815920,"hi guys,this is a simple question that for some reason i was unable to find an answer for in the internet. what is the correct pronunciation of kubernetes in english? i think it would be nice to add it in your readme.md file too.thank you"
219657630,is this a bug report or feature request choose one):feature request kubernetes version use kubectl version ):client version version.info{major minor gitversion:v gitcommit:efcfefbabedd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:ecdbaeaaaffbbbbb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release container linux by coreos stable kernel e.g uname a linux ip coreos-r smp fri mar utc x intel(r xeon(r cpu e v ghz genuineintel gnu/linux install tools others : what happened :when you deploy non-statefulset pods they will be given a label called name with the pod name however with statefulsets pods do not have a name label this means you cant write services that use a selector based on the name label you can of course put labels in the statefulset definition but those wont be unique per pod example i can deploy a statefulset of kafka pods add a label app=kafka but each pod has that label and its not unique what you expected to happen :pods deployed from a statefulset should have a name label with the name of the pod the name that you see when you do kubectl get pods) how to reproduce it as minimally and precisely as possible create a statefulset kubectl describe pods the pod notice the lack of a name label anything else we need to know :an example of when this is needed would be deploying something where each pod needs to be individually reachable kafka is a good example you want to have stateful storage and identity for the brokers but you also need to be able to reach each one which means you need a service per pod currently i have not found a way to automatically have a service associated with a statefulset pod via labels ive had to manually tag the statefulset pods to take advantage of selectors
219624325,bug report kubernetes version use kubectl version environment cloud provider or hardware configuration aws os e.g from etc/os-release ubuntu kernel e.g uname a generic install tools salt others : what happened :when running multiple api servers behind a loadbalancer it is possible to get the same nodeport assigned to multiple services we have seen a nodeport re-used up to times when a nodeport gets re-used it prevents the kube-proxy iptables from being created which means all services that get a duplicated nodeport after the first one fail on all nodes. what you expected to happen :nodeports assigned via the api should always be unique across the cluster. how to reproduce it as minimally and precisely as possible run multiple apiservers behind a loadbalancer we have create many nodeport enabled services at the same time we used gnu parallel to do this check the created services and wait for duplicates to show upwe created services at a time concurrently each with nodeports ended up with or of those nodeports conflicting with another
219417207,at kubecon europe in berlin last week i presented some work weve done at huawei scaling kubernetes in-cluster load balancing to services and beyond the challenges associated with doing this using the current iptables approach and what weve achieved using an alternative ipvs-based approach iptables is designed for firewalling and based on in-kernel rule lists while ipvs is designed for load balancing and based on in-kernel hash tables ipvs also supports more sophisticated load balancing algorithms than iptables least load least conns locality weighted as well as other useful features e.g health checking retries etc).after the presentation there was strong support a.k.a a riot for us to open source this work which we are happy to do we can use this issue to track that.for those who were not able to be there here is the video the slides will follow up on this with a more formal design proposal and a set of prs but in summary we added a about lines of code to the existing lines of kube-proxy and added a third mode flag to its command-line mode=ipvs to the existing mode=userspace and mode=iptables).performance improvement of load balancer updates is dramatic update latency reduced from hours per rule to ms per rule network latency and variability also reduced dramatically for large numbers of services kubernetes/sig-network-feature-requests kubernetes/sig-scalability-feature-requests thockin wojtek-t
218773453,implementing this proposal that will solve issues raised in and in short allow binding on addresses other then currently supported and the idea is to implement a new method newonaddress in pkg/client/unversioned/portforward/portforward.go that will add an address parameter and refactor new as a wrapper on the newonaddress pointing to localhost address.the newonaddress would work as new did previously that is calling listenonportandaddress with a difference of setting the new portforwarder private variable address that can take string representations of ipv ipv localhost or nil then listenonport will be changed in a way that if ipv or ipv is provided in portforwarder.address the address given wil be the only one used if it is a valid one if value localhost or nil is passed it will behave exactly as it does now listening on and pkg/kubectl/cmd/portforward.go will be modified to call newonaddress method instead of new with new option address provided by a or address flag on kubectl port-forward command and passed via a new string variable address in updated portforwardoptions type
218553580,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):active_fileinactive_fileworking_setworkingsetcadvisormemory.available--- is this a bug report or feature request choose one):well say bug report though this is arguable) kubernetes version use kubectl version ):.. environment cloud provider or hardware configuration os e.g from etc/os-release):name=ubuntuversion lts trusty tahrid=ubuntuid_like=debianpretty_name=ubuntu ltsversion_id kernel e.g uname a ):linux hostname_redacted generic ubuntu smp tue dec utc x x x gnu/linux install tools others : what happened :a pod was evicted due to memory pressure on the node when it appeared to me that there shouldnt have been sufficient memory pressure to cause an eviction further digging seems to have revealed that active page cache is being counted against memory.available. what you expected to happen :memory.available would not have active page cache counted against it since it is reclaimable by the kernel this also seems to greatly complicate a general case for configuring memory eviction policies since in a general sense its effectively impossible to understand how much page cache will be active at any given time on any given node or how long it will stay active in relation to eviction grace periods). how to reproduce it as minimally and precisely as possible):cause a node to chew up enough active page cache that the existing calculation for memory.available trips a memory eviction threshold even though the threshold would not be tripped if the page cache active and inactive were freed for anon memory. anything else we need to know :i discussed this with derekwaynecarr in sig-node and am opening this issue at his request conversation starts here poking around on slack or opening this issue i did my best to read through the release code kubernetes documentation and cgroup kernel documentation to make sure i understood what was going on here the short of it is that i believe this calculation node.status.capacity memory node.stats.memory.workingsetis using cadvisors value for working set which if i traced the code correctly amounts to:$cgroupfs/memory.usage_in_bytes total_inactive_filewhere according to my interpretation of the kernel documentation usage_in_bytes includes all page cache:$kernel/documentation/cgroups/memory.txt design the core of the design is a counter called the res_counter the res_countertracks the current memory usage and limit of the group of processes associatedwith the controller accounting details all mapped anon pages rss and cache pages page cache are accounted. ultimately my issue is concerning how i can set generally applicable memory eviction thresholds if active page cache is counting against those and theres no way to to know generally how much page cache will be active across a clusters nodes to use as part of general threshold calculations how long active page cache will stay active to use as part of eviction grace period calculations.i understand that there are many layers here and that this is not a particularly simple problem to solve generally correctly or even understand top to bottom so i apologize up front if any of my conclusions are incorrect or im missing anything major and i appreciate any feedback you all can provide.as requested by derekwaynecarr cc sjenning derekwaynecarr
218334482,this is a feature request to add support in podpreset for injecting containers and init containers.usecase as a cluster admin i need the ability to to inject an init container and a proxy sidecar container into a pod to enable istio microservices mesh functionality
217865047,kubernetes installation aio with kubeadmcentos when kubeadm init run the following error gets reported and kubelet fails to start:kubelet error failed to run kubelet failed to create kubelet misconfiguration kubelet cgroup driver cgroupfs is different from docker cgroup driver systemd
217737938,cc kubernetes/sig-nodepid cgroup allows limiting the number of processes per cgroup and very critical to avoid fork bombs docker seems to have added support since release docker/docker#i can think of two options to expose this in ks api add it to pod.spec.container.resources.limit as a container resource limit pids-limit or expose it as a podsecuritypolicy securitycontext which gets applied to all pods at admission
217512804,what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.): aws elb security policy --- is this a bug report or feature request choose one feature request kubernetes version use kubectl version kubectl versionclient version version.info{major minor gitversion:v gitcommit:cebaabceecddba gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:cbbabfeddcacefccc gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release cat etc/os-releasename=container linux by coreosid=coreosversion=..version_id=..build_id=---pretty_name=container linux by coreos ladybug)ansi_color=;;home_url kernel e.g uname a uname alinux ip----.us-west-.compute.internal coreos-r smp tue feb utc x intel(r xeon(r cpu e v ghz genuineintel gnu/linux install tools aws cloudformation others : what happened default aws elb security policy at the moment is this policy is not pci-dss compliant as it allows protocols tls and tls therefore all elastic load balancers created by kubernetes are not pci-dss compliant aws has a predefined policiy for elbs tls who does not allow tls versions and however there is no way to make this policy default. what you expected to happen i would like to be able to use annotations for specifying a given security policy so that new elbs created by loadbalancer type services would have the specified security policy the annotation could be named service.beta.kubernetes.io/aws-load-balancer-security-policy . how to reproduce it as minimally and precisely as possible example of a service config with service.beta.kubernetes.io/aws-load-balancer-security-policy annotation: apiversion vkind servicemetadata name app annotations service.beta.kubernetes.io/aws-load-balancer-ssl-cert arn:aws:acm:us-east-:xxxxxxxxxx:certificate/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx service.beta.kubernetes.io/aws-load-balancer-backend-protocol http service.beta.kubernetes.io/aws-load-balancer-ssl-ports service.beta.kubernetes.io/aws-load-balancer-security-policy elbsecuritypolicy-tls spec ports name http port protocol tcp targetport name https port protocol tcp targetport selector app app type loadbalancer anything else we need to know
216988596,im experienced node.js developer and i wanted to use kubernetes for my project i was able to get kubernetes running quite easly but then ive run into concerns that i was not able to solve or to find some resources about how to organize local development workflow properly being able to have local changes applied to container on every file change without additional work running commands to update container image then to push it to pod etc thats must-be for day-to-day development being able to test local development with other services lets say i have pods and i need of them running and being locally available to have at least some minimal functionality required for development how should i organize that do i need to create testing env in the cloud that cost money how to automate services discovery in such case of switching between production/test services some services are not published to external world but still may be required to have local app running how to manage such situation what is the best approach to have as similar as possible production and development environment that is easy to synchronize when changes will occur what is the best approach to be able to quickly install such environment on new local machines new developers how to export entire cluster to some config file yaml is it possible to have point of entry for entire cluster tutorials of kubernetes require running some commands of kubectl and it makes entire workflow imperative instead of immutable eg i need to run some sequence of commands to have exactly the same cluster on my laptop and on my friends laptop missing some command may break entire workflow if bus hits me nobody will know how to continue im not experienced about devops but my imagination says there could be some central config and boot-up file that will allow me to replicate all my application setup there are a lot of resources about what can you define in yaml file etc but i was not able to find some general guide about how you should organize those files what is proper way to execute them to set order of their execution etc i thought id create just a series of yaml files and run them one by one by some shell file but i was not able to confirm if its good approach in general i was trying to find some start to end guide about setting up fully functional development workflow that will work in real life.im totally ready to accept that managing devops myself will require learning a lot of new tools but i was not able to find out what is one of the dream workflow and what is path to achieve it
216936821,this is a proof-of-concept for containerd kubernetes integration sent it here just for feedback please do not merge this try it out pull this pr down build and install containerd version build and install runc version make sure containerd containerd-shim dist runc are in path start containerd start a local cluster with hack/local-cluster-up.sh create/delete the pod: apiversion vkind podmetadata name redisspec restartpolicy never containers name redis image docker.io/library/redis:latest command redis-server bind current status of the poc pod lifecycle and image lifecycle are supported including pod creation/deletion/list/inspect image pull/list pod lifecycle management is using containerd grpc api image management is mainly using containerd cli dist just for convenience only docker.io/library/xxx image is supported now streaming and container logs are not supported yet detailed pod/container configuration are not supported yet e.g dnsoptions env etc required metadata is stored in memory instead of checkpoint dependencies versions containerd runc runtime-spec existing dependency update see grpc v etcd v rkt v..please feel free to comment if you have any suggestion/question/idea.@mikebrow thanks for your help with the poc!@yujuhong dchen thockin crosbymichael stevvooe feiskyer resouer cc kubernetes/sig-node-feature-requests
216908117,based on kubernetes/community#this issue is meant to track work items and help collaboration between the community on adding support for persistent local storage.note priority of these features can change based on the number of collaborators.alpha user guide is here task tracking is available in the sig storage planning spreadsheet
216884434,is this a request for help? no what keywords did you search in kubernetes issues before filing this one? ingress controller hosts multiple host.i found this issue but it was closed as it was in the wrong repo.--- is this a bug report or feature request choose one):feature requestwith the current implementation if you have a few sub-/domains which need to point to the same service you get a lot of duplicated code.example: spec rules host foobar.com http paths backend servicename foobar serviceport host api.foobar.com http paths backend servicename foobar serviceport host admin.foobar.com http paths backend servicename foobar serviceport host status.foobar.com http paths backend servicename foobar serviceport so i propose that the host field get changed from a single fqdn to a array of fqdn.so i could do something like: spec rules host foobar.com api.foobar.com admin.foobar.com status.foobar.com http paths backend servicename foobar serviceport it save me lines and makes the config more clear.maybe dup of but this seems easier to implement.cc aledbf you closed the last issues not sure if you work in this part of ks) kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:cebaabceecddba gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v..+coreos gitcommit:cbbabfeddcacefccc gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment :digitalocean coreos
216502517,kubernetes version use kubectl version environment cloud provider or hardware configuration aws os debian gnu/linux jessie kernel linux ip ks smp fri oct utc x gnu/linux install tools kops others what happened :when i update a deployment behind a service type nodeport for a short time requests are lost. what you expected to happen :there shouldnt be any request getting lost. how to reproduce it as minimally and precisely as possible use kubectl apply kind serviceapiversion vmetadata name zero-downtime-testspec type nodeport ports port targetport selector app zero-downtime-test---kind deploymentapiversion extensions/vbetametadata name zero-downtime-testspec replicas template metadata labels app zero-downtime-test spec containers name backend image nginx livenessprobe httpget path port scheme http readinessprobe httpget path port scheme http ports containerport protocol tcp now do constantly request the node port and check if response is a valid status code e.g with jmeter and req/second modify the deployment so that a rolling update is triggered for a few milliseconds requests wont be answered. anything else we need to know :when adding the following to my deployment it works fine lifecycle prestop exec command sleep therefore i think this might be a timing issue where the pod gets the termination signal before it is removed from the service load balancing
213261910,what this pr does why we need it :as the rbac role need to be related to resources i think we can use the command to get the supported resources cluster/kubectl.sh api-resources name shortnames apigroup namespaced kindbindings true bindingcomponentstatuses cs false componentstatusconfigmaps cm true configmapendpoints ep true endpointsevents ev true eventlimitranges limits true limitrangenamespaces ns false namespacenodes no false nodepersistentvolumeclaims pvc true persistentvolumeclaimpersistentvolumes pv false persistentvolumepods po true podpodtemplates true podtemplatereplicationcontrollers rc true replicationcontrollerresourcequotas quota true resourcequotasecrets true secretserviceaccounts sa true serviceaccountservices svc true serviceexternaladmissionhookconfigurations admissionregistration.ks.io false externaladmissionhookconfigurationinitializerconfigurations admissionregistration.ks.io false initializerconfigurationcustomresourcedefinitions crd apiextensions.ks.io false customresourcedefinitionapiservices apiregistration.ks.io false apiservicecontrollerrevisions apps true controllerrevisiondaemonsets ds apps true daemonsetdeployments deploy apps true deploymentreplicasets rs apps true replicasetstatefulsets sts apps true statefulsettokenreviews authentication.ks.io false tokenreviewlocalsubjectaccessreviews authorization.ks.io true localsubjectaccessreviewselfsubjectaccessreviews authorization.ks.io false selfsubjectaccessreviewsubjectaccessreviews authorization.ks.io false subjectaccessreviewhorizontalpodautoscalers hpa autoscaling true horizontalpodautoscalerjobs batch true jobcertificatesigningrequests csr certificates.ks.io false certificatesigningrequestdaemonsets ds extensions true daemonsetdeployments deploy extensions true deploymentingresses ing extensions true ingressnetworkpolicies netpol extensions true networkpolicypodsecuritypolicies psp extensions false podsecuritypolicyreplicasets rs extensions true replicasetnetworkpolicies netpol networking.ks.io true networkpolicypoddisruptionbudgets pdb policy true poddisruptionbudgetclusterrolebindings rbac.authorization.ks.io false clusterrolebindingclusterroles rbac.authorization.ks.io false clusterrolerolebindings rbac.authorization.ks.io true rolebindingroles rbac.authorization.ks.io true rolepodpresets settings.ks.io true podpresetstorageclasses sc storage.ks.io false storageclass which issue this pr fixes fixes notes for your reviewer : release note steps to write your release note use the release-note labels to set the release note state if you have access enter your extended release note in the below block leaving it blank means using the pr title as the release note if no release note is required just write none .--> release-noteadd kubectl api-resources command to discovery of resources
212561085,per sig-testing today.we generally agree wed like to restructure ee to group by sig wed like to auto-associate every pr with a sig so there is global accountability as opposed to individual accountability for any regressions or flakes wed like to group every job/suite by sig possibly redundant to the first bullet as part of this issue jay has to curate the existing ee tests with a rough first pass at sig responsibilities that should be fun
212476364,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):cronjobscheduledjobtoo many missed start times to list--- is this a bug report or feature request choose one):feature request kubernetes version use kubectl version ): environment cloud provider or hardware configuration aws os e.g from etc/os-release centos linux release core kernel e.g uname a ):linux el.elrepo.x smp sat feb est x x x gnu/linux install tools custom others what happened :cluster shutdown nightly to reduce costs on startup the cluster refuses to run jobs.the controller manager errors with:e controller.go cannot determine if default/
212251821,as per kubectl delete should ensure that the resource is deleted before returning by default we can also add a wait-for-deletion flag that users can set if they dont want to wait.work items x update apiserver to return the uid of the resource being deleted in response to a delete request update kubectl delete code to first send delete request to apiserver with the resource name server will return back resource uid as part of the response then keep sending delete requests to apiserver with uid precondition until server returns or or we timeout skip the wait if user sets wait-for-deletion=false revert wait for deletion code added in liggitt smarterclayton bgrant kubernetes/sig-cli-bugs
211954577,feature request:pods in statefulsets should be able resolve their peers via short hostnames for example if i create a rabbitmq statefulset i should be able to resolve rabbitmq from rabbitmq or any other pod in the statefulset.this could be achieved by adding service.ns.svc.cluster.local to the search domains in etc/resolv.conf in addition to the ns.svc.cluster.local and others already present).in the case of rabbitmq in particular this would make clustering much easier to setup rabbitmq clustering requires that node names match the hostname that other cluster members can use to connect to it and it defaults to hostname which returns the short hostname when running the official rabbitmq docker image under ks so the nodes are already getting names like rabbit@rabbitmq and if running rabbitmqctl join_cluster rabbit@rabbitmq could resolve the rabbitmq part it would just work.but this seems like a nice convenience and logical feature for statefulsets in general i.e not rabbitmq specific since there is inherent identity attached to these ordinal hostnames.i imagine the primary downside is the strict limit on search domains imposed by some resolver libraries if that means the extra search domain shouldnt be added by default for all statefulsets is there some other way to achieve it
211529899,kubernetes version platform awsdeploy tool kopsmy masters var/log/kube-controller-manager.log is receiving this type of log: i node_status_updater.go could not update node status failed to find node ip----.ec.internal in nodeinformer cache nil>i node_status_updater.go could not update node status failed to find node ip----.ec.internal in nodeinformer cache nil>i node_status_updater.go could not update node status failed to find node ip----.ec.internal in nodeinformer cache nil>i node_status_updater.go could not update node status failed to find node ip----.ec.internal in nodeinformer cache nil>i node_status_updater.go could not update node status failed to find node ip----.ec.internal in nodeinformer cache nil>i node_status_updater.go could not update node status failed to find node ip----.ec.internal in nodeinformer cache nil>i node_status_updater.go could not update node status lots of times per second this node ip----.ec.internal does not exist in the cluster anymore and is not shown if i kubectl get nodes shouldnt it be removed from status_updater
210969003,kubernetes version use kubectl version ):client version version.info{major minor gitversion:v..-beta gitcommit:bfdcabccaadbbabefb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:dcbbabefbcfbdf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration azure os e.g from etc/os-release ubuntu kernel e.g uname a linux ks-master-efac generic ubuntu smp fri jun utc x x x gnu/linux install tools azure acs what happened :i tried to setup glusterfs cluster with heketi and point glusterfs storage class to it with dns entry but it seems that storageclass is not able to resolve dns address of service. events firstseen lastseen count from subobjectpath type reason message s s persistentvolume-controller warning provisioningfailed failed to provision volume with storageclass glusterfs glusterfs create volume err error creating volume post dial tcp lookup heketi.heketi.svc.cluster.local no such host. what you expected to happen :glusterfs storage class is able to resolve dns address to heketi service how to reproduce it as minimally and precisely as possible setup glusterfs cluster inside kubernetes using heketi at heketi namespace create storage class as follows apiversion storage.ks.io/vbetakind storageclassmetadata name glusterfsprovisioner kubernetes.io/glusterfsparameters resturl try to create any stateful app with this storageclass anything else we need to know :dns is resolvable from busybox container: wget qo ive change fqdn name to ip address in storage configration it started working but id prefer to use dns name
210509459,a user reported seeing these in their kubectl describe nodes output d m kubelet ip----.eu-west-.compute.internal warning containergcfailed operation timeout context deadline exceeded h m kubelet ip----.eu-west-.compute.internal warning imagegcfailed operation timeout context deadline exceeded along with some of these errors in the kubelet: feb ip kubelet i fshandler.go du and find on following dirs took s var/lib/docker/overlay/edaeababfbcdfdaefafeb ip kubelet i fshandler.go du and find on following dirs took s var/lib/docker/overlay/bccacbbccefffeb ip kubelet i fshandler.go du and find on following dirs took s var/lib/docker/overlay/bfedeaacaccb
209432261,is this a request for help no what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there ingress wildcard ingress regexp is this a bug report or feature request? feature although if it exists and i dont see it then bug for docs---as far as i can tell ks ingress supports in theory subdomain wildcards for ingresses supposing the given controller supports them of course such as mydomain.com there appear to be confusing issues re but in theory it supports it i cannot find this in official documentation anywhere is there support for general wildcards?the specific use case is as follows i have a set of ks resources being deployed to multiple environments e.g prod uat qa each one has its own inbound root domain e.g prod.mydomain.com qa.mydomain.com uat.mydomain.com .when i create my ks resources i want to use identical ones between all the environments if i have exposed services e.g web api magic in each domain routed by subdomain i shouldnt require separate ingresses one for each environment per exposed service instead i want to do something like this: ymlapiversion extensions/vbetakind ingressmetadata name webspec rules host web http paths path backend servicename web-service serviceport repeat for services api and magic )this allows my to tell my devs pick your environment with kubectl config set-context prod or uat or qa ).deploy consistently with kubectl apply f configs/ did i miss that somewhere or is it not currently supported
208707572,what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):kubectl configmap from-file--- is this a bug report or feature request choose one):feature request i think)if this is a feature request please describe in detail the feature/behavior/change youd like to see.make it possible if it is not already and i havent missed it to load a configmap on the fly using a config yml?lets say i have a config file for some service e.g an nginx config nginx.conf i want my custom nginx.conf to be available to my containers at runtime without extending the base image.the ks way is a configmap which then is mounted as a volume to do that i need to load the contents of that file as a configmap i have two choices run kubectl create configmap some-name from-file=path/to/my/nginx.conf which requires a separate create command rather than simple and idempotent apply f copy the entire nginx.conf into nginx-confmap.yml under data which is not very practical for editing and makes testing/debugging the config very difficult not to mention dry...).ideally i should run kubectl apply f confdir where all my yml are and it does the right thing including loading an individual file as configmap at apply time_.i would imagine something like: ymlapiversion vkind configmapmetadata name some-config namespace defaultdatafromfile data/nginx.conf run kubectl apply f confdir the file nginx-confmap.yml has a reference to the relative config file kubectl sees the reference and loads it upthis allows me to stick with a standard set of files and operational flow kubectl apply f dir while maintaining my config files in their original format.btw terraform works like this as do other config management tools when i use compose locally i volume-mount the original file making it clean
207662188,environment cloud provider or hardware configuration digital ocean unimplemented?)digital ocean has just released their load balancer feature to general availability that block storage is also available i think digital ocean should now be a good candidate for a new cloud provider implementation it seems no work has begun towards this task this is obviously a major effort but id be willing to contribute hopefully some engineers from digital ocean would be willing to contribute as well as its their platform we would be adding support for if nothing else i think we should flag the appropriate teams and mark this as a will add eventually feature.also related i would argue for closing the old issues and consolidating the digital ocean cloud provider notes into one issue.cc wardviaene leshik justinsb
205379141,the ux for using kubectl with multiple namespaces is harder than it need be and i dont understand why.per config set-context kubectl config current-context namespace=myns can we have kubectl namespace myname or similar?other ideas kubectl use ns myns and kubectl use cluster myothercluster kubectl use myothercluster/myns i believe openshift also calls namespaces projects we could adopt the same terminology they have kubectl project myns .cc kubernetes/sig-cli-feature-requests
203828102,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see no what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there statefulset ordinal config--- is this a bug report or feature request choose one feature request kubernetes version use kubectl version with a statefulset the hostname is set to name- _ordinal it is a common pattern with the types of services that go into statefulsets to expect the unique id of the service to be passed e.g as an env var this causes all sorts of interesting contortions like extending the official zookeeper image to try and guess the ordinal index of the set from the hostname and then set another var with the index.it would be good if there were some templating format so that the property could be set directly in the config file e.g simplified): ymlapiversion apps/vbetakind statefulsetmetadata name zkspec servicename zk-headless replicas template metadata labels app zk spec containers name zk imagepullpolicy always image zookeeper resources env name zoo_my_id value ordinal name zoo_servers value server.=zk server.=zk server.=zk name zoo_tick_time value name zoo_init_limit value name zoo_sync_limit value
203676453,what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there downward api node labels--- is this a bug report or feature request choose one feature requesti think the downward api should support exposing the node labels on which the pod it is running while i understand that the pod should not be coupled too much with the node it is scheduled to but i think there are certain scenarios that require the pod to have more information about its host failure-zones like rack names on premise or availability zones in cloud instance-types for adapting thread settings per instance type other labels like local ssd storage available this helps applications that are cloud-native to understand where they are located while this could be achieved using api calls to the node object i think it should be supported by the downward api a node object would expose way more information that should not be available to any pod in openshifts default configuration there is access to the node objects prevented by default.@mattbates smarterclayton bprashanth
203064999,thanks for filing an issue before hitting the button please answer these questions.--> what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there multiple replicaset duplicate replicaset--- is this a bug report or feature request choose one bug report kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:efcfefbabedd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:efcfefbabedd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release ubuntu lts kernel e.g uname a linux ip generic ubuntu smp tue may utc x x x gnu/linux install tools custom terraform+ansible setup using hyperkube binary others this cluster has disabled auth and serviceaccounts what happened upgrading control plane to v from v created duplicate replicasets and hence duplicate pods for some deployments what you expected to happen existing deployments/replicasets/pods to be the same or being replaced not duplicates how to reproduce it as minimally and precisely as possible keep a deployment with volume mounts probably ebs running on v upgrade control plane components to v.. anything else do we need to know interestingly this seems to have happened only for deployments that use shared volume mounts either ebs or otherwise
202812972,in the current design there are basically two choices that a user can make either allow indefinite number of pods to be scheduled on one node or not allow to have more than pod at all in many cases it might be good to have some middle ground option have maximum of n pod scheduled in one node/topologykey my primary use case is clusterautoscaler in ca when a replicaset/deployment is created and its pods cannot schedule due to lack of capacity new nodes are added if the nodes used in the clusters are big enough it may happen that all replicas end up on a single node which is probably not the user wanted when creating pods with multiple replicas if the user specifies hard pod anti-affinity it will get as many nodes as replicas which may lead to poor node utilization.the other use case is to ensure that not all pods go to a single rack as the rack may die at some time causing a big outage on the other hand we dont want to have only one pod in a rack as there might not be enough racks.cc davidopp wojtek-t gmarek
202252547,what keywords did you search in kubernetes issues before filing this one service.beta.kubernetes.io/aws-load-balancer-backend-protocol elb is this a bug report or feature request choose one bug report and/or feature request depends on how this is expected to work. kubernetes version use kubectl version v..-rancher environment cloud provider or hardware configuration amazon ec os e.g from etc/os-release rancheros v kernel e.g uname a rancher install tools rancher how to reproduce it :first create a service manifest containing: apiversion vkind servicemetadata namespace gitlab name gitlab labels name gitlab annotations service.beta.kubernetes.io/aws-load-balancer-ssl-cert arn:aws service.beta.kubernetes.io/aws-load-balancer-ssl-ports httpsspec type loadbalancer ports name ssh port targetport ssh name http port targetport http name https port targetport http selector name gitlab and observe that it creates an elb that shows this under listeners:! image that the only port that is given a load balancer protocol of ssl is port the other ports are apparently left with a reasonable default protocol of tcp.however when i add backend-protocol http and change the annotations to annotations service.beta.kubernetes.io/aws-load-balancer-ssl-cert arn:aws service.beta.kubernetes.io/aws-load-balancer-ssl-ports https service.beta.kubernetes.io/aws-load-balancer-backend-protocol http what happened it changes the protocol of all ports even those that have nothing to do with http and should be left as tcp like ssh:! image we cant use http for ssh and i dont think we can split this into separate services/loadbalancers because we want to use the same domain name for all the ports... what you expected to happen :i expect it to only change load balancer protocol to http/https for ports where that makes sense:! image was able to manually change it to this but any time i update the service manifest my manual change will get lost so that isnt a good solution.) how should it know which ports to change the protocol for? ideally it would let you choose the protocol for each port ideally by naming it directly in the ports section spec ports name ssh port targetport ssh backendprotocol tcp name http port targetport http backendprotocol http name https port targetport http backendprotocol http but if were limited to only using annotations then maybe consider changing aws-load-balancer-backend-protocol to accept a json object with the mappings like ssh tcp http http https http if given a string it could use the same protocol for all ports like it does now.)this would be similar to how aws-load-balancer-ssl-ports lets you only make some of your load balancer ports use ssl support mixed http/https ports in elbs)). other related issues support mixed protocols in service.type=loadbalancer ssl support for elb listeners through annotations annotations for elb advanced features
202182550,when we execute kubectl rollout status were receiving this error: kubectl rollout status deployment/app namespace=appwaiting for rollout to finish out of new replicas have been updated...waiting for rollout to finish out of new replicas have been updated...error timed out waiting for the condition i also tried to put the option w but it didt change anything.since we upgraded to kubernetes were receiving this in it works with no problems ive tried to update kubectl and use previous versions too but seems to be a problem with the cluster not with kubectl command this is a problem for us because jenkins is using this command to monitor the deployment status so our builds are crashing in the end were running the cluster on aws with kops provisioning
201997260,kubernetes version use kubectl version v v likely many versions environment cloud provider or hardware configuration azure azure container service os e.g from etc/os-release ubuntu xenial kernel e.g uname a latest lts kernel install tools cloud-init hyperkube others : configuration details kubelet runs in a container master services run as static manifests kube-addon-manager runs as a static manifest kube-proxy runs in iptables mode via a daemonset what happened :after upgrading to docker on the nodes outbound container traffic stops working what you expected to happen :outbound container traffic to work aka i can hit the internet and service ips from inside the container) how to reproduce it as minimally and precisely as possible):deploy an acs kubernets cluster if the workaround has rolled out then force upgrade docker to youll have to remove a pin were setting in etc/apt/preferences.d).unclear if this repros on other configurations right now. anything else do we need to know :no i just dont know where/how to best troubleshoot this
201236122,create a systemd syslogidentifier inside the service create a systemd description inside the unit what this pr does why we need it overviewlogged against the host its difficult to identify whos who.this pr add useful information to quickly get straight to the point with the description field: systemctl list-units ks*unit load active sub descriptionks_babdf-e--df-afdafc.service loaded active running kube-controller-manager-...ks_becda-dc-b-a-ecfa.service loaded active running nginx-daemonset-gxmsks_dec--aa-ade-efcb.service loaded active running kube-apiserver-...ks_fdebab-f-fd-bd-fc.service loaded active running kube-scheduler overview and journalalways on the host to easily retrieve the pods logs this pr add a syslogidentifier named as the podbasename a daemonset prometheus-node-exporter is running on the kubernetes clustersystemctl list-units ks grep prometheus-node-exporterks_caba-d-fce-afa-dfc.service loaded active running prometheus-node-exporter-cpp get the logs from the prometheus-node-exporter daemonset journalctl t prometheus-node-exporter wc l sadly the journalctl flag t identifier doesnt allow a pattern to catch the logs.also this field improve any queries made by any tools who exports the journal e.g es kibana cursor s=fddbafbbffeb;i=bc;b=debeaccfecebee;m=feca;t=ea;x=bddff realtime_timestamp monotonic_timestamp boot_id debeaccfecebee priority uid gid systemd_slice system.slice selinux_context system_u:system_r:kernel_t:s machine_id bbbdaefda hostname kubelet-host transport stdout syslog_facility comm ld-linux-x cap_effective fffffffff syslog_identifier prometheus-node-exporter pid exe var/lib/rkt/pods/run/caba-d-fce-afa-dfc/stage/rootfs/usr/lib/ld-..so cmdline stage/rootfs/usr/lib/ld-linux-x-.so stage/rootfs/usr/bin/systemd-nspawn systemd_cgroup system.slice/ks_caba-d-fce-afa-dfc.service systemd_unit ks_caba-d-fce-afa-dfc.service message prometheus-node-exporter time=\--t::z level=info msg time source=\node_exporter.go
201093179,rkt runtime fail to start if the directory for the associated volume is missingcreate a systemd execstartpre inside the service to do so. what this pr does why we need it :with rkt as container runtime we cannot use volumes if the directory isnt here.i decide to restrict the field to only creates directories instead of pass custom commands. apiversion extensions/vbetakind daemonsetmetadata name nginx-daemonset namespace defaultspec template metadata annotations rkt.kubernetes.io/host-create-directories tmp/nginx tmp/nginx labels run nginx-host spec hostnetwork true containers name nginx-daemonset-pod image quay.io/julienbalestra/nginx:latest command usr/sbin/nginx g daemon off; the feature will create a specific field execstartpre like: execstartpre=/bin/mkdir pv tmp/nginx tmp/nginx will lead to obviously this: ls d tmp/nginx*/tmp/nginx tmp/nginx if the annotation isnt fill the execstartpre is not created. special notes for your reviewer :we are trying to move from fleet to kubernetes with the rkt-runtime in production at blablacar we have severals git format-patch for this purpose.the labels should be area/rkt area/kubelet none
200310072,feature request id like to be able to set custom hostnames in kubernetes for a service besides the default my-service my-service.my-ns and my-service.my-ns.svc.cluster.local so i can use the same dns name externally and internally whereas the external dns record resolves to the public ip and the internal dns record resolves to the service cluster ip.for our services of type loadbalancer we create a letsencrypt certificate that runs in an haproxy sidecar container for the application in that service the hostname in the certificate matches the external hostname as set by us in a dns server external to kubernetes.directly connecting to the service from another kubernetes hosted application leads to a mismatch in hostname for the certificate as any public certificate authority is not allowed to use internal host names in the certificates they generate this makes it hard to encrypt our internal kubernetes traffic without using our own certificate signing service.my proposal would be to allow something like the following apiversion vkind servicemetadata name my-service namespace my-nsspec dnsaliases hostname my-service.mydomain.com hostname my-service.mydomain.org selector app my-app where kubernetes then automatically manages that my-service.mydomain.com and my-service.mydomain.org resolve to the clusterip of my-service inside kubernetes independent from what they resolve to outside of the cluster this wouldnt necessarily be tied to the type loadbalancer because we can use it to outfit our internal-only services with letsencrypt certificates as well.because of the requirement for the host to match the host in the certificate ticket wouldnt work for this specific scenario
199967985,there are a number of best practices for creating config files but they are not enforced by the api it would be helpful to have a kubectl lint command to check for basic best practices and give warningsthings to check for deployments specify max old replicas deployments specify both selectors and pt.labels not just the labels service ports always have names configs comment out remove values for objects managed by other actors e.g replicas if managed by hpa do we want to do this
199235047,kubernetes version it has been reported that this bug is in the branch and above i have validated on and we have not validated the x branch and an exponential back off patch just was just merged but we do not beleive that is addressing the root problem. environment cloud provider or hardware configuration aws os e.g from etc/os-release debian kernel e.g uname a custom maintained kernel by aws team linux ks smp fri oct utc x gnu/linux install tools kops others : what happened :as you add more pv storage to a cluster the cluster starts to spam the api heavily it has been reported that only pv attached will start causing ec api timeouts this is a showstopper issue which is making pv attached storage unusable in aws as the aws account nears its limit on api calls this problem cascades to the point that retries flood the api one of our accounts is at around k calls per hour.the controller starts to make api calls at such high rate that it starts to retry and then you just spam the api the controller is making far too many api calls to validate that a node exists and that a volume is attached to a node the specific call that is timing out the most is describeinstances .the cluster is at steady state we do not have volume churn i.e we are not adding and removing volumes. what you expected to happen :able to have pv attached to a cluster and not kill ec api. how to reproduce it as minimally and precisely as possible use kops to create a cluster dont use kube-up.sh it is super flakey and eol update the master controller to v and restart the controller enable cloud trails install deployments with attached pvc.you will get: i log_handler.go aws request ec describeinstancesi log_handler.go aws request ec describeinstancesi log_handler.go aws request ec describeinstancesi reconciler.go volume kubernetes.io/aws-ebs/aws://us-west-c/vol-cfc/node ip----.us-west-.compute.internal is attached--touching.i reconciler.go volume kubernetes.io/aws-ebs/aws://us-west-a/vol-acdabae/node ip----.us-west-.compute.internal is attached--touching.i reconciler.go volume kubernetes.io/aws-ebs/aws://us-west-b/vol-fdecff/node ip----.us-west-.compute.internal is attached--touching.i reconciler.go volume kubernetes.io/aws-ebs/aws://us-west-b/vol-dab/node ip----.us-west-.compute.internal is attached--touching. showing up in the logs of the controller i have only tested in ha but will validate in single master setup shortly. anything else do we need to know :this is bad enough issue that we are crashing controllers.aws is rate limited by api call and by failed api calls some api calls are limited to a region and some are account wide.the code that is getting called is from here that is using in aws will exceed their rate limits with as little as pv attached to a cluster we have one account that is at about k api calls per hour because of timeouts this makes pv unuable on aws
197297742,"what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there dynamic volume provisioning--- is this a bug report or feature request choose one bug report kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:dcbbabefbcfbdf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:dcbbabefbcfbdf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration aws os e.g from etc/os-release coreos kernel e.g uname a linux ip----.ec.internal coreos-r smp wed dec utc x intel(r xeon(r cpu e v ghz genuineintel gnu/linux install tools kaws others : what happened :created a stateful set with a persistent volume claim dynamic volume provisioning created an ebs volume in the us-east-a availability zone despite all the masters and nodes in the cluster being in us-east-e i tried it twice with the same results both times.the pvc kubectl describe pvc n errbitname mongodb-mongodb-namespace errbitstorageclass standardstatus boundvolume pvc-fceb-caa-e--edlabels app=errbit component=mongodbcapacity giaccess modes rwono events. the pv created by dynamic provisioning kubectl describe pvname pvc-fceb-caa-e--edlabels failure-domain.beta.kubernetes.io/region=us-east failure-domain.beta.kubernetes.io/zone=us-east-astorageclass standardstatus boundclaim errbit/mongodb-mongodb-reclaim policy deleteaccess modes rwocapacity gimessage:source type awselasticblockstore a persistent disk resource in aws volumeid aws://us-east-a/vol-dcdcfaf fstype ext partition readonly falseno events. the stateful set kubectl describe statefulset mongodb n errbitname mongodbnamespace errbitimage(s mongo:..selector app=errbit,component=mongodblabels app=errbit,component=mongodbreplicas current desiredannotations kubectl.kubernetes.io/last-applied-configuration={kind:statefulset,apiversion:apps/vbeta,metadata:{name:mongodb,namespace:errbit,creationtimestamp:null,labels:{app:errbit,component:mongodb}},spec:{replicas:,template:{metadata:{creationtimestamp:null,labels:{app:errbit,component:mongodb}},spec:{containers: {name:mongodb,image:mongo:..,args: --auth ,ports: {name:mongodb,containerport:} ,resources:{},volumemounts: {name:mongodb,mountpath:/data/db} } }},volumeclaimtemplates: {metadata:{name:mongodb,creationtimestamp:null},spec:{accessmodes: readwriteonce ,resources:{requests:{storage:gi}}},status:{}} ,servicename:mongodb},status:{replicas:}}creationtimestamp thu dec pods status running waiting succeeded failedno volumes.events firstseen lastseen count from subobjectpath type reason message m m statefulset normal successfulcreate pet mongodb m m statefulset normal successfulcreate pvc mongodb-mongodb- the stateful sets pod pending due to the volume being in the wrong zone kubectl describe pod mongodb n errbitname mongodb-namespace errbitnode labels app=errbit component=mongodbstatus pendingip:controllers statefulset/mongodbcontainers mongodb image mongo port tcp args auth volume mounts data/db from mongodb rw var/run/secrets/kubernetes.io/serviceaccount from default-token-mdqdk ro environment variables none>conditions type status podscheduled falsevolumes mongodb type persistentvolumeclaim a reference to a persistentvolumeclaim in the same namespace claimname mongodb-mongodb readonly false default-token-mdqdk type secret a volume populated by a secret secretname default-token-mdqdkqos class bestefforttolerations none>events firstseen lastseen count from subobjectpath type reason message m m default-scheduler warning failedscheduling schedulerpredicates failed due to persistentvolume pvc-afe-ca-e--ed not found which is unexpected schedulerpredicates failed due to persistentvolume pvc-afe-ca-e--ed not found which is unexpected m m default-scheduler warning failedscheduling schedulerpredicates failed due to persistentvolumeclaim is not bound mongodb-mongodb which is unexpected schedulerpredicates failed due to persistentvolumeclaim is not bound mongodb-mongodb which is unexpected m s default-scheduler warning failedscheduling pod mongodb failed to fit in any nodefit failure summary on nodes novolumezoneconflict the nodes all in us-east-e kubectl describe nodes grep zone failure-domain.beta.kubernetes.io/zone=us-east-e failure-domain.beta.kubernetes.io/zone=us-east-e failure-domain.beta.kubernetes.io/zone=us-east-e failure-domain.beta.kubernetes.io/zone=us-east-e what you expected to happen :dynamic volume provisioning should have created the required volume in the us-east-e availability zone. how to reproduce it as minimally and precisely as possible):add the following storage class to the cluster yaml---kind storageclassapiversion storage.ks.io/vbetametadata name standard annotations storageclass.beta.kubernetes.io/is-default-class trueprovisioner kubernetes.io/aws-ebsparameters type gp encrypted true create the following stateful set and service yaml---kind namespaceapiversion vmetadata name errbit---kind serviceapiversion vmetadata name mongodb namespace errbit labels app errbit component mongodbspec ports name mongodb port clusterip none selector app errbit component mongodb---kind statefulsetapiversion apps/vbetametadata name mongodb namespace errbit labels app errbit component mongodbspec servicename mongodb replicas template metadata labels app errbit component mongodb spec containers name mongodb image mongo args auth ports containerport name mongodb volumemounts name mongodb mountpath data/db volumeclaimtemplates metadata name mongodb spec accessmodes readwriteonce resources requests storage gi"
197279166,kubernetes version use kubectl version environment cloud provider or hardware configuration minikube aws os e.g from etc/os-release bootdocker alpine debian jessie kernel e.g uname a bootdocker ks install tools minikube kops others : what happened :im trying to use prestop for the purpose of sending custom signals to containers that do not handle sigterm gracefully the use case is nearly identical to whats described under lifecycle hooks and termination notice the challenge im running into which i believe applies to the example is that sigterm is sent immediately after the prestop hook finishes execution this means that sending a signal to the container e.g via nginx s quit or pkill quit f unicorn_rails will result in sigterm being nearly immediately sent to the container afterward without the process having time to react the result is an ungraceful shutdown and forcibly terminated connections.to work around this i attempted to add a sleep in the prestop command: yamlapiversion extensions/vbetakind deploymentmetadata labels app nginx name nginxspec replicas selector matchlabels app nginx template metadata labels app nginx spec containers image nginx imagepullpolicy always name nginx lifecycle prestop exec command bash c usr/sbin/nginx s quit sleep restartpolicy always this works great except that it now throws a failedprestophook warning event: m m nginx--uel pod normal scheduled default-scheduler successfully assigned nginx--uel to minikubem m nginx--uel pod spec.containers{nginx normal pulling kubelet minikube pulling image nginx:.m m nginx--uel pod spec.containers{nginx normal pulled kubelet minikube successfully pulled image nginx:.m m nginx--uel pod spec.containers{nginx normal created kubelet minikube created container with docker id adb security: seccomp=unconfined m m nginx--uel pod spec.containers{nginx normal started kubelet minikube started container with docker id adbm m nginx--uel pod spec.containers{nginx warning failedprestophook kubelet minikube}m m nginx--uel pod spec.containers{nginx normal killing kubelet minikube killing container with docker id adb need to kill pod. when deleting the pod the kubelet logs show msgs like: e docker_manager.go prestop hook for container aaddecbcfeeefafcfcababfe nginx default/nginx--nlu failed error executing in docker container i operation_executor.go mountvolume.setup succeeded for volume kubernetes.io/secret/-c-e--af-default-token-queh spec.name default-token-queh pod c-e--af uid c-e--af).e docker_manager.go logging security options key:seccomp value:unconfined msg:}i docker_manager.go determined pod ip after infra change nginx--uel_default(-c-e--af w docker_manager.go no ref for pod aaddecbcfeeefafcfcababfe nginx default/nginx--nlue docker_manager.go prestop hook for container aaddecbcfeeefafcfcababfe nginx default/nginx--nlu failed container not running aaddecbcfeeefafcfcababfe)w docker_manager.go no ref for pod nginx--nluw docker_manager.go no ref for pod ddeafdccffcbbbfcadeccfa default/nginx--nlui reconciler.go unmountvolume operation started for volume kubernetes.io/secret/efabbd-c-e--af-default-token-queh spec.name default-token-queh from pod efabbd-c-e--af uid efabbd-c-e--af).i operation_executor.go unmountvolume.teardown succeeded for volume kubernetes.io/secret/efabbd-c-e--af-default-token-queh outervolumespecname default-token-queh pod efabbd-c-e--af uid efabbd-c-e--af).innervolumespecname default-token-queh pluginname kubernetes.io/secret volumegidvalue e docker_manager.go prestop hook for container aaddecbcfeeefafcfcababfe nginx default/nginx--nlu failed container not running aaddecbcfeeefafcfcababfe) notably it says that the hook failed because the container is not running the exit code of indicates a sigkill was sent somewhere along the line perhaps as a result of pid being killed from within the prestop .ultimately this works containers appear to be gracefully shut down when a sleep is provided however the requirement for the sleep is undocumented and surprising there are many alternative ways to work around the problem of handling sigterm write a bash wrapper around the startup command or set stopsignal in the dockerfile but prestart seems specifically designed to handle this case and id love to see it work more gracefully. what you expected to happen the prestop hook should be able to trigger a container stop without errors occurring the documented behavior of sigterm being sent immediately after prestop completes makes prestop challenging to use for the example use case of handling signals perhaps a configurable delay could be introduced so sleep is not required the documentation which describes this use case should be updated to correctly handle graceful shutdown behavior which requires time for the pid to quiesce. how to reproduce it as minimally and precisely as possible apply a deployment as in the yaml kubectl delete pod the created pod observe that failedprestophook warnings occur in kubectl get events anything else do we need to know
195678076,this would allow for easier sharing of kubecfg files across teams
195619651,kubectl logs would print a list of containers in and earlier in it now prints an error error from server badrequest the server rejected our request for an unknown reason get pods kube-dns-v--bfz) )thanks stonith for finding copied from kubectl versionclient version version.info{major minor gitversion:v gitcommit:dcbbabefbcfbdf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:dcbbabefbcfbdf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd kubectl logs n kube-system kube-dns-v--bfz error from server badrequest the server rejected our request for an unknown reason get pods kube-dns-v--bfz) ( kubectl logs n kube-system kube-dns-v--bfz kubedns works)works with kubectl on the same cluster kubectl versionclient version version.info{major minor gitversion:v gitcommit:bccccdbfffecbbacaead gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:dcbbabefbcfbdf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd kubectl logs n kube-system kube-dns-v--bfz error from server a container name must be specified for pod kube-dns-v--bfz choose one of kubedns dnsmasq healthz
193777451,use case as kubernetes admin i want to define a storageclass that provisions pvs with persistentvolumereclaimpolicy retain so users dont loose their data when they accidentally delete their pvcs. api changes either magic annotation kubernetes.io/reclaim-policy retain or a new storageclass field sc.persistentvolumereclaimpolicy retain requested actions update the api update internal provisioning logic to respect this new annotation/field update docs announce that external provisioners need to respect this new annotation/field.all these items are fairly easy if we agree on the design.@kubernetes/sig-storage
193531536,feature request_custom controllers can be implemented on top of tprs e.g prometheus-operator and etcd-operator in our custom resources we still want to provide a status field which is updated by the controller to inform about the current status of whatever is being managed by the tprs controller this can currently not be done safely to my knowledge as users may accidentally modify the status section of a tpr to an inconsistent state as it is part of the main resource json.ideally there would be an option to have sub-resources in tprs just like deployments statefulsets etc have which is modified independently through its own api endpoint
193134811,fixes adds an automountserviceaccounttoken bool field to serviceaccount and podspec if set in both the service account and pod the pod wins if unset in both the service account and pod we automount for backwards compatibility release-notean automountserviceaccounttoken bool field was added to serviceaccount and podspec objects if set to false on a pod spec no service account token is automounted in the pod if set to false on a service account no service account token is automounted for that service account unless explicitly overridden in the pod spec
192648932,feature request when creating a pvc on container engine with slow and fast storage classes set up with following manifest automatic creation works perfectly. kind persistentvolumeclaimapiversion vmetadata name test-pvc-provisioning annotations volume.beta.kubernetes.io/storage-class slowspec accessmodes readwriteonce resources requests storage gi but when trying to increase the storage capacity and re-applying the manifest it returns the following error: the persistentvolumeclaim test-pvc-provisioning is invalid spec forbidden field is immutable after creation of course from the perspective of most persistent volumes that dont support dynamic resizing this makes sense but on gce where persistent disks can grow on the fly it would be very useful to be able to grow the size of the volume claim. kubernetes version client version version.info{major minor gitversion:v gitcommit:bccccdbfffecbbacaead gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:eadecbbcdfaf gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment google container engine gci image
190726064,statement of issuei have an application that listens on multiple ports say and i would like to have a liveness check and readiness check that tests that the application is listening on both ports and terminate it if it is not.i know of a number of ways to do this i can change the application code so that any failure leads to termination unfortunately some of this application code is third party so its hard for me to change i can just execute a script that checks all the ports that requires that i install the script or tool in question on the container for now it is probably what i will have to do alternatively i could create a probe container in the same pod that just runs a script to verify that the real container is listening and terminates with an error if it does not however that doesnt really work because kubernetes restarts my probe container rather than the entire pod and i dont think i can change this behaviour possible resolutionsi can see three possible resolutions given in my personal order of preference allow configuration of multiple liveness checks to be run in parallel extend the pod spec to have a recreate entire pod on error option not just options about how to restart containers allowing use of a probe container allow multiple ports in a tcp connection probe workaroundas a workaround i can create a tool or script and install it on the container i want to check that means messing about with third party images but is plausible and so is what i am doing for now environmentim running kubernetes in gke but i believe this applies to all versions and environments.i am using deployments rather than replication controllers some of the above might be implemented as extensions to the deployment or replica set configuration rather than the pod spec
188601125,feature request ability to run a scheduled job as a daemonset so it can run on each node e.g image garbage collection with something like docker-gc version use kubectl version ):client version version.info{major minor gitversion:v gitcommit:bccccdbfffecbbacaead gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:bccccdbfffecbbacaead gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} what you expected to happen :the ability to target more than one node in a scheduled job if i specify a nodeselector for workers the job will only run on worker node i would like the job to run on all worker nodes. how to reproduce it as minimally and precisely as possible): apiversion batch/valphakind scheduledjobmetadata name hellospec schedule jobtemplate spec template spec containers name hello image busybox args bin/sh c date echo hello from the kubernetes cluster restartpolicy onfailure nodeselector nodetype worker i think there is a notion of a node service being worked on but i a couldnt find the link/doc describing how it works b dont think it applies to scheduled jobs
188258676,currently using gke you can only use ssds as attached disks which can then be used for storage when using hostpath volumes.it would be nice if i could create a node-pool that uses ssds as the boot disk this way all running pods that dont use any external storage or hostpath ssds can benefit from improved disk io._i couldnt find any issue related to this but that might be because my search term was too generic and i had to weed through hundreds of other issues__also i realise this is more of a google cloud platform feature than a kubernetes feature so maybe im in the wrong place
187544004,when a user does kubectl get events what they probably want is a timeline of the cluster over the last n minutes in other words something analogous to dumping the logs from each component into a file and sorting it by timestamp what kubectl get events provides today is far from this due to compression it may be that we wont be able to fulfill this use case until we have infrastore but we really need something like this for debugging.cc piosz fgrzadkowski
187303495,here is one idea:when you are running a cluster in your bare metal infrastructure you cannot expose a service or anything using the loadbalancer for obvious reasons.when you are running a in a cloud provider it uses the native load balancer aka elastic load balancer in aws provisioning and configuring it.my point is:why not have an extra flag in the expose command that allows you to specify the image to use as a load balancer in your bare metal cluster e.g: kubectl expose rc my-service type loadbalancer image my-custom-ha-proxy and that will deploy a container with my-custom-ha-proxy and use it on a similar way as it does for the elb in aws.this came up in a conversation in the slack channel kubernetes-users and the originator question was how do i know the ip of the caller within the cluster and this solution came up as you would be able to configure the x-forwarded-for header and send it to the destination service.if this issue sounds like there is a better way of doing it please reply on a comment and close it
187040895,what keywords did you search in kubernetes issues before filing this one? port forward bind addressport forward localhostport forwarding listen is this a bug report or feature request choose one feature request kubernetes version use kubectl version): client version version.info{major minor gitversion:v..+cfb gitcommit:cfbacbbcbccadfbb gittreestate:not a git tree builddate:--t::z goversion:go compiler:gc platform:darwin/amd}server version version.info{major minor gitversion:v gitcommit:acafafcefdfcdab gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} describe in detail the feature/behavior/change youd like to see. currently kubectl port-forward only allows binding to localhost kubectl port-forward mypod forwarding from forwarding from it would be good to be able to bind to another local interface and all something like kubectl port-forward bind-address mypod forwarding from forwarding from kubectl port-forward bind-address mypod forwarding from this would allow more advanced interactions for debugging/interaction between local development environments and containers running on ks. kubectl proxy supports bind-address which seems like a good a name as any to be consistent.see also
186439956,this is a feature request to support automatic resizing of volumes for example if a gce pd is created as gb and then resized to gb the pv object should be able to updated by user to reflect the new size and kubernetes should automatically handle the resize doing whatever is necessary to make the extra disk space available for use to pods this is non-trivial since it requires repartitioning the disk see
184547993,kubeadm is new and nothing else in kubernetes depends on it it should have gone into a separate repository we need to figure out how to build releases from multiple repos anyway understand we wanted to move quickly and this was done for expedience but productivity in the kubernetes repo is a zero sum game.cc jbeda mikedanese roberthbailey lukemarsden kubernetes/sig-cluster-lifecycle
184185549,feature request:what is steps to bootstrap a ks cluster with two masters please document that.if it is yet possible this should be supported
183757068,comes from user)_the most important thing for cloud support is insight logs verbosity the description of the v argument reads: --v log level for v logs what are the different log levels what level is most useful to see all api calls what is v what would be a useful standard reply to a customer please re-run this command with v and send me the outputwith gcloud i normally ask for the output of log-http verbosity=debug .@kubernetes/kubectl ymqytw pwittrock
183301193,implements kubectl cp examples sh copy from pod to local machine kubectl cp namespace/ pod:/some/file/or/dir some/local/file/or/dir copy from local machine to pod kubectl cp some/local/file/or/dir namespace/ pod:/some/remote/file/or/dir @deadsk smarterclayton kubernetes/sig-cli reviewable:start this change is img src height align=absmiddle alt=reviewable reviewable:end
181800154,apache spark is a fast in-memory data processing engine with elegant and expressive development apis to allow data workers to efficiently execute streaming machine learning or sql workloads that require fast iterative access to datasets concepts spark application a spark application is what a user submits to execute his program on the spark framework spark driver runs the main program associated with a spark application spark context a spark context is created with a particular configuration by a spark application and is the entry-point to the spark execution environment spark executor one or more processes which listen to the spark driver and perform individual spark tasks as instructed by the spark driver spark standalone mode spark standalone mode is lightweight cluster manager within the main spark project itself which lets one turn up a spark cluster i.e individual master and worker processes spark client mode in client mode the driver is launched on the client machine outside the cluster spark cluster mode in cluster mode the driver is launched onto the cluster for any supported type of cluster which is specified at the time of submitting the spark application into the cluster spark dynamic allocation it is a configuration flag which can be turned on dynamic allocation scales the number of executors based on the workload presented dynamic allocation of executors requires an external shuffle service when used in cluster mode problemkubernetes currently supports standalone mode only there are various efforts within the project dealing with spark-standalone such as spark examples limitations of standalone mode! standalone spark workers cannot be elastic depending on job demands easily as the spark components cannot speak directly to kubernetes since the spark-master and spark-worker arent designed to work with a resource manager it doesnt provide easy integration to kubernetes features namespace rolling out etc if several users share a standalone cluster it is not strongly multi-tenant and isolation requires that each user has his/her own standalone cluster each worker wants to use an entire node this is not resource efficient sharing resources between different clusters as they scale dynamically during the execution of a multi-stage spark application requires an external manager/monitor native support! native is possible for spark to support kubernetes natively which would obviate the need for standalone deployments of masters and workers spark knows best about its resource requirements and if it directly accessed the kubernetes api to create workers and drivers that would be more efficient we want generic resource management tools in or alongside kubernetes that apply to spark as well as other application types we dont want application-specific resource management tools standalone cluster scaler having one resource manager spark standalone nested inside another resource manager kubernetes causes many subtle issues and bugs when run in production approach spark provides support for writing integrations with external cluster managers writing a cluster manager plugin for kubernetes would solve all the above problems proposed flow with native spark integration a kubernetes cluster already exists and the user has logged in to it his/her kube/config that has credentials and a current-context namespace selected already the namespace has sufficient resource quota to create several pods spark-submit is used to submit a new spark application to the ks cluster the target being a ks cluster may be indicated by specifying the master=ks://default and deploy-mode=cluster arguments to spark-submit spark-submit creates a driver pod based on a standard spark image which the user can override if needed the driver pod exposes spark ui typically on port which is exposed outside the cluster or can be accessed via the apiserver proxy user can set some option to control this this driver pod now creates creates executor pods based on options provided to it the driver pod authenticates as the namespaces default service account when talking to the api-server or the user can select a non-default service account the user has to have ensured that this service account has permission to create pods in this namespace the driver pod should create and post a secret that the executor pods can read and use as a token to authenticate themselves to their driver the executor pods start up and report back to the driver at the necessary rpc endpoint to indicate that they are ready to process the submitted spark application the driver pod starts to issue tasks to the registered executor pods when processing completes the logs of the driver pod contain the output the driver pod cleans up by killing executors and deleting the service endpoint and finally itself proof of concept kubernetes is a sub-project/cluster plugin project in apache spark compiling spark with kubernetes support is possible driver and executor pods are created on a kubernetes cluster it is possible to execute sample programs e.g sparkpi on the cluster by invoking spark-submit.fork of the spark project future work support for dynamic executor scaling this requires an external shuffle service and this part of the design is still under discussion support for various commandline options support for job-ids with ease of submitting killing and checking status of jobs through spark-submit working spark-shell by attaching stdin and stdout from the driver pod.cc kubernetes/sig-apps
181739121,a few weeks ago on slack i brought up the idea of writing a generic webhook admission controller much like authentication authorization and even the imagepolicywebhook admission controller the idea would be that kubernetes could call out to some external resource that itself implements the necessary bits based on their need as an admission controller the implementation details are not fully fleshed out but the plan would be to start out small with support for some very specific kubernetes types and later attempt to support all kubernetes types either explicitly or through some generic fashion.the reasons something like this might be useful are two fold getting custom admission controllers into kubernetes right now requires building a custom version of kubernetes this topic has come up a lot and is not the sole purpose of this feature using a webhook for something like this would allow the cluster owner to implement the admission controller however they deem fit in whatever language they want even if the first reason was taken care of this could still be reason enough to implement such a thing)_webhooks in kubernetes are already used in authentication authorization and even in the imagepolicywebhook admission controller so this is not something thats not been done before but i do realize that implementing this admission controller right will be quite the undertaking.the purpose of this feature request was not to ask the kubernetes team to work on this ive already started working on this locally but to give an official place for interested parties to discuss this and link related issues to.while discussing this on slack there were a number of concerns and related issues that i feel should be mentioned were also a few interested parties that i thought made sense to cc deadsk derekwaynecarr erictune justinsb ncdc thockin tsandalli will add more details and discussion as necessary please feel free to contribute to this discussion
181448365,this is a feature request currently it is not possible to set a secretkeyref/configmapkeyref as optional in a pods definition its also not possible to specify a default this means that even if the program running inside the containers in the pod has sensible defaults which need only be overwritten in some environments all secret k/v pairs must be updated in all environments before pods with new secretkeyrefs can be started.the use case here might be a dev environment maintained by a less sophisticated user where the overhead of managing secrets can be additional work that wastes time and resources in the production case a more sophisticated user has no trouble adding the secrets before editing the pod definitions in many cases the default secret values can be left in the program itself and thus the secret is an un-necessary second layer of admin work that is needed.ive looked around for other tickets/threads talking about this issue havent found any open to why this would be a bad idea or too complex to implement but in practice there are setups where i think it adds value
181244248,and other platforms that may not have deb/rpm installation support
181165800,is this a request for help? i think it is an issue either with software or documentation but i am not quite sure i have started with a question on stackoverflow this a bug report or feature request choose one):i think it is a bug or request to improve documentation kubernetes version use kubectl version ):.. environment cloud provider or hardware configuration vagrant os e.g from etc/os-release ubuntu kernel e.g uname a install tools kubeadm init/join others : what happened :i have got vms nodes both see each other either by hostname through etc/hosts or by ip address one has been provisioned with kubeadm as a master another as a worker node following the instructions i have added weave-net the list of pods looks like the following: vagrant@vm-master kubectl get pods all-namespacesnamespace name ready status restarts agekube-system etcd-vm-master running mkube-system kube-apiserver-vm-master running mkube-system kube-controller-manager-vm-master running mkube-system kube-discovery--xjy running mkube-system kube-dns--pul running mkube-system kube-proxy-amd-ail running mkube-system kube-proxy-amd-oxxnc running mkube-system kube-scheduler-vm-master running mkube-system kubernetes-dashboard--swts running mkube-system weave-net-euqt running mkube-system weave-net-baao crashloopbackoff m crashloopbackoff appears for each worker node connected i have spent several ours playing with network interfaces but it seems the network is fine i have found similar question on stackoverflow where the answer advised to look into the logs and no follow up so here are the logs: vagrant@vm-master kubectl logs weave-net-baao c weave namespace=kube-system i error contacting apiserver get dial tcp getsockopt connection refused trying with blank env vars i error contacting apiserver get dial tcp getsockopt connection refusedfailed to get peers what you expected to happen :i would expect the weave-net to be in running state how to reproduce it as minimally and precisely as possible):i have not done anything special just followed the documentation on getting started if it is essencial i can share vagrant project which i used to provision everything please let me know if you need one
179968541,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there.):created api client waiting for the control plane to become ready related to or a similar discussion happened bug report choose one): kubernetes version use kubectl version ):kubectl versionclient version version.info{major minor gitversion:v gitcommit:acafafcefdfcdab gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}kubeadm version version.info{major minor gitversion:v..-alpha..+cffc-dirty gitcommit:cffccfdcbddc gittreestate:dirty builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration virtual box vagrant bento/ubuntu gb ram cpu os e.g from etc/os-release distributor id ubuntu description ubuntu lts release codename xenial kernel e.g uname a linux vagrant generic ubuntu smp tue sep utc x x x gnu/linux install tools others : what happened :as i try to run kubeadm init it hangs with root@vagrant kubeadm init
179839447,i am trying to install kubernetes on linux with kubeadm at step after installing weave net i see that kube-dns status just shows containercreating
179819759,problema frequent question that comes up on slack and stack overflow is how to trigger an update to a deployment/rs/rc when the image tag hasnt changed but the underlying image has.consider there is an existing deployment with image foo:latest user builds a new image foo:latest user pushes foo:latest to their registry user wants to do something here to tell the deployment to pull the new image and do a rolling-update of existing podsthe problem is that there is no existing kubernetes mechanism which properly covers this current workarounds always change the image tag when deploying a new version refer to the image hash instead of tag e.g localhost:/andy/busybox@sha:aacefbcbdabeebdbfeafaeebaccfa use latest tag or imagepullpolicy always and delete the pods new pods will pull the new image this approach doesnt do a rolling update and will result in downtime fake a change to the deployment by changing something other than the image possible solutions if rolling restart were implemented users could do a rolling-restart to pull the new image have a controller that watches the image registry and automatically updates the deployment to use the latest image hash for a given tag see justinsb
179221868,i would be great to be able to use gce new internal load balancer feature.configuration could be done via the same type of config as aws internal load balancer.this would avoid having to create bastion routes to access non public services living in gke
177674886,kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:aecccabcdadfdbbb gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd} environment cloud provider or hardware configuration apple macbook pro os e.g from etc/os-release macos sierra kernel e.g uname a darwin install tools gcloud components install kubectl brew install kubernetes-cli or download from release others tested on both a gke cluster and minikube to be sure also tested with running kubectl from google cloud shell and a docker container and kubectl works as expected on both kubernetes environments from these client environments. what happened : kubectl has unpredictable errors performing any operations theres no single error sometimes the process hangs sometimes it panics sometimes theres connection issues to the cluster. what you expected to happen : kubectl commands to run as expected without hangs segfaults or network issues. how to reproduce it as minimally and precisely as possible upgrade to macos sierra use kubectl anything else do we need to know :compiling kubectl for osx with go x will fix this as fixes for macos sierra are in go i compiled a version of kubectl from master with version: client version version.info{major minor gitversion:v..-alpha..+ffcecccdecf gitcommit:ffcecccdecfdaffeddfdadffd gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:darwin/amd} and it works as expected.im happy to pull request something to fix this but i dont want to make the assumption that everything needs to be updated to go also im not sure how the gcloud kubectl component is related or diverges from the release build version so if i can understand the nuances i can put something together to fix this
177633866,kubernetes version use kubectl version v.. environment cloud provider or hardware configuration virtual machine with gb physical memory and cpu cores os e.g from etc/os-release ubuntu kernel e.g uname a linux ubuntu-m generic ubuntu smp fri aug utc x x x gnu/linux install tools manual self created ansible scripts) what happened i created a service as below kind service apiversion v metadata name ultraesbsv-odytotow namespace default selflink api/v/namespaces/default/services/ultraesbsv-odytotow uid fba-cee-e-ae-dafe resourceversion creationtimestamp t::z labels app ultraesb cluster testcluster spec ports name protocol tcp port targetport nodeport name protocol tcp port targetport nodeport selector app ultraesb cluster testcluster clusterip type nodeport sessionaffinity none status loadbalancer then using the rest api i removed a port entry application/json-patch+json op:remove path:/spec/ports after executing above http request updated service is as below kind service apiversion v metadata name ultraesbsv-odytotow namespace default selflink api/v/namespaces/default/services/ultraesbsv-odytotow uid fba-cee-e-ae-dafe resourceversion creationtimestamp t::z labels app ultraesb cluster testcluster spec ports name protocol tcp port targetport nodeport selector app ultraesb cluster testcluster clusterip type nodeport sessionaffinity none status loadbalancer as expected port entry has been removed i checked in iptables rules for node port using command sudo iptables-save grep and i could see that it was successfully removed from iptables within few seconds i tried to add it back to the service application/json-patch+json op:add path:/spec/ports value name nodeport port protocol:tcp then is gave following message kind status apiversion v metadata status failure message service ultraesbsv-odytotow is invalid spec.ports .nodeport invalid value provided port is already allocated reason invalid details name ultraesbsv-odytotow kind service causes reason fieldvalueinvalid message invalid value provided port is already allocated field spec.ports .nodeport code after few minutes i tried the same http request then it was successfully executed it seems like kubernetes api proxy is some how caching node port is acquired for some grace period although it is not is this an expected behaviour? what you expected to happen at the very first http request i send to add port back into the service it should be successful how to reproduce it as minimally and precisely as possible): anything else do we need to know
177058169,bind dnsmasq on every node to cbr ip and point the containers resolve.conf to cbr instead of kube-dns setup a dnsmasq.conf like: #listen on container interfacelisten-address=...interface=ethinterface=cbruser=root#only use these namesserversno-resolvserver=/svc.cluster.local/...server=...server=...#static entriesaddress=/foo.bar/...log-queries log-facility=/tmp/dnsmasq.log at least reasons to consider this external lookups wont fail if kube-dns/endpoint controller/kube-proxy are down internal lookups will fail fast from by default dnsmasq will send queries to any of the upstream servers it knows about and tries to favour servers that are known to be up as an added bonus we can expose dnsmasq caching/ttl options through a configmap lookups get served from the same node egnormal svc lookup with dnsmasq root@dnsutils nslookup echoheadersx.defaultserver address name echoheadersx.default.svc.cluster.localaddress logs sep dnsmasq query a echoheadersx.default.default.svc.cluster.local from sep dnsmasq forwarded echoheadersx.default.default.svc.cluster.local to sep dnsmasq reply echoheadersx.default.default.svc.cluster.local is nxdomainsep dnsmasq query a echoheadersx.default.svc.cluster.local from sep dnsmasq forwarded echoheadersx.default.svc.cluster.local to sep dnsmasq reply echoheadersx.default.svc.cluster.local is kill kube-dns kubectl scale kube-dns-v replicas= redo the lookup it returns in a second root@dnsutils nslookup echoheadersx.defaultserver address server cant find echoheadersx.default nxdomain logs sep dnsmasq query a echoheadersx.default.default.svc.cluster.local from sep dnsmasq query a echoheadersx.default.svc.cluster.local from sep dnsmasq query a echoheadersx.default.cluster.local from sep dnsmasq forwarded echoheadersx.default.cluster.local to sep dnsmasq forwarded echoheadersx.default.cluster.local to sep dnsmasq reply echoheadersx.default.cluster.local is nxdomainsep dnsmasq query a echoheadersx.default.google.internal from sep dnsmasq forwarded echoheadersx.default.google.internal to sep dnsmasq reply echoheadersx.default.google.internal is nxdomainsep dnsmasq query a echoheadersx.default.c.kubernetesdev.internal from sep dnsmasq forwarded echoheadersx.default.c.kubernetesdev.internal to sep dnsmasq reply echoheadersx.default.c.kubernetesdev.internal is nxdomainsep dnsmasq query a echoheadersx.default from sep dnsmasq forwarded echoheadersx.default to sep dnsmasq reply echoheadersx.default is nxdomain but on another host the same lookup takes s to fail root@dnsutils dig showsearch echoheaders.default dig p showsearch google.com global options cmd connection timed out no servers could be reached a tcpdump shows that we retry thrice with a s timeout root@ee-test-beeps-minion-group-iwp tcpdump i cbr grep i googletcpdump verbose output suppressed use v or vv for full protocol decodelistening on cbr link-type enmb ethernet capture size bytes ip domain a google.com.default.svc.cluster.local ip domain a google.com.default.svc.cluster.local ip domain a google.com.default.svc.cluster.local kubernetes/sig-network
176522543,is this a bug report or feature request choose one):bug report kubernetes version use kubectl version ): client version version.info{major minor gitversion:v gitcommit:eedebebffadaddaf gittreestate:clean}server version version.info{major minor gitversion:v gitcommit:eedebebffadaddaf gittreestate:clean} environment cloud provider or hardware configuration aws kube-up.sh os e.g from etc/os-release debian gnu/linux jessie kernel e.g uname a linux ip amd smp debian ckt-+debu x gnu/linux install tools kube-up.sh what happened :a pod failed to start with the following log error writing home/myapp-ui:git-afba/push/slug.tgz to slug.tgz write slug.tgz no space left on device) . kubectl describe nodes shows that none of the nodes are running out of disk space: name status ageip----.us-west-.compute.internal ready dip----.us-west-.compute.internal ready dip----.us-west-.compute.internal ready dip----.us-west-.compute.internal ready d...conditions type status lastheartbeattime lasttransitiontime reason message outofdisk false mon sep wed aug kubelethassufficientdisk kubelet has sufficient disk space available ready true mon sep wed aug kubeletready kubelet is posting ready status warning cpu hardcapping unsupported...conditions type status lastheartbeattime lasttransitiontime reason message outofdisk false mon sep thu aug kubelethassufficientdisk kubelet has sufficient disk space available ready true mon sep wed sep kubeletready kubelet is posting ready status warning cpu hardcapping unsupported...conditions type status lastheartbeattime lasttransitiontime reason message outofdisk false mon sep tue sep kubelethassufficientdisk kubelet has sufficient disk space available ready true mon sep tue sep kubeletready kubelet is posting ready status warning cpu hardcapping unsupported...conditions type status lastheartbeattime lasttransitiontime reason message outofdisk false mon sep thu aug kubelethassufficientdisk kubelet has sufficient disk space available ready true mon sep mon sep kubeletready kubelet is posting ready status warning cpu hardcapping unsupported i manually sshed into the minions and ran df h : admin@ip df hfilesystem size used avail use mounted on/dev/xvda g g g udev m m devtmpfs g m g runtmpfs g m g dev/shmtmpfs m m run/locktmpfs g g sys/fs/cgroup/dev/mapper/vg--ephemeral-ephemeral g g g mnt/ephemeraladmin@ip df hfilesystem size used avail use mounted on/dev/xvda g g g udev m m devtmpfs g m g runtmpfs g m g dev/shmtmpfs m m run/locktmpfs g g sys/fs/cgroup/dev/mapper/vg--ephemeral-ephemeral g g mnt/ephemeraladmin@ip df hfilesystem size used avail use mounted on/dev/xvda g g g udev m m devtmpfs g m g runtmpfs g m g dev/shmtmpfs m m run/locktmpfs g g sys/fs/cgroup/dev/mapper/vg--ephemeral-ephemeral g g g mnt/ephemeraladmin@ip df hfilesystem size used avail use mounted on/dev/xvda g g g udev m m devtmpfs g m g runtmpfs g m g dev/shmtmpfs m m run/locktmpfs g g sys/fs/cgroup/dev/mapper/vg--ephemeral-ephemeral g g g mnt/ephemeral minion ip is the only one with usage of mnt/ephemeral i confirmed that the failing pod was attempting to start on that node.i sshed into that minion and ran the following: root@ip----:/mnt/ephemeral du sh docker.g dockerroot@ip----:/mnt/ephemeral du sh kubernetesg kubernetesroot@ip----:/mnt/ephemeral exitexitadmin@ip----:/mnt/ephemeral df hfilesystem size used avail use mounted on/dev/xvda g g g udev m m devtmpfs g m g runtmpfs g m g dev/shmtmpfs m m run/locktmpfs g g sys/fs/cgroup/dev/mapper/vg--ephemeral-ephemeral g g mnt/ephemeral so it seems something is leaking disk space in mnt/ephemeral/kubernetes . admin@ip----:/mnt/ephemeral/kubernetes sudo du sort n r kubelet kubelet/pods kubelet/pods/cfeec-eb-e-ae-eff kubelet/pods/cfeec-eb-e-ae-eff/volumes kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/influxdb-persistent-storage kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/influxdb-persistent-storage/data kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/influxdb-persistent-storage/data/ks kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/influxdb-persistent-storage/data/ks/default kubelet/pods/cfe-eb-e-ae-eff kubelet/pods/cfe-eb-e-ae-eff/volumes kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices kubelet/pods/cfda-eb-e-ae-eff kubelet/pods/cfda-eb-e-ae-eff/volumes kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/influxdb-persistent-storage/data/_internal kubelet/pods/cfeec-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/influxdb-persistent-storage/data/_internal/monitor kubelet/pods/faf-eb-e-ae-eff kubelet/pods/faf-eb-e-ae-eff/volumes kubelet/pods/faf-eb-e-ae-eff/volumes/kubernetes.io~empty-dir kubelet/pods/faf-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/etcd-storage kubelet/pods/faf-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/etcd-storage/member kubelet/pods/faf-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/etcd-storage/member/wal kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfda-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash kubelet/pods/cfe-eb-e-ae-eff/volumes/kubernetes.io~empty-dir/es-persistent-storage/kubernetes-logging/nodes//indices/logstash etc kubectl namespace kube-system get po o widename ready status restarts age nodeelasticsearch-logging-v-pth running d ip----.us-west-.compute.internalelasticsearch-logging-v-xmmw running d ip----.us-west-.compute.internalfluentd-elasticsearch-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalfluentd-elasticsearch-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalfluentd-elasticsearch-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalfluentd-elasticsearch-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalheapster-v..--low running d ip----.us-west-.compute.internalkibana-logging-v-xvzrk running d ip----.us-west-.compute.internalkube-dns-v-emyp running d ip----.us-west-.compute.internalkube-proxy-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalkube-proxy-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalkube-proxy-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalkube-proxy-ip----.us-west-.compute.internal running d ip----.us-west-.compute.internalkubernetes-dashboard-v..-ukcf running d ip----.us-west-.compute.internalmonitoring-influxdb-grafana-v-yp running d ip----.us-west-.compute.internal so it looks like most of the storage is takend up by influxdb elasticsearch shouldnt logging/monitoring components be restrained to a of available disk space and just delete older stuff as soon as they reach that limit by default also why doesnt kubectl describe nodes report that ip----.us-west-.compute.internal is out of disk
175891912,i was wondering if there are any plans to support digital ocean volumes similar to awselasticblockstore if not are you open to pull requests for this feature
173818363,"an earlier discussion about consul integration can be found here consul support integrationwe worked on getting consul support into kubernetes our implementation is based on consul and should provide the same featureset as the etcd implementation does.to ensure correct working of the implementation we wrote integration tests which are very similar to the ones for etcd these can be found at test/integration/master/consul_tools_test.gohowever the implementation is not finished yet herein after i will address the remaining issue unit testscurrently unit tests which need to talk to a storage backend are using the etcd implementation directly example this means the tests cant directly be reused with consul we tried to solve this problem by refactoring all the etcd specific tests to more generic storage tests and also using a factory to create all the available storage backends and run the unit tests against each backend in the end we had to drop this change because we could not keep up with the frequency of changes in these locations so for now unit tests dont run against consul.we think this is probably a good discussion point on how to handle multiple storage backends.this could be handled in a separate pr to keep the scope smaller for each one our test implementationin the following are snippets showing how our approach to run the tests for multiple backends looks like running tests and creating storage var factory storagefactory.testserverfactoryfunc testmain(m testing.m storagefactory.runtestsforstoragefactories(func(fac storagefactory.testserverfactory int factory fac return m.run func newstorage(t testing.t deploymentstorage storagefactory.testclientserver internalstorage server registrytest.newstorage(t factory extensions.groupname restoptions generic.restoptions{storage internalstorage decorator generic.undecoratedstorage deletecollectionworkers deploymentstorage newstorage(restoptions return deploymentstorage server factory func runtestsforstoragefactories(iterfn func(testserverfactory int factorytypeslist os.getenv(kube_storage if factorytypeslist factorytypeslist etcd,consul types strings.split(factorytypeslist retcodes make( int len(types for index factorytype range types factory err getfactory(factorytype if err nil fmt.printf(skipping tests for s because v factorytype err retcodes index continue fmt.printf(running tests in s mode\n factory.getname retcodes index iterfn(factory for code range retcodes if code os.exit(code os.exit create creates a storage backend based on given config.func getfactory(type string testserverfactory error switch type case storagebackend.storagetypeunset storagebackend.storagetypeetcd return newetcdtestserverfactory case storagebackend.storagetypeetcd todo we have the following features to implement support secure connection by using key cert and ca files honor https scheme to support secure connection in grpc support non-quorum read return newetcdstorage(c codec case storagebackend.storagetypeconsul return newconsulsharedtestserverfactory default return nil fmt.errorf(unknown storage type s type reviewable:start this change is img src height align=absmiddle alt=reviewable reviewable:end"
172194515,our enterprise team is migrating our workflow to kubernetes but weve run into a crippling problem and so far that someone on the ks team must have solved by now i searched for registries in this projects issues but didnt see anything in the first pages of results i read and posted on the docker project and all i found was people complaining about the same thing have multiple private environments for getting our apps from dev out into production the problem is that docker appears to have no mechanism for eliminating the public docker registry and overriding with your own instead every dockerfile docker-build command and ks yaml file needs to be templated with wrapper scripts around them to fill in the templates for us for instance in my test environment i use a local registry visible to both my dev machine and vagrant ks cluster i have to use private.devregistry.com as a prefix on every image name throughout the entire project dockerfiles build commands ks yaml files this has to be hardcoded or else docker will attempt to use public docker-hub but then to make this usable in our qa environment we have to have a script which replaces every registry string to private.qa.reg.com so that it works with their registry on a different network and again in production which is on yet another network now with everything templated doing simple dev and testing has become much more complex unable to use standard docker and ks commands.all because docker does not appear to have any way to set a global registry endpoint for all your commands kubernetes makes it simple to manage multiple clusters i appreciate that.is there a known solution for this or is the team working on something to help manage this so far this is crippling our ability to use ks.one candidate idea was to actually modify the docker source and attempt to build it ourselves another was to use a dummy hostname on everything and change everything by modifying local etc/hosts both of these ideas are silly but may be better than a complex web of templates
172045539,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there kubectl yaml mapping values are not allowed in this context is this a bug report or feature request choose one): kubernetes version use kubectl version ):v. environment cloud provider or hardware configuration os e.g from etc/os-release centos kernel e.g uname a install tools yum install others : what happened :when i execute kubectl create f x.yaml or kubectl get nodes i get error yaml line mapping values are not allowed in this context this is a serious problem i cannot use kubernetes to manage my cluster. what you expected to happen :who can help me to repair this problem. how to reproduce it as minimally and precisely as possible): anything else do we need to know
171714400,thanks for filing an issue before hitting the button please answer these questions.--> is this a request for help if yes you should use our troubleshooting guide and community support channels see no what keywords did you search in kubernetes issues before filing this one if you have found any duplicates you should instead reply there looked at the api docs discussed on the kubernetes/sig-auth slack channel with ericchiang and whitlockjc--- is this a bug report or feature request choose one feature requestonce oidc is integrated it would be helpful to have an endpoint that will reply with information about the logged in user mainly the username uid groups and roles something similar to openshifts oapi/v/users service ideally this would return json similar to json username:me uid:myuid groups admins devs etc roles adminrole userrole etc that way someone can easily test that oidc is setup correctly the dashboard can show who the logged in user is and external tool developers can verify/display the logged in user what you expected to happen curl h authorization bearer asdfv username:me uid:myuid groups admins devs etc roles adminrole userrole etc
171638393,feature request : fish shell is a relatively new modern interactive user friendly command line shell it really shines at user friendly features like visual tab completion menus live syntax highlighting and other useful features. kubectl has a massive set of sub-commands and options remembering all those is a chore specially when getting to know kubernetes all the help is appreciated.please provide autocompletion support for fish shell in addition to bash and zsh.thanks kubernetes version use kubectl version ): client version version.info{major minor gitversion:v..+ddb gitcommit:ddbefdbfaffbaafcba gittreestate:not a git tree builddate:--t::z goversion:go compiler:gc platform:darwin/amd}the connection to the server localhost was refused did you specify the right host or port? environment : others fish shell
171535837,summary i would like us to get rid of custom entrypoint.sh scripts by supporting templating of in-container files. the problem despite current configmap features it remains necessary for many or most container images under kubernetes to include entrypoint.sh scripts and/or customizations to the containerized applications which are particular to the kubernetes environment this results in forking of upstream images and limits portability of images between environments it also results in some very hackish startup scripts see kelseys init scripts for hipsters).based on feedback through numerous issues and prs the main shortcoming in configmap which would allow elimination of entrypoint scripts is the inability to include elements into a configuration file which are only available at deployment time for example some services such as clustered databases or load balancers need to know the pod ip or the pod name as part of their configuration. suggested solution configtemplate several proposals have been made to enhance configmaps to embrace templating functionality and they have met with significant opposition my proposal is that we create a new object named here a configtemplate as placeholder until someone suggests a better one this template object would produce a file inside the deployed container and could consume the downward api secrets and configmaps in order to populate a template this would be largely the same as pr but with a new object type instead of overloading configmaps.at a sketch a configtemplate for an ini-style config file could be inlined and might look like this: apiversion extensions/vbetakind configtemplatemetadata name postgresnodeconfspec configmaprefs name patronimap name productionmap template data node_ip status.podip dcs patronimap.dcsconn cluster_name productionmap.cluster memory mb note what syntax we use for the exact substitutions is unimportant lets just use whatever is easiest to support/code.for inline versions it is likely that only key value substitutions would be supported for more complex behavior including differently-formatted configuration files wed support file-based substitution ala: apiversion extensions/vbetakind configtemplatemetadata name postgresnodeconfspec configmaprefs name patronimap name productionmap template file templates/pg.conf.prod the file in question would be some kind of text file with substitution tags in whatever syntax we decide on either way the configtemplate would then be used inside pod definitions like this: apiversion vkind podmetadata name pgprodpodspec containers name postgres image jberkus/patroni templates name postgresnodeconf path etc/postgres/pgnode.conf why not volumes? youll notice above that im not taking a regular volume approach to this thats because in many cases including some cases i have personally the rendered config file needs to share directories with files which come from the container image doing this in a volume is complicated and will lead again to custom entrypoint.sh scripts.however if there are strong reasons to handle this as a volume that could be worked around. templating engines and sidecars there is some discussion about how templating would be handled and what templating engine wed use this includes a suggestion by thockin that this be entirely sidecar functionality.i will argue that providing config file templates which allow containers to start inside kubernetes pods without modification is fairly central functionality to what kubernetes does and as such the general template object should be a core kubernetes object however i can certainly see the value of allowing the user to plug in their choice of rendering library for the actual template rendering for file-based templates if nothing else it would forestall a lot of arguments about syntax.even in that case however i would like us to provide a built-in very simple template renderer one which is capable of just swapping in upstream facts for some specific variable syntax such as fact or fact or similar and nothing else such a built-in renderer would satisfy of users and not push the user into installing extra dependencies. alternatives i cannot personally think of any alternatives which will lead to the elimination of the majority of entrypoint.sh scripts suggestions welcome. references pr issue issue
171285006,it looks like docker exec is being used as the backend for kubectl exec docker exec has the user flag which allows you to run a command as a particular user this same functionality doesnt exist in kubernetes our use case is that we spin up pods and execute untrusted code in them however there are times when after creating the pod we need to run programs that need root access they need to access privileged ports etc).we dont want to run the untrusted code as root in the container which prevents us from just escalating permissions for all programs.i looked around for references to this problem but only found this stackoverflow answer from last year there are some workarounds to this such as setting up a server in the container that takes commands in or defaulting to root but dropping to another user before running untrusted code however these workarounds break nice kubernetes/docker abstractions and introduce security holes
171154072,kubernetes version client version version.info{major minor gitversion:v gitcommit:cedacddacab gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd} environment cloud provider or hardware configuration gke os e.g from etc/os-release debian kernel e.g uname a linux abee generic ubuntu smp wed jun utc x gnu/linux what happened after successfully authenticating with a service account on gcloud using gcloud auth activate-service-account google_auth_email key-file keyconfig.json project google_project_id the cluster credentials were downloaded using: gcloud container clusters get-credentials cluster_name even tho no errors are raised when attempting to run kubectl version i get this error: kubectl versionclient version version.info{major minor gitversion:v gitcommit:cedacddacab gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}error google could not find default credentials see for more information. what you expected to happen kubectl should be configured to use the cluster properly how to reproduce it as minimally and precisely as possible authenticate with a service account on gcloud get credentials from a cluster through gcloud run any kubectl command that reaches the server anything else do we need to know :after looking into the configuration environment i observed the user is not properly configured on kube/config : kubectl config viewapiversion vclusters cluster certificate-authority-data redacted server redacted name gke_cluster_namecontexts context cluster gke_cluster_name user gke_cluster_name name gke_cluster_namecurrent-context gke_cluster_namelkind configpreferences users name gke_cluster_name user auth-provider config null name gcp after looking in the gcloud docs i found this instruction export google_application_credentials=/path/to/keyfile.json after exporting the variable and running get-credentials once again running kubectl version worked and then running kubectl config view correctly displayed the user as authenticated ...users name gke_cluster_name user auth-provider config access-token redacted expiry t::.z name gcp note that this workflow was running perfectly in the past my guess is that kubectl is no longer correctly detecting the gcloud service account that is authenticated its only looking for the environment variable either way something seems broken
170980810,it is possible to create a config map using kubectl create configmap foo from-file bar.txt we support similarly updating the config map using kubectl update configmap foo from-file bar.txt
170928094,elbv supports layer load balancing surprisingly its already ga and integrated with ecs it would be great if kubernetes supported hooking up services to a specific elb instance with path rules
170571928,while its easy enough to get through something like: ifs read ra list pod_name index=${list } it requires a custom entrypoint if the ordinal index is a common part of fa apps we should just expose it more naturally through the downward api ref
170316157,im trying to integrate configmaps into our continuous delivery flow its important for us that the deploys are idempotent.typically wed create the configuration from files by doing kubectl create configmap myconfig from-file=file.yml from-file=file.yml however if this is run more than once we get this since the command is a create error from server configmaps myconfig already exists there are apply and replace commands for file or stdin resources but no apply configmap we could grab all the contents of each file and pipe that into a resource yml then pass that through to apply but it seems awkward.what is the current accepted practice for updating a configmap that is file driven?there is some overlap with this and plus
170021320,a max number of failures or failure backoff policy for jobs would be useful.imagine you have an etl job in production thats failing due to some pathological input.by the time you spot it its already been rescheduled thousands of times as an example i had broken jobs that kept getting rescheduled killing them took forever and crashed the kubernetes dashboard in the process.today restartpolicy is not respected by jobs because the goal is to achieve successful completion strangely restartpolicy never is still valid yaml this means failed jobs keep getting rescheduled when you go to delete them you have to delete all the pods theyve been scheduled on deletes are rate limited and in v the verbose youre being throttled messages are hidden from you when you run the kubectl command to delete a job so it just looks like its taking forever this is not a pleasant ux if you have a runaway job in prod or if youre testing out a new job and it takes minutes to clean up after a broken test.what are your thoughts on specifying a maximum number of failures/retries or adding a failure backoff restart policy?(cc thockin who suggested i file this feature request edit per this so thread the throttling issue is avoidable by using the onfailure restart policy to keep rescheduling to the same pods rather than to new pods i.e this prevents the explosion of the number of pods and deadlines can help weed out failures after a certain amount of time.however suppose my etl job takes an hour to run properly but may fail within seconds if the input data is bad id rather specify a maximum number of retries than a high deadline
169523951,pod spec validation rejects env variable keys that contain dots eg foo.bar but foo.bar is a valid env variable name for some though not all shell environments so this validation rule is overly strict.@smarterclayton
169057297,hit an issue with the pet set states switching from running state to init state initially i thought the pods arent running but later figured out that the containers are actually running but the container state is not consistent with the pod state in kubernetes api issue container state runningpod state pending kubernetes cluster version kubectl versionclient version version.info{major minor gitversion:v gitcommit:cedacddacab gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd}server version version.info{major minor gitversion:v gitcommit:cedacddacab gittreestate:clean builddate:--t::z goversion:go compiler:gc platform:linux/amd kubernetes get pod state kubectl get po l app=zkname ready status restarts agezoo init hzoo init hzoo init h kubernetes describe pod state kubectl describe po zoo-name zoo-namespace defaultnode ip----.region.compute.internal/...start time tue aug labels app=zk name=zoostatus pendingip controllers petset/zooinit containers install container id image gcr.io/google_containers/zookeeper-install image id port args version=..-alpha install-into=/opt work-dir=/work-dir state waiting reason podinitializing ready false restart count environment variables none bootstrap container id image java:openjdk--jre image id port command work-dir/peer-finder args on-start=/work-dir/on-start.sh service=zk state waiting reason podinitializing ready false restart count environment variables pod_namespace default v:metadata.namespace)containers zk container id docker://aefaccaeebecfffbdcdcabbced image java:openjdk--jre image id docker://sha:ddcfebedddffaddbb ports tcp tcp command opt/zookeeper/bin/zkserver.sh args start-foreground state running started tue aug ready true restart count readiness exec sh c opt/zookeeper/bin/zkcli.sh ls delay=s timeout=s period=s success failure environment variables none>conditions type status initialized false ready true podscheduled true volumes datadir type persistentvolumeclaim a reference to a persistentvolumeclaim in the same namespace claimname datadir-zoo readonly false workdir type emptydir a temporary directory that shares a pods lifetime medium opt type emptydir a temporary directory that shares a pods lifetime medium qos tier besteffortno events container state core@ip docker ps grep zoo-aefac java:openjdk--jre opt/zookeeper/bin/z hours ago up hours ks_zk.feb_zoo-_default_dbce-ce-e--dfade_ceabedfd gcr.io/google_containers/pause-amd pause hours ago up hours ks_pod.ea_zoo-_default_dbce-ce-e--dfade_dcc node state system info machine id id system uuid uuid boot id boot_id kernel version coreos os image coreos moreos operating system linux architecture amd container runtime version docker kubelet version v kube-proxy version v
168580336,here is one example see below:- lastseen firstseen count name kind subobject type reason source messagem m nginx-deployment--tfwp pod normal scheduled default-scheduler successfully assigned nginx-deployment--tfwp to kubernetes-minion-group-kcbfm m nginx-deployment--tfwp pod spec.containers{nginx normal pulling kubelet kubernetes-minion-group-kcbf pulling image nginxm m nginx-deployment--tfwp pod spec.containers{nginx normal pulled kubelet kubernetes-minion-group-kcbf successfully pulled image nginxm m nginx-deployment--tfwp pod spec.containers{nginx normal created kubelet kubernetes-minion-group-kcbf created container with docker id ffcaefm m nginx-deployment--tfwp pod spec.containers{nginx normal started kubelet kubernetes-minion-group-kcbf started container with docker id ffcaefm m nginx-deployment--ctt pod normal scheduled default-scheduler successfully assigned nginx-deployment--ctt to kubernetes-minion-group-qm m nginx-deployment--ctt pod spec.containers{nginx normal pulling kubelet kubernetes-minion-group-q pulling image nginxm m nginx-deployment--ctt pod spec.containers{nginx normal pulled kubelet kubernetes-minion-group-q successfully pulled image nginxm m nginx-deployment--ctt pod spec.containers{nginx normal created kubelet kubernetes-minion-group-q created container with docker id adcm m nginx-deployment--ctt pod spec.containers{nginx normal started kubelet kubernetes-minion-group-q started container with docker id adcm m nginx-deployment--dzp pod normal scheduled default-scheduler successfully assigned nginx-deployment--dzp to kubernetes-minion-group-bdrm m nginx-deployment--dzp pod spec.containers{nginx normal pulling kubelet kubernetes-minion-group-bdr pulling image nginxm m nginx-deployment--dzp pod spec.containers{nginx normal pulled kubelet kubernetes-minion-group-bdr successfully pulled image nginxm m nginx-deployment--dzp pod spec.containers{nginx normal created kubelet kubernetes-minion-group-bdr created container with docker id fm m nginx-deployment--dzp pod spec.containers{nginx normal started kubelet kubernetes-minion-group-bdr started container with docker id fm m nginx-deployment replicaset normal successfulcreate replicaset-controller created pod nginx-deployment--tfwpm m nginx-deployment replicaset normal successfulcreate replicaset-controller created pod nginx-deployment--cttm m nginx-deployment replicaset normal successfulcreate replicaset-controller created pod nginx-deployment--dzpm m nginx-deployment deployment normal scalingreplicaset deployment-controller scaled up replica set nginx-deployment to m m secret-env-pod pod normal scheduled default-scheduler successfully assigned secret-env-pod to kubernetes-minion-group-bdrm m secret-env-pod pod spec.containers{test-container normal pulling kubelet kubernetes-minion-group-bdr pulling image gcr.io/google_containers/busyboxm m secret-env-pod pod spec.containers{test-container normal pulled kubelet kubernetes-minion-group-bdr successfully pulled image gcr.io/google_containers/busyboxm m secret-env-pod pod spec.containers{test-container normal created kubelet kubernetes-minion-group-bdr created container with docker id ffm m secret-env-pod pod spec.containers{test-container normal started kubelet kubernetes-minion-group-bdr started container with docker id ffm m secret-test-pod pod normal scheduled default-scheduler successfully assigned secret-test-pod to kubernetes-minion-group-bdrm m secret-test-pod pod spec.containers{test-container normal pulling kubelet kubernetes-minion-group-bdr pulling image kubernetes/mounttest:.m m secret-test-pod pod spec.containers{test-container normal pulled kubelet kubernetes-minion-group-bdr successfully pulled image kubernetes/mounttest:.m m secret-test-pod pod spec.containers{test-container normal created kubelet kubernetes-minion-group-bdr created container with docker id cbaeam m secret-test-pod pod spec.containers{test-container normal started kubelet kubernetes-minion-group-bdr started container with docker id cbaea after m it should be m and then m this feature would allow us to see whats wrong easily i have had other examples where its all mixed up and its not easy to see where your just deployed pod is failing tested on
168388172,aeecbebbfffe is not a very friendly elb name the aws console does not allow for searching/sorting by tags for every resource we use something like this: environment - app staging-my-app it would be nice if we could pick a template to use for naming a combination of cluster name and pod name might work well
167336235,i am trying to intsall kubernetes dashboard by the command: kubectl create f when i do kubectl get po o wide all-namespaces i see the status of the kubernetes-dashboard pod as crashloopbackoff the output of command kubectl logs kubernetes-dashboard--gxrwv namespace=kube-system looks like this: starting http server on port creating api server client for while initializing connection to kubernetes apiserver this most likely means that the cluster is misconfigured e.g it has invalid apiserver certificates or service accounts configuration or the apiserver-host param points to a server that does not exist reason the server has asked for the client to provide credentials does anyone know how to fix this issue thanks in advance
165716862,hey!ive been working on integrating kubernetes and infinispan java based data grid distributed cache in-memory key/value store and i noticed that kubernetes horizontal pod autoscaling feature operates only on cpu metrics i would like to propose extending the autoscalers functionality and adding memory based thresholds i understand that a similar effect might be achieved by using custom metrics but it requires a lot more configuration the autoscaler could possibly be configured to use a combination of two cpu and memory metrics but this needs a longer thought how to avoid flipping the number of replicas probably the easiest way is to set priorities to the metrics).n.b since there are usually types of applications cpu memory and i/o intensive we could add rd metric to the autoscaler i/o however im not sure how this fits into persistent volumes concept in kubernetes.what do you think about this idea?thankssebastian
165594681,"deployment/service manifest: ---apiversion extensions/vbetakind deploymentmetadata name prometheusspec replicas selector matchlabels app prometheus template metadata name prometheus labels app prometheus spec containers name prometheus image quay.io/coreos/prometheus args storage.local.retention=h storage.local.memory-chunks config.file=/etc/prometheus/prometheus.yml ports name web containerport volumemounts name config-volume mountpath etc/prometheus name data mountpath prometheus volumes name config-volume configmap name prometheus name data persistentvolumeclaim claimname prometheus-data-pvc---kind persistentvolumeclaimapiversion vmetadata name prometheus-data-pvc annotations volume.alpha.kubernetes.io/storage-class ebsspec accessmodes readwriteonce resources requests storage gi logs from events: m m prometheus--bqkbd pod warning failedmount kubelet ip----.eu-west-.compute.internal unable to mount volumes for pod prometheus--bqkbd_default(bdcc-d-e-df-b timeout expired waiting for volumes to attach/mount for pod prometheus--bqkbd/default list of unattached/unmounted volumes= data m m prometheus--bqkbd pod warning failedsync kubelet ip----.eu-west-.compute.internal error syncing pod skipping timeout expired waiting for volumes to attach/mount for pod prometheus--bqkbd/default list of unattached/unmounted volumes= data in aws the volume is attached,on the box: core@ip ls ltr dev/xvdb*brw-rw root disk jul dev/xvdbabrw-rw root disk jul dev/xvdbb in proc/mounts only one is mounted: core@ip cat proc/mounts grep xvdb/dev/xvdbb var/lib/rkt/pods/run/cda---c-efbfeedc/stage/rootfs/opt/stage/hyperkube/rootfs/var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/eu-west-a/vol-cff ext rw,seclabel,relatime,data=ordered dev/xvdbb var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/eu-west-a/vol-cff ext rw,seclabel,relatime,data=ordered dev/xvdbb var/lib/rkt/pods/run/cda---c-efbfeedc/stage/rootfs/opt/stage/hyperkube/rootfs/var/lib/kubelet/pods/bce-d-e-df-b/volumes/kubernetes.io~aws-ebs/pvc-db-b-e-df-b ext rw,seclabel,relatime,data=ordered dev/xvdbb var/lib/kubelet/pods/bce-d-e-df-b/volumes/kubernetes.io~aws-ebs/pvc-db-b-e-df-b ext rw,seclabel,relatime,data=ordered where can i debug this further"
165085305,ive got a service that i want to expose only on the internal service ip which is available through a vpn connection but i also want the ease of use of an ingress resource handling ssl termination for me.it seems that the ssl termination functionality is tightly coupled to ingress functionality which is only used to expose a service to the outside world especially on gke where by default you dont have any control over how the ingress external ip is set-up does the ingress resource suit this use-case or should service resources be able to handle ssl termination or should a third resource be introduced to handle this or is this simply not possible and should i handle ssl termination in these use-cases myself
164577402,source user feedback)first it outputs waiting for pod which gets mingled with commands output second simple output often gets lost it date doesnt work console kubectl run test rm restart=never it image=ubuntu datewaiting for pod default/test to be running status is pending pod ready falseerror from server unrecognized input header it bash sleep then date works with output mingled with command output console kubectl run test rm restart=never it image=ubuntu bash c sleep date sleep waiting for pod default/test to be running status is pending pod ready falsehit enter for command prompt fri jul utc pod test deleted t bash sleep then date doesnt work console kubectl run test rm restart=never t image=ubuntu bash c sleep date sleep error i/--stdin is required for containers with t/--tty=truesee kubectl run h for help and examples i date works but output is still mingled console kubectl run test rm restart=never i image=ubuntu datewaiting for pod default/test to be running status is pending pod ready falsefri jul utc pod test deleted compared to docker run all cases return the same simple output console docker run i ubuntu datefri jul utc kubernetes/kubectl
164347685,sometimes users require daemons to run on each node scheduling them is easy with daemonsets discovery is more complicated and currently requires e.g iptables hacks that bypass kubernetes altogether possible use cases monitoring agents such as datadog logging agents such as fluentd authenticating proxies such as aws-es-proxy or loasd at the very least this new kind of services should support service->daemon mapping for cases like loasd it would be ideal to support n a number of services all getting forwarded to the same daemon with the daemon able to tell which service each connection was meant for).cc thockin write proposal implementation extend service api extend kube-proxy and/or kube-dns
163098359,docker implemented modifiable shmsize see in version it should be possible to define the shmsize of a pod on the api and kubernetes shall pass this information to docker
162963516,aws announced general availability of the efs today and im looking into possibility of using it with my ks cluster the tricky part here is that i run a multi-az setup hence it would be optimal if i could mount from nfs server aka efs mount target in the same zone so obviously name somevol nfs server eu-west-b.fs-xxxxxxxx.efs.eu-west-.amazonaws.com path will not do the trick initially my idea was that it would be nice if i could reference node label in server name like failure-domain.beta.kubernetes.io/zone>.fs-xxxxxxxx.efs.eu-west-.amazonaws.com after quick chat justinsb suggested that maybe kubernetes own dns could handle such cases.another approach could be based on ks service with hardcoded endpoints as long as endpoint can be labeled with az and kube-proxy can be forced into sort of use az local endpoints first if available mode
162730896,in the current state invocation of kubectl logs pods/my-pod provides mixed stderr and stdout from the pod for certain applications it is important to be able to separate stderr from stdout i havent found any way of separating stderr from stdout through kubectl logs
161962706,"hi guys im one of the leader of kubespray organisation and we think our project is mature enough to get included into kubernetes github tree.does it make sense for you how to proceed please regards,smana"
160405221,i brought this up back when petset was just a proposal but it didnt get much response at the risk of being labeled or dismissed as an sjw or preachy vegan or of stirring up drama please hear me out:the name petset is derived from the common metaphor in infrastructure of pets vs cattle the metaphor encourages infrastructure developers to think of cloud servers as anonymous and fungible resources rather than things to be manually managed by a person the implication is that instead of treating a server like a pet which you take care of and treat when it becomes sick you simply destroy the server and replace it with a new one as a cattle rancher would simply kill an animal that didnt serve its economic purpose to the rancher the pet has a personal and emotional value to you but the cattle is just a commodity.the lesson in infrastructure here is quite a good one and its value should be preserved but using this particular metaphor is quite unfortunate because it perpetuates the belief that the life and well being of an animal has value only in relation to its value to humans.this may seem like making a mountain out of a molehill but try to think about how our language perpetuates our culture and our beliefs try to think about it in the context of how future generations will see it in the same way that homophobic or racist language was and in some cases still is commonplace and accepted in days past in the modern world we generally recognize this language as unacceptable because it promotes a negative world view that we have progressed past imagine how angry people would be if this feature were called wifeset and the analogy were wives vs bar hook-ups were in an era of increasing empathy and that empathy is not bounded only to other humans it affects any being that can feel pain sadness or loss like we can.while i dont claim to have a perfect substitute for this analogy that might replace petset the one i have been using myself is to compare the role of owning a car vs using a taxi or ridesharing service when you own a car it is your personal possession you take care of it you keep it fueled cleaned and maintained in good shape when it breaks you get it fixed in contrast a taxi or ridesharing service is using a car as a fungible resource you hail one only when you need the service it provides and you use it only as long as you require that service you dont care which car picks you up or who is driving it as long as it is fulfills the service so my own suggestion for a replacement for petset is carset if someone has a better idea that seems to click with people more all the better and of course the feature could be named something more generalit doesnt have to use an analogy i recall that originally nominalset was being considered.)this issue is not about me or anyone else being offended or requesting that the name be changed to carset specificallyits just asking specifically not to invoke the pets vs cattle metaphor in the name of this feature.thank you very much for considering this.for context here is my previous comment from back in the proposal phase
160059218,im curious if there is interest in creating some environment variables kubectl_context and kubectl_namespace to make kubectl a bit easier to use when one has multiple clusters configured in kube/config .the alternative to these environment variables either require a lot of typing specify context and namespace to all kubectl commands or requires changing the global context such that it affects other looping kubectl invocations or as ive done so far wrapper functions in my shell but ive been unable to setup my environment such that i can have helper functions without breaking the tab completion for kubectl .is there interest in this or asked differently would such a pr be accepted
159328560,our initial understanding is that liveness probe will start to check after readiness probe was succeeded but it turn out not to be like that.we are testing with our system that has a long boot time an approximation of boot time is between minutes we specify readiness probe with same url of liveness probe and specify initial delay of liveness probe to be seconds we found that the pod is killed by failing of liveness probe while readiness probe still in a failure.so why dont we specify initial delay of liveness probe to be more than minutes well it might be a case when readiness probe succeed at first minute and after that the pod will fail to do a job before liveness probe will start so it will affect running service.another point is why we dont only put readiness probe and leave out liveness probe when readiness probe fail again it might take out of service as well so it dont cause any affect for running service but it wont be restarted and we have to do manual restart them.there are some concerns we thought about here are list what if readiness probe never succeed while starting a pod can we specify a maximum number of check will be performed and if readiness probe fail more than that number we may consider that pod should be killed what if someone want to set readiness probe to put the pod in maintenance mode the pod might be killed by above point if readiness probe fail more than that number or liveness probe will kill it maybe we add one more state of readiness probe called maintenance so that liveness probe should stop working when enter this state and no number of failing readiness probe should not be considered.what do you think
159265587,unless you change something in the template for deployment a rollout wont be triggered shouldnt i be able to trigger a rollout arbitrarily in this case i want to rebuild my replica set with a new version of a docker image with the same label maybe im just missing something
158773863,if kubectl can login but isnt authorized to access non-resource urls paths like apis it gives an extremely cryptic message about failed version negotiation kubectl get podserror failed to negotiate an api version server supports map client supports map autoscaling/v batch/v rbac.authorization.ks.io/valpha v authorization.ks.io/vbeta authentication.ks.io/vbeta apps/valpha batch/valpha federation/valpha policy/valpha componentconfig/valpha extensions/vbeta:{} this is the default error message for a user without any permissions i would expect this action to return an error message similar to other s for example kubectl get nodeserror from server the server does not allow access to the requested resource get nodes
158140277,as dns is so crucial to ks and i have seen failures due to resource starvation preventing kube-dns from starting we should run kube-dns in an ha configuration i believe we should run it on a daemonset on every node.it might be that the kubesky etcd skydns setup is too resource intensive for this if so we should probably make a controller that removes etcd and integrates kubesky skydns now that this pattern is well-known proven thockin i thought you had some notes on how to do this but github search is failing me.cc ajohnstone
158034316,currently kubernetes adds every elbs security group rule to instances which means the number of rules included in instances security group grows as the number of elb grows.aws supports up to inbound rules per security group and security groups per network interface this aws limit and the current setup of kubernetes there is a hard limit of services for each kubernetes cluster this problem arises mainly because every elb creation results in a new rule added to every instance security group we can resolve this issue by introducing a shared security group per kubernetes cluster a simple solution is to modify codes in aws.go such that when elb is created it either finds or creates a shared security group with no rule and attaches it to elb also instead of adding the elbs own security group rule to instances it tests if each instance already has the shared security group id as one of the source group id here we also have to make sure each instance accepts every traffic coming from the shared security group source if it finds that some instances do not have the shared security group id added yet it does so with this revision the number of security group rules for instances becomes independent of the number of elbs which means the number of service it can support is not limited by the aws limit
157394277,in we took most of the fat out of the internal apiserver and client machinery however we have fairly complex internal flows for controllers caches the watches and other distribution mechanisms that are more difficult to trace profile in addition we are adding larger numbers of control loops and feedback mechanisms into the state of the system we are seeing more inconsistent outcomes that are hard to reason about.for i think it would be valuable to broaden the traces started in possibly by adding opentracing support it would also give us a bit more contextual information in failures we would also need to consider how traces could propagate across control loops such as whether we do level driven traces from the kubelet or elsewhere
156842999,the current approach for populating environment variables from configmaps ends up pretty verbose with the following configmap: apiversion vdata space-ships ship-type battle-cruiser weapon laser-cannonkind configmapmetadata name space-config i have to manually expose each variable separately: apiversion vkind podmetadata name space-simulatorspec containers name space-simulator image foo/space-simulator env name space_ships valuefrom configmapkeyref name space-config key space-ships name ship_type valuefrom configmapkeyref name space-config key ship-type name weapon valuefrom configmapkeyref name space-configs key weapon this seems unnecessary and error-prone if all i want to express is that the pod should pull all the data from the configmap and populate environment variables from them.it would be nice if this could be done in more concise manner perhaps by introducing an envfrom field for example: apiversion vkind podmetadata name space-simulatorspec containers name space-simulator image foo/space-simulator envfrom configmap space-config the idea being that envfrom could support other ways of auto-population e.g secrets
156592621,i got errors complaining about the selector and labels not matching even if i explicitly specified both showed janetkuo
156338624,kubelet has a few dependencies and we should be aware of them document them so users know whats the minimum amount of other programs kubelet needs around it to execute successfully.im testing this by compiling kubelet statically and running it in a busybox container with absolutely nothing ive been surprised how well it runs after all.im listing some both hard and soft dependencies glibc hard dependency on the right libraries at runtime we should get rid of this as its unnecessary root access hard dependency and unavoidable the kubelet has to have full control of the machine cap_sys_admin is one of the privs that are needed at least when running in a container cgroups cpu and memory required and used in the meantime cpuacct and cpuset required but not used but vishh said they will be used in the future journalctl bug in cadvisor should not be required iptables is required for the kubelet it seems although now flannel isnt initialized when not used ethtool required for hairpin code i do not really understand what hairpin is required/used for thockin bprashanth nsenter used as util/io/writer when containerized used by docker-exec-handler but not default used with socat for port forwarding within a pod mount umount and findmnt used in nsenterwriter in util/io/writer socat used for port forwarding within a pod code touch used by rkt code docker or rkt obviously systemd does rkt require a systemd host yifan-gu paths ro it must be able to read from the whole host var/run/:rw required for communicating with docker setting up certs etc sys/:rw for various cgroup things var/lib/kubelet:rw and var/lib/docker:rw it obviously must be able to write to its own and dockers dirfor kube-proxy iptables is of course a hard requirement but the problem is that as it seems iptables has to be linked dynamically which sets a glibc dep on the kube-proxy image conntrack used for limiting requests i think)inside client containers env seems like its required inside a container that kubernetes is running but only when using docker-exec-handler=native code plugins dmidecode required for the vsphere plugin glusterfs-server glusterfs-client are required for the glusterfs plugin git is required for the git pluginplease correct me if im wrong and comment if there is anything ive missed would appreciate discussion about this and we should have multi-platform kubernetes in mind)cc vishh smarterclayton mikedanese pmorie
155832874,consider a job with two containers in it one which does the work and then terminates and another which isnt designed to ever explicitly exit but provides some sort of supporting functionality like log or metric collection.what options exist for doing something like this what options should exist?currently the job will keep running as long as the second container keeps running which means that the user has to modify the second container in some way to detect when the first one is done so that it can cleanly exit as well.this question was asked on stack overflow a while ago with no better answer than to modify the second container to be more kubernetes-aware which isnt ideal another customer has recently brought this up to me as a pain point for them.@kubernetes/goog-control-plane erictune
154284964,something like yamlrules host foo.bar.com path api backend servicename alpha weight or servicename alpha weight or thereer ways to achieve similar splitting with either a deployment or by managing service labels so they span a mix of alpha and alpha endpoints but this seems more intuitive.on gce i think wed have to use iptables stats to satisfy that but we cant push it down into a service because today a single service only understands homogenous endpoints.nginx and haproxy understand wrr so the and would have to apply to all the endpoints of the service and wed just need to make sure the tie is broken evenly
153511068,"release-noteadd support for terminal resizing for exec attach and run note that for docker exec sessionsinherit the environment from the primary process so if the container was created with tty=false,that means the exec sessions term variable will default to dumb users can override this bysetting term=xterm or whatever is appropriate to get the correct smart terminal behavior. fixes"
152973810,there is no specific issue that addresses this issue which i think is rather critical i havent tested spdy and http yet but those are probably broken also in amy environments kubectl proxy would be the only user-friendly way to to get to the internal services although its understood that it can only carry http traffic it should support modern uses of http
151876428,there are a few prs in flight to use annotations to add extra features to the elbs that ks sets up dns ssl proxy protocol and ive written up here what i think a consistent set of those annotations would be i think the prs that we have are pretty close to what we have here proxy protocol support service.beta.kubernetes.io/aws-load-balancer-proxy-protocol=* this is required to get the client ip but the service must be aware of the extra header bytes that are sent likely our ingress controllers will turn this on and then set an http x-forwarded-for header and also some advanced services might set it also to indicate they speak proxy protocol.cc williamsandrew and cf ssl support at the elb service.beta.kubernetes.io/aws-load-balancer-ssl-cert=arn:aws:acm:us-east-::certificate service.beta.kubernetes.io/aws-load-balancer-ssl-ports or e.g https )if the user wants the elb to do ssl decryption they should set these annotations ssl-cert specifies the certificate to use ssl-ports specifies on which ports we should enable ssl ssl-ports can be optional and default to we need not implement ssl-ports right away because the primary use for ssl and non-ssl is likely http and https for webservers and they will likely use ingress which will likely use sni instead but i do think some people will want direct-to-the-service http https so i bet well want it eventually..)cc therc and cf dns support at the service level service.beta.kubernetes.io/hostname=myservice.myzone.com if set kubernetes will configure a dns alias for the elb because this isnt specific to aws i dont think it gets an aws prefix.cc chbatey and quinton-hoole and cf sharing reusing an elb service.beta.kubernetes.io/load-balancer-secret=some-sort-of-secret-value this allows an elb to be safely shared between two ks clusters if set ks will look for an elb with a tag derived from the secret if it finds one it will use that elb otherwise it will create one and tag it in this way two ks clusters with services with the same tag will share an elb if the secret is kept reasonably secret then a user wont be able to steal someone elses elb this will be more realistic when we have rbac etc).cf
151777021,this has come up a bunch of times in conversations the idea is that some external orchestration is being performed and the user needs to signal sighup usually but arbitrary pods sometimes this is related to configmap and sometimes not this also can be used to bounce pods.we cant currently signal across containers in a pod so any sort of sidecar is out for now.we can do it by docker kill but something like kubectl signal is clumsy its not clearly an api operation unless we revisitoperation constructs for managing imperative async actions).another issue is that a pod doesnt really exist do we signal one container which one maybe all containers or do we nominate a signalee in the pod spec
151086396,currently when creating an elb in aws kubernetes doesnt support adding custom tags or naming for the resources in turn this requires a very broad set of permissions for the instance profile running the master allowing full access to any elb in the same account.from my perspective this is unsuitable for larger corporations running in aws under a single account as this increases risks of others teams elbs being manipulated by an instance they arent aware of.my suggestion would be to allow some basic configuration in the cluster to include custom tagging or at least prefixing the name of resources created by kubernetes so allowing iam-like permission schemes to filter based on the tags or naming not only for aws but i dont have expertise with gce or other public cloud providers to know if they support iam-like authorization
150954523,ingress controller doesnt seem to support web sockets for example its not possible to run jupyter behind an ingress controller its not possible to edit notebooks socket error).i find it weird than the ingress controller cannot do that while a standard nginx server with reverse proxy can do that really well but they seem to sit at very different position in the system
150952640,currently afaik you can only create a secret from a file using: kubectl create secret generic name from-file== the drawback is this if a change is made to the file you need to delete and then recreate the secret: kubectl delete secret name>kubectl create secret generic name from-file== this is awkward because to update the other items service deployment pv in the same app you just run: kubectl apply f filename> i realize that i can create a script to base encode the contents of the file and inject it into a secret spec file and then use create/apply/delete as with other api objects but this just feels awkward because the create/apply/delete workflow is so clean.for context this is a redis config file that contains a password and im mounting it with a volumemount in a deployment name redis-conf-secret readonly true mountpath etc/redis
150641857,when running kubernetes locally using docker if i try to visit get redirected to that shows an error page with the following text kind status apiversion v metadata status failure message endpoints kubernetes-dashboard not found reason notfound details name kubernetes-dashboard kind endpoints code if i visit the url dashboard appears correctly my understanding is that ui is expecting the namespace to be kube-system the default namespace for hyperkube is default
150424721,in order to run ks in our environment i need to get logging information into splunk i need to pick up node logs and application logs i know at this point there is no drop-in solution so i know ill need to hack something together what im seeking in this issue is not a heres the solution though that would be great of course im seeking advice on which of several solutions ive come up with are most likely to work given how kubernetes is evolving research and referencesissue seems to have resulted in the current ability to choose elasticsearch as a provider in kube-up.shissue seems to indicate that the future is not yet clear.issue contemplates some limitations of the current approach alsoissue provides a proxy to send data to aws instead of esbased on many sources it seems clear that systemd/journalctl is the future of logging in nix latest versions of rhel/centos/fedora and ubuntu have moved this way.while im of course willing to hack id really rather avoid needing to re-build the ks images if possible option ks->fluentd-->splunki could use fluentd and then use a fluentd splunk plugin if i use kube-up.sh with kubernetes_logging_destination=elasticsearch i get a fluentd/es setup there are plugins for fluentd that can forward content on to splunk.but i do not know how i would configure ks to not install elasticsearch as a part of startup i also think this represents an extra layer of forwarding id rather avoid with this solution i think i would need to send all application output to stdout/stderr since this is how ks currently gathers stuff this solution does not use journald which causes me to think this option is a doomed-to-change-in-the-near-future which leads me to option option ks->journald docker->journaldi could forward all logs from the nodes and pods too to journald using the docker journald log driver and then capture data out of the journald logs and send to splunk from there honestly this seems like the right solution why re-invent logging capture if not already true any strong security setup is going to require use of centralized logging capture and that will need to be based on journald there are two problems with this though doing it this way will not allow kubernetes metadata to be included in the log stream and i believe this will break kubectl logs because it relies on the docker json logs option docker ks journaldlogging directly from docker to journald means there isnt a chance to add ks meta what might be the coolest option is if ks provided a docker logging driver that then proxies the data and sends it to journald that way as a system administrator i just need to go to one place to get all my logs the journald service on each node but i still get the ks metadata too.i have no reasonable ability to execute this option though it would be a big change but id be willing to help if this is a good way to do it option splunk docker log driversplunk has an experimental log driver i could use but it doesnt allow ks to see the data or to enrich it ie it will certainly break ks logs this might work as a workaround but it is not appealing closing thoughtsif option is the best i could use some ideas about how i could get ks to start up fluentd but not start elasticsearch.if option is best i could use some help pointing me in the right direction about how to avoid breaking kubernetes when i configure the docker daemons on the nodes to send logs to journald instead of the json logs.if option is best i need some pointers on how i could contribute.thanks for insights you can offer
150415574,this causes more confusion than it should easy to mess up the gce l wont accept anything other than rsa and ecdsa p and the cert will get rejected unless ordered as: -----begin my certificate----------end my certificate----------begin intermediate certificate----------end intermediate certificate----------begin intermediate certificate----------end intermediate certificate----------begin root certificate----------end root certificate----- additionally we should tell people how to check for the validity of the chain should do that automatically some loadbalancers will do that for you cloud lbs wont
149486727,it would be nice to support spot fleet on aws for the nodes the challenge is that spot fleet does not support tags afaik and we rely on our nodes being tagged in the ks aws cloudprovider
148220193,ive searched the docs but cant find an answer to an important question how do i backup and migrate or restore a kubernetes cluster?theres quite a bit of gray around what qualifies as the cluster running containers node ip addresses etc but at the very least id expect a way to dump the configuration of all high level resources services replication controllers deployments etc).does this exist if so is there an existing issue tracking progress if this is a bad idea whats the right way tm to have an insurance policy for disaster recovery in the event of catastrophic cluster failure
146354112,"i am not sure if what i am doing is supposed to work but i have created the following pod: apiversion vkind podmetadata name nginx-hostspec containers image caseydavenport/nginx imagepullpolicy ifnotpresent name nginx-host ports containerport hostport restartpolicy always this gives me a pod with the following hostip x.x.x podip x.x.x, from the host x.x.x running curl gets me the response from nginx that i expect.from the host x.x.x running curl gets me port connection refused .should this work or have i missed something?versions: client version version.info{major minor gitversion:v gitcommit:cdafdabccefddef gittreestate:clean}server version version.info{major minor gitversion:v gitcommit:cdafdabccefddef gittreestate:clean} networking calicohost os ubuntu docker thanks a lot"
146150392,tracks the implementation of the templating proposal this wip is essentially a quick merge of os templates with head some modifications to get the core substitution logic and resource to work with apiserver proposal for details on what remains from memory syntax processedtemplates and templates api endpoints pkg/client kubectl integration validation unittests for templates etcd etc ees docsother items to track template processing template create validation etcd_test.go tests template update validation etcd_test.go tests processing templates without saving them@bgrant mentioned someone from kubernetes/kubectl might be interested.@bparees since you had the original proposal
146081676,it should be possible to use a single ip to direct traffic to multiple protocols with a single service of type=loadbalancer we just need to promote ephemeral to static ip create another forwarding rule with the same static ip but a different protocol/portunfortunately we need this dance because a single forwarding rule only supports protocol.when we do this we should make sure the firewall rules are opened up for the right protocol that just takes the first protocol for the firewall rule is there a reason we didnt do this initially
145948766,there are several applications like sip apps or rtp which needs a lot of ports to run multiple calls or media streams currently there is no way to allow a range in ports in spec so essentially i have to do this name sip-udp containerport protocol udp name sip-udp containerport protocol udp doing above for ports is not pretty can we have a way to allow port ranges like
145162975,going through this guide to set up kubernetes locally via docker i end up with the error message as stated above.steps taken export ks_version=..-alpha tried as well copy-paste the docker run command download the appropriate kubectl binary and put in on path which kubectl works optionally setup the cluster run kubectl get nodes in short no magic i am running this locally on ubuntu docker if you need more information let me know
144368681,currently the default behavior of revisionhistorylimit of deployments in other words if spec.revisionhistorylimit is not specified is to keep around all replica sets this strikes me as unexpected and unreasonable behavior in normal usage of the deployment object you will quickly end up with dozens of replica sets with no obvious button or mechanism to clean them up i think that using deployments in the typical recommended way should not result in unbounded creation of replica sets.i personally vote for as the default it gives you basic rollback functionality but doesnt confuse the user by keeping around lots of replica sets and feels the most intuitive to me but i think any number up to is reasonable
144300066,it would be great if we could spin up cloud resources like an rds database directly from within ks rather than having to create resources through the api and then pass the credentials into ks.this could be implemented using a custom resource type e.g database
144299362,it would be great to be able to configure iam permissions on a per-pod basis we would then have a mock metadata service on which would inject the correct iam roles into the pod or no roles at all).i believe this can be implemented using the aws security token service
144114123,after calling expose type=loadbalancer a call to get services will print something like this kubectl get servicesname cluster-ip external-ip port(s agehello-node tcp dkubernetes none tcp d kubectl should show a pending state to indicate to the user that something is happening behind the scenes
143999909,helloi set up my first cluster on aws and when using the dashboard it keeps hanging and then displaying the following kind status apiversion v metadata status failure message no endpoints available for service kubernetes-dashboard reason serviceunavailable code does this mean i need more larger nodes ive seen people have the problem but not with the dashboard.i have minions which are all t.micros and master also a t.micro kubectl get pods namespace=kube-systemname ready status restarts ageelasticsearch-logging-v-oa running melasticsearch-logging-v-uvqt running mfluentd-elasticsearch-ip----.ec.internal running mfluentd-elasticsearch-ip----.ec.internal running hfluentd-elasticsearch-ip----.ec.internal running hheapster-v..-bprg running hkibana-logging-v-nbjuk crashloopbackoff hkube-dns-v-atchx running hkube-proxy-ip----.ec.internal running mkube-proxy-ip----.ec.internal running hkube-proxy-ip----.ec.internal running hkubernetes-dashboard-v..-bjnr running hmonitoring-influxdb-grafana-v-wwq running h
143494351,we should support annotations to configure some of the more obscure settings on elbs.for example apparently akamai wants idle timeouts of s and the default is s.we might want to set the default timeout to s if thats generally a good thing but then have an annotation to allow these settings to be tweaked.if these annotations are generally applicable we should consider making them not-aws specific but some of them will be specific to elb
143491973,services should be able to declare that they want a specific dns name.we should define a label/annotation or other way for a service to specify that it wants a dns name and have a controller that creates it.we should have integrations with route presumably the gce equivalent
143092761,secret.yaml apiversion vkind secretmetadata name mysecrettype opaquedata password mwyyzdflmmunrmcg username ywrtawk secret-env-pod.yaml apiversion vkind podmetadata name secret-env-podspec containers name mycontainer image redis env name secret_username valuefrom secretkeyref name mysecret key username name secret_password valuefrom secretkeyref name mysecret key password restartpolicy never run pod and export environment variables. kubectl create f secret.yamlkubectl create f secret-env-pod.yamlkubectl exec it secret-env-pod bashroot@secret-env-pod:/data export environment variables. declare x gosu_version=.declare x home=/rootdeclare x hostname=secret-env-poddeclare x kubernetes_port=tcp://...:declare x kubernetes_port__tcp=tcp://...:declare x kubernetes_port__tcp_addr=...declare x kubernetes_port__tcp_port=declare x kubernetes_port__tcp_proto=tcpdeclare x kubernetes_service_host=...declare x kubernetes_service_port=declare x kubernetes_service_port_https=declare x oldpwddeclare x path=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bindeclare x pwd=/datadeclare x redis_download_sha=ebbeaedbffcffdfaedcdeclare x redis_download_url x redis_version=..declare x secret_password=fdeedfdeclare x secret_username=admindeclare x shlvl
142439246,this is more of an open question should we teach the ingress about tcp?if we fold tcp into ingress everything in the ingress is about entry into the cluster everything in the service is intra cluster ips algorithms affinity etc type=nodeport will continue to be an escape hatch for servicesthis would enable us to leverage the ingress for things like ssl routing and leverage service.type=loadbalancer for internal lb whatever form that takes how to express type=loadbalancer? create an ingress like yamlapiversion extensions/vbetakind ingressmetadata name no-rules-mapspec backend servicename backendsvc serviceport kubectl expose would do this discussion on needs to converge how to express sni yamlapiversion extensions/vbetakind ingressmetadata name rules-mapspec ssl routing rules host www.foo.com tcp backend servicename svc serviceport host foo.bar.com tcp backend servicename svc serviceport if the above spec is defined with a tls section the ingress controller will terminate the connection using sni to figure out which certs to serve discussion on we need to express tcp frontend ports? the challenge here is defining a frontend port that we dont really need for http and are implicit one way is yamlapiversion extensions/vbetakind ingressmetadata name rules-mapspec rules host www.foo.com tcp port backend servicename svc serviceport another option is to disallow port the ingress controller just picks one and writes it to status on gce a port is not needed all that matters is that tcp traffic reaches the backend for legacy apps that need a port they create multiple ingresses with a ingress.port annotation that the ingress controller tries to honor.@kubernetes/goog-cluster kubernetes/rh-networking smarterclayton aledbf thockin therc
141445596,problem descriptionthe download kubectl section mentions the following create configuration kubectl config set-cluster test-doc server kubectl config set-context test-doc cluster=test-doc kubectl config use-context test-doc for max os x users instead of localhost you will have to use ip address of your docker machine which you can find by running docker-machine env machinename see documentation for details).the docker run command doesnt publish any ports though so the above does not work even if localhost is replaced with the correct ip address of the docker machine. kubectl get nodes will then fail with: the connection to the server was refused did you specify the right host or port? _note is the ip of the docker machine that i configured as described hypothesisit looks like the api server port needs to be published workarounda workaround is to keep using localhost in the configuration line above and to then set up an ssh tunnel: docker-machine ssh machine_name n l localhost: (where machine_name is the name of the docker machine being used docker-machine ls can be used to list all running machines).this is sufficient to make kubectl get nodes work as described
140423920,kubelet continuously throws lots of errors: w kubelet.go orphaned volume dad-eb-e-b-cfb/default-token-rse found tearing down volumee kubelet.go could not tear down volume dad-eb-e-b-cfb/default-token-rse rename var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/default-token-rse var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting device or resource busyw kubelet.go orphaned volume dad-eb-e-b-cfb/wrapped_default-token-rse.deleting found tearing down volumew mount.go could not determine device for path var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting~w kubelet.go orphaned volume dad-eb-e-b-cfb/default-token-rse found tearing down volumee kubelet.go could not tear down volume dad-eb-e-b-cfb/default-token-rse rename var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/default-token-rse var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting device or resource busyw kubelet.go orphaned volume dad-eb-e-b-cfb/wrapped_default-token-rse.deleting found tearing down volumew mount.go could not determine device for path var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting~w kubelet.go orphaned volume dad-eb-e-b-cfb/default-token-rse found tearing down volumee kubelet.go could not tear down volume dad-eb-e-b-cfb/default-token-rse rename var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/default-token-rse var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting device or resource busyw kubelet.go orphaned volume dad-eb-e-b-cfb/wrapped_default-token-rse.deleting found tearing down volumew mount.go could not determine device for path var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting~w kubelet.go orphaned volume dad-eb-e-b-cfb/default-token-rse found tearing down volumee kubelet.go could not tear down volume dad-eb-e-b-cfb/default-token-rse rename var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/default-token-rse var/lib/kubelet/pods/dad-eb-e-b-cfb/volumes/kubernetes.io~secret/wrapped_default-token-rse.deleting device or resource busy im starting the cluster this way: docker run name=kube_kubelet volume=/:/rootfs:ro volume=/sys:/sys:ro volume=/var/lib/docker/:/var/lib/docker:rw volume=/var/lib/kubelet/:/var/lib/kubelet:rw volume=/var/run:/var/run:rw net=host pid=host privileged=true d gcr.io/google_containers/hyperkube:v..-beta hyperkube kubelet containerized hostname-override address api-servers cluster-dns cluster-domain=cluster.local allow-privileged=true v config=/etc/kubernetes/manifests exit docker run name=kube_kubeproxy d net=host privileged gcr.io/google_containers/hyperkube:v..-beta hyperkube proxy master v exit
140222110,right now services that setup amazon elbs setup a tcp port listener it would be swell if there was a way to setup an https protocol listener that included specifying an ssl certificate.currently i have to manually go into the aws console and re-configure the elb for each service to use https with our certificates which isnt really manageable because ks occasionally reconfigures them to match the service definition
139778907,using kubectl logs shows the following error: error from server get dial tcp lookup node no such host kubernetes master uses nodename to fetch logs from kubelet which doesnt always resolve there are two workarounds as i can think of manually add host entry in masters etc/hosts but this doesnt scale use hostname override to use raw ip address this is better but raw ip is hard to manage and we see this error master already have the mapping from nodename to ipaddress we can get rid of the problem by just using internal ip
138631210,allow users to specify a subpath in container.volumemounts so they can use a single volume for many mounts instead of creating many volumes for instance a user can now use a single persistentvolume to store the mysql database and the document root of an apache server of a lamp stack pod by mapping them to different subpaths in this single volume.also solves
138558966,"hi,sorry for doing it in the wrong order as mentioned here did to address this problem on kubectl side)the need is to be able to update a daemon set in a single command line.basically it is equivalent to do kubectl delete f dsfile cascade=falsekubectl create f dsfilefor pod in podskubectl delete podwait for delete edit bgrant suggested to update the ds instead of delte recreate"
137938290,to do a rolling update of a configmap the user needs to create a new configmap update a deployment to refer to it and delete the old configmap once no pods are using it this is similar to the orchestration deployment does for replicasets.one solution could be to add a configmap template to deployment and do the management there.another could be to support garbage collection of unused configmaps which is the hard part that would be useful for secrets and maybe other objects also.cc kubernetes/sig-apps-feature-requests
135049089,"hi!id like to report an issue i cant resolve dns names inside a pod when its containers are on the same node with kube-dns containers.im running a kubernetes cluster in vagrant master node minions): vagrant@master kubectl cluster-infokubernetes master is running at is running at kubectl get nodesname labels status ageminion.ks-vagrant-kbelyaev.tld kubernetes.io/hostname=minion.ks-vagrant-kbelyaev.tld ready hminion.ks-vagrant-kbelyaev.tld kubernetes.io/hostname=minion.ks-vagrant-kbelyaev.tld ready h vagrant@master kubectl get rc namespace=kube-systemcontroller container(s image(s selector replicas agekube-dns-v etcd gcr.io/google_containers/etcd ks-app=kube-dns,version=v h kubesky gcr.io/google_containers/kubesky skydns gcr.io/google_containers/skydns:---cfc healthz gcr.io/google_containers/exechealthz:. vagrant@master kubectl get pods namespace=kube-systemname ready status restarts agekube-dns-v-cfrsn running h vagrant@master kubectl get pods namespace=kube-system o yaml grep i ip hostip podip im using flanneld with host-gw backend for overlay network the configuration is the same on both kubernetes nodes minion and minion): vagrant@master etcdctl get coreos.com/network/config network backend type host-gw root@minion vagrant ps aux grep flanneldroot ssl usr/bin/flanneld etcd-endpoints etcd-prefix=/coreos.com/network alsologtostderr=false iface=eth ip-masq=false subnet-file=/run/flannel/subnet.env docker is running with following configuration on both nodes: vagrant@minion ps aux grep dockerroot ssl usr/bin/docker daemon selinux-enabled bip ip-masq=true mtu insecure-registry=kubernetes-registry insecure-registry=registry insecure-registry=registry.ks-vagrant-kbelyaev.tld insecure-registry=...: kube-proxy is running on the iptables proxy mode with the same configuration on both nodes: root@minion vagrant cat etc/kubernetes/proxy kubernetes proxy config default config should be adequate add your own!kube_proxy_args kubeconfig=/etc/kubernetes/proxy.kubeconfig proxy-mode=iptables then im starting two pods: vagrant@master kubectl run nginx image=nginxreplicationcontroller nginx created vagrant@master kubectl run nginx image=nginxreplicationcontroller nginx created vagrant@master kubectl get pod nginx-ohau o yaml grep i ip hostip podip vagrant@master kubectl get pod nginx-rtw o yaml grep i ip hostip podip after i that im checking if kube-dns is working properly.on nginx-ohau pod: vagrant@master kubectl exec nginx-ohau i t bashroot@nginx-ohau cat etc/resolv.confnameserver nameserver search default.svc.cluster.local svc.cluster.local cluster.local machinezone.com ks-vagrant-kbelyaev.tld ks-vagrant-kbelyaev.tldoptions ndots:root@nginx-ohau nslookup ya.ruserver address non-authoritative answer:name ya.ruaddress name ya.ruaddress name ya.ruaddress root@nginx-ohau nslookup kubernetesserver address name kubernetes.default.svc.cluster.localaddress on nginx-rtw pod: vagrant@master kubectl exec nginx-rtw i t bashroot@nginx-rtw cat etc/resolv.confnameserver nameserver search default.svc.cluster.local svc.cluster.local cluster.local machinezone.com ks-vagrant-kbelyaev.tld ks-vagrant-kbelyaev.tldoptions ndots:root@nginx-rtw nslookup ya.ru reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected server address non-authoritative answer:name ya.ruaddress name ya.ruaddress name ya.ruaddress root@nginx-rtw nslookup kubernetes reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected reply from unexpected source expected server address server cant find kubernetes nxdomain as you can see dns is working on one pod and doesnt on the other.ive started an investigation and thats what i found the only difference between these two pods is that they are running on different kubernetes nodes the pod with the dns issue nginx-rtw is on the kubernetes node with kube-dns-v-cfrsn pod.then i look further here is iptables for minion generated by iptables-save v on thu feb nat:prerouting accept input accept output accept postrouting accept docker kube-nodeports kube-sep-izsggwivqqkmyu kube-sep-krkwmjpumwwppt kube-sep-pddzxvecuqrvbz kube-services kube-svc-nsjqifixforg kube-svc-erifxisqepfof kube-svc-tcoujcqxezgvunu a prerouting m comment comment kubernetes service portals j kube-services-a prerouting m addrtype dst-type local j docker-a output m comment comment kubernetes service portals j kube-services-a output d m addrtype dst-type local j docker-a postrouting s o docker j masquerade-a postrouting m comment comment kubernetes service traffic requiring snat m mark mark xd j masquerade-a kube-sep-izsggwivqqkmyu s m comment comment kube-system/kube-dns:dns j mark set-xmark xd/xffffffff-a kube-sep-izsggwivqqkmyu p udp m comment comment kube-system/kube-dns:dns m udp j dnat to-destination a kube-sep-krkwmjpumwwppt s m comment comment kube-system/kube-dns:dns-tcp j mark set-xmark xd/xffffffff-a kube-sep-krkwmjpumwwppt p tcp m comment comment kube-system/kube-dns:dns-tcp m tcp j dnat to-destination a kube-sep-pddzxvecuqrvbz s m comment comment default/kubernetes j mark set-xmark xd/xffffffff-a kube-sep-pddzxvecuqrvbz p tcp m comment comment default/kubernetes m tcp j dnat to-destination a kube-services d p tcp m comment comment default/kubernetes cluster ip m tcp dport j kube-svc-nsjqifixforg-a kube-services d p udp m comment comment kube-system/kube-dns:dns cluster ip m udp dport j kube-svc-tcoujcqxezgvunu-a kube-services d p tcp m comment comment kube-system/kube-dns:dns-tcp cluster ip m tcp dport j kube-svc-erifxisqepfof-a kube-services m comment comment kubernetes service nodeports note this must be the last rule in this chain m addrtype dst-type local j kube-nodeports-a kube-svc-nsjqifixforg m comment comment default/kubernetes j kube-sep-pddzxvecuqrvbz-a kube-svc-erifxisqepfof m comment comment kube-system/kube-dns:dns-tcp j kube-sep-krkwmjpumwwppt-a kube-svc-tcoujcqxezgvunu m comment comment kube-system/kube-dns:dns j kube-sep-izsggwivqqkmyucommit completed on thu feb generated by iptables-save v on thu feb filter:input accept forward accept output accept docker a forward o docker j docker-a forward o docker m conntrack ctstate related,established j accept-a forward i docker o docker j accept-a forward i docker o docker j acceptcommit completed on thu feb here is what happens when im making dns request inside nginx-rtw iptables trace root@minion vagrant tail f var/log/messages grep tracefeb localhost kernel trace raw:prerouting:policy in=docker out mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace nat:prerouting:rule in=docker out mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace nat:kube-services:rule in=docker out mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace nat:kube-svc-tcoujcqxezgvunu:rule in=docker out mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace nat:kube-sep-izsggwivqqkmyu:rule in=docker out mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace filter:forward:rule in=docker out=docker mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace filter:docker:return in=docker out=docker mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace filter:forward:rule in=docker out=docker mac=:::f:a::::ac src dst len tos=x prec=x ttl id df proto=udp spt dpt len=feb localhost kernel trace nat:postrouting:policy in out=docker src dst len tos=x prec=x ttl id df proto=udp spt dpt len=#tcpdump root@minion vagrant tcpdump i docker vvv s l n port grep kubernetestcpdump listening on docker link-type enmb ethernet capture size bytes domain bad udp cksum xdc xf a kubernetes.default.svc.cluster.local domain bad udp cksum x x a kubernetes.default.svc.cluster.local domain bad udp cksum x xea q a kubernetes.default.svc.cluster.local kubernetes.default.svc.cluster.local s a as you can see the response from the dns server is not dnated back to set src to so the response packet is going directly via docker bridge and not getting into iptables preserving pods ip to fix these i added this rule in iptables: iptables t nat i postrouting s d j masquerade thus all packets which is going between containers on one kubernetes node are masqueraded.but im not sure that this fix is a good decision what would you suggest is it bug in kubernetes or maybe i misconfigured kubernetes im sure that this problem conserns all cases when we request kubernetes service inside a pod which containers are on the same node with pod containers kubernetes service is proxying to.thanks,konstantin"
134973257,it would be very use for kubectl to be able to export all api objects in a namespace from a running cluster to json/yaml files and import that state from files to another cluster.this fuctionality can currently be hacked out of kubectl for specific use cases by transforming the output of kubectl get o=json all switching contexts and feeding the fixed-up json back into kubectl create your json transformer have to do things like discard the status sub-objects and the service-token secrets ideally kubectl would do this for you.im envisioning something like this sh#exportkubectl config use-context clusterakubectl namespace=my_application export o=json my_application.clustera.json#importkubectl config use-context clusterbkubectl import my_application.clustera.json
134670771,we already have google.golang.org/grpc/health/grpc_health_valpha in godeps my understanding is that all grpc implementations should be interoperable for health checks and if theyre not its a bug add a new health check type grpc with an optional service name per the official documentation an empty name triggers a check for generic health of the whole server no matter how many grpc services it understands the backend is expected to implement service grpc.health.valpha.health s check method in addition of course to whatever other services it supports to do real work sending health checks through other random protobuf service definitions is not in scope.i can work on this if nobody else steps up this issue is really about getting feedback i assume the only contentious point is whether we want to tie ourselves to the valpha service name
134336468,we have a working version of this that is implemented for the aws cloud provider.if the kubernetes.io/aws-lb-cname-zone label is set we create a cname in route with the format service-name - namespace - hosted-zone . zone well be creating a pr for this soon and wanted to get peoples opinions justinsb
130545938,ive set up a persistent volume using an nfs mount nfs server is managed by me).ive also been able to create a persistentvolumeclaim using the above nfs volume and launch a pod that uses the claim.the issue is that i need to mount a particular subpath of the volume per claim so i wanted to know if theres a way to have multiple claims each having a different subpath mounted from the main persistentvolume in this way different pods can be launched with claims corresponding to only that subpath as required by them and not the entire nfs volume currently ks doesnt accept a path parameter when creating a claim so theres no way to separate out parts of the volume.any thoughts about how this can be done or can a path parameter be added to the persistentvolumeclaim spec to achieve this?the only solution that fits now is to have multiple nfs volumes per cluster which doesnt seem right.use case the nfs server stores state information for a lot of application clusters and is used to restore state in case of crashes need to mount state information of only one application per application pod otherwise pods would have access to state information of other applications as well
127823642,filing random idea the missing piece is hooking up dns which can even be human driven for an ingress like yamlapiversion extensions/vbetakind ingressmetadata name echomapspec tls host foo.bar.com port tlssecret name foosecret rules irrelevant run letsencrypt client get a challenge like provision well-known/acme-challenge at foo.bar.com create the l for program dns for foo.bar.com point urlmap of foo.bar.com:/.well-known/acme-challenge to localhost backend serving file with expected contents delete challenge url from urlmap hook up https backend according to ingress rules keep renewing cert
123756269,if a container is waiting for an image to be pulled before it can start it would be nice to see the progress of that pull in kubectl so that the user can know if they have time for another cup of coffee.an api endpoint to give a progress update possibly with a watch option would be ideal.it would also be helpful to include this in the container information in each pod.for example running kubectl desribe pod/
123344471,authenticating users via x certs is important but the project seems to be missing a mechanism to revoke certs without throwing the entire chain away and regenerating all certs for all users).it would be great to be able to declare which certs are invalid and have the kube-apiserver kubelets and all other cert-dependent services deny service for requests with the now-invalid cert appears to be one way to solve this.fyi this is the same idea as the issue with coreos
120284625,can i connect kube-proxy to multiple api servers like: master
119650314,ideamake it easy to export local files as a limited lifetime volume which pods in a kubernetes cluster can mount use cases interactive developmenti am developing an interpreted program such as a python or node.js program i want to use the editor/ide on my desktop but i want to run the program in a pod because it relies on things that exist in the kubernetes environment and not my desktop e.g dns service account credentials services more memory/cpu etc i want to avoid the step of rebuilding an image and restarting a pod every time i make an edit i want a solution like this i develop my source files locally in my home/src directory i export home/src as remote storage that can be mounted by pods it contains my source code e.g home/src/hello.py i kubectl run a pod that has an image that includes my interpreter like python:latest but not my source code the pod also mounts the exported volume home/src which contains the source code the command line is set in the pod spec to bin/sh so i can run via kubectl attach i use kubectl attach to connect to the pod and then run python src/hello.py in another window i make edits on my local host using my editor then i restart the program in the kubectl attach session porcelain for docker-like experiencepeople use to docker may ask why kubectl has no equivalent to docker run d p name web v webapp training/webapp we could add a local-temporary-volumes local-path>::
119579586,edited to reflect current status since kubernetes is written in go its possible to add support for arm devices like raspberry pi.i have done this its published here it would be great to merge this to mainline ks.this work focuses on making it easier for other architectures too like arm and ppcle todo x upgrade to go go x compile arm binaries on every ci build and for every release x compile arm and ppcle binaries on every ci build and build images for every release x use the gcr.io/google_containers/binary-arch:version names for images discussions x make etcd cross-platform x add a gcr.io flannel image that uses quay.io/coreos/flannel:version for amd and cross-compile for other architectures x merge the pr that pushes docker images including hyperkube when releasing x edit the hyperkube makefile to support arm x make skydns kubesky and exechealthz cross-platform x add a ks-in-docker deployment like that supports multiple architectures x update docker-multinode instructions x update docker guide to support other architectures also x write a multiarch proposal x build dashboard for arm gcr.io/google_containers/kubernetes-dashboard-arm x make pause totally cross-platform and written in c x be able to detect server platform x label nodes with os/arch status of multiarch images:get docker-multinode working gcr.io/google_containers/hyperkube fixed for all gcr.io/google_containers/etcd fixed for all gcr.io/google_containers/flannel fixed for all depends on gcr.io/google_containers/debian-iptables fixed for all gcr.io/google_containers/pause fixed for all get dns working gcr.io/google_containers/kubesky fixed for all gcr.io/google_containers/skydns fixed for all gcr.io/google_containers/exechealthz fixed for all wrapped kubernetes binaries gcr.io/google_containers/kube-proxy fixed for all gcr.io/google_containers/kube-apiserver fixed for all gcr.io/google_containers/kube-controller-manager fixed for all gcr.io/google_containers/kube-scheduler fixed for all gcr.io/google_containers/kubelet doesnt really exist yet i worked with the dashboard team and it has had multiarch support from the first release.im maybe gonna add multiarch support for official heapster too now when it has reached v.on kubernetes-on-arm i already have heapster influxdb and grafana nicely running as a cluster addon things to have in mind all binaries have to be compiled for arm and therefore a common naming structure is important brendandburns has already proposed this as binary-arch:version which is logical also etcd and flannel should be built etcd is already in cluster/images/etcd maybe flannel could be included there also pushed to gcr.io/google_containers/flannel one big question is how they should be compiled it might be possible to compile arm images on a amd machine go is probably going to support qemu cross-compilation doesnt work afaik because e.g hyperkube needs to be built dynamically and one cant build arm docker images on amd also all docker images must be compiled on arm except those from scratch base docker images have to be chosen one deployment method that would fit well for raspberry pis is ks-in-docker im using a ks-in-docker -like setup combined with systemd services ive also written a kube-config script that handles the whole process bugs and patches sometimes things fail randomly on arm example often its easy to patch but there should be a specific place for arm patches goarm values its difference between armv and armv when compiling we will have to set env goarm when we compile the images so e.g raspberry pi is supported outdated tasks use qemu emulation for hyperkube and flannel cross-building makefile s related thought about making a proposal earlier and now when was merged it might be time/cc@dalanlan ks-in-docker interest)@resouer ks-in-docker interest)@guybrush ks-in-docker interest)@davidopp who was involved in roberthbailey who was involved in zmerlynn who was mentioned in dieterreuter interested in raspberry pi things
117874608,users are getting tripped up by pods not being able to schedule due to resource deficiencies it can be hard to know when a pod is pending because it just hasnt started up yet or because the cluster doesnt have room to schedule it helps but isnt that discoverable i tend to try a get on a pod in pending first and only after waiting a while and seeing it stuck in pending do i use describe to realize its a scheduling problem).this is also complicated by system pods being in a namespace that is hidden users forget that those pods exist and count against cluster resources.there are several possible fixes offhand i dont know what would be ideal develop a new pod state other than pending to represent tried to schedule and failed for lack of resources have kubectl get po or kubectl get po o=wide display a column to detail why something is pending perhaps the container.state that is waiting in this case or the most recent event.message create a new kubectl command to more easily describe resources im imagining a kubectl usage that gives an overview of total cluster cpu and mem per node cpu and mem and each pod/containers usage here we would include all pods including system ones this might be useful long term alongside more complex schedulers or when your cluster has enough resources but no single node does diagnosing the no holes large enough problem
117720497,we should see if we can make ipvs do everything we need it should perform even better than iptables a benchmark is in order.notes: root@kubernetes-minion-zi:/home/thockin ipvsadm a t s rrroot@kubernetes-minion-zi:/home/thockin ipvsadm a t m r root@kubernetes-minion-zi:/home/thockin ipvsadm a t m r root@kubernetes-minion-zi:/home/thockin ip addr add dev ethroot@kubernetes-minion-zi:/home/thockin curl hostbroot@kubernetes-minion-zi:/home/thockin curl hostaroot@kubernetes-minion-zi:/home/thockin docker run ti busybox wget qo hostbroot@kubernetes-minion-zi:/home/thockin docker run ti busybox wget qo hosta masq mode is dnat not snat src ip is preserved.we have to assign the vip to some interface in the root ns this is a bit ugly in that ports not exposed by the vip get sent to the host e.g i think we can fix that by adding another catchall for the vip i dont know if there are limits to local ipsnot sure if there is a atomic batch update command but it does handle batch invocation at least.several scheduling policies but rr seems sufficient maybe lc sh seems to give us client affinity.we can configure timeouts.well need to do something for node-ports probably still iptables i think this and the other tricks we pull for load-balancers will be the biggest challenge.@bentheelder busy
117695091,pods are crashing as reported by kubectl get po name ready status restarts ageelasticsearch-attiy crashloopbackoff helasticsearch-lkli crashloopbackoff hfrontend-lqv running hfrontend-ddanc running h but the message isnt clear when trying to exec kubectl exec it elasticsearch-attiy sh error error executing remote command error executing command in container container not found elasticsearch) and the describe gives an even more confusion replicas created and it did find the image kubectl describe po elasticsearch-attiyname elasticsearch-attiynamespace defaultimage(s eu.gcr.io/foo/foo_elasticsearch:latestnode gke-small-cluster-f-node-wke/...labels name=elasticsearchstatus runningreason:message:ip replication controllers elasticsearch replicas created)containers elasticsearch image eu.gcr.io/foo/foo_elasticsearch:latest state waiting reason crashloopbackoff ready false restart count conditions type status ready false events firstseen lastseen count from s wed nov wed nov kubelet gke-small-cluster-f-node-wke spec.containers{elasticsearch pulled successfully pulled image eu.gcr.io/foo/foo_elasticsearch:latest wed nov wed nov kubelet gke-small-cluster-f-node-wke spec.containers{elasticsearch created created with docker id aff wed nov wed nov kubelet gke-small-cluster-f-node-wke spec.containers{elasticsearch started started with docker id aff it would help to have clearer messages to help understand causes of crashes.the exec command could say that the given container isnt running properly on the pod with a status or a command that could describe more in details the reasons).the describe command here could clarify that replicas have been created but arent healthy more importantly i found missing the reason of why the container got restarted i do suspect that my command statement is the cause
116458099,i create rcs with proper names like nginx and my pods end up being named nginx-kl or something so everytime i need the logs of a pod i need to get all pods find this pod copy the name get logs same for exec i feel like kubectl action nginx can do smart things list pods if theres only pod with that prefix use it if there are multiple pods output no pod named nginx do you mean nginx-k nginx-... would be cooler if i could just pick the pod interactively based on an index or specify a regex
116263695,as far as i can tell right now its only possible to create an ingress to address services inside the namespace in which the ingress resides it would be good to be able to address services in any namespace.its possible that im missing something and this is already possible if so itd be great if this was documented
114964284,is it possible to launch a pod without a service account or with a null service account?ive tried creating a serviceaccount and deleting the secret it is recreated ive tried creating a secret without the token line it is recreated not specifying a serviceaccount means the default serviceaccount is used.as far as i can tell every pod created gets full access to the ks api while i know that rbac are the correct long-term solution here i think denying a pod all access to the ks api is a common enough use case that we should be able to support it without telling people to wait for rbac.the best i have found is to create the serviceaccount figure out the randomly named secret download the secret modify the token to be base encoded and then replace the secret but that is very hacky indeed
111005205,we need to be able to define the following options when specifying the pod definition in rc and pod log-driver logging driver for container--log-opt log driver optionsthese options should be settable at container level and have been introduced with docker since docker client lib support both options as well adding those options to the pod definition is now doable
109917861,"in the api docs several of the endpoints allow you to pass a fieldselector parameter to limit the information returned when i run kubectl describe pod pod_name v i see thats used by kubectl to get all the events associated with a pod.i assumed the syntax of fieldselector uses the same syntax as the label selectors as described in the docs however i cant use the set-based syntax like involvedobject.name in pod_,pod_) .in addition there are some fields that apparently cant be used if i try getting pods with a fieldselector of metadata.name foobar i get the error field label not supported metadata.nameit would be great to have some documentation on how to use this parameter and what its constraints are"
108697483,given the state of cloud-lb today most gce aws openstack lb implementations target nodes indiscriminately we should ensure that the cloud-load-balancer is only targetting healthy nodes.we should health-check to the nodes kubelet or kube-proxy or add a new do nothing but answer node health daemon
108190643,users often need a means to inject custom behavior into the lifecycle of a deployment process the deployment api could be expanded to support the execution user-specified docker images which are given an opportunity to complete at various points during the recon process for a deployment.use cases and various design approaches were discussed previously in an openshift deployment hooks proposal rfc is to capture initial thoughts on the topic and to link any existing related issues
105707908,containers are supposed to be immutable i know but sometimes copy is useful...syntax sh copy from a container to localkubectl cp container:/path/to/file path/to/local/file copy from local to containerkubectl cp path/to/local/file container:/path/to/file#copy from container to containerkubectl cp container:/path/to/file container:/path/to/other/file @smarterclayton bgrant kubernetes/kubectl
104846604,when using kubectl exec the terminal dimensions are always set to x instead of matching the clients layout.of course it is possible to insert env columns=$columns lines=$lines before every command but this is not very usable
104387588,kubectl rolling-update is useful for incrementally deploying a new replication controller but if you have an existing replication controller and want to do a rolling restart of all the pods that it manages you are forced to do a no-op update to an rc with a new name and the same spec it would be useful to be able to do a rolling restart without needing to change the rc or to give the rc spec so anyone with access to kubectl could easily initiate a restart without worrying about having the spec locally making sure its the same/up to date etc this could work in a few different ways a new command kubectl rolling-restart that takes an rc name and incrementally deletes all the pods controlled by the rc and allows the rc to recreate them same as but instead of deleting each pod the command iterates through the pods and issues some kind of restart command to each pod incrementally does this exist is this a pattern we prefer the advantage of this one is that the pods wouldnt get unnecessarily rebalanced to other machines kubectl rolling-update with a flag that lets you specify an old rc only and it follows the logic of either or kubectl rolling-update with a flag that lets you specify an old rc only and it auto-generates a new rc based on the old one and proceeds with normal rolling update logic.all of the above options would need the maxsurge and maxunavailable options recently introduced see along with readiness checks along the way to make sure that the restarting is done without taking down all the pods.@nikhiljindal kubernetes/kubectl
104046753,im unable to use kubectl because of tls handshake timeout. kubectl get podserror couldnt read version from server get net/http tls handshake timeout edit after several tries i got one to go through so its not happening of the time but the error has been recorded over k times in the logs.ive also noticed the error is showing up often in var/log/kube-apisever.log from requests from minions. i logs.go http tls handshake error from eofi logs.go http tls handshake error from eofi logs.go http tls handshake error from eofi logs.go http tls handshake error from eofi logs.go http tls handshake error from eofi logs.go http tls handshake error from eof i also noticed a lot of dial tcp connection refused errors in the logs on various endpoints. e reflector.go failed to list api.resourcequota get dial tcp connection refusede reflector.go failed to list api.secret get dial tcp connection refusede reflector.go failed to list api.serviceaccount get dial tcp connection refusede reflector.go failed to list api.limitrange get dial tcp connection refusede reflector.go failed to list api.namespace get dial tcp connection refusede reflector.go failed to list api.namespace get dial tcp connection refused if its relevant the ephemeral filesystem on mnt/ephemeral/kubernetes is on one of my minions most of my kube-system pods kube-ui kube-dns elasticsearch etc are running that minion its full because of elasticsearch and heapster empity-dir volumes and the mount is only gb this filesystem being full caused other problems this weekend including containers from the kube-dns pod shutting down which in turn brought down all of my other production pods over the weekend
100286472,"id like to propose one use case and two issues which i think are a little different from use case a java dev workflow java code git repo app docker image a war nothing more registryweb server docker image a standard tomcat image registry deploy:a pod.yaml with two containers,: apiversion vbetakind podmetadata name appspec containers image tomcat:latest name tomcat volumemounts mountpath home name test-volume image app:latest name war command sh c cp app/myapp.war work volumemounts mountpath work name test-volume volumes name test-volume restartpolicy onfailure have to but i really want to set always for tomcat and onfailture for war it works ok tomcat can load war from work dir but the only concern is that we need to make tomcat container starts after app finish cp so this is about two issues container dependency therere many other cases users want to guarantee container a starts before b in a pod see why not git volume nobody stores everything in a git repo and the file may also be generated by b._why not other type of distributed fs volume we dont want to maintain nfs server or other kinds because we dont want to maintain folders and directories with unique names in case of conflicts.two dependency types: soft just create start the containers defined in pod one by one the order is just the same as it is in listed pod.yaml hard b will wait until a s state become ok its a signal defined by user like running better work with container handler)definitely we can also integrate them into one rule start by order and start the next when meeting a handler defined rule.we should implement soft order at least map in golang is not order promised i believe for the use case above its enough volumes-from yes this feature will make things much easier we dont need use empty_dir anymore a and b should always be in the same pod and thats also volumes-from want to see.we also need to make sure the order of containers which can be exactly the same as container dependency part says.we can use empty_dir to imitate volumes-from see in one wordinstead of container type volume i think the native volumes-from of docker is much better it will solve a tricky problem for all:should i package my war in the image or use it as a volume?thats where pod can play better"
98692811,its premature to start working on this but i wanted to jot down some notes collected from past conversations and experience on this topic and i noticed we didnt have an issue for this yet).a rescheduler is an agent that proactively causes currently-running pods to be moved so as to optimize some objective function for goodness of the layout of pods in the cluster the objective function doesnt have to be expressed mathematically it may just be a collection of ad-hoc rules but in principle there is an objective function implicitly an objective function is described by the schedulers predicate and priority functions it might be triggered to run every n minutes or whenever some event happens that is known to make the objective function worse for example whenever a pod goes pending for a long time.)a rescheduler is useful because without a rescheduler scheduling decisions are only made at the time pods are created but as the cluster layout changes over time free holes are often produced that were not available when a pod was initially scheduled these holes are produced by run-to-completion pods terminating empty nodes being added by a node auto-scaler etc moving already-running pods into these holes may lead to a better cluster layout a rescheduler might not just exploit existing holes but also create holes by evicting pods assuming it knows they can reschedule elsewhere as in free space defragmentation although alluded to above its worth emphasizing that rescheduling is the only way to make use of new nodes added by a cluster auto-scaler unless pods were already pending but even then its likely advantageous to put more than just the previously pending pods on the new nodes.) because rescheduling is disruptive--it causes one or more already-running pods to die when they otherwise wouldnt--a key constraint on rescheduling is that it must be done subject to disruption slos there are a number of ways to specify these slos--a global rate limit across all pods a rate limit across a set of pods defined by some particular label selector a maximum number of pods that can be down at any one time among a set defined by some particular label selector etc these policies are presumably part of the reschedulers configuration.there are a lot of design possibilities for a rescheduler to explain them its easiest to start with the description of a baseline rescheduler and then describe possible modifications the baseline rescheduler only kicks in when there are one or more pending pods for some period of time its objective function is binary completely happy if there are no pending pods and completely unhappy if there are pending pods it does not try to optimize for any other aspect of cluster layout is not a scheduler it simply identifies a node where a pending pod could fit if one or more pods on that node were moved out of the way and then kills those pods to make room for the pending pod which will then be scheduled there by the regular scheduler(s obviously this killing operation must be able to specify dont allow the killed pod to reschedule back to whence it was killed otherwise the killing is pointless of course it should only do this if it is sure the killed pods will be able to reschedule into already-free space in the cluster note that although it is not a scheduler the rescheduler needs to be linked with the predicate functions of the scheduling algorithm(s so that it can know that the pending pod would actually schedule into the hole it has identified once the hole is created and that the evicted pod(s will be able to schedule somewhere else in the cluster.possible variations on this baseline rescheduler are it can kill the pod(s whose space it wants and also schedule the pod that will take that space and reschedule the pod(s that were killed rather than just killing the pod(s whose space it wants and relying on the regular scheduler(s to schedule the pod that will take that space and to reschedule the pod(s that were evicted it can run continuously in the background to optimize general cluster layout instead of just trying to get a pending pod to schedule it can try to move groups of pods instead of using a one-at-a-time greedy approach it can formulate multi-hop plans instead of single-hopa key design question for a rescheduler is how much knowledge it needs about the scheduling policies used by the clusters scheduler(s for the baseline rescheduler it needs to know the predicate functions used by the clusters scheduler(s else it cant know how to create a hole that the pending pod will fit into nor be sure that the evicted pod(s will be able to reschedule elsewhere if it is going to run continuously in the background to optimize cluster layout but is still only going to kill pods then it still needs to know the predicate functions for the reason mentioned above in principle it doesnt need to know the priority functions it could just randomly kill pods and rely on the regular scheduler to put them back in better places however this is a rather inexact approach thus it is useful for the rescheduler to know the priority functions or at least some subset of them so it can be sure that an action it takes will actually improve the cluster layout if it is going to run continuously in the background to optimize cluster layout and is going to act as a scheduler rather than just killing pods then it needs to know the predicate functions and some compatible but not necessarily identical priority functions one example of a case where compatible but not identical might be useful is if the main scheduler(s has a very simple scheduling policy optimized for low scheduling latency and the rescheduler having a more sophisticated/optimal scheduling policy that requires more computation time the main thing to avoid is for the scheduler(s and rescheduler to have incompatible priority functions as this will cause them to fight though it still cant lead to an infinite loop since the scheduler(s only ever touches a pod once).the vast majority of users probably only care about rescheduling for three scenarios redistribute pods onto new nodes added by a cluster auto-scaler move pods around to get a pending pod to schedule move pods around when cpu starvation is detected on a node
97762632,we have an autoscaling group for the minions we should consider enabling auto-scaling based e.g on cpu or a custom metric we publish
95767910,forked from one of the critical features were lacking on the configuration front is template parameterization we should decide on what approach we want to take.working list of requirements it should be schema-respecting not arbitrary textual transformation in order to facilitate schema validation and independent substitution/decoration passes input parameters should be clearly declared such as to facilitate form generation as discussed in and substitution should be independent of reconciliation as per ghodsss comment
95764950,kubectl should support a dry-run option for every mutation to show what it would do without actually doing itwe might need api changes in order to make this high-fidelity for instance if we wanted to know what a pod config would look like after admission control populated defaults wed need to execute most of the creation path
93182017,currently kubectl cluster-info only returns where the kubernetes master is currently running and no other information not apparent how to switch cluster or delete the current cluster kubectl config set cluster sets values and not the actual cluster if you leave the google cloud platform context you cannot switch back out please correct me if im wrong with statements above but this seems like a major problem.i would assumed the relevant information for switching cluster should be here in the documentation here is an earlier discussion on this last year still think it could be more clear where the current state of kubectl is at regarding cluster switching and terminating both via script and terminal
91390161,is it possible to get kubernetes keep its secrets in hashicorp vault
82959398,our current rolling update scheme requires deleting a pod and creating a replacement we should also have rolling in-place update of both containers and pods there are two kinds of in-place update only applies to container update pod stays in place but container doesnt for example change container image without killing pod applies to pod or container update pod stays in place and container stays in place for example resource limit updates iirc we dont implement this yet or at least dont implement it in-place)the motivation is that theres no reason to kill the pod if you dont need to the user may have local data stored in it that they dont want blown away due to update
81576637,"i often find myself wanting to exec commands on single-container pods and since pod names are not stable i use some wrapper script to avoid constantly updating pod names: kubectl exec p get_current_pod_name my_script.sh my get_current_pod_name returns the name of the first pod matching a label and that covers the case when i want to execute the script on any one and only one container.for fetching server logs of different pods/containers for example im currently giving an argument to get_current_pod_name to actually give me the n-th pod from the list of matches but thats kind ugly and no multiplexing.i found this comment by brendanburns on which led me to believe that there might be more people whod like l argument support i would think to do that wed actually want to use label syntax e.g kubectl exec l stage=production,user=bburns or whatever would it be reasonable to add label filtering some way to say run in any one container or run in all containers in this case maybe no support for it flags"
73920994,i have an use-case where i would like to mount google cloud storage gcs bucket and a directory in that bucket in my container and use it as a regular fs currently it seems doable using sfs-fuse thanks brendanburns it would be great if gcs was supported as a first class volume in kubernetes
72195893,there should be a persistent volume plugin that lets a user make a claim on a nodes local storage ensures the scheduler places the pod on a node with the available storage provides host-stickiness so that the pod is restarted on the same nodethe use-case for this is for applications that require local storage for performance reasons.initially we may need to expand this to include the ability to specify performance characteristicsof the storage such as iops io bandwidth etc.this is a use-case that jmccormick needs for a product hes working on did i capture yourrequirements correctly jeff?cc markturansky thockin
71598641,as far as i can see kube-proxy uses round robin to schedule requests to pods within the controller.how about an option to use only pods that are running on the same minion if available that would save a lot of unnecessary network traffic
68696861,as discussed in supporting plain lxc containers or lmctfy lxd requires some or a lot of changes.i am very interested in managing lxc containers with kubernetes and also implementing this but i would like to get some hints what needs to be done/how much work this would be what changes are needed/what components need to be extended what architectural decision needs to be made before the implementation is reasonable what are the minimal changes are there any changes that should/need to be done before a integration is practical
65653665,itd be great if user could diff current configuration with the one in a file i imagine output to be a regular smart diff of json files ignoring version status etc
62792277,it would be nice if the container api payload had support for exposing host devices to the container like docker run device does).the kubelet could pass it go-dockerclient once they add support for it or create container with the docker remote api by passing an addition member in the create hostconfig payload pathonhost dev/devicename pathincontainer dev/devicename cgrouppermissions mrw
55846843,while its possible for ks admin/users to ssh into nodes from cloudprovider it is nicer to provide a ssh subcommnd within kubectl this helps us provide a consistent view of kubernetes cluster so instead of gcloud compute ssh instance zone=us-central-b or ssh i blabla user@ip client can just do: kubectl ssh node nodename where nodename is the canonical node resource name in kubernetes e.g from kubectl get node .one option is to provide pod id so kubectl ssh node nodename p podid will ssh to the host where podid is running
54686145,that this is in we should define how we want to use it
51651113,i want to use kubernetes on my private paas and some of the containers need rate limit(like mbps and bandwidth quota(like gb/month).is it possible to do this using kubernetes
47752827,talked with brendandburns and jbeda about this briefly.being able to submit a job that fires in the ks cluster periodically like chronos would be a good feature to offer something that mirrors the replicationcontroller periodiccontroller was jbeda off the cuff name)json would also be pretty similar in my mind as well mirroring replicationcontrollers with the addition of a timing attribute and possibly a means to notify output.thoughts typing this up quick before i run to a bunch of meetings
46479927,consul is a self checking service discovery and dynamic dns server which runs on all machines in our bare-metal datacenter we use consul to provide fast and reliable dns resolving service mainly for kubernetes pods addresses consul also provides a simple rest k/v storage system which is very similar to etcd and we like to use it instead whole etcd cluster.currently we use consul and etcd together where consul k/v is used by all our services and etcd is only used by kubernetes this forces us to maintain two independent datacenter-wide services which provides almost exactly the same functionality without any additional benefits.both services have very similar rest api and it shouldnt be difficult to provide support for consul
46286820,spawned from it should be easy for users do two things create new resources determine when they are readyreadiness is a complex topic and readiness can mean different things in different contexts the kubernetes client cli and client library see pkg/client/conditions.go should provide tools for common readiness conditions and enable developers and administrators to easily script more complex readiness this issue only covers client side readiness server side readiness should be handled elsewhere.readiness must have an explicit upper bound the system may never converge probably manifested as a maximum timeout certain errors may be transient network server and some fatal resource deleted it should be possible for end users to understand the ways that readiness can fail and work through those conditions.most resources are likely to have an implicit ready state pods when the state is running succeeded or failed depending on the restart policy replicationcontrollers when there are enough pods in running state to satisfy the label query services when at least one pod is running and reachable via the service?however readiness can vary in infinitely complex ways services user must wait for pods to be running for ha or pods must be running in x zones two services must be running and serving requests web frontend and database tier and the backend database must have its schema created and at the latest versionit should be possible for users to define their own client ready conditions via scripting potentially outside of kubectl as long as the tools kubectl provide a common layer for behavior.possible cli examples kubectl create c foo.json wait kubectl create c foo.json wait=m kubectl get pod my-pod wait wait-for=pod-running kubectl get pod my-pod wait wait-for format-template if status.condition running else end things id like to avoid end users doing bash for grep loops on output as much as possible implementing bash timeout logic describing common yet complex conditions replication controller at desired state in template logic
45874490,touched upon in and elsewhere.privileges are currently required to do docker build docker in docker doesnt really work so in order to do this on kubernetes one has to mount the host docker socket into their container which is a big security and abstraction problem
43849821,self explanatory maltej mentioned ipv in and multiple partners have mentioned ipv as an attractive solution for the ks networking model which allocates ip addresses fairly freely for both pods and with ip-per-service services
39792920,this would map closely to dockers native volumes support and allow people to build and version pre-baked data as containers maybe read-only havent thought that far
37950302,i want to start this issue to drum up support for adding socket activation support to kubernetes my cto at the last place i worked at davidstrauss invented this technique.normally for a service to be available its daemon has to be running and listening on a socket with socket activation another service generally systemd listens for all incoming traffic and on an incoming request passes the request to its service waking it if necessary.by idling inactive containers there can be significant savings on memory at pantheon which hosts drupal and wordpress sites at any time some of containers are idled waiting for traffic this means they can proportionally increase the density of containers resulting in significant operations and server cost savings.this is of course not a technique for everyone you need to have services which wont get any use for significant periods of time and you can accept a few second delay from the initial waking time but for many people especially those running multi-tenant architectures this would be a very valuable feature if baked in and easy to use and help kubernetes be even more elastic.some more information on socket activation
