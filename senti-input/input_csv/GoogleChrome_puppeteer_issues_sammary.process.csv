issueId,body
321725258,this patch introduces browser contexts and methods to manage them browser.createincognitobrowsercontext to create new incognito context browser.browsercontexts to get all existing contexts browsercontext.dispose to dispose incognito context.fixes
315664195,i would like to officially open an issue for st class support of running puppeteer in aws lambda i know in the troubleshooting doc it says it is not possible and that there are community work arounds however it would be especially awesome if as a community we could build support for aws lambda and puppeteer right here in this repository if this is not on the roadmap and never will be feel free to close this issue if not than lets put our brains together and brainstorm!!!you guys are the best
313671226,hey after using puppeteer for a little while now in a number of different scenarios ssr automated testing a common piece of functionality that often came up as needed or at very minimum a nice to have was the ability to wait for a specific network request to be made or alternatively for a specific network response to come back as a result i propose the addition of the following two functions. javascriptpage.waitforrequest match options match string a regular expression to match against the request url options object optional parameters method string|array either a single http request method get post etc as a string or an array of values get post defaults to all http request methods timeout number maximum time to wait in millisecondsand additionally javascriptpage.waitforresponse match options ) with a similar set of options id probably also additionally add support for filtering by response code. an example javascriptawait page.waitforrequest https?://example.com/(users|companies method post happy to submit a pr if you all think its a worthwhile addition!thanks
312146778,this patch introduces page.setbypasscsp method that allows clientsto ignore content-security-policy for a given page.fixes
309406293,"im currently encountering some potential rounding issues with the pdf page size where i have to use an additional few ths of a millimetre on the width and height for the pages the page s declaration in my css and also the css height of my section s to render mostly correctly.even after adding the extra mm to cover the issue the width then increases by about mm on either side of the page only when puppeteer is used to generate the pdf but not when printed as pdf directly from my local chrome the removal of the additional mm breaks the layout in both local chrome and puppeteer rendered pdfs.possibly similar issue to tell us about your environment puppeteer version platform os version macos urls if applicable node.js version what steps will reproduce the problem? jsconst puppeteer require(puppeteer);const fs require(fs);const delta closest additional mm to not break layout(async let browser try browser await puppeteer.launch headless true args no-sandbox disable-setuid-sandbox disable-gpu hide-scrollbars disable-web-security const page await browser.newpage await page.goto data:text/html,${fs.readfilesync(index.html).tostring(utf await page.pdf path index.pdf displayheaderfooter false printbackground true pageranges height delta mm additional page margins of mm width delta mm additional page margins of mm margin top right bottom left await browser.close catch err if browser await browser.close throw err node index.js open pdf open html file directly in chrome browser print as pdf compare may have to zoom in to see). what is the expected result? pdf should not have any margin/whitespace on the page edges. what happens instead? there is a horizontal page gap on either side of each page of about mm.should not need to use any additional mm adjustment raw sizes should render as expected"
303932442,step are you in the right place for general technical questions or how to guidance please search stackoverflow for questions tagged puppeteer or create a new post for issues or feature requests related to the devtools protocol file an issue there problem in headless chrome file an issue against chromiums issue tracker issues feature requests or setup troubles with puppeteer file an issue right here steps to reproduce tell us about your environment puppeteer version not installed platform os version winx urls if applicable node.js version what steps will reproduce the problem? i ran npm install save-dev puppeteer and get a error message as follows: error failed to download chromium r set puppeteer_skip_chromium_download env variable to skip download error self signed certificate in certificate chain at tlssocket.
303596069,steps to reproduce puppeteer version platform os version windows version build node.js version v.. what steps will reproduce the problem? first i setup a basic test page like this: jsxconst browser await puppeteer.launch({ignorehttpserrors true})const page await browser.newpage()await page.setcontent( test page )const pdf await page.pdf format letter printbackground true headertemplate headertemplate displayheaderfooter true margin top in right in bottom in left in await browser.close() the headertemplate looks like this: jsxconst headertemplate div style=font-size px display flex flex-direction row justify-content space-between width id=template div class=pagenumber id=num style=font-size px;>page number what is the expected result? the docs dont mention whether or not script tags are supported in the header and footer templates i would expect then that the code above would render a header with a red background and remove the page number node the code above was just for testing purposes). what happens instead? the script appears not to evaluate instead the resulting pdf has no background and a page number div.if evaluating scripts in these templates is not supported might i suggest updating the pdf section of the docs to state as much thanks for all the hard work the puppeteer team has put into this project
303505229,steps to reproduce tell us about your environment puppeteer version puppeteer platform os version manjaro linux with linux node.js version v.. what steps will reproduce the problem? this fails on the example.js from the getting started section: javascriptconst puppeteer require(puppeteer);(async const browser await puppeteer.launch const page await browser.newpage await page.goto await page.screenshot({path example.png await browser.close run node example.js what is the expected result? i believe it should use the namespace sandbox since my kernel is well above what happens instead? heres the console output: (node unhandledpromiserejectionwarning error failed to launch chrome! /.:fatal:zygote_host_impl_linux.cc no usable sandbox update your kernel or see for more information on developing with the suid sandbox if you want to live dangerously and need an immediate workaround you can try using no-sandbox xdeaec base::debug::stacktrace::stacktrace xde logging::logmessage::~logmessage xddcf content::zygotehostimpl::init xddcb content::browsermainloop::earlyinitialization xddcda content::browsermainrunnerimpl::initialize xdf headless::headlesscontentmaindelegate::runprocess xdeb content::runnamedprocesstypemain xdebab content::contentmainrunnerimpl::run xdebdd service_manager::main xdeb content::contentmain xdf headless::(anonymous namespace)::runcontentmain xdae headless::headlessbrowsermain xdebac headless::headlessshellmain xdcdac chromemain xfbfa libc_start_main xdcdaa startreceived signal xdeaec base::debug::stacktrace::stacktrace xdea base::debug::(anonymous namespace)::stackdumpsignalhandler xfbbfdd unknown xfb gi_raise xfbec gi_abort xdeee base::debug::breakdebugger xdebbb logging::logmessage::~logmessage xddcf content::zygotehostimpl::init xddcb content::browsermainloop::earlyinitialization xddcda content::browsermainrunnerimpl::initialize xdf headless::headlesscontentmaindelegate::runprocess xdeb content::runnamedprocesstypemain xdebab content::contentmainrunnerimpl::run xdebdd service_manager::main xdeb content::contentmain xdf headless::(anonymous namespace)::runcontentmain xdae headless::headlessbrowsermain xdebac headless::headlessshellmain xdcdac chromemain xfbfa libc_start_main xdcdaa start r r ffdfab r r r ffdfab r r ffdfac r ffdfa di si ffdfab bp ffdfa bx dx ax cx fb sp ffdfab ip fb efl cgf b erf trp msk cr end of stack trace calling exit core file will not be generated.troubleshooting at onclose home/megadyne/myapp/fixpuppeteer/node_modules/puppeteer/lib/launcher.js at interface.helper.addeventlistener home/megadyne/myapp/fixpuppeteer/node_modules/puppeteer/lib/launcher.js at interface.emit events.js at interface.close readline.js at socket.onend readline.js at socket.emit events.js at endreadablent stream_readable.js at process._tickcallback internal/process/next_tick.js::)(node unhandledpromiserejectionwarning unhandled promise rejection this error originated either by throwing inside of an async function without a catch block or by rejecting a promise which was not handled with catch rejection id node dep deprecationwarning unhandled promise rejections are deprecated in the future promise rejections that are not handled will terminate the node.js process with a non-zero exit code
299534507,these are the breaking changes wed like to have for v set initial viewport size to x make browser.disconnect async sockets dont close instantly make page.evaluate to throw for the non-serializable values e.g objects with circular references refactor response.securitydetails into response.certificate and actually include certificate data consider switching to pipes instead of websocket connection by default consider dropping node support consider renaming waitforselector s visible option into interactable since we also have a real visibility with intersectionobserver consider passing calling frame as the first argument to the function exposed via page.exposefunction
292867350,steps to reproduce tell us about your environment puppeteer version platform os version puppeteer docker urls if applicable node.js version what steps will reproduce the problem? its really hard to know what is causing the issue.im running the test in docker with this command: docker run i rm cap-add=sys_admin memory=g v hostdir:$clientdir web-puppeteer bin/bash clientdir/script.sh with script.sh being just npm i and npm t which runs karma with puppeteer as a browser heres the custom browser config: chromeheadlesscustom base chromeheadless flags no-sandbox disable-setuid-sandbox disable-gpu disable-dev-shm-usage i thought the issue was because of a memory issue so i increased it but it doesnt seem to resolve it what is the expected result? running a karma test suite with puppeteer shouldnt cause any issues what happens instead? chromeheadless crashes with this error: build jan  m error launcher  mchromeheadless crashed.build jan error:gpu_process_transport_factory.cc lost ui shared context
290720740,steps to reproduce puppeteer version puppeteer and puppeteer@..-next platform os version debian urls if applicable node.js version what steps will reproduce the problem? test.js: javascriptconst puppeteer require(puppeteer);(async const url process.argv console.log(url url const args disable-gpu disable-setuid-sandbox force-device-scale-factor ignore-certificate-errors no-sandbox const browser await puppeteer.launch({args const page await browser.newpage const res await page.goto(url console.log(res await browser.close nodejs test.js wait cry what is the expected result nodejs test.js log with response from goto what happens instead nodejs test.js unhandledpromiserejectionwarning unhandled promise rejection rejection id error navigation timeout exceeded ms exceeded(node dep deprecationwarning unhandled promise rejections are deprecated in the future promise rejections that are not handled will terminate the node.js process with a non-zero exit code.^c
290412583,i have a requirement to navigate to a pdf url in headless chrome and then enter a search term but for some reason the pdf never gets loaded or rendered with a screenshot.even attempting to use an external javascript library to embed a pdf doesnt work.chrome headless disable-gpu screenshot headless disable-gpu screenshot there any options via selenium or perhaps with an external pdf viewer that can be used in conjunction with headless chrome to open a pdf url and then enter a search phrase
290108432,i want to change proxy on every repuest without closing and opening the browser again first set proxy here const browser await puppeteer.launch args proxy-server=+proxy so my goal is to change proxy on every request not to have only one proxy per browser.thanks
289823723,tell us about your environment: puppeteer version platform os version linux mint node.js version what steps will reproduce the problem? i have a piece of code i use to generate pdf reports from html documents: javascript const page await browser.newpage await page.goto(file waituntil networkidle timeout const default_options format a printbackground true const filename file-${uuid()}.pdf const final_options object.assign default_options path filename options console.log(final_options await page.pdf(final_options await page.close(); file is a link to a local html file in this file head i load specific fonts that i need for the document.inside of the options i pass the templates for headers and footers: javascriptoptions headertemplate p>footer text
289437175,there are number of issues running pptr/headless in env like docker where resources are limited shall we add disable-dev-shm-usage to the default launch flags so users run into fewer issue
289240651,"hello,recently we talked about this problem in the issues and environment puppeteer version chrome version platform os version aws lambda node.js version case:===================we are using puppeteer on aws lambda we take a screenshot of given html template and upload it to s and use this image for future requestsit handles over million requests each month thats why every process should be atomic and immutable aws lambda has a disk and process limit.) example code: javascriptconst browser await puppeteer.launch args disable-gpu no-sandbox single-process disable-web-security disable-dev-profile });const page await browser.newpage();await page.goto response await page.screenshot type jpeg quality browser.close(); problem===================when we are using example code we got disk error from aws lambda example tmp folder: --t::.z aef-fa-e-bef-fda start stdout total drwx sbx_user jan drwxr-xr-x root root jan rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.-rw sbx_user jan core.headless-chromi.drwx sbx_user jan pki when we investigated these files we understood that it is a core dump we removed these files after the process completed.when we monitored process list we saw zombie processes zombie chrome processes have been growing increasingly we cant kill them aws lambda has a maximum process limit max process thats why we reach the lambda limits ssl var/lang/bin/node max-old-space-size max-semi-space-size max-executable-size expose-gc var/runtime/node_modules/awslambda/index.js z headless-chromi defunct z headless-chromi defunct z headless-chromi defunct z headless-chromi defunct z headless-chromi defunct z headless-chromi defunct z headless-chromi defunct z headless-chromi defunct z sh defunct> we couldnt use dump-init on lambda because lambda already has an init system.how did we fix it very hacky method)===================we used browser.disconnect instead of browser.close we manualy managed chrome processes such as kill . example code: javascriptbrowser.on(disconnected console.log(sleeping ms sleep to eliminate race condition settimeout(function console.log( browser disconnected process id process child_process.exec( kill process error stdout stderr if error console.log( process kill error error console.log( process kill success stdout stdout stderr:${stderr firstly we didnt use this method we only killed the process after browser disconnect we got the following error: error read econnreset at exports._errnoexception util.js at tcp.onread net.js i think it looks like a puppeteer process management problem when we used this method we didnt receive any puppeteer related errors how can we fix it?thanks"
287842999,since now headers and footers with page numbers work i now desperately miss an option to generate a table of contents toc out of the h h headers when generating a pdf file i.e like wkhtmltopdf is doing this the toc should be at the start of the pdf and it should not only be clickable jump to the page but also generate the outline pdf element so that the toc is displayed in the contents view in any viewer although this may sound complicated if this functionality is implemented at the right place it is not that complicated take a look on how wkhtmltopdf is implementing this).before posting here i tried a couple of workarounds to achieve this.some dead ends css target_counter proposed a long time ago and only some specialised tools do it to be honest ive given up to think that it will be implemented in chrome some day reference issue is now find a tool or tools to extract the table of contents from the generated pdf and generate a preface.pdf with the toc witch to merge in-front of the original pdf with i was able to generate a readable and searchable pdf text-file so that theoretically it was possible to find the header in the text file and via reverse search and the added comments find out on wich page it is etc etc...any chance to get this soon?thanksognian
285165099,sup>issue template didnt really apply.i cant find anything in the api docs page about error handling the main type of error thats encountered is a timeout the issue is that theres no documented way to distinguish the type of error received in the catch after awaiting a promise that rejects.the issue is that good code most of the time needs to do conditional catches to avoid silencing unexpected errors there doesnt seem to be a good stable way to do this currently.i tried inspecting some of the errors for these tests the output is logging error and then logging error} . jsawait page.click(doesntexist result assertionerror err_assertion no node found for selector doesntexist at console.assert console.js at page.click private/tmp/temp-puppet-/node_modules/puppeteer/lib/page.js at anonymous at process._tickcallback internal/process/next_tick.js generatedmessage false name assertionerror err_assertion code err_assertion actual false expected true operator generatedmessage false name assertionerror err_assertion code err_assertion actual false expected true operator gives a generic assertionerror with no information about the cause except if you want to parse the error message which seems very unstable as it can change without much notice.the next issue is waitfor : jsawait page.waitfor(doesntexist resulterror waiting failed timeout ms exceeded at timeout.waittask._timeouttimer.settimeout private/tmp/temp-puppet-/node_modules/puppeteer/lib/framemanager.js at ontimeout timers.js at tryontimeout timers.js at timer.listontimeout timers.js::){} again the only way i see to extract information is the error message.---there are a few potential solutions here document the error messages and give an example regex to check them this requires no code changes but makes it clear that the error messages changing would be a breaking change dont change the errors but give official utility functions like puppeteer.istimeouterror(error very little code changes but the api will be stable across versions better than make each error either an error subclass or give it properties e.g type puppeteertimeout selector doesntexist this is about tied with since it gives a readable way to check the error type.thoughts sorry if this has come up before but i couldnt find anything relevant
276223882,crashed targets should be handled gracefully all pending commands should be rejected this includes protocol messages and watchdogs such as navigatorwatcher waittask all subsequent commands to the crashed target should reject right awayneither or happen with puppeteer v
274167754,requests that are fired after the basic request is done are initiated by some source f.e a script or something since chrome does provide this information in the network tab i wonder if you can give it via puppeteer?example:! image
273626789,we support elementhandle.uploadfile for input type=file but we dont support emulating dragging in files onto an arbitrary element
272388071,i am getting the following error: jserror protocol error runtime.callfunctionon cannot find context with specified id undefined at session._onmessage srv/node_modules/puppeteer-edge/lib/connection.js at connection._onmessage srv/node_modules/puppeteer-edge/lib/connection.js at emitone events.js at websocket.emit events.js at receiver._receiver.onmessage srv/node_modules/ws/lib/websocket.js at receiver.datamessage srv/node_modules/ws/lib/receiver.js at receiver.getdata srv/node_modules/ws/lib/receiver.js at receiver.startloop srv/node_modules/ws/lib/receiver.js at receiver.add srv/node_modules/ws/lib/receiver.js at socket._ultron.on srv/node_modules/ws/lib/websocket.js at emitone events.js at socket.emit events.js at addchunk stream_readable.js at readableaddchunk stream_readable.js at socket.readable.push stream_readable.js at tcp.onread net.js::) it is a large codebase and it is unclear whats triggering this error.any guides?on that note there needs to be a better way to throw error without knowing the origin of the error in the code it is impossible to trace down these exceptions
270887248,puppeteer version platform os version centos what steps will reproduce the problem? i try to use javascript function in my code to trigger this event is there any batter way in puppeteer? what is the expected result? i focus the html elementuse mouse.down()then mouse.move(x y to the canvasthe canvas will create a node what happens instead? but elements ondrop is not triggered the node isnt exist
270736774,im trying to use puppeteer to interact with a webpage that requires webgl support when puppeteer launches chromium in headless mode the disable-gpu flag is always used based on that flag is not necessary on linux or macos anymore steps to reproduce tell us about your environment puppeteer version platform os version macos what steps will reproduce the problem execute the below script used node locally). javascriptconst puppeteer require(puppeteer);(async const browser await puppeteer.launch headless false const page await browser.newpage test for webgl support e.g const webgl await page.evaluate const canvas document.createelement(canvas const gl canvas.getcontext(webgl const expgl canvas.getcontext(experimental-webgl return gl gl gl instanceof webglrenderingcontext expgl expgl expgl instanceof webglrenderingcontext console.log(webgl support webgl await browser.close();})(); what is the expected result? webgl support gl true expgl true what happens instead? webgl support gl null expgl null
270053554,a page with csp will break some of puppeteer methods such as page.addscripttag at the very least we should fail gracefully.it would be ideal to have a way to disable csp altogether.reported in
269673629,the documentation show that frames can be associated with extensions but theres no documentation on how to register an extension run a script under one of the extensions environments option page/background page/page action/ect...).it would be awesome to run automated tests for web-extensions developpers
269183034,as of today we have request interception that allows puppeteer to intercept and amend requests before they are sent to the server.similarly to this it would be really handy to have response interception that would allow to intercept and amend responses from server before they are delivered to the browser
269029699,tell us about your environment puppeteer version alpha platform os version macos high sierra urls if applicable): what steps will reproduce the problem? _please include code that reproduces the issue._ javascript start.jsconst puppeteer require(puppeteer)const run async const browser await puppeteer.launch headless false slowmo args disable-infobars const page await browser.newpage await page.goto await page.waitfor await browser.close();}run node start.js what is the expected result window inner size and viewport should be equal all the default px x px what happens instead window inner size is larger than the viewport size! google-puppeteer.png
268388673,steps to reproduce tell us about your environment puppeteer version abbcddadebeaebed platform os version debian stretch nodejs urls if applicable steps will reproduce the problem? just launch this code to see the problem: javascript https.jsuse strict;const puppeteer require(puppeteer);const url const args disable-setuid-sandbox no-sandbox const options args headless true ignorehttpserrors true const browser await puppeteer.launch(options const page await browser.newpage await page.setrequestinterception(true page.on(request request if request.resourcetype image request.abort else request.continue await page.goto(url timeout waituntil load const html await page.content console.log(html await page.close await browser.close();})(); what is the expected result? with the request interception disabled await page.setrequestinterception(true page.on(request request if request.resourcetype image request.abort else request.continue nodejs misc/https_headless.js html>potkwkhady khady roule roule what happens instead nodejs misc/https_headless.js node unhandledpromiserejectionwarning unhandled promise rejection rejection id error navigation timeout exceeded ms exceeded(node dep deprecationwarning unhandled promise rejections are deprecated in the future promise rejections that are not handled will terminate the node.js process with a non-zero exit code.^c
267589912,currently it seems the default behaviour of puppeteer is to follow redirects and return the dom at the end of the chain.how can this be changed when need to stop the behaviour and make the goto to stop after the first redirect and simply return the html from that first page for example
266916675,two-folded issue some emojis arent typed correctly others seem to work randomly and newlines are also ignored so is keyboard.press(enter the solution for newline treatment is to split the text by n and then do line by line with a page.type(#target string.fromcharcode at the end of it steps to reproduce puppeteer version alpha platform os version osx also debian an empty html file only has the following inside it also doesnt work with the meta charset=utf tag added no difference): the code that causes the problem: var sample buffer.from(j+rusbubtbybzdhjlzxqksmfwywgj+hr/cfhug base).tostring(utf);await page.type(#thetarget sample); even without the buffer and decoding if you just use the following it wont work: await page.type(#thetarget tokyo street); what is the expected result? tokyo streetjapan what happens instead? tokyo streetjapan
266402561,steps to reproduce tell us about your environment puppeteer version platform os version fedora urls if applicable steps will reproduce the problem? js const browser await puppeteer.launch headless false args disable-notifications const page await browser.newpage the page needs to be at least px wide so the chat tab doesnt overlap with the posts area that this script needs to click into await page.setviewport({width height console.log(chrome ready const account email foo@bar.com password pwd await page.goto const emailfield await page.$(input name=email await emailfield.click delay await emailfield.type(account.email delay await emailfield.dispose const passwordfield await page.$(input name=pass await passwordfield.click delay await passwordfield.type(account.password delay await passwordfield.dispose const loginbutton await page.$(button name=login await loginbutton.focus await loginbutton.click delay await loginbutton.dispose await page.waitfornavigation waituntil networkidle networkidletimeout what is the expected result? the click goes through and a login is attempted. what happens instead? the click does not go through.i have a hunch that the underlying problem is workaround based on mixing code from and js const elementhandle await page.$(body elementhandle.constructor.prototype.boundingbox async function const box await this.executioncontext().evaluate(element const rect element.getboundingclientrect const x math.max(rect.left const width math.min(rect.right window.innerwidth x const y math.max(rect.top const height math.min(rect.bottom window.innerheight y return x x width width y y height height this return box elementhandle.dispose(); is there any chance you could offer switchable implementations of elementhandle.boundingbox until the dom.getboxmodel issues are straightened out?many thanks for puppeteer heart
266156611,actually the only way to request a page through a post or/and with custom headers is by trapping the request with page.setrequestinterceptionenabled(true as proposed here or by building a custom html page with a form tag and submitting it this solution prevent us to set custom headers).both of these solutions are very hackishit would be better to have a second parameter to page.goto in which we can specify the request params like request.continue method post postdata headers what do you think
263323515,waits for a selector to be present and hidden opposite behavior of the option visible
263118379,tell us about your environment windows puppeteer problem? fullpage screenshots of single page websites will not be displayed correctly.i tried to clip areas scrolling through the page and waitfornavigation similiar problems with navalia chromeless and phantomjs.is there anything else i can do? steps reproduce const puppeteer require(puppeteer);var url page urlvar name thevar reswidth width of screenshotvar resheight async const browser await puppeteer.launch const page await browser.newpage await page.goto(url waituntil load await page.waitfornavigation({waituntil networkidle await page.setviewport({width reswidth height resheight await page.emulatemedia(screen await page.screenshot({path name reswidth jpeg type jpeg fullpage true console.log(screenshot done await browser.close();})(); ! the
262948880,fire browser.closed event when the underlying browser process gets closed crashes fire page.closed event when the page gets closed crashesthis will help to build reliable scripts that restart browser or reopen pages when they crash
260498349,fixes to get some feedback on this before finishing tests and api docs.the shape of the api is similar to tracing this pr introduces page.screencast : jspage.screencast.on(frame frame await page.screencast.start();await page.goto other stuff const frames await page.screencast.stop user can save frames themselvesframes.foreach((frame i fs.writefilesync( frame_${i}.png new buffer(frame base));}); passing a path creates a webm video file when you call stop the solution uses no dependencies only web platform apis the downside is that capturing a recording is real-time meaning longer videos will take more time to complete. jsawait page.screencast.start({path video.webm
260245812,there are certain usecases where it is needed to override default puppeteer launcher flags.there should be a way to do that.suggestion: js get default launcher flags.const myargs puppeteer.defaultargs add/remove arguments as needed.myargs myargs.filter(argsfilter).concat(secretargs launch with exactly these arguments.const browser await puppeteer.launch disabledefaultargs true do not use default args to launch puppeteer args myargs
259968973,currently we dont have a way to find/query nodes with shadow roots users need to traverse shadowroot s themselves.it would be ideal if the dtp supported an option to make this easier but maybe we can provide options to page.$eval to pierce through shadow roots
259467813,screen shot at pm
256413352,as mentioned in and other places)we need a way to wait for page to load all the resources after the page.setcontent .the lifecycle events might help help
255039482,hi!could someone tell me whether theres a possibility to set proxy not only for a chromium instance but also for a page?so the current solution is: const browser await puppeteer.launch args proxy-server desired solution in my case is something like this: const page await browser.newpage args proxy-server with proxy per page theres a possibility to run a single chrome instance but use different proxies depending on page.thanks in advance
254831767,im curious to know what changes there are between running as headless true vs false when i run a login to amazon using headless true i get an error from amazon via the screenshot but when i set headless false i watch it work just fine no error.so im trying to figure out what headless true is doing that is different from when its not headless.thanks to any suggestions
254603973,we need a way to fetch add and remove cookies from browser: jsconst cookies await browser.cookies();await browser.deletecookie(cookies )await browser.addcookie(cookies ); when the is implemented the cookies will belong to the browser context this will nicely explain cookie scoping different pages from the same browser context share cookies whereas pages from different browser contexts dont.and once we have cookies on browser browsercontext we can remove pages cookies api
254136242,paulirish suggested to introduce page.$eval and page.$$eval that will reduce the need for a lot one-line methods.e.g elementhandle.attribute would be replaced with a one-liner: jslet idattribute await page.$eval(div div div.id); so the plan is introduce frame.$eval frame.$$eval page.$eval page.$$eval drop the elementhandle.attribute
253061096,this adds new events pageopened and pageclosed to browser.it also adds browser.pages
253026115,hii use raspbian stretch and i install puppeteer with npm but it did not download chromiumthe os has the latest chromium chromium-browser is already the newest version ubuntu....). but when i run codes i have get error (node unhandledpromiserejectionwarning unhandled promise rejection rejection id assertionerror err_assertion chromium revision is not downloaded run npm install(node dep deprecationwarning unhandled promise rejections are deprecated in the future promise rejections that are not handled will terminate the node.js process with a non-zero exit code. i also test npm install as it says but not workingwhat should i do
252829365,is it planned/wanted to implement xpath selectors?this can be done quite easily but i dont know if its a choice to not handle them?ive implemented two methods in my own code to handle them javascriptasync waitforxpath(selector options polling mutation return this.waitforfunction(selector return null document.evaluate(selector document null xpathresult.first_ordered_node_type null).singlenodevalue options selector);}async xpath(selector const remoteobject await this._rawevaluate(selector return document.evaluate(selector document null xpathresult.first_ordered_node_type null).singlenodevalue selector if remoteobject.subtype node return new elementhandle(this._client remoteobject this._mouse await utils.releaseobject(this._client remoteobject return null;} i can make a pr if youre interested
252505354,i would like to detect if a page is playing some sound is there a way to do that
252072596,would be great to screencast the page
251976103,"if i set page.setrequestinterceptionenabled(true some request headers disappear.consider the following code javascript(async const browser await puppeteer.launch const page await browser.newpage await page.emulate(devices iphone await page.setrequestinterceptionenabled(true page.on(request request request.continue console.log(url request.url console.log(method request.method console.log(headers json.stringify( ...request.headers n await page.goto waituntil networkidle browser.close();})(); as sample the first post request catched url postheaders cache-control,max-age origin referer user-agent,mozilla iphone cpu iphone os like mac os x applewebkit khtml like gecko version mobile/b safari content-type,text/plain;charset=utf- if i uncomment await page.setrequestinterceptionenabled(true and request.continue the same request misses referer header: url postheaders cache-control,max-age origin user-agent,mozilla iphone cpu iphone os like mac os x applewebkit khtml like gecko version mobile/b safari content-type,text/plain;charset=utf- , accept,*/* this also happens for all the other requests that i have notice only referer is missing is this supposed to happen"
251703657,is there a convenient way to clean http cache and application data like service workers cookies etc i couldnt find anything related in the docs
251521233,http authentication could look like this similar to nightmare.js page.authenticate(user pass
251395719,not sure how this would work or if possible at all with how puppeteer works a nice feature would be if the browser emit an event with a new page when a new tab is opened by clicking on a target=_blank or through some other means
251381418,having a multi-element selector using elementhandles would be a very useful feature api should probably work something like the intuitive: jslet elements await page.$$(a array>elements.foreach(el el.click
251299888,puppeteer.launch results in: events.js throw er unhandled error event t::.z error spawn home/app/node_modules/puppeteer/.local-chromium/linux-/chrome-linux/chrome enoent at errnoexception util.js at process.childprocess._handle.onexit internal/child_process.js at onerrornt internal/child_process.js at combinedtickcallback internal/process/next_tick.js at process._tickdomaincallback internal/process/next_tick.js::) i verified home/app/node_modules/puppeteer/.local-chromium/linux-/chrome-linux/chrome exists and tried chmod x.also tried apk add chromium and puppeteer.launch executablepath usr/bin/chromium-browser dumpio true args no-sandbox which results in: /.:warning:dns_config_service_posix.cc failed to read dnsconfig. /.:error:devtools_http_handler.cc error writing devtools active port to file /.:fatal:platform_font_linux.cc check failed typeface could not find any font sans sans /.:error:broker_posix.cc invalid node channel message(node unhandledpromiserejectionwarning unhandled promise rejection rejection id error failed to launch chrome warning:dns_config_service_posix.cc failed to read dnsconfig. /.:error:devtools_http_handler.cc error writing devtools active port to file /.:fatal:platform_font_linux.cc check failed typeface could not find any font sans sans /.:error:broker_posix.cc invalid node channel message--t::.z node dep deprecationwarning unhandled promise rejections are deprecated in the future promise rejections that are not handled will terminate the node.js process with a non-zero exit code
251220418,hi everyone currently i use phantomjs to generate pdf documents from html output because phantomjs isnt really up-to-date id like to switch to headless chrome.the thing that keeps me from making the switch is that its not possible to print the page number on the bottom of every page i found out that its possible to set a header and/or a footer on every page with css. htmlheader display block position fixed top px left px right px;}footer display block position fixed bottom px left px right px;} the only downside is that this only works for static data as its not possible to somehow detect and print the page number in the footer or header phantomjs allows to set a header and footer of a document and print the page number
251218147,sorry for my bad english.i have a problem with authentication on pornhub the login page have an auth form with submit button the button have a click event which makes an ajax request to front/authenticate but the event is not firing and the browser navigates to login? params the form does not have action and method attributes).the submit button works normally if i click it manually in non-headless mode).also i cant access internal variables for example j or loginajax script below inside page.evaluate().my script: javascriptconst puppeteer require(puppeteer);(async const browser await puppeteer.launch headless false});const page await browser.newpage();await page.setviewport({width height await page.goto page.evaluate document.getelementbyid(username).value user document.getelementbyid(password).value pass document.getelementbyid(submit).click does not help await page.click(#submit);})(); internal auth script: javascriptfunction loginajax return j.ajax type post url front/authenticate cache datatype json data j(.js-loginform).serialize success function(n n.premium_redirect_cookie n.redirect document.location.assign(n.redirect j(.signinerror).show().text(n.message j.ajax url premiumredirectcookieurl cache crossdomain xhrfields withcredentials success function n.redirect document.location.assign(n.redirect j(.signinerror).show().text(n.message head.ready(document function var n j(.js-loginsubmit).on(click function(n n.preventdefault loginajax j(input.js-signinusername input.js-signinpassword).on(keydown function(i n i.which loginajax n j(input.js-signinusername input.js-signinpassword).on(keyup function(i n n
250829149,hey there i was wondering if anyone has had success using puppeteer within a serverless environment like aws lambda or google cloud functions i was curious of a few things how would downloading headless chrome work in a serverless environment how long does it roughly take to download and start navigating to a page?thanks
250792946,closes
250748045,problemi see there is an api for starting and stoping a timeline recording await page.tracing.start({path trace.json but i would like to request the ability to take a memory heap snapshot and the ability to write expectations on the snapshot data.my use case is roughly load a page load a component do stuff remove the component force a garbage collection would love a method for this as well take a heap snapshot verify that the component is not retained in memory somehowthis would be amazing to be able to have automated headless memory leak regression testing!maybe something like: var heapsnap await page.heap.snapshot(heap.snapshot);expect(!heapsnap.contains(someclassname)) bonus points for an api for the memory allocation timeline that you could query for the number of objects of a type added removed during the timeline
250720341,question how do i get puppeteer to download a file or make additional http requests and save the response
250689745,running this example code from the readme jsconst puppeteer require(puppeteer);(async const browser await puppeteer.launch();const page await browser.newpage();await page.goto page.screenshot({path example.png});browser.close();})(); i get the following error output: (node unhandledpromiserejectionwarning unhandled promise rejection rejection id error failed to connect to chrome!(node dep deprecationwarning unhandled promise rejections are deprecated in the future promise rejections that are not handled will terminate the node.js process with a non-zero exit code. platform info uname alinux localhost smp preempt thu jul pdt x gnu/linux lsb_release adistributor id debiandescription debian gnu/linux stretch)release codename stretch node versionv cat package.json dependencies puppeteer
250221390,since the hash-only navigation doesnt cause any network requests and doesnt cause load event the following gets stuck: jsconst puppeteer require(puppeteer);(async let browser await puppeteer.launch let page await browser.newpage await page.goto await page.goto stuck here browser.close
243330345,support browser contexts target.createbrowsercontext so to avoid launching multiple instances if one wants a pristine session see proposal and original discussion at viable user scenario might be testing several users logged in simultaneously into the service.we might expose browser contexts as a string literal option to the browser.newpage: jsbrowser.newpage creates a new page in a default browser contextbrowser.newpage context default same as previous callbrowser.newpage context another-context creates a page in another browser context
241113578,there should be an api to access cookies
237368330,while using puppeteer there are still occasions where i want to use the raw protocol.id certainly want to do this against the page and potentially the browser as well.i want to send methods and get responses additionally i want to listen for specific events.not sure of the right api and if it needs to be transactional as to not mixup state
