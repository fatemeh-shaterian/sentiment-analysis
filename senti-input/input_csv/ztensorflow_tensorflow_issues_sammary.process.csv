issueId,issue_number,url,user,like,dislike,laugh,hooray,confused,heart,body
349181771,21518,https://api.github.com/repos/tensorflow/tensorflow/issues/21518,skeydan,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux fedora mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device n/a tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory quadro m g exact command to reproduce bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_packagehi a recent checkout of master hours ago fails for me with error home/key/code/tensorflow/tensorflow/build executing genrule tensorflow:tensorflow_python_api_gen failed exit home/key/anaconda/lib/python./site-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converterstraceback most recent call last file home/key/.cache/bazel/_bazel_key/decdfcfeceffadffb/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py line in module from tensorflow.python.tools.api.generator import doc_srcs file home/key/.cache/bazel/_bazel_key/decdfcfeceffadffb/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py line in module from tensorflow.python import keras file home/key/.cache/bazel/_bazel_key/decdfcfeceffadffb/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py line in module from tensorflow.python.keras import applications file home/key/.cache/bazel/_bazel_key/decdfcfeceffadffb/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py line in module import keras_applicationsmodulenotfounderror no module named keras_applicationstarget tensorflow/tools/pip_package:build_pip_package failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path sinfo processes local.failed build did not complete successfully could you please advise
349060365,21515,https://api.github.com/repos/tensorflow/tensorflow/issues/21515,EdwardLin2014,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu server window mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device tensorflow installed from source or binary binary tensorflow version use command below v..--gbce v..--gce python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version v gpu model and memory nvidia tesla v-sxm-gb nvidia geforce gtx ti gb exact command to reproduce source code logs import tensorflow as tfimport numpy as np define tensorflow wrapperprint(define tensorflow wrapper......)def weight_variable(shape create a bias variable with appropriate initialization initial tf.constant shape=shape return tf.variable(initial)def bias_variable(shape create a bias variable with appropriate initialization initial tf.constant shape=shape return tf.variable(initial)def initialize_variable(shape name with tf.name_scope(name layer with tf.name_scope(weights weights weight_variable(shape with tf.name_scope(biases biases bias_variable( ,shape return weights biasesdef convdlayer(inputs weights biases name act=tf.nn.relu preactivate tf.nn.convd(inputs weights strides padding=valid data_format=nchw dilations name=convd biases return act(preactivate name=activation)def max_pool_x(x name return tf.nn.max_pool(x ksize strides padding=valid data_format=nchw name=name create neural networkprint(create neural network initial trainable parametersconvw,convb initialize_variable( ,,, ,conv)convw,convb initialize_variable( ,,, ,conv)convw,convb initialize_variable( ,,, ,conv)convw,convb initialize_variable( ,,, ,conv)fcw,fcb initialize_variable( ,,, ,fc)fcw,fcb initialize_variable( ,,, ,fc)fcw,fcb initialize_variable( ,,, ,fc placeholder for input and outputx tf.placeholder(tf.float none,,,none name=xinput)y tf.placeholder(tf.float none,,,none name=yinput architectureconv convdlayer(x,convw,convb,conv)conv convdlayer(conv,convw,convb,conv)maxpool max_pool_x(conv,maxpool)conv convdlayer(maxpool,convw,convb,conv)conv convdlayer(conv,convw,convb,conv)maxpool max_pool_x(conv,maxpool)fc convdlayer(maxpool,fcw,fcb,fc)fc convdlayer(fc,fcw,fcb,fc)y convdlayer(fc,fcw,fcb,fc,act=tf.identity define losscrossentropy tf.nn.sigmoid_cross_entropy_with_logits(labels=y logits=y name=crossentropy)lossvalue tf.reduce_mean(crossentropy name=lossvalue define optimizer#optimizer tf.train.adadeltaoptimizer(learning_rate=.,rho=.,epsilon=e-,use_locking=false,name=adadelta)#optimizer tf.train.adagradoptimizer(learning_rate=.,initial_accumulator_value=.,use_locking=false,name=adagrad)#optimizer tf.train.adamoptimizer(learning_rate=.,beta=.,beta=.,epsilon=e-,use_locking=false,name=adam)#optimizer tf.train.momentumoptimizer(learning_rate momentum name=nestrov use_nesterov=true)optimizer tf.train.gradientdescentoptimizer(learning_rate name=gradientdescent)gvs optimizer.compute_gradients(lossvalue)train_step optimizer.apply_gradients(gvs illustrate compute_gradients random issueprint(load x and y input......)data np.load(testgrad.npz)xinput data x yinput data y print(session with tf.session as sess sess.run(tf.global_variables_initializer lossvalue_eval gvs_eval sess.run( lossvalue,gvs feed_dict={x:xinput,y_:yinput sess.run(train_step feed_dict={x:xinput,y_:yinput lossvalue_eval gvs_eval sess.run( lossvalue,gvs feed_dict={x:xinput,y_:yinput sess.close()print(session with tf.session as sess sess.run(tf.global_variables_initializer lossvalue_eval gvs_eval sess.run( lossvalue,gvs feed_dict={x:xinput,y_:yinput sess.run(train_step feed_dict={x:xinput,y_:yinput lossvalue_eval gvs_eval sess.run( lossvalue,gvs feed_dict={x:xinput,y_:yinput sess.close()print(--------------------------------------------------)print(check whether they have the same values in each session)print(before training print(lossvalue r all(lossvalue_eval==lossvalue_eval))check_gvs_bt np.zeros((,))for i in range for j in range check_gvs_bt i j all(gvs_eval i j gvs_eval i j )print(gvs r all(check_gvs_bt))print(after training print(lossvalue r all(lossvalue_eval==lossvalue_eval))check_gvs_at np.zeros((,))for i in range for j in range check_gvs_at i j all(gvs_eval i j gvs_eval i j )print(gvs r all(check_gvs_at))tf.reset_default_graph describe the problemby executing the above code with the fixed training instance and label which you can download from we can see that the gradient value computed by compute_gradients gvs_eval gvs_eval are not always the same in each session this happens regardless which optimizer you use.correct me if i have made some stupid mistake in this code as i have tried simple network architecture for example training y weight x and compute_gradients seems to be working fine"
348929614,21499,https://api.github.com/repos/tensorflow/tensorflow/issues/21499,nmoran,1,0,0,1,0,0,this pull request addresses issues encountered when trying to build tensorflow with cuda support on debian based systems stretch and buster)the current tensorflow build process does not work with the debian maintained cuda packages this is because there are some differences in the paths where the libraries are installed.summary of changes update to configure.py to search cuda_root>/lib/x_-gnu-linux for libcudart in addition to cuda_root>/lib update to bazel configure script to find cupti header and library update to bazel configure script to find nvvm device files
348904395,21491,https://api.github.com/repos/tensorflow/tensorflow/issues/21491,shoyer,2,0,0,0,0,0,system information tensorflow version use command below unknown rc python version arithmetic between objects with different dtypes only works with python scalars not tensors this works tf.constant tf.tensor id shape dtype=float numpy this doesnt tf.constant tf.constant()invalidargumenterror cannot compute mul as input was expected to be a int tensor but is a float tensor op:mul name mul describe the problemnumpy has well defined casting rules for converting between dtypes if an operation cannot be performed natively on the given dtypes e.g np.add between float and float then one or more of the inputs will be promoted to a higher data-type e.g float float.this is highly convenient and remains one of the major annoyances when porting numpy code to tensorflow tensorflow code ends up littered with calls to tf.cast its particularly annoying for any use-cases that require types other than float e.g models that use complex numbers because you cant simply cast every argument to float.an argument for not implementing this would be that tensorflow strives to be more explicit than numpy and this could lead to unintended performance degradation but im struggling to imagine cases where this would actually be the case its also easy to avoid by using explicit casting on everything which could still be utilized by users who concerned about it there are lots of areas where numpy got things wrong by being undisciplined e.g indexing rules but i have not heard any complaints about this one
348509238,21461,https://api.github.com/repos/tensorflow/tensorflow/issues/21461,nmerino,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce bazel run hello_worldinfo build options have changed discarding analysis cache.info analysed target hello_world packages loaded).info found target...target hello_world up-to-date bazel-bin/hello_worldinfo elapsed time s critical path sinfo process local.info build completed successfully total actionsinfo build completed successfully total actionstraceback most recent call last file home/nmerino/.cache/bazel/_bazel_nmerino/daafedbbdceea/execroot/__main__/bazel-out/k-fastbuild/bin/hello_world.runfiles/__main__/hello_world.py line in module import tensorflow as tfimporterror no module named tensorflow describe the problemhey there im trying to import tensorflow in my current project which use bazel as its build tool for some reason bazel cannot find tensorflow even though it is specified in the requirements file as bazel is the building tool for tensorflow and also gaining popularity in the community i would like if you could help me solving this issue._just in case you are wondering im not trying to use tensorflow to infer stuffs rather to serialize data to tfrecords source code logs hello_world.py import tensorflow as tfmsg tf.constant(hello world!)with tf.session as sess print sess.run(msg) workspace git_repository name io_bazel_rules_python remote commit deffaecfbadceaf)load io_bazel_rules_python//python:pip.bzl pip_repositories pip_import)pip_repositories()pip_import name pip requirements requirements.txt)load pip//:requirements.bzl pip_install pip_install)pip_install() build.bazel package(default_visibility visibility:public )load pip//:requirements.bzl requirement)load io_bazel_rules_python//python:python.bzl py_binary py_library)py_binary name hello_world srcs hello_world.py deps requirement(tensorflow requirements.txt tensorflow
348258948,21440,https://api.github.com/repos/tensorflow/tensorflow/issues/21440,Kayoku,1,0,0,0,0,0,"hi,im working on a fcn fully convolutional network and wanted to make it work on mobile the advantage of this type of network is that we can use any dimensions as input the network still work for example an image x or x or x...)tflite allow us to convert a freezed graph pb to tflite format tflite but we need to put a fixed size for input_shape bazel-bin/tensorflow/contrib/lite/toco/toco input_file=frozen.pb input_format=tensorflow_graphdef output_format=tflite output_file=/tmp/test.tflite inference_input_type=float input_arrays=input_image output_arrays=output_image input_shapes=, , ,and this what i would like bazel-bin/tensorflow/contrib/lite/toco/toco input_file=frozen.pb input_format=tensorflow_graphdef output_format=tflite output_file=/tmp/test.tflite inference_input_type=float input_arrays=input_image output_arrays=output_image input_shapes=, none,none ,im wondering if anyone is working on this feature because i think it can be very useful for a lot of people.thank you for your time and sorry if i duplicate any thread i didnt find this feature request"
348023929,21414,https://api.github.com/repos/tensorflow/tensorflow/issues/21414,sauronpy,0,0,1,0,0,0,hi there..my wife got a meticulous problem in the official logo of tensorflow.see the logo t of the logo its have column but in shadow its have column.corroct it.we also accept any gift for that
347266116,21354,https://api.github.com/repos/tensorflow/tensorflow/issues/21354,annemenini,1,0,0,0,0,0,fix issue the following tests passed on mac rebuilding the python package and executing the test case from the issue above with docker and tensorflows ci scripts tensorflow/core:framework_tensor_test tensorflow/core:lib_strings_strcat_test clang-format was used to verify the c coding style.note that it is my first pull request in this project so let me know if anything is missing
347238572,21350,https://api.github.com/repos/tensorflow/tensorflow/issues/21350,wenmengzhou,1,0,0,0,0,0,try to implement throttle_step settings discussed in using throttle_step to trigger evaluation can be more robust to different gpu load
346869480,21326,https://api.github.com/repos/tensorflow/tensorflow/issues/21326,kishore0905,0,0,0,0,1,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu windows mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device :nope tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source) :dont know gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory geforce gtx ti exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflostackoverfloww as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.i am running this neural translation application i made some changes here to store the model as checkpoints so that everytime i dont want to again train the application it works fine in windows gpu system.when i run the same code in cpu system its giving different result i checked the versions and environment everything is same.how could i solve this issue source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
346783173,21320,https://api.github.com/repos/tensorflow/tensorflow/issues/21320,ashwaniag,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu linux ubuntu mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source) :bazel release gcc/compiler version if compiling from source cuda/cudnn version :cuda cudnn gpu model and memory exact command to reproduce :python object_detection/model_main.py alsologtostderr pipeline_config_path=pipeline.config model_dir=.you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.i am trying a object_detection mobilev_model for my own custom dataset it fails with the following exception traceback most recent call last file object_detection/model_main.py line in module tf.app.run file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(argv file object_detection/model_main.py line in main tf.estimator.train_and_evaluate(estimator train_spec eval_specs file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/training.py line in train_and_evaluate return executor.run file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/training.py line in run return self.run_local file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/training.py line in run_local hooks=train_hooks file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train loss self._train_model(input_fn hooks saving_listeners file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model return self._train_model_default(input_fn hooks saving_listeners file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model_default saving_listeners file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_with_estimator_spec loss mon_sess.run( estimator_spec.train_op estimator_spec.loss file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run raise six.reraise(*original_exc_info file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run return self._sess.run(*args kwargs file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run return self._sess.run(*args kwargs file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run run_metadata file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror expected size in but got node slice slice index=dt_int t=dt_float device=/job:localhost/replica:/task:/device:gpu: (unstack zeros stack node loss/unstack recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__loss/unstack tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu: () caused by op uslice defined at file object_detection/model_main.py line in module tf.app.run file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(argv file object_detection/model_main.py line in main tf.estimator.train_and_evaluate(estimator train_spec eval_specs file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/training.py line in train_and_evaluate return executor.run file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/training.py line in run return self.run_local file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/training.py line in run_local hooks=train_hooks file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train loss self._train_model(input_fn hooks saving_listeners file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model return self._train_model_default(input_fn hooks saving_listeners file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model_default features labels model_fn_lib.modekeys.train self.config file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in call_model_fn model_fn_results self._model_fn(features=features kwargs file home/local/york/ashwani.agarwal/models/research/object_detection/model_lib.py line in model_fn unpad_groundtruth_tensors=train_config.unpad_groundtruth_tensors file home/local/york/ashwani.agarwal/models/research/object_detection/model_lib.py line in unstack_batch unpadded_tensor tf.slice(padded_tensor slice_begin slice_size file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/ops/array_ops.py line in slice return gen_array_ops._slice(input begin size name=name file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in slice slice input=input begin=begin size=size name=name file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file home/local/york/ashwani.agarwal/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinvalidargumenterror see above for traceback expected size in but got node slice slice index=dt_int t=dt_float device=/job:localhost/replica:/task:/device:gpu: (unstack zeros stack node loss/unstack recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__loss/unstack tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.this is the pipeline.config model ssd num_classes image_resizer fixed_shape_resizer height width feature_extractor type ssd_mobilenet_v depth_multiplier min_depth conv_hyperparams regularizer l_regularizer weight e initializer truncated_normal_initializer mean stddev activation relu batch_norm decay center true scale true epsilon train true use_depthwise true box_coder faster_rcnn_box_coder y_scale x_scale height_scale width_scale matcher argmax_matcher matched_threshold unmatched_threshold ignore_thresholds false negatives_lower_than_unmatched true force_match_for_each_row true similarity_calculator iou_similarity box_predictor convolutional_box_predictor conv_hyperparams regularizer l_regularizer weight e initializer truncated_normal_initializer mean stddev activation relu batch_norm decay center true scale true epsilon train true min_depth max_depth num_layers_before_predictor use_dropout false dropout_keep_probability kernel_size box_code_size apply_sigmoid_to_scores false use_depthwise true anchor_generator ssd_anchor_generator num_layers min_scale max_scale aspect_ratios aspect_ratios aspect_ratios aspect_ratios aspect_ratios post_processing batch_non_max_suppression score_threshold e iou_threshold max_detections_per_class max_total_detections score_converter sigmoid normalize_loss_by_num_matches true loss localization_loss weighted_smooth_l classification_loss weighted_sigmoid hard_example_miner num_hard_examples iou_threshold loss_type classification max_negatives_per_positive min_negatives_per_image classification_weight localization_weight train_config batch_size data_augmentation_options random_horizontal_flip data_augmentation_options ssd_random_crop keep_checkpoint_every_n_hours optimizer rms_prop_optimizer learning_rate exponential_decay_learning_rate initial_learning_rate decay_steps decay_factor momentum_optimizer_value decay epsilon fine_tune_checkpoint data/model.ckpt num_steps fine_tune_checkpoint_type classification}train_input_reader label_map_path data/avo_labelmap.pbtxt tf_record_input_reader input_path data/ssdv_tfrecord/train_v_.tfrecords eval_config num_visualizations num_examples eval_interval_secs save_graph true use_moving_averages true min_score_threshold visualize_groundtruth_boxes true retain_original_images true}eval_input_reader label_map_path data/avo_labelmap.pbtxt shuffle false num_readers tf_record_input_reader input_path data/ssdv_tfrecord/test_v_.tfrecords
345885585,21251,https://api.github.com/repos/tensorflow/tensorflow/issues/21251,dhingratul,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device n/a tensorflow installed from source or binary pip tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory ti gb dual exact command to reproduce create a so file with custom ops freeze the graph(with custom ops by calling tf.load_op_library at top of the freezing graph function load graph in c status session_->create(graph_def load custom ops by importing so file tf_status status tf_newstatus tf_library lib tf_loadlibrary(libsbnet.so status tf_buffer op_list_buf tf_getoplist(lib); question how do i attach these custom ops with the graph as i still get an error that the custom op is not in the graph?you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
345567662,21225,https://api.github.com/repos/tensorflow/tensorflow/issues/21225,feranick,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu any mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory titan xp exact command to reproduce make sure you have numpy installed run tensorboard a few runtimewarning appearif you have version installed instead no runtimewarnings appear describe the problemrunning tensorboard with numpy gives a few runtimewarning see below all is fine with numpy and below source code logs /usr/lib/python./importlib/_bootstrap.py runtimewarning numpy.dtype size changed may indicate binary incompatibility expected got return f(*args kwds)/usr/lib/python./importlib/_bootstrap.py runtimewarning numpy.dtype size changed may indicate binary incompatibility expected got return f(*args kwds)/usr/lib/python./importlib/_bootstrap.py runtimewarning numpy.dtype size changed may indicate binary incompatibility expected got return f(*args kwds)/usr/lib/python./importlib/_bootstrap.py runtimewarning numpy.dtype size changed may indicate binary incompatibility expected got return f(*args kwds
345402975,21196,https://api.github.com/repos/tensorflow/tensorflow/issues/21196,suryaprakaz,0,0,0,0,1,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu ubuntu lts mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device :doesnt apply tensorflow installed from source or binary) :binary tensorflow version use command below) :v..--gce python version :python bazel version if compiling from source) :doesnt apply gcc/compiler version if compiling from source) :doesnt apply cuda/cudnn version :cuda cudnn gpu model and memory :geforce gtx ti exact command to reproduce describe the problem summarymobilenet v is faster than v only when loading from the checkpoint format i.e variable ops meta index data whereas it is slower than v when running in the frozen graph format const ops pb descriptioni have two tensorflow trained models that are in the checkpoint format meta index data namely mobilenetv_..ckpt and mobilenetv_._.ckpt the model definitions are as described in the mobilenetv and mobilenetv papers both models are trained with a width_multiplier of mobilenetv has an expansion factor of this means that mac wise v is better than v v mil v mil and is expected to be slightly faster than v.to compare how the models actually perform i evaluated them in two waysmethod after training inference by loading the models from checkpointsmethod during deployment inference by loading the models from frozen graphdefs freeze_graph for converting variables to consts) tools used for testing tfs timeline trace tool benchmark_model tool naive python time module timeline tooli used the tensorflows timeline tool to view the execution times in the chrome trace format method mobilenetv trace
344633127,21136,https://api.github.com/repos/tensorflow/tensorflow/issues/21136,meichen91,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device n/a tensorflow installed from source or binary pip tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/ayou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.when using tf.contrib.image.interpolate_spline the input cannot have partially known shape e.g a placeholder i wonder if new features to allow inputs with partially known shape can be added the following is a code snippet to demonstrate the problem thank you for helping source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.import tensorflow as tfx tf.placeholder(tf.float none )y tf.constant tf.float)x tf.constant tf.float)x tf.reshape( x y tf.reshape( y x tf.reshape( x y tf.contrib.image.interpolate_spline(x y x order=)with tf.session as session result session.run(y feed_dict={x print(result
344503398,21130,https://api.github.com/repos/tensorflow/tensorflow/issues/21130,lballes,1,0,0,0,0,0,system informationtensorflow cpu version installed via pip on linux replicated the error on both python and on linux did not try with gpu version.details:have i written custom code noos platform and distribution ubuntu tensorflow installed from piptensorflow version cpu versionbazel version n/acuda/cudnn version n/agpu model and memory n/aexact command to reproduce see code belowmobile device no describe the problem tf.hessians fails on a very simple function that uses tf.reduce_prod the error only occurs at the minimum of the function where the hessian is zero heres a minimal example import numpy as np import tensorflow as tf x tf.placeholder(tf.float shape y tf.reduce_prod(x h tf.hessians(y x with tf.session as sess print(sess.run(h feed_dict={x np.ones print(sess.run(h feed_dict={x np.zeros produces nan nan nan nan nan nan nan nan nan the hessian at x is well-defined and should evaluate to a zero-matrix this behavior is unexpected if tf.hessians cant handle tf.reduce_prod it should raise an exception source code logssee above for minimal example
343334215,21021,https://api.github.com/repos/tensorflow/tensorflow/issues/21021,kbsriram,1,0,0,0,0,0,added gradients and tests for resizebilinear resizebicubic resizenearestneighbornote some of the tests are for the operator itself rather thanthe gradient paralleling existing tests in image_grad.pysee
343208764,20999,https://api.github.com/repos/tensorflow/tensorflow/issues/20999,nairouz,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu google colab mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problemto make it short there is an incompatibility between keras learningratescheduler callback and tf.train.optimizer keras optimizers have some specific attributes which are required for keras callbacks and that does not seem to be the case for tf.train.optimizer.unfortunately keras optimizers are incompatible with the eager execution mode so basically the user is compelled to choose between using keras callbacks and the eager execution mode.here is a code proving that it runs fine with a typical keras optimizer and fails with the tensorflow one source code logs import numpy as npimport tensorflow as tfimport tensorflow.keras as keras from tensorflow.keras.models import modelfrom tensorflow.keras.layers import dense inputfrom tensorflow.keras.callbacks import learningrateschedulerdef step_decay(epoch initial_rate e factor int(epoch lr initial_rate factor return lrlr_schedule learningratescheduler(step_decay)input input(shape name=input)out dense activation=relu)(input)model model(inputs=input outputs=out)model.compile(optimizer tf.train.adamoptimizer(e loss=mse)np.random.seed()x np.random.random astype(np.float)y np.random.random astype(np.float)model.fit(x=x y=y batch_size epochs callbacks= lr_schedule logsepoch valueerrortraceback most recent call last)
342587051,20954,https://api.github.com/repos/tensorflow/tensorflow/issues/20954,solarbear123,1,0,0,0,0,0,python version have i written custom code no os platform and distribution windows tensorflow installed from pip install ignore-installed upgrade tensorflow-gpu tensorflow version gpu bazel version na cuda/cudnn version gpu model and memory geforce gtx m gb exact command to reproduce see belowx tf.placeholder(tf.float none n_inputs )x tf.placeholder(tf.float none n_inputs )basic_cell tf.contrib.rnn.basicrnncell(num_units=n_neurons errs out hereerror message:the procedure entry point?addcleanup@arenalmpl@internal@protobuf@google@@qeaaxxepeaxpax@z@z could not be located in the dynamic link library c:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\contrib\data\_dataset_ops.so! image
342570836,20950,https://api.github.com/repos/tensorflow/tensorflow/issues/20950,Mukundan314,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu arch linux mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device n/a tensorflow installed from source or binary source r and r tensorflow version use command below n/a python version bazel version if compiling from source gcc/compiler version if compiling from source gcc cuda/cudnn version gpu model and memory gtx gb exact command to reproduce bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package describe the problemunable to build tensorflow from source output of bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package warning the following configs were expanded more than once cuda for repeatable flags repeats are counted twice and may lead to unexpected behavior.loading loading packages loadedwarning home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/grpc/build in srcs attribute of cc_library rule grpc//:grpc_nanopb please do not import grpc//third_party/nanopb:pb_common.c directly you should either move the file to this package or depend on an appropriate rule there since this rule was created by the macro grpc_generate_one_off_targets the error might have been caused by the macro implementation in home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/grpc/bazel/grpc_build_system.bzl::warning home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/grpc/build in srcs attribute of cc_library rule grpc//:grpc_nanopb please do not import grpc//third_party/nanopb:pb_decode.c directly you should either move the file to this package or depend on an appropriate rule there since this rule was created by the macro grpc_generate_one_off_targets the error might have been caused by the macro implementation in home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/grpc/bazel/grpc_build_system.bzl::warning home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/grpc/build in srcs attribute of cc_library rule grpc//:grpc_nanopb please do not import grpc//third_party/nanopb:pb_encode.c directly you should either move the file to this package or depend on an appropriate rule there since this rule was created by the macro grpc_generate_one_off_targets the error might have been caused by the macro implementation in home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/grpc/bazel/grpc_build_system.bzl::warning home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/build in py_library rule tensorflow/contrib/learn:learn target tensorflow/contrib/learn:learn depends on deprecated target tensorflow/contrib/session_bundle:exporter no longer supported switch to savedmodel immediately.warning home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/build in py_library rule tensorflow/contrib/learn:learn target tensorflow/contrib/learn:learn depends on deprecated target tensorflow/contrib/session_bundle:gc no longer supported switch to savedmodel immediately.info analysed target tensorflow/tools/pip_package:build_pip_package packages loaded).info found target bazelworkspacestatusaction stable-status.txterror home/mukundan/.cache/bazel/_bazel_mukundan/fdfecbfaaeefedcfd/external/protobuf_archive/build c compilation of rule protobuf_archive//:python/google/protobuf/pyext/_message.so failed exit external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::new(pytypeobject pyobject pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc warning iso c forbids converting a string constant to char wwrite-strings static char kwlist descriptor_db external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findmessagebyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findfilebyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findfieldbyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findextensionbyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findenumtypebyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findoneofbyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findservicebyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findmethodbyname(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc in function pyobject google::protobuf::python::cdescriptor_pool::findfilecontainingsymbol(google::protobuf::python::pydescriptorpool pyobject*):external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc error invalid conversion from const char to char fpermissive charpp pyunicode_asutfandsize(ob sizep null external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc note in expansion of macro pystring_asstringandsize if pystring_asstringandsize(arg name name_size at global scope:ccplus warning unrecognized command line option wno-writable-stringstarget tensorflow/tools/pip_package:build_pip_package failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path sinfo processes.failed build did not complete successfullyfailed build did not complete successfully
342451609,20938,https://api.github.com/repos/tensorflow/tensorflow/issues/20938,crawforc3,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu debian mobile device e.g iphone pixel samsung galaxy if the issue happens on mobile device :n/a tensorflow installed from source or binary) :source tensorflow version use command below rc python version bazel version if compiling from source gcc/compiler version if compiling from source) :gcc version debian debu cuda/cudnn version :n/a gpu model and memory :n/a exact command to reproduce : cd usr/local/src git clone tensorflowcat dev/null configurebazel build config=opt tensorflow/tools/pip_package:build_pip_packagebazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkgpip install tmp/tensorflow_pkg/tensorflow*.whl describe the problembuilding tensorflow from source fails with the error below were trying to update the docker images for over at kaggle source code logs tf.log global scope:ccplus warning unrecognized command line option wno-writable-stringstarget tensorflow/tools/pip_package:build_pip_package failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path sinfo processes local.failed build did not complete successfullyfailed build did not complete successfully
342295673,20922,https://api.github.com/repos/tensorflow/tensorflow/issues/20922,apisarek,7,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :i have written a test that imho should not fail but it fails os platform and distribution e.g linux ubuntu linux mint tensorflow installed from source or binary) : pip install tensorflow in a clean environment tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :please run copyright the tensorflow authors all rights reserved licensed under the apache license version the license you may not use this file except in compliance with the license you may obtain a copy of the license at unless required by applicable law or agreed to in writing software distributed under the license is distributed on an as is basis without warranties or conditions of any kind either express or implied see the license for the specific language governing permissions and limitations under the license tests for slim.data.tfexample_decoder.from future import absolute_importfrom future import divisionfrom future import print_functionimport numpy as npfrom tensorflow.contrib.slim.python.slim.data import tfexample_decoderfrom tensorflow.core.example import example_pbfrom tensorflow.core.example import feature_pbfrom tensorflow.python.framework import constant_opfrom tensorflow.python.framework import dtypesfrom tensorflow.python.ops import array_opsfrom tensorflow.python.ops import control_flow_opsfrom tensorflow.python.ops import image_opsfrom tensorflow.python.ops import lookup_opsfrom tensorflow.python.ops import math_opsfrom tensorflow.python.ops import parsing_opsfrom tensorflow.python.platform import testclass tfexampledecodertest(test.testcase def encodedfloatfeature(self ndarray return feature_pb.feature(float_list=feature_pb.floatlist value=ndarray.flatten().tolist def encodedintfeature(self ndarray return feature_pb.feature(int_list=feature_pb.intlist value=ndarray.flatten().tolist def encodedbytesfeature(self tf_encoded with self.test_session encoded tf_encoded.eval def byteslist(value return feature_pb.byteslist(value= value return feature_pb.feature(bytes_list=byteslist(encoded def bytesfeature(self ndarray values ndarray.flatten().tolist for i in range(len(values values i values i .encode(utf return feature_pb.feature(bytes_list=feature_pb.byteslist(value=values def stringfeature(self value value value.encode(utf return feature_pb.feature(bytes_list=feature_pb.byteslist(value= value def encoder(self image image_format assert image_format in jpeg jpeg png png raw raw if image_format in jpeg jpeg tf_image constant_op.constant(image dtype=dtypes.uint return image_ops.encode_jpeg(tf_image if image_format in png png tf_image constant_op.constant(image dtype=dtypes.uint return image_ops.encode_png(tf_image if image_format in raw raw return constant_op.constant(image.tostring dtype=dtypes.string def generateimage(self image_format image_shape generates an image and an example containing the encoded image args image_format the encoding format of the image image_shape the shape of the image to generate returns image the generated image example a tf-example with a feature key image/encoded set to the serialized image and a feature key image/format set to the image encoding format jpeg jpeg png png raw num_pixels image_shape image_shape image_shape image np.linspace num_pixels num=num_pixels).reshape(image_shape).astype(np.uint tf_encoded self._encoder(image image_format example example_pb.example(features=feature_pb.features(feature image/encoded self._encodedbytesfeature(tf_encoded image/format self._stringfeature(image_format return image example.serializetostring def decodeexample(self serialized_example item_handler image_format decodes the given serialized example with the specified item handler args serialized_example a serialized tf example string item_handler the item handler used to decode the image image_format the image format being decoded returns the decoded image found in the serialized example serialized_example array_ops.reshape(serialized_example shape decoder tfexample_decoder.tfexampledecoder keys_to_features image/encoded parsing_ops.fixedlenfeature dtypes.string default_value image/format parsing_ops.fixedlenfeature dtypes.string default_value=image_format items_to_handlers={image item_handler tf_image decoder.decode(serialized_example image return tf_image def rundecodeexample(self serialized_example item_handler image_format tf_image self.decodeexample(serialized_example item_handler image_format with self.test_session decoded_image tf_image.eval we need to recast them here to avoid some issues with uint return decoded_image.astype(np.float def testdecodeexamplewithpngencodingatbit(self image_shape unused_image serialized_example self.generateimage image_format=png image_shape=image_shape unused_decoded_image self.rundecodeexample serialized_example tfexample_decoder.image(dtype=dtypes.uint image_format=png self.assertallclose(unused_image unused_decoded_image)if name main test.main() it is a modified describe the problemi would like to decode and encode a single channel bit uint png and it is not possible i have modified the tests also to create a np.uint image then trying to decode it but it does not work def testdecodeexamplewithpngencodingatbit(self image_shape unused_image serialized_example self.generateimage image_format=png image_shape=image_shape dtype=np.uint unused_decoded_image self.rundecodeexample serialized_example tfexample_decoder.image(dtype=dtypes.uint image_format=png self.assertallclose(unused_image unused_decoded_image) (modifying the generateimage method accordingly).i understand that it should be possible since decode_png function allows uint as an output type the test fails because the program raises valueerror outputs of true_fn and false_fn must have the same type uint uint i understand that it is due to the fact that tf.cond needs the same output types from branches
342167477,20905,https://api.github.com/repos/tensorflow/tensorflow/issues/20905,martis-chromium,1,0,0,0,0,0,describe the problemwould the tf project be open to supporting a tf lite shared library target as is done already with libtensorflow.so and libtensorflow_cc.so )?i believe many people would benefit from this based on recent related issues am happy to do the upfront work but i would require assistance from tf devs for e.g correct build configuration and on-going support source code logswe could add a tf_cc_shared_object target to tensorflow/contrib/lite/build : tf_cc_shared_object name libtensorflow_lite.so framework_so linkopts tflite_linkopts s strip library visibility visibility:public deps framework tensorflow/contrib/lite/kernels:builtin_ops we could also add a header-only target to define the headers for use with libtensorflow_lite.so although this wont be useful until issue has been resolved).as mentioned above id need some input to decide the correct build config e.g linkopts use of framework_so
341742442,20865,https://api.github.com/repos/tensorflow/tensorflow/issues/20865,steathy,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary pip tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :when importing tensorflow it fails and asks for cuda dlltraceback most recent call last file stdin line in module file d:\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import pywrap_tensorflow pylint disable=unused-import file d:\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file d:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module self_check.preload_check file d:\python\lib\site-packages\tensorflow\python\platform\self_check.py line in preload_check build_info.cudart_dll_name build_info.cuda_version_number))importerror could not find cudart_.dll tensorflow requires that this dll be installed in a directory that is named in your path environment variable download and install cuda from this url
341603329,20847,https://api.github.com/repos/tensorflow/tensorflow/issues/20847,aselle,1,0,0,0,0,0,fixes
341422745,20831,https://api.github.com/repos/tensorflow/tensorflow/issues/20831,fanshiqing,3,0,1,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu cent os tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source na gcc/compiler version if compiling from source na cuda/cudnn version na gpu model and memory na exact command to reproduce na describe the problemcurrently tf.estimator.train_and_evaluate makes it easy to use an estimator to perform both training and evaluation possibly in a distributed environment in my case evaluator have to read from chief workers model_dir do some extra processing generate new model_dir finally concrete evaluation is based on the new model_dir however train_and_evaluate only supports one estimator with single model_dir i.e chief worker and evaluator shares the same checkpoint path which makes above requirements imcompatible it would be ideal if we could perhaps pass more than one model_dir to train_and_evaluate to handle the above scenes.or maybe there are some workarounds to deal with this problem
341378944,20825,https://api.github.com/repos/tensorflow/tensorflow/issues/20825,longchr123,1,0,0,0,0,0,we can get mobilenet tflite&pb(mobilenet from i use the command:bazel-bin/tensorflow/python/tools/freeze_graph input_graph=/path/to/mobilenet_v_.__frozen.pb input_checkpoint=/path/to/mobilenet_v_._.ckpt output_graph=/path/to/freezed_mobilenet.pb output_node_names=mobilenetv/predictions/reshape input_binary=trueerror occured:/home/leve/anaconda/lib/python./site-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use sse sse avx avx fma i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx gb major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc adding visible gpu devices i tensorflow/core/common_runtime/gpu/gpu_device.cc device interconnect streamexecutor with strength edge matrix i tensorflow/core/common_runtime/gpu/gpu_device.cc i tensorflow/core/common_runtime/gpu/gpu_device.cc n i tensorflow/core/common_runtime/gpu/gpu_device.cc created tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name geforce gtx gb pci bus id compute capability traceback most recent call last file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py line in module run_main file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py line in run_main app.run(main=my_main argv= sys.argv unparsed file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py line in run sys.exit(main(argv file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py line in lambda my_main lambda unused_args main(unused_args flags file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py line in main flags.saved_model_tags checkpoint_version file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py line in freeze_graph checkpoint_version=checkpoint_version file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py line in freeze_graph_with_def_protos var_list=var_list write_version=checkpoint_version file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py line in init self.build file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py line in build self._build(self._filename build_save=true build_restore=true file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py line in build build_save=build_save build_restore=build_restore file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py line in build_internal saveables self._validateandsliceinputs(names_to_saveables file home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py line in validateandsliceinputs variable) typeerror names_to_saveables must be a dict mapping string names to tensors/variables not a variable tensor(mobilenetv/convd_/batchnorm/beta shape dtype=float system information linux ubuntu tensorflow installed from source tensorflow version python version bazel version gcc/compiler version ubuntu ubuntu cuda/cudnn gpu g exact command to reproduce
341310357,20816,https://api.github.com/repos/tensorflow/tensorflow/issues/20816,phizaz,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below and python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problemin short tfe.variable should support: a tfe.variable(.)float(a) it is useful in my case but to state my case it would be very convoluted if contributors deem appropriate then plaese add a support to this
341287710,20811,https://api.github.com/repos/tensorflow/tensorflow/issues/20811,sadatnfs,1,0,0,0,0,0,hello it seems like since commit specific pr here building tf with mkl option forces the configure step to look for a license file inside the third_party/mkl_dnn folder which actually doesnt exist currently im using this hack in my docker for the build to proceed without needing the explicit dnn license file by copying the mkl license file into the mkl_dnn folder since they are identical licenses but itd be great if we could have this fixed sometime i opened a small pr with this small fix copied the license file from the original mkl-dnn repo in identical to mkl license but just for pedantic reasons so maybe this will help other users out as well environment info:have i written custom code noos platform and distribution tested on debian:stretchtensorflow installed from sourcetensorflow version master branch)bazel version cuda/cudnn version n/agpu model and memory n/aexact command to reproduce d opt git clone cd opt/tensorflow cp opt/tensorflow/third_party/mkl/license opt/tensorflow/third_party/mkl_dnn/license bin/bash configure bazel build config=opt config=mkl tensorflow/tools/pip_package:build_pip_package thanks!nafis
341287242,20809,https://api.github.com/repos/tensorflow/tensorflow/issues/20809,Mostafa-Alaa,1,0,0,0,0,0,"this pr fixes a bug when using batchnormalization with renorm set and the input tensor x data type is float or bfloat.a simple code to exploit the bug pythonimport tensorflow as tffrom tensorflow.keras.layers import batchnormalizationx tf.placeholder(tf.float x)renorm_clipping rmin tf.constant tf.float rmin rmax tf.constant tf.float rmax dmax tf.constant tf.float dmax),}batchnormalization(renorm=true renorm_clipping=renorm_clipping)(x true"
341216502,20796,https://api.github.com/repos/tensorflow/tensorflow/issues/20796,jock4319,1,0,0,0,0,0,running automatic_differentiation.ipynb on colab and theres a error. def f(x y output for i in range(y output tf.multiply(output x return outputdef g(x y return the gradient of f with respect to its first parameter return tfe.gradients_function(f)(x y) assert f numpy f(x is essentially x xassert g numpy and its gradient will be xassert f numpy f(x is essentially x x xassert g numpy and its gradient will be x x ---------------------------------------------------------------------------typeerror traceback most recent call last)
341008567,20779,https://api.github.com/repos/tensorflow/tensorflow/issues/20779,jasonachonu,1,0,0,0,0,0,hi system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos hugh sierra tensorflow installed from source or binary i did pip install tensor flow tensorflow version use command below v..--gce python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :please find below the error am having.building the networktraceback most recent call last file src/neirbilstm-occupancy.py line in module trainoccupancy.run file users/jasonachonu/git/seqhub_summer/src/nblib/trainoccupancy.py line in run config.cost_type file users/jasonachonu/git/seqhub_summer/src/nblib/network.py line in train_net from tensorflow.contrib import rnn file anaconda/lib/python./site-packages/tensorflow/contrib/__init__.py line in module from tensorflow.contrib import distributions file anaconda/lib/python./site-packages/tensorflow/contrib/distributions/__init__.py line in module from tensorflow.contrib.distributions.python.ops.estimator import file anaconda/lib/python./site-packages/tensorflow/contrib/distributions/python/ops/estimator.py line in module from tensorflow.contrib.learn.python.learn.estimators.head import compute_weighted_loss file anaconda/lib/python./site-packages/tensorflow/contrib/learn/__init__.py line in module from tensorflow.contrib.learn.python.learn import file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/__init__.py line in module from tensorflow.contrib.learn.python.learn import file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/__init__.py line in module from tensorflow.contrib.learn.python.learn import estimators file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py line in module from tensorflow.contrib.learn.python.learn.estimators.dnn import dnnclassifier file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py line in module from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py line in module from tensorflow.contrib.learn.python.learn.estimators import estimator file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in module from tensorflow.contrib.learn.python.learn.learn_io import data_feeder file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py line in module from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data file anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py line in module import dask.dataframe as dd file anaconda/lib/python./site-packages/dask/dataframe/__init__.py line in module from rolling import rolling_count rolling_sum rolling_mean rolling_median file anaconda/lib/python./site-packages/dask/dataframe/rolling.py line in module rolling_count wrap_rolling(pd.rolling_count count)attributeerror module pandas has no attribute rolling_count
340987258,20778,https://api.github.com/repos/tensorflow/tensorflow/issues/20778,RyenAng,11,0,0,0,0,0,windows cuda cudnn python.(anaconda.)install the tensorflow pip install upgrade tensorflow-gpu when i import the tensorflow in ipyhon from tensorflow.python.keras._impl.keras.backend import absimporterror cannot import name abs ! image
340919929,20773,https://api.github.com/repos/tensorflow/tensorflow/issues/20773,NaxAlpha,0,1,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows latest tensorflow installed from source or binary binary tensorflow version use command below r v..--gce python version python anaconda inc msc v bit amd on win bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version cuda cudnn gpu model and memory gtx gb(laptop exact command to reproduce n/a describe the problemwhile developing a prime number generator using tensorflow-gpu i came across a wired bug given the source code if you use tf.print result is correct and run-time is less than without using tf.print source code logs python check first n numbers for primity using tensorflowimport warningswith warnings.catch_warnings warnings.filterwarnings(ignore,category=futurewarning import hpyimport osos.environ tf_cpp_min_log_level import tensorflow as tfimport numpy as npimport timedef prime_tf_graf(max_count,dump ttyp tf.int data tf.range max_count dtype=ttyp if dump data tf.print(data data data loaded x y tf.meshgrid(data data temp y%x mask tf.cast(tf.equal(temp y dtype=ttyp temp temp y mask temp tf.cast(tf.not_equal(temp dtype=ttyp sumr tf.reduce_sum(temp axis nums data rato tf.cast(sumr/nums ttyp indx tf.cast(tf.not_equal(rato ttyp shap tf.reshape(tf.where(indx return shapdef prime_tf(max_count,dump with tf.device(/gpu graf prime_tf_graf(max_count dump sess tf.session(config=tf.configproto(allow_soft_placement=true sess.run(tf.global_variables_initializer prim sess.run(graf return primdef bench(task now time.time res task return res time.time nowp t bench(lambda prime_tf true))print(with dump found d primes in f time%(len(p t))p t bench(lambda prime_tf false))print(without dump found d primes in f time%(len(p t)) and here is output on my system: ps e:\research\tflearn python bug.pydata loaded with dump found primes in timewithout dump found primes in time"
340880836,20766,https://api.github.com/repos/tensorflow/tensorflow/issues/20766,MatthiasWinkelmann,1,0,0,0,0,0,reserved word async async became a reserved word in python i replaced with is_async which was already used for such flags in other places an alternative suggestion if the is construct is deemed too ugly might be to use the full term asynchronous python c api changespython c api now returns const types for pyunicode_asutfandsize and similar fixed analogous to
340797703,20751,https://api.github.com/repos/tensorflow/tensorflow/issues/20751,martin-gorner,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/aif you print a tensor of shape n with tf.print by default summarize is the default value you get which wrongly looks like your tensor is of shape n the correct output should be here is what you get with tf.print(summarize now the vectors of size are visible although the last one is still wrong looks like a vector of size
340602120,20733,https://api.github.com/repos/tensorflow/tensorflow/issues/20733,JulietAlphaKilo,2,0,0,0,0,0,this is the suggested fix for the missing numhyperthreadspercore definition which is still present in release
340305663,20701,https://api.github.com/repos/tensorflow/tensorflow/issues/20701,jjallaire,1,0,0,0,0,0,hi there i am the maintainer of the r interface to tensorflow we are currently in the process of porting various eager examples to r we havent had trouble with python versions of tensorflow but with python versions we get some strange errors i realize that this is within the r interface so technically falls outside of the scope of tf for python however in order for us to address this we need some insight as to what might be different for eager under python ill provide a detailed repro and explanation of its under the hood behavior below.cc martinwicke random-forests system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below tensorflow v..-dev python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see below describe the problemusing the r interface to tensorflow: rlibrary(tensorflow)tf$enable_eager_execution()x tf$constant()tf$add(x x) results in this error: systemerror built-in function tfe_py_fastpathexecute returned a result with an error set this error occurs within the definition of add within gen_math_ops.py : python result pywrap_tensorflow.tfe_py_fastpathexecute ctx._context_handle ctx._eager_context.device_name add name ctx._post_execution_callbacks x y) this code works as expected under tf w python again i realize that this is the r interface so you might not have an intuition about what could be wrong you can think of the r interface conceptually as just using the c python api to invoke functions so in the above code we are essentially using pyimport_import to import the tensorflow module pyobject_callfunctionobjargs to call python functions e.g tf.enable_eager_execution tf.constant etc.)my theory is that under python there is something being done at the python language level that we arent emulating or capture when calling through the python c interface hopefully this provides you with some clues as to what that might be and we will be able to make whatever changes are required to make this work within r
340247849,20698,https://api.github.com/repos/tensorflow/tensorflow/issues/20698,lgeiger,2,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos and debian gnu/linux stretch tensorflow installed from source or binary binary tensorflow version use command below v..-rc--gcfdbd dev also reproduces on v python version and bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version none gpu model and memory none exact command to reproduce see belowyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problem tf.keras multi input models dont work when used together with tf.data.dataset due to input broken validation checks this problem reproduces both on tf and the latest nightly.@fchollet do you have any ideas whats going on here or am i missing something obvious source code logs multi input modelconsider the following toy model: pythonimport numpy as npimport tensorflow as tffrom tensorflow import kerasdata_a np.array dtype=np.float)labels np.array dtype=np.float)data_b np.array dtype=np.float)data_a np.reshape(data_a data_b np.reshape(data_b x keras.layers.input(shape name=input_x)y keras.layers.input(shape name=input_y)admi keras.layers.lstm return_sequences=false)(x)pla keras.layers.lstm return_sequences=false)(y)out keras.layers.concatenate( admi pla axis=-)output keras.layers.dense activation=sigmoid)(out)model keras.models.model(inputs= x y outputs=output)model.compile(optimizer=adam loss=binary_crossentropy metrics= accuracy using numpy datawhen fitting using numpy data this works as expected when passing a list or dictionary of inputs: pythonmodel.fit( data_a data_b labels batch_size epochs=)model.fit({input_x data_a input_y data_b labels batch_size epochs using tf.data.dataset.from_tensor_slices dictionarywhen trying the same with a tf.data.dataset the following fails due to incorrect input validation: pythondataset tf.data.dataset.from_tensor_slices(({input_x data_a input_y data_b labels)).batch().repeat()model.fit(dataset epochs steps_per_epoch=) python-traceback---------------------------------------------------------------------------valueerror traceback most recent call last)
340244614,20695,https://api.github.com/repos/tensorflow/tensorflow/issues/20695,ppaquette,10,0,0,0,0,0,the following wheel files are missing on pypi:tensorflow tensorflow-..-cp-cpm-manylinux_x_.whltensorflow_gpu tensorflow_gpu-..-cp-cpm-manylinux_x_.whl
340176131,20690,https://api.github.com/repos/tensorflow/tensorflow/issues/20690,tiehexue,14,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu macos high sierra tensorflow installed from source or binary) :pip tensorflow version use command below python version exact command to reproduce :import tensorflow as tf describe the problemwhen import tensorflow in python prompt it says syntaxerror invalid syntax.traceback most recent call last file stdin line in module file usr/local/lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import pywrap_tensorflow pylint disable=unused-import file usr/local/lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file usr/local/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file usr/local/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line def tfe_contextoptionssetasync(arg async syntaxerror invalid syntax source code logsline of pywrap_tensorflow_internal.py has async as parameter which seems to be a keyword.after changed to async importing tensorflow works.def tfe_contextoptionssetasync(arg async return pywrap_tensorflow_internal.tfe_contextoptionssetasync(arg async)tfe_contextoptionssetasync pywrap_tensorflow_internal.tfe_contextoptionssetasync
340096624,20684,https://api.github.com/repos/tensorflow/tensorflow/issues/20684,kanul,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :cuda gpu model and memory :geforce gtx mib phone :xiaomi snapdragon exact command to reproduce :bazel run config=opt tensorflow/contrib/lite/toco:toco input_file=/external_home/data/model/deeplabv_mnv_pascal_train_aug/frozen_inference_graph.pb--output_file=/external_home/data/model/deeplabv_mnv_pascal_train_aug/kanul.tflite--inference_type=quantized_uint--input_shape=,,,--input_array=sub_--output_array=resizebilinear_ describe the problem i have tried to quantize mobilenetv for deeplabv with tflite but i fail to convert the model.from the following issue i saw that the operations were not supported for the option of quantization name mobilenetv_coco_voc_trainaugas we can see graphs from the tensorboard there is one big problem.in import/mobilenetv/expanded_conv_/depthwise/depthwise,the operation of depthwise consists of the subgraph with nodes depthwise and batchtospacend spacetobatchnd.but in import/mobilenetv/expanded_conv_/depthwise/depthwise,the operation of depthwise is depthwiseconvdnative itself.from the difference we can not quantize deeplabv based on mobilenetv.the one thing is that mobilenetv/expanded_conv does not have min/max value to be needed for quantization with tflite.although i implement the needed min/max value in hardcode_min_max.cc,this model does not run well in mobile environments.the ultimate problem is caused by the fact that depthwise_conv consist of nodes with batchtospacend and spacetobatchnd.i request you to notify the method to resolve above issues. source code logs bazel run config=opt tensorflow/contrib/lite/toco:toco input_file=/external_home/data/model/deeplabv_mnv_pascal_train_aug/frozen_inference_graph.pb--output_file=/external_home/data/model/deeplabv_mnv_pascal_train_aug/kanul.tflite--inference_type=quantized_uint--input_shape=,,,--input_array=sub_--output_array=resizebilinear i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before pre-quantization graph transformations operators arrays quantized w tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc tweaking the minmax of array resizebilinear which is an input to concatenation operator with output concat because we want all inputs and outputs of a concatenation operator to have the same minmax so that it can be implemented as a pure byte-copy no arithmetic i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc after pre-quantization graph transformations pass operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before quantization graph transformations operators arrays quantized w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/depthwise_weights lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_mul__param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy w tensorflow/contrib/lite/toco/graph_transformations/quantize.cc constant array mobilenetv/expanded_conv_/depthwise/batchnorm/fusedbatchnorm_add_param lacks minmax information to make up for that we will now compute the minmax from actual array elements that will result in quantization parameters that probably do not match whichever arithmetic was used during training and thus will probably be a cause of poor inference accuracy"
339817341,20669,https://api.github.com/repos/tensorflow/tensorflow/issues/20669,fifothekid,1,0,0,0,0,0,system information have i written custom code no os platform and distribution windows bit tensorflow installed from latest master source tensorflow version commit dfceccdbebaaef python version cmake version rc ms c compiler version cpu model and memory i with gb of ram exact command to reproduce opening developer command line as admin choosing the bit compilerc:\program files x)\microsoft visual studio\\enterprise\common\tools\vsdevcmd\ext\vcvars.bat amd cd d:\opencv\tensorflow\tensorflow\contrib\cmake\build cmake a x t host=x dcmake_build_type=release dswig_executable=d:/opencv/swigwin-../swig.exe dpython_executable=c:/users/fifo/appdata/local/programs/python/python/python.exe dpython_libraries=c:/users/fifo/appdata/local/programs/python/python/libs/python.lib dtensorflow_win_cpu_simd_options=/arch:avx dtensorflow_build_cc_tests=off dtensorflow_build_python_tests=off dtensorflow_build_more_python_tests=off dtensorflow_build_cc_example=on dtensorflow_build_python_bindings=on dtensorflow_build_cc_tests=off dtensorflow_optimize_for_native_arch=on dtensorflow_enable_mkl_support=on dtensorflow_enable_mkldnn_support=on dtensorflow_verbose=on dtensorflow_build_shared_lib=on msbuild p:configuration=release all_build.vcxprojsuccess msbuild p:configuration=release install.vcxprojsuccess msbuild p:configuration=release tf_python_build_pip_package.vcxprojfails with generating init__.py files for python api traceback most recent call last file d:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\fifo\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removed importerror dll load failed no foi possvel encontrar o mdulo especificado during handling of the above exception another exception occurred traceback most recent call last file d:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file d:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file d:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\fifo\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level modulenotfounderror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.it used to work before with version i dont want to install the pip versioncompilation works without any problem on ubuntu
339403858,20644,https://api.github.com/repos/tensorflow/tensorflow/issues/20644,jeffpollock9,2,0,0,0,0,0,have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu n/a tensorflow installed from source or binary n/a tensorflow version use command below python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/ai was wondering if there is any appetite for a robbins-monro type learning rate decay in tensorflow the decay would be roughly a more general solution is implemented at the bottom): pythondecayed_learning_rate learning_rate global_step decay_rate as far as i can tell it is not already implemented in tensorflow which surprised me since i think this is the learning rate decay rate required for theoretical convergence using the adam optimizer in section of the paper which has: pythonalpha_t alpha sqrt(t) which is the same as the first equation with decay_rate and i assume they start at t while tensorflow starts with global_step i have an implementation i have been using mostly copied from the already implemented ones robbins_monro_decay(learning_rate global_step decay_steps decay_rate staircase=false name=none a robbins-monro type decay if global_step is none raise valueerror(global_step is required for robbins_monro_decay with tf.name_scope name robbinsmonrodecay learning_rate global_step decay_steps decay_rate as name learning_rate tf.convert_to_tensor learning_rate name=learning_rate dtype learning_rate.dtype decay_steps tf.cast(decay_steps dtype decay_rate tf.cast(decay_rate dtype global_step tf.cast(global_step dtype p global_step decay_steps if staircase p tf.floor(p return tf.multiply learning_rate tf.pow(p decay_rate name=name) i can make a full pull request if that would be useful i would be more than happy to add some additional documentation/code or change any of the code/naming/whatever.thanks
339321599,20638,https://api.github.com/repos/tensorflow/tensorflow/issues/20638,dhkwon1122,1,0,0,0,0,0,hi tf.i wonder if you have any plan to add deformable convolution feature you always for nice work
339187273,20619,https://api.github.com/repos/tensorflow/tensorflow/issues/20619,kenfehling,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary) :binary pip tensorflow version use command below) :..rc python version python anaconda custom x bazel version if compiling from source) :n/a gcc/compiler version if compiling from source) :n/a cuda/cudnn version :n/a gpu model and memory :n/a exact command to reproduce : import tensorflow as tffrom tensorflow.keras import sequentialfrom tensorflow.layers import densefrom tensorflow.python.training.adam import adamoptimizerimport numpy as npmodel sequential()model.add(dense input_shape model.add(dense activation=softmax))model.compile(optimizer=adamoptimizer loss=mse)lr_schedule tf.keras.callbacks.reducelronplateau()x np.random.uniform y np.random.uniform model.fit(x=x y=y callbacks= lr_schedule validation_split=.) you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemusing a native optimizer adamoptimizer i cant get reducelronplateau to work but it does work using an optimizer from tf.keras.optimizers only tf native optimizers are supported in eager mode so right now i just dont use reducelronplateau while in eager mode but i thought this should be reported thank you source code logs file users/ken/documents/projects/keras-try/src/lr.py line in module model.fit(x=x y=y callbacks= lr_schedule validation_split file users/ken/anaconda/lib/python./site-packages/tensorflow/python/keras/engine/training.py line in fit validation_steps=validation_steps file users/ken/anaconda/lib/python./site-packages/tensorflow/python/keras/engine/training_arrays.py line in fit_loop callbacks.on_epoch_end(epoch epoch_logs file users/ken/anaconda/lib/python./site-packages/tensorflow/python/keras/callbacks.py line in on_epoch_end callback.on_epoch_end(epoch logs file users/ken/anaconda/lib/python./site-packages/tensorflow/python/keras/callbacks.py line in on_epoch_end logs lr k.get_value(self.model.optimizer.lr)attributeerror tfoptimizer object has no attribute lr
338538164,20565,https://api.github.com/repos/tensorflow/tensorflow/issues/20565,sjperkins,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu lts xenial xerus tensorflow installed from source or binary) :binary tensorflow version use command below) :tensorflow-gpu binarytf.version rctf.git_version v..-rc--gefbctf.compiler_version v..-rc--gefbc python version python bazel version if compiling from source) :n/a gcc/compiler version if compiling from source) :n/a cuda/cudnn version :cuda cudnn gpu model and memory :geforce gtx m gb ramnvidia driver version exact command to reproduce : pythonfrom future import print_functionimport tensorflow as tfwith tf.graph().as_default as graph with tf.device(/gpu n tf.constant dtype=tf.int def cond(i s return tf.less(i n def body(i s n i return i n loop_vars tf.constant dtype=tf.int loop tf.while_loop(cond body loop_vars init tf.global_variables_initializer graph.finalize()config tf.configproto(log_device_placement=true)with tf.session(graph=graph config=config as s s.run(init print(s.run(loop describe the problemwhen the above script is run it produces instead of the expected if one changes the device to cpu the expected result is produced.ive tested that the gpu works with the following code: pythonimport tensorflow as tfwith tf.device(/gpu a tf.ones b a*init tf.global_variables_initializer()with tf.session(config=tf.configproto(log_device_placement=true as s s.run(init result s.run(b print(result.shape result source code logsn/a
337916881,20521,https://api.github.com/repos/tensorflow/tensorflow/issues/20521,gaffordb,0,0,0,0,0,1,system information have i written custom code yes os platform and distribution linux ubuntu tensorflow installed from source tensorflow version python version bazel version gcc/compiler version cuda/cudnn version gpu model and memory gtx mib exact command to reproduce :(any sparse tensor being prefetched to gpu will work here this is just one short example) import tensorflow as tf set up simple sparse tensords tf.data.dataset.from_tensors(tf.contrib.layers.dense_to_sparse prefetch to gpugpu_ds ds.apply(tf.contrib.data.prefetch_to_device(/gpu get a valuegpu_iter gpu_ds.make_one_shot_iterator()val gpu_iter.get_next barfwith tf.session as sess sess.run(val describe the problembug resulting from prefetch_to_device with the following conditions device is gpu dataset element contains a sparse tensoriterator functions appropriately only when you try to use the data do you run into the error see log for error)also as a side note which may or may not be relevant ive found that even a single sparse tensor in an element will spoil any other nice kind and dense tensors in the pack so if in the previous example i were to do the following i would still run into an error: import tensorflow as tf make a sparse tensords tf.data.dataset.from_tensors(tf.contrib.layers.dense_to_sparse add in a non-sparse componentds ds.map(lambda x x prefetch to devicegpu_ds ds.apply(tf.contrib.data.prefetch_to_device(/gpu pull out valsgpu_iter gpu_ds.make_one_shot_iterator()val_sparse val_normal gpu_iter.get_next uh oh!with tf.session as sess sess.run(val_normal logtraceback most recent call last file sparse_bunk.py line in module sess.run(val_normal file local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run run_metadata file local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.internalerror no unary variant device copy function found for direction and variant type_name tensorflow::tensor node functionbufferingresourcegetnext functionbufferingresourcegetnext output_types= dt_variant dt_int device=/job:localhost/replica:/task:/device:gpu: (functionbufferingresource) thank you
337880584,20517,https://api.github.com/repos/tensorflow/tensorflow/issues/20517,homofortis,24,0,0,0,0,0,im sure developers are working hard to catch up with python is there any timeline?pip install tensorflow apparently does not work building from source:os platform and distribution mac os x python python homebrew)tensorflow installed from source version tensorflow rcbazel version: build label homebrewbuild target bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time tue jun build timestamp build timestamp as int cuda/cudnn version nonegpu model and memory noneexact command to reproduce: bazel build config=opt tensorflow/tools/pip_package:build_pip_package starting local bazel server and connecting to it..............................warning private/var/tmp/_bazel_zardoz/eaacebceb/external/grpc/build in srcs attribute of cc_library rule grpc//:grpc_nanopb please do not import grpc//third_party/nanopb:pb_common.c directly you should either move the file to this package or depend on an appropriate rule there since this rule was created by the macro grpc_generate_one_off_targets the error might have been caused by the macro implementation in private/var/tmp/_bazel_zardoz/eaacebceb/external/grpc/bazel/grpc_build_system.bzl::warning private/var/tmp/_bazel_zardoz/eaacebceb/external/grpc/build in srcs attribute of cc_library rule grpc//:grpc_nanopb please do not import grpc//third_party/nanopb:pb_decode.c directly you should either move the file to this package or depend on an appropriate rule there since this rule was created by the macro grpc_generate_one_off_targets the error might have been caused by the macro implementation in private/var/tmp/_bazel_zardoz/eaacebceb/external/grpc/bazel/grpc_build_system.bzl::warning private/var/tmp/_bazel_zardoz/eaacebceb/external/grpc/build in srcs attribute of cc_library rule grpc//:grpc_nanopb please do not import grpc//third_party/nanopb:pb_encode.c directly you should either move the file to this package or depend on an appropriate rule there since this rule was created by the macro grpc_generate_one_off_targets the error might have been caused by the macro implementation in private/var/tmp/_bazel_zardoz/eaacebceb/external/grpc/bazel/grpc_build_system.bzl::warning users/zardoz/projects/tensorflow/tensorflow/contrib/learn/build in py_library rule tensorflow/contrib/learn:learn target tensorflow/contrib/learn:learn depends on deprecated target tensorflow/contrib/session_bundle:exporter no longer supported switch to savedmodel immediately.warning users/zardoz/projects/tensorflow/tensorflow/contrib/learn/build in py_library rule tensorflow/contrib/learn:learn target tensorflow/contrib/learn:learn depends on deprecated target tensorflow/contrib/session_bundle:gc no longer supported switch to savedmodel immediately.warning users/zardoz/projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/build in py_library rule tensorflow/contrib/timeseries/python/timeseries:ar_model target tensorflow/contrib/timeseries/python/timeseries:ar_model depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .warning users/zardoz/projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/build in py_library rule tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter target tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .warning users/zardoz/projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/build in py_library rule tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor target tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .warning users/zardoz/projects/tensorflow/tensorflow/contrib/bayesflow/build in py_library rule tensorflow/contrib/bayesflow:bayesflow_py target tensorflow/contrib/bayesflow:bayesflow_py depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .warning users/zardoz/projects/tensorflow/tensorflow/contrib/seqseq/build in py_library rule tensorflow/contrib/seqseq:seqseq_py target tensorflow/contrib/seqseq:seqseq_py depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .warning users/zardoz/projects/tensorflow/tensorflow/contrib/kfac/python/ops/build in py_library rule tensorflow/contrib/kfac/python/ops:loss_functions target tensorflow/contrib/kfac/python/ops:loss_functions depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .warning users/zardoz/projects/tensorflow/tensorflow/contrib/build in py_library rule tensorflow/contrib:contrib_py target tensorflow/contrib:contrib_py depends on deprecated target tensorflow/contrib/distributions:distributions_py tensorflow distributions has migrated to tensorflow probability deprecated copies remaining in tf.contrib.distributions are unmaintained unsupported and will be removed by late you should update all usage of tf.contrib.distributions to tfp.distributions .info analysed target tensorflow/tools/pip_package:build_pip_package packages loaded).info found target...info from linking external/grpc/libgrpc_base_c.a for host :/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(endpoint_pair_uv.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(endpoint_pair_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(ev_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(fork_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(gethostname_fallback.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(gethostname_host_name_max.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(iocp_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(iomgr_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_set_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_uv.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(resolve_address_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_utils_linux.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_utils_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_client_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_server_utils_posix_noifaddrs.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_server_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_uv.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(timer_uv.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(unix_sockets_posix_noop.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc_base_c.a(wakeup_fd_eventfd.o has no symbolsinfo from linking external/grpc/libalts_util.a for host :/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libalts_util.a(check_gcp_environment_linux.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libalts_util.a(check_gcp_environment_windows.o has no symbolsinfo from linking external/grpc/libtsi.a for host :/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libtsi.a(ssl_session_openssl.o has no symbolsinfo from linking external/grpc/libgrpc++_base.a for host :/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgrpc++_base.a(rpc_method.o has no symbolsinfo from linking external/grpc/libgpr_base.a for host :/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_iphone.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_linux.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(env_linux.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(env_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(log_android.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(log_linux.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(log_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(string_util_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(string_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(sync_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(time_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(tls_pthread.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(tmpfile_msys.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(tmpfile_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(wrap_memcpy.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(thd_windows.o has no symbols/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/libgpr_base.a(stap_timers.o has no symbolsinfo from linking external/grpc/third_party/address_sorting/libaddress_sorting.a for host :/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin/ranlib file bazel-out/host/bin/external/grpc/third_party/address_sorting/libaddress_sorting.a(address_sorting_windows.o has no symbolserror users/zardoz/projects/tensorflow/tensorflow/python/build executing genrule tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed exit traceback most recent call last file private/var/tmp/_bazel_zardoz/eaacebceb/execroot/org_tensorflow/bazel-out/host/bin/external/cython/cython_binary.runfiles/cython/cython.py line in module main(command_line file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/main.py line in main result compile(sources options file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/main.py line in compile return compile_multiple(source options file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/main.py line in compile_multiple context options.create_context file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/main.py line in create_context self.cplus self.language_level options=self file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/main.py line in init from import builtin cythonscope file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/cythonscope.py line in module from utilitycode import cythonutilitycode file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/utilitycode.py line in module from treefragment import parse_from_strings stringparsecontext file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/treefragment.py line in module from visitor import visitortransform file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/visitor.py line in module from import exprnodes file private/var/tmp/_bazel_zardoz/eaacebceb/external/cython/cython/compiler/exprnodes.py line await none syntaxerror invalid syntaxtarget tensorflow/tools/pip_package:build_pip_package failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path sinfo processes local.failed build did not complete successfully
337336768,20464,https://api.github.com/repos/tensorflow/tensorflow/issues/20464,joe-antognini,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemcurrently tf.estimator.train_and_evaluate makes it easy to use an estimator to perform both training and evaluation possibly in a distributed environment however this function only supports a single evaluation dataset this makes the function suboptimal because we oftentimes want to evaluate on both the training and the validation set in order to get a sense for the amount of overfitting that is happening it would be ideal if we could perhaps pass a list of evalspec objects to train_and_evaluate
337219890,20439,https://api.github.com/repos/tensorflow/tensorflow/issues/20439,Curious-Nikhil,0,0,0,0,2,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory :n/a exact command to reproduce import tensorflow describe the problem i am consistently getting an import error when i enter import tensorflow i have tried reinstalling but i am unfortunately limited by my experience and expertise so i request help. after some research i have found that avx hardware support is necessary so, system specs amd athlon x nvidia gt doesnt support cuda)_can you confirm on this i cant find enough to confirm source code logs import tensorflow traceback most recent call last file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\mishr\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed a dynamic link library dll initialization routine failed.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\mishr\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import pywrap_tensorflow pylint disable=unused-import file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\mishr\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed a dynamic link library dll initialization routine failed.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\mishr\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\mishr\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help"
337209809,20436,https://api.github.com/repos/tensorflow/tensorflow/issues/20436,phizaz,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version cuda gpu model and memory exact command to reproduce describe the problem tf.data.dataset doesnt seem to free its own memory usage even it is out of scope in eager execution mode.the memory im mentioning is the main memory not gpus memory source code logsthe following codes are run in jupyterlab.block import tensorflow as tfimport numpy as nptf.enable_eager_execution() block def run data tf.data.dataset.from_tensor_slices(np.zeros data data.shuffle data data.batch x next(iter(data))run() i repeat block many times and the memory usage grows each time.note gc.collect doesnt have any effect
337110992,20426,https://api.github.com/repos/tensorflow/tensorflow/issues/20426,datlife,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary pip install tensorflow version use command below v..-rc--gdb rc python version cuda/cudnn version cuda cudnn gpu model and memory gtx exact command to reproduce : pythonimport tensorflow as tfdef conv_block(inputs filters kernel_size strides scope create a simple conv bn relu block with tf.variable_scope(scope x tf.keras.layers.convd(filters kernel_size strides name=convd)(inputs x tf.keras.layers.batchnormalization(name=bn)(x x tf.keras.layers.activation(tf.nn.relu)(x return xdef reproduce_keras_variable_scope_error construct a simple model inputs tf.keras.input(shape batch_size name=inputs hidden conv_block(inputs scope=block outputs conv_block(hidden scope=block this is fine the tensor scopes are matched as expected for v in tf.trainable_variables print format(v.name v.shape problem happens here please consult the error output below model tf.keras.model(inputs outputs model.summary()if name main reproduce_keras_variable_scope_error describe the problemthe problem is keras layers do not probably name its variables according the variable_scope therefore i cannot call conv_block multiple times this problem occurs when calling tf.layers as well i would like to use tf.keras.model because i might convert to estimator later source code logs shellblock_/convd/kernel block_/convd/bias block_/bn/gamma block_/bn/beta block_/convd/kernel block_/convd/bias block_/bn/gamma block_/bn/beta traceback most recent call last file tf_keras_layer.py line in module reproduce_variable_scope_error file tf_keras_layer.py line in reproduce_variable_scope_error model tf.keras.model(inputs outputs file home/dat/miniconda/envs/portrait/lib/python./site-packages/tensorflow/python/keras/engine/training.py line in init super(model self).__init__(*args kwargs file home/dat/miniconda/envs/portrait/lib/python./site-packages/tensorflow/python/keras/engine/network.py line in init self._init_graph_network(*args kwargs file home/dat/miniconda/envs/portrait/lib/python./site-packages/tensorflow/python/keras/engine/network.py line in init_graph_network self.inputs self.outputs file home/dat/miniconda/envs/portrait/lib/python./site-packages/tensorflow/python/keras/engine/network.py line in map_graph_network str(all_names.count(name times in the model valueerror the name convd is used times in the model all layer names should be unique
336203215,20343,https://api.github.com/repos/tensorflow/tensorflow/issues/20343,tastyminerals,3,0,0,0,0,0,after trying to figure out how to use tensorflow profiler to test my model i am giving up current readme.md does not only lack details but is actually quite confusing
336163360,20340,https://api.github.com/repos/tensorflow/tensorflow/issues/20340,ironside007,1,0,0,0,0,0,i have win amd raedon r mi am using python and tensorflow i am also working through spyder as using python console would make handling the code very difficult please guide me how should i use my gpu in the training of deep learning model i am working on currently it uses my cpu. ps no solutions to previously made issues worked for me
336136913,20336,https://api.github.com/repos/tensorflow/tensorflow/issues/20336,shanshan0309,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow):n/a os platform and distribution e.g linux ubuntu windows server tensorflow installed from source or binary):source tensorflow version use command below):master branch latest revison python version:anaconda python bit) bazel version if compiling from source):n/a gcc/compiler version if compiling from source):vs cuda/cudnn version:nvidia cuda toolkit nvidia cudnn gpu model and memory:n/a exact command to reproduce:n/a describe the problem: tensorflow failed to build on x this issue can be reproduced from the master revision bb this should be tensorflow source issue could you please help take a look at this thanks! the failures like: the whole log file please see attachment. tensorflow_x_build.log steps git clone d:\tensorflow\src pushd d:\tensorflow set preferredtoolarchitecture=x set rel=release set cudnn_home=c:\program files\nvidia gpu computing toolkit\cuda\v.\cuda set py=c:\programdata\anaconda set cl=/fs permissive cmake d:\tensorflow\src\tensorflow\contrib\cmake a x dcmake_build_type=release dpython_executable=c:\programdata\anaconda\python.exe dpython_libraries=c:\programdata\anaconda\libs\python.lib dswig_executable=d:\tensorflow\swigwin-..\swig.exe dtensorflow_build_python_tests=on dtensorflow_build_shared_lib=on msbuild m p:configuration=release;platform=x p:windowstargetplatformversion tensorflow.sln t:rebuild
336081995,20330,https://api.github.com/repos/tensorflow/tensorflow/issues/20330,freedomtan,1,0,0,0,0,0,benchmark_model doesnt build for desktop as reported at
335514928,20284,https://api.github.com/repos/tensorflow/tensorflow/issues/20284,perfinion,1,0,0,0,0,0,"this series adds a framework to be able to unbundle the dependencies from tensorflow previously bazel rebuilds every single dep from scratch this allows distro packages to disable bundling on a per-dep basis and link with the libraries that already exist on the system.for more information why deps should be unbundled see these links deps i have unbundled so far work on my gentoo machine thanks to dennisjenkins@google.com for testing this on gentoo as well there are still more that need to be unbundled but this covers enough to be useful already this has not been tested on any other distro than gentoo but i dont forsee any big incompatibilities.the libs that are unbundled are configured by setting build action_env tf_system_libs=comma,sep,list eg: build action_env tf_system_libs=com_googlesource_code_re,nasm,jpeg,png_archive,org_sqlite,gif_archive,six_archive,astor_archive,termcolor_archive,pcre,swig,curl,grpc,lmdb,zlib_archive,snappy,flatbuffers,cython,jemalloc once configured the tarball for the dep will not be downloaded or extracted and the build file will be swapped to the system_build_file instead which contains different rules to compile and link against the system package.there are also macros if_system_lib(name a b and if_not_system_lib(name a b to configure other things in the build.do not merge this yet im opening this for comments are review here before its ready for merging"
335513892,20283,https://api.github.com/repos/tensorflow/tensorflow/issues/20283,ZigaSajovic,4,0,0,0,0,0,"at this point only tf.truncated_normal is implemented within tensorflow are there any plans for implementing others aswell ex truncated gamma truncated exponential etc truncated distributions come up often in probabilistic programming a branch in which tensorflow is becoming prominent i for one do most of my probabilistic work and sampling in tensorflow due to the gpu support and the control it offers me.each truncated distribution offers a different method of efficiently sampling from it so distribution specific algorithms would be ideal.in the mean time i implemented a general method for personal use i attach the specifications bellow in case there is any interest in this general solution the implementation documentation and examples are available here note that i am willing to work on improving it in case interest is present. truncateddistribution the class truncateddistribution extends any existing tensorflow distribution i.e classes inheriting from tf.distribution to enable their truncated counterparts with full support of broadcasting. methods init\_\_(disttribution,left,right n_points sample(sample_shape cdf(x log_cdf(x survival_function(x log_survival_function(x prob(x log_prob(x mean(n_samples variance(n_samples stddev(n_samples"
335344594,20273,https://api.github.com/repos/tensorflow/tensorflow/issues/20273,danielwatson6,2,0,0,0,0,0,have i written custom code n/aos platform and distribution n/atensorflow installed from n/atensorflow version bazel version n/acuda/cudnn version gpu model and memory nvidia gtx tiexact command to reproduce n/ait would be very helpful to add usage examples on how to use cudnn_rnn particularly on cross-compatibility between non-cuda and cuda-supporting devices users should be able to figure out how to save/restore weights to run their models with say tf.nn.rnn_cell.lstmcell or tf.contrib.cudnn_rnn.cudnnlstm .there are classes that seem to do this e.g tf.contrib.cudnn_rnn.cudnnlstmsaveable but there are no easily accessible code examples showing how they should be used.*a possible feature request would it be possible to have a high-level wrapper that makes this choice automatically based on the availability of a cuda device saving and restoring weights accordingly this should be possible with tf.test.is_gpu_available(cuda_only=true
335228005,20265,https://api.github.com/repos/tensorflow/tensorflow/issues/20265,MalcolmSlaney,1,0,0,0,0,0,"can the documentation for dataset.map be fleshed out a bit most importantly what are the input and output arguments for the mapping function the current documentation implies there is only one return but all the code examples show input,output what is the meaning of the input and output args"
334901066,20225,https://api.github.com/repos/tensorflow/tensorflow/issues/20225,KOLANICH,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary source tensorflow version use command below master gcc/compiler version if compiling from source exact command to reproduce mingw-make describe the problemthe tool builds the dependencies instead of asking the user to provide location of prebuilt ones building dependencies when building tf causes problems if a dependency build fails i have to start the build from scratch this consumes lot of time also i cannot fix an error in the dependencies because it fetches the latest version from git on each insfallation this also consumes lot of time please provide the way to use prebuilt dependencies
334572211,20192,https://api.github.com/repos/tensorflow/tensorflow/issues/20192,dimitryn,2,0,0,0,0,0,hi i took latest tensorflow sources from github and try to build tensorflow lite.there was error compilation.we upgraded our ndk version from to couple weeks ago and tensorflow lite compiled without any error but latest sources from today failed to compile. info found target...error xxx/tensorflow/contrib/lite/kernels/build c compilation of rule tensorflow/contrib/lite/kernels:gemm_support failed exit in file included from tensorflow/contrib/lite/kernels/gemm_support.cc::in file included from tensorflow/contrib/lite/kernels/gemm_support.h::in file included from external/gemmlowp/public/gemmlowp.h::in file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h::in file included from external/gemmlowp/public/../internal/../internal/kernel_default.h::in file included from external/gemmlowp/public/../internal/common.h::in file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cmath::in file included from external/androidndk/ndk/sources/android/support/include/math.h::external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h error no member named logf in the global namespaceinline libcpp_inline_visibility float log(float lcpp_x noexcept return logf(__lcpp_x external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h error no member named logl in the global namespaceinline libcpp_inline_visibility long double log(long double lcpp_x noexcept return logl(__lcpp_x external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h error call to log is ambiguouslog(_a lcpp_x noexcept return log((double)__lcpp_x external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h note candidate functioninline libcpp_inline_visibility float log(float lcpp_x noexcept return logf(__lcpp_x external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h note candidate functioninline libcpp_inline_visibility long double log(long double lcpp_x noexcept return logl(__lcpp_x errors generated.target tensorflow/contrib/lite:libtensorflowlite.so failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path sinfo processes local.failed build did not complete successfully system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary) :sources tensorflow version use command below) :master python version bazel version if compiling from source gcc/compiler version if compiling from source) :ndk cuda/cudnn version :no gpu model and memory :no exact command to reproduce :bazel build tensorflow/contrib/lite:framework crosstool_top=//external:android/crosstool cpu=arm-va host_crosstool_top=@bazel_tools//tools/cpp:toolchain cxxopt=-std=c++thanks for help
334354047,20173,https://api.github.com/repos/tensorflow/tensorflow/issues/20173,marctuscher,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary pip tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory geforce gtx ti gb exact command to reproduce import tensorflow as tf problem descriptionimporting tensorflow gives me following error this error is only reproducible in a file where a computational graph is defined. keyerror couldnt find field google.protobuf.fileoptions.php_metadata_namespace solutioni already found a solution to this problem it is related to the pip package protobuf i solved this problem by issuing following commands: pip uninstall protobufpip install protobuf
334234243,20158,https://api.github.com/repos/tensorflow/tensorflow/issues/20158,dillondaudert,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below) :v..--gdefe dev python version bazel version if compiling from source) :n/a gcc/compiler version if compiling from source) :n/a cuda/cudnn version :cuda cudnn gpu model and memory :nvidia geforce exact command to reproduce :running the provided script should produce this error describe the problemtf.keras.model will throw a failedpreconditionerror table not initialized error when passing a dataset that includes a lookup table to model.fit this can be worked around by creating a session and running tf.tables_initializer and keras.backend.set_session but it would be nice if keras could check for tables and initialize them automatically source code logs script : pythonimport tensorflow as tfimport numpy as npfrom tensorflow.python.ops import lookup_opsfrom tensorflow import kerasalphabet a b c table lookup_ops.index_table_from_tensor(tf.constant(alphabet generate samples of strings of different lengthsinputs join( np.random.choice(alphabet for in range for in range() targets np.zeros dataset tf.data.dataset.from_tensor_slices((inputs targets))def map_fn(x y x tf.string_split( x delimiter=).values x table.lookup(x x tf.nn.embedding_lookup(tf.eye x return x ydataset dataset.map(lambda x y map_fn(x y))dataset dataset.repeat()dataset dataset.batch()x keras.layers.input(shape name=input)flat keras.layers.flatten()(x)y keras.layers.dense name=dense)(flat)model keras.model(x y)model.compile(loss=mse optimizer=rmsprop)model.fit(dataset epochs steps_per_epoch verbose output : bashscript started on dillon@dillon-linux:~/github/dillondaudert python keras_table_init_ex.py i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use avx fma i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc adding visible gpu devices i tensorflow/core/common_runtime/gpu/gpu_device.cc device interconnect streamexecutor with strength edge matrix i tensorflow/core/common_runtime/gpu/gpu_device.cc i tensorflow/core/common_runtime/gpu/gpu_device.cc n y i tensorflow/core/common_runtime/gpu/gpu_device.cc y n i tensorflow/core/common_runtime/gpu/gpu_device.cc created tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name geforce gtx pci bus id compute capability i tensorflow/core/common_runtime/gpu/gpu_device.cc created tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name geforce gtx pci bus id compute capability epoch w tensorflow/core/framework/op_kernel.cc op_requires failed at lookup_table_op.cc failed precondition table not initialized.traceback most recent call last file keras_table_init_ex.py line in module model.fit(dataset epochs steps_per_epoch verbose file home/dillon/.conda/envs/tf.-nightly/lib/python./site-packages/tensorflow/python/keras/engine/training.py line in fit validation_steps=validation_steps file home/dillon/.conda/envs/tf.-nightly/lib/python./site-packages/tensorflow/python/keras/engine/training_arrays.py line in fit_loop outs f(ins file home/dillon/.conda/envs/tf.-nightly/lib/python./site-packages/tensorflow/python/keras/backend.py line in call fetched self._callable_fn(*array_vals file home/dillon/.conda/envs/tf.-nightly/lib/python./site-packages/tensorflow/python/client/session.py line in call run_metadata_ptr file home/dillon/.conda/envs/tf.-nightly/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in exit c_api.tf_getcode(self.status.status))tensorflow.python.framework.errors_impl.failedpreconditionerror table not initialized node hash_table_lookup lookuptablefindv tin=dt_string tout=dt_int (hash_table_lookup_placeholder stringsplit hash_table_lookup_placeholder node iteratorgetnext iteratorgetnext output_shapes output_types= dt_float dt_double device=/job:localhost/replica:/task:/device:cpu: (iterator node iteratorgetnext recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:gpu send_device=/job:localhost/replica:/task:/device:cpu send_device_incarnation tensor_name=edge__iteratorgetnext tensor_type=dt_double device=/job:localhost/replica:/task:/device:gpu: () dillon@dillon-linux:~/github/dillondaudert exitexitscript done on
333994074,20141,https://api.github.com/repos/tensorflow/tensorflow/issues/20141,mynameischaos,1,0,0,0,0,0,system information------------------------------------------have i written custom code as opposed to using a stock example script provided in tensorflow):noos platform and distribution e.g linux ubuntu centos linux release tensorflow installed from source or binary):anaconda python conda install)tensorflow version use command below):..python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version:cuda-.gpu model and memory pphone n/ahi when i use tf.contrib.quantize.experimental_create_training_graph it will add fake quantization in the graph reference this picture(find positions to insert fake quantization nodes image think it will add a fake quantization node in the bypass but i can not find it in the tensorboard graph(in the red box it is the bypass in other convs i can find the quantization node image network architecture is resnet tensorflow version is pls help me thank you
333963701,20140,https://api.github.com/repos/tensorflow/tensorflow/issues/20140,zyj183247166,1,0,0,0,0,0,"write the code below and save it to a ckpt as a model import tensorflow as tf v tf.variable(tf.constant shape name v)v tf.variable(tf.constant shape name v)v tf.variable(tf.constant shape name v)result=v+vresult result vinit_op tf.global_variables_initializer()saver tf.train.saver()with tf.session as sess sess.run(init_op writer tf.summary.filewriter(./graphs/const_add sess.graph saver.save(sess saved_model/model.ckpt) then in another py we restore the model from the model.ckpt file import tensorflow as tfsaver tf.train.import_meta_graph(saved_model/model.ckpt.meta)with tf.session as sess saver.restore(sess saved_model/model.ckpt print sess.run(tf.get_default_graph().get_tensor_by_name(add sess.run(tf.assign(v vv with tf.variable_scope(,reuse=tf.auto_reuse with tf.variable_scope(,reuse=false v=tf.get_variable(name=v,shape print(v.name sess.run(tf.assign(v trainable_variables=tf.trainable_variables variable_list_name c.name for c in tf.trainable_variables variable_list sess.run(variable_list_name for k,v in zip(variable_list_name,variable_list print(variable name:,k print(shape:,v.shape print(v print sess.run(tf.get_default_graph().get_tensor_by_name(v print sess.run(tf.get_default_graph().get_tensor_by_name(add print sess.run(tf.get_default_graph().get_tensor_by_name(add print sess.run(tf.get_default_graph().get_tensor_by_name(v_:))) the results will be as below:! image will find that:if we restore some variables from the already existed model file saved_model/model.ckpt.meta),such as v,v,v in this example.it will influence the process of calling get_variable because of these two causes as below the variables restored from the model file such as v,v and v will not exist in the scope of get_variable it means you can only use with tf.variable_scope(,reuse=false v=tf.get_variable(name=v,shape= ) and create a new variable you can not reuse the restored variable v from the model file unless you define a v before you restore from the model file like below v=tf.get_variable(name=v,shape= )saver tf.train.saver()with tf.session as sess saver.restore(sess saved_model/model.ckpt print sess.run(result)) that is you can not reuse the restored variable v which is from restoring the model file unless you define it befor you restore although tensorflow doesnot allow reusing the restored variable v which is from restoring the model file if you dont define v before you restore the model file.but if you call get_varialbe after you restore the model file it will create a variable whose name is v but not as name=v which you specify in my opinion it should be corrected because it is so confusing how to correct it?i think get_variable should also reuse the variables which is loaded by restoring some model file.the last sentence is what i finally want to say my english is to bad you can run the code i offer and will find what i want to convey thanks"
333951453,20139,https://api.github.com/repos/tensorflow/tensorflow/issues/20139,kimbaol,7,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below) :..-rc python version both and bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :cuda cudnn gpu model and memory :titanxp g exact command to reproduce :env test_tmpdir=/tmp/bazel n tf_need_gcp tf_need_hdfs tf_need_s tf_enable_xla tf_need_gdr tf_need_verbs tf_need_jemalloc tf_need_opencl tf_need_cuda tf_need_mpi python_bin_path=/usr/bin/python python_lib_path=/usr/local/lib/python./dist-packages tf_need_opencl_sycl cc_opt_flags=-march=native tf_cuda_version cuda_toolkit_path=/usr/local/cuda gcc_host_compiler_path=/usr/bin/gcc tf_cudnn_version tf_cuda_clang tf_set_android_workspace=false cudnn_install_path=/usr/lib/aarch-linux-gnu tf_need_kafka tf_need_tensorrt tf_nccl_version nccl_install_path=/usr/local/nccl tensorrt_install_path=/usr/lib/x_-linux-gnu tf_cuda_compute_capabilities configure bazel build c opt copt=-mavx copt=-mavx copt=-mfma copt=-mfpmath=both copt=-msse config=cuda config=mkl cxxopt=-d_glibcxx_use_cxx_abi tensorflow/tools/pip_package:build_pip_packagebuild_pip_package then pip install python tensorflow/tensorflow/contrib/tensorrt/test/test_tftrt.py describe the problemi built tensorflow from source with above script there is no problem with git branch r after i checked out to branch r and rebuilt tftrt test case will crash source code logsroot@fdfaef:~/share/tensorflow/tensorflow/contrib/tensorrt/test python test_tftrt.py f tensorflow/core/framework/op.cc non-ok-status registeralreadylocked(op_data_factory status already exists op with name scopedallocatoraborted core dumped
333919226,20136,https://api.github.com/repos/tensorflow/tensorflow/issues/20136,dillondaudert,1,0,0,0,0,0,this change allows passing nested tuples x x y y from a dataset to a keras model
333329604,20098,https://api.github.com/repos/tensorflow/tensorflow/issues/20098,masonk,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes custom code os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below python version x describe the problemi would like to produce and persist a model represented by a metagraph then restore it and feed it from a different data source such as from a different dataset despite being a natural thing to want to do it is not easy to find out how to do this from official documentation in particular theres no best practice shown anywhere in the docs.today the only way i have found of doing this is to build the graph with a feedable iterator as described in the comments to and then saving+restoring the iterator handle so that i can feed in new iterator by handle on every train step.as a secondary issue i think it would make more sense to save and restore a reinitializable iterator to the metagraph then in the restored session i could pull that reinitializable iterator out of the restored metagraph and reinitialize it from a new dataset no way that i tried of doing this actually worked although i could save the iterator with make_saveable_from_iterator the necessary make_initializer function wasnt present on the restored object it didnt survive the roundtrip to disk source code logs@annarailton gives a full source code for the handle-based method of iterator persistence in this comment independently came up with functionally equivalent code after several hours of work then found her code by searching to see if anyone else was doing it with feedables i was searching because it felt wrongish inefficient and the docs gave no endorsement for this approach.so in the end i have two related requests document the current best practice for attaching new data to the inputs of a restored metagraph.e.g should show this best practice i believe this is by far the most common thing to want to do with a restored metagraph likely to be far more common than resuming an existing iterator as shown in the docs provide an efficient way to attach a new data to a restored metagraph.it may be that the handle lookup in the feedable iterator method is efficient in that case this second request is a no-op finally im happy to give you a pr for datasets#saving_iterator_state to show the handle based feeding method if youd like one
333054483,20081,https://api.github.com/repos/tensorflow/tensorflow/issues/20081,adrianblevine,0,3,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
332709549,20050,https://api.github.com/repos/tensorflow/tensorflow/issues/20050,oldsqlwnb,1,0,0,0,0,0,describe the problem.in the documentation at it says long short-term memory unit lstm recurrent network cell the default non-peephole implementation is based on s hochreiter and j schmidhuber long short-term memory neural computation this is not true since the implementation already contains the forget gate which was not mentioned in the original paper but in this one learning to forget continual prediction with lstm by gers et al..the naming of the parameter num_units or the lstmcell is is unclear and causes a lot of confusion google num_units lstm cell and you will find countless posts with the same question what does it mean?)since the number of units within an lstm cell is fixed num_units should either be memory_cell_block_size as it is described in the long short-term memory paper and/or rename lstmcell into lstmmemorycellblock request add the paper learning to forget continual prediction with lstm by gers et al to the documention and mention that the implementation already contains forget gates rename num_units and/or lstmcell or at the very least add a comment
332524555,20034,https://api.github.com/repos/tensorflow/tensorflow/issues/20034,surry,2,0,0,0,0,0,avoid creating a new std::stringstream and copying data into it every time srandomaccessfile::read is called fixes i was seeing the same issue in when using an s path for my tensorboard logdir and when reading datasets from s looking at the s code i noticed that srandomaccessfile was copying data read from an aws::iostream into a temporary std::stringstream and then copying it back into the final buffer scratch ).based on the comments for randomaccessfile::read it seems that the data from s could be directly written into scratch without a copy i also think the stringstream was allocating lots of small chunks of memory to store the data read from the s files which was probably the bigger performance hit.i updated the code and rebuilt tensorflow and now tensorboard loads tfevent files from s much more quickly
332500667,20032,https://api.github.com/repos/tensorflow/tensorflow/issues/20032,falahgs,0,1,0,0,0,0,unsuccessful tensorslicereader constructor failed tofind any matching files for slim_pretrained\inception_v.ckpt node save/restorev restorev dtypes= dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev/tensor_names save/restorev/shape_and_slices
332056974,19993,https://api.github.com/repos/tensorflow/tensorflow/issues/19993,yongtang,1,0,0,0,0,0,while building tensorflow on mac with python and llvm macos high sierra the following compilation errors surface: in file included from tensorflow/python/lib/core/py_util.cc::in file included from tensorflow/core/lib/core/errors.h::in file included from applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/sstream::in file included from applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/ostream::in file included from applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/ios::/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/__locale error c requires a type specifier for all declarations char_type toupper(char_type c const bazel-out/host/genfiles/external/local_config_python/python_include/pyport.h note expanded from macro toupper...... the error is related to the issue in pyport.h .the build error could be fixed by including include locale> before including include python.h> .the changes in this pr allows the build to succeed.signed-off-by yong tang yong.tang.github@outlook.com
331783057,19959,https://api.github.com/repos/tensorflow/tensorflow/issues/19959,kryczko,1,0,0,0,0,0,"dear tensorflow team,i believe i have hit a limitation of tensorflow and was wondering if you or the community could assist me i am trying to initialize a graph with a topology from the paper seen here in this paper one splits an image into tiles which can overlap and has some sort of network for each tile the output of each network is then summed to obtain an output the problem is when you split the image into many tiles the computational graph becomes very large and one will often run out of memory in this topology the network is shared across all tiles for inference one never hits a memory problem because one can evaluate the value of each tile one by one this is where i am having an issue i cannot seem to define gradients properly if i first evaluate the tiles one by one the way that tensorflow is currently set up i have to define the full computational graph to get the correct gradients is there a way i could evaluate the tiles first and then apply gradients afterwards?os platform and distribution macostensorflow installed from tensorflow tensorflow version bazel version unknowncuda/cudnn version n/a this experiment was ran on cpu)gpu model and memory n/aexact command to reproduce"
331737039,19955,https://api.github.com/repos/tensorflow/tensorflow/issues/19955,isaacsultan,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory nvidia exact command to reproduce tensorboard logdir=c:\users\isultan\pycharmprojects\deep-shading\logs describe the problemcalling tensorboard with an argument for the directory of logs from anaconda prompt causes a type error logs where produced in tf.keras with tensorboard callback model_tb tensorboard(log_dir=./logs write_graph=true passed to model.fit_generator source code logs (tf c:\users\isultan tensorboard logdir=c:\users\isultan\pycharmprojects\deep-shading\logs traceback most recent call last file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\runpy.py line in run_module_as_main main mod_spec file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\runpy.py line in run_code exec(code run_globals file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\scripts\tensorboard.exe\__main__.py line in module file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\site-packages\tensorboard\main.py line in module from tensorboard import default file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\site-packages\tensorboard\default.py line in module from tensorboard.plugins.audio import audio_plugin file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\site-packages\tensorboard\plugins\audio\audio_plugin.py line in module from tensorboard.plugins.audio import metadata file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\site-packages\tensorboard\plugins\audio\metadata.py line in module from tensorboard.plugins.audio import plugin_data_pb file c:\users\isultan\appdata\local\continuum\miniconda\envs\tf\lib\site-packages\tensorboard\plugins\audio\plugin_data_pb.py line in module options=none file=descriptor),typeerror new got an unexpected keyword argument file"
331681666,19946,https://api.github.com/repos/tensorflow/tensorflow/issues/19946,alexbeloi,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gbce python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version cuda cudnn gpu model and memory nvidia k x exact command to reproduce python training/train.pyyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemfeature request extend tf.contrib.estimator.replicate_model_fn to be usable with a dataset feeder with arbitrary nested structure of tensors as input_fn currently the split_batch in replicate_model_fn.py can only split flat nested structures source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.below is traceback from dataset using dict of dict of tensors. traceback most recent call last file training/train.py line in module run_experiment(args.train_files args.eval_files hparams file training/train.py line in run_experiment tf.estimator.train_and_evaluate(_estimator train_spec eval_spec file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/training.py line in train_and_evaluate executor.run file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/training.py line in run self.run_local file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/training.py line in run_local hooks=train_hooks file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train loss self._train_model(input_fn hooks saving_listeners file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model return self._train_model_default(input_fn hooks saving_listeners file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model_default features labels model_fn_lib.modekeys.train self.config file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in call_model_fn model_fn_results self._model_fn(features=features kwargs file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py line in replicated_model_fn features labels len(devices device=consolidation_device file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py line in split_batch feature_shards split_dictionary(features file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py line in split_dictionary ensure_divisible_by_shards(tensor file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py line in ensure_divisible_by_shards batch_size ops_lib.convert_to_tensor(sequence).get_shape file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor as_ref=false file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in internal_convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant value dtype=dtype shape=shape verify_shape=verify_shape file home/alex/t/virtualenv/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto supported type type(values values))typeerror failed to convert object of type class dict to tensor contents input_seq tf.tensor iteratorgetnext shape dtype=int stop_token tf.tensor iteratorgetnext shape dtype=float target_seq tf.tensor iteratorgetnext shape dtype=float consider casting elements to a supported type
331508244,19933,https://api.github.com/repos/tensorflow/tensorflow/issues/19933,codescv,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos on linux ubuntu you get similar results tensorflow installed from source or binary binary tensorflow version use command below cpu python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problemi created a dataset in tfrecord format for testing every entry contains columns named c c each being a strings list and a label column to denote the labels the code to create the data can be found here i used a linear model to train the data the first approach looks like this: dataset tf.data.tfrecorddataset(data_file)dataset dataset.prefetch(buffer_size=batch_size*)dataset dataset.map(parse_tfrecord num_parallel_calls=)dataset dataset.repeat(num_epochs)dataset dataset.batch(batch_size)features labels dataset.make_one_shot_iterator().get_next logits tf.feature_column.linear_model(features=features feature_columns=columns cols_to_vars=cols_to_vars)train_op with tf.session as sess sess.run(train_op) the full code can be found here i run the code above i get steps/sec batch size being in the second approach i manually get batches from dataset into python then feed them to a placeholder like this: example tf.placeholder(dtype=tf.string shape= none )features tf.parse_example(example features=tf.feature_column.make_parse_example_spec(columns+ tf.feature_column.numeric_column(label dtype=tf.float default_value=) ))labels features.pop(label)train_op dataset tf.data.tfrecorddataset(data_file).repeat().batch(batch_size)next_batch dataset.make_one_shot_iterator().get_next()with tf.session as sess data_batch sess.run(next_batch sess.run(train_op feed_dict={example data_batch}) the full code can be found here i run the code above i get steps/sec that is x faster than the first approach this is what i do not understand because theoretically the second should be slower due to the extra serialization/deserialization of data batches.is this possibly a bug or am i using it mistakenly thanks source code logsi have also included some profile traces below:(using tf dataset api)! lr_single feed_dict)! lr_single_feed
331314133,19914,https://api.github.com/repos/tensorflow/tensorflow/issues/19914,alsrgv,1,0,0,0,1,0,tensorflow official mnist model and tflearn model use as a data store for mnist dataset.this url is returning errors: userprojectaccountproblemuser project billing account not in good standing. any eta for the fix?cc reedwm martinwicke
331092939,19903,https://api.github.com/repos/tensorflow/tensorflow/issues/19903,jonasrauber,2,0,0,0,0,0,i want to use a tf.estimators.estimator to fine-tune a model that contains batch normalization e.g resnet to initialize the model i use the new warmstartsettings and pass it to the estimators warm_start_from argument unfortunately this will only warm-start trainable variables and the moving_mean and moving_variance created by the batch normalization layer are not part of the trainable variables collection thus the moving averages will not be warm-started.because of this problem tensorflow rc introduced the possibility to pass a list of variables to the warmstartsettings unfortunately when using estimators this does not help because the variables are recreated all the time and not know at the position where the warm start settings have to be defined.a possible solution might be to make it possible to pass a function to the warm_start_from argument of estimators that has access to the current graph and returns a warmstartsettings object.my current workaround is to specify a list of variable names rather than variables in the warmstartsettings but this is not exactly how its supposed to be and comes with its own problems e.g getting the list of variable names before the model was built i just use all the variable names that are saved in the checkpoint and exlude e.g global_step but this is problematic because it circumvents certain checks and assertions
330886407,19876,https://api.github.com/repos/tensorflow/tensorflow/issues/19876,xielm12,1,0,0,0,0,0,"here is my code: import tensorflow as tffeatures color r,a a,g a,g g,b , b,r weight color_feature tf.feature_column.categorical_column_with_hash_bucket key color hash_bucket_size dtype=tf.string)column tf.feature_column.weighted_categorical_column(color_feature weight,dtype tf.float)indicator tf.feature_column.indicator_column(column)tensor tf.feature_column.input_layer(features indicator )with tf.session as session session.run(tf.global_variables_initializer session.run(tf.tables_initializer print(session.run( tensor )) then got the error as follow: d:\programdata\anaconda\python.exe e:/wnd_test/test.pyd:\programdata\anaconda\lib\site-packages\hpy\__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters i t:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use avx w t:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc op_requires failed at sparse_to_dense_op.cc invalid argument indices is repeated w t:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc op_requires failed at sparse_to_dense_op.cc invalid argument indices is repeatedtraceback most recent call last file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_call return fn(*args file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run_fn options feed_dict fetch_list target_list run_metadata file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in call_tf_sessionrun run_metadata)tensorflow.python.framework.errors_impl.invalidargumenterror indices is repeated node input_layer/color_weighted_by_weight_indicator/sparsetodense sparsetodense t=dt_float tindices=dt_int validate_indices=true device=/job:localhost/replica:/task:/device:cpu: (input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparsetodense/default_value) during handling of the above exception another exception occurred:traceback most recent call last file e:/wnd_test/test.py line in module print(session.run( tensor file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run run_metadata_ptr file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run feed_dict_tensor options run_metadata file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_run run_metadata file d:\programdata\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror indices is repeated node input_layer/color_weighted_by_weight_indicator/sparsetodense sparsetodense t=dt_float tindices=dt_int validate_indices=true device=/job:localhost/replica:/task:/device:cpu: (input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparsetodense/default_value) caused by op input_layer/color_weighted_by_weight_indicator/sparsetodense defined at file e:/wnd_test/test.py line in module tensor tf.feature_column.input_layer(features indicator file d:\programdata\anaconda\lib\site-packages\tensorflow\python\feature_column\feature_column.py line in input_layer trainable cols_to_vars file d:\programdata\anaconda\lib\site-packages\tensorflow\python\feature_column\feature_column.py line in internal_input_layer trainable=trainable file d:\programdata\anaconda\lib\site-packages\tensorflow\python\feature_column\feature_column.py line in get_dense_tensor return inputs.get(self file d:\programdata\anaconda\lib\site-packages\tensorflow\python\feature_column\feature_column.py line in get transformed column._transform_feature(self pylint disable=protected-access file d:\programdata\anaconda\lib\site-packages\tensorflow\python\feature_column\feature_column.py line in transform_feature return sparse_ops.sparse_tensor_to_dense(weighted_column file d:\programdata\anaconda\lib\site-packages\tensorflow\python\ops\sparse_ops.py line in sparse_tensor_to_dense name=name file d:\programdata\anaconda\lib\site-packages\tensorflow\python\ops\sparse_ops.py line in sparse_to_dense name=name file d:\programdata\anaconda\lib\site-packages\tensorflow\python\ops\gen_sparse_ops.py line in sparse_to_dense name=name file d:\programdata\anaconda\lib\site-packages\tensorflow\python\framework\op_def_library.py line in apply_op_helper op_def=op_def file d:\programdata\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in create_op op_def=op_def file d:\programdata\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinvalidargumenterror see above for traceback indices is repeated node input_layer/color_weighted_by_weight_indicator/sparsetodense sparsetodense t=dt_float tindices=dt_int validate_indices=true device=/job:localhost/replica:/task:/device:cpu: (input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparseslice input_layer/color_weighted_by_weight_indicator/sparsetodense/default_value) process finished with exit code the invalidargumenterror is caused by the fact that feature b and feature r are hashed into the same hash_bucket itll be great if a combiner argument as in tf.feature_column.embedding_column can be implemented in the tf.feature_column.indicator_column funciton"
330848010,19870,https://api.github.com/repos/tensorflow/tensorflow/issues/19870,samikama,1,0,0,0,0,0,this pr replaces std::cout with vlog in remapper.cc and removes scoped_allocator_ops_op_lib dependency from scopedallocator this dependency is already satisfied through other libraries in core and causes a fatal for libraries that uses meta_optimizer due to double registration of scopedallocator op due to double inclusion of static objects
330838370,19869,https://api.github.com/repos/tensorflow/tensorflow/issues/19869,claynerobison,1,0,0,0,0,0,reverting acdadaceabefefdabsl is not yet ready for use by open source tensorflow see can you look at this the absl change to mkl_util.h broke our mkl builds
330383219,19840,https://api.github.com/repos/tensorflow/tensorflow/issues/19840,b0noI,9,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no i have used unchanged code from branch r commit ebccacdbbe os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below) :branch r commit ebccacdbbe python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory v gb exact command to reproduce : bashgit clone benchmarks/scripts/tf_cnn_benchmarksgit checkout caecbdbccceecpython tf_cnn_benchmarks.py data_format=nchw batch_size num_batches model=resnet optimizer=momentum variable_update=replicated nodistortions hierarchical_copy=true gradient_repacking datasets_use_prefetch=false display_every gpu_thread_mode=gpu_shared num_gpus use_fp=true results: python tf_cnn_benchmarks.py data_format=nchw batch_size num_batches model=resnet optimizer=momentum variable_update=replicated nodistortions hierarchical_copy=true gradient_repacking datasets_use_prefetch=false display_every gpu_thread_mode=gpu_shared num_gpus use_fp=true/usr/lib/python/dist-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converterstraceback most recent call last file tf_cnn_benchmarks.py line in module import benchmark_cnn file home/vkovalevskyi/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py line in module import data_utils file home/vkovalevskyi/benchmarks/scripts/tf_cnn_benchmarks/data_utils.py line in module from tensorflow.contrib.data.python.ops import batching file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/__init__.py line in module from tensorflow.contrib import cudnn_rnn file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/cudnn_rnn/__init__.py line in module from tensorflow.contrib.cudnn_rnn.python.layers import file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py line in module from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py line in module from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py line in module from tensorflow.contrib.rnn.python.ops import lstm_ops file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/rnn/__init__.py line in module from tensorflow.contrib.rnn.python.ops.gru_ops import file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py line in module resource_loader.get_path_to_datafile(_gru_ops.so file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/util/loader.py line in load_op_library ret load_library.load_op_library(path file home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/python/framework/load_library.py line in load_op_library lib_handle py_tf.tf_loadlibrary(library_filename)tensorflow.python.framework.errors_impl.notfounderror home/vkovalevskyi/.local/lib/python./site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so undefined symbol znstream_executorstreamthenblasgemmens_blastransposees_yyyfrkns_devicememoryifeeis_ifps_i
330326815,19836,https://api.github.com/repos/tensorflow/tensorflow/issues/19836,caisq,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu doesnt matter tensorflow installed from source or binary pip tensorflow version use command below python version doesnt mattersupposed you run the following code to construct a nested sequential model with tf.keras and save it as a h file. pyimport tensorflow as tfwith tf.graph().as_default tf.session inner_model tf.keras.sequential tf.keras.layers.dense input_shape activation=relu tf.keras.layers.dense activation=tanh outer_model tf.keras.sequential outer_model.add(inner_model outer_model.add(tf.keras.layers.dense activation=sigmoid outer_model.compile(loss=binary_crossentropy optimizer=sgd outer_model.save(/tmp/tf_nested_keras.h) inside the saved h file you can see the layers have the names dense dense dense_.however the weight names do not match up with the layer names dense/kernel dense/bias dense_/kernel dense_/bias dense_/kernel dense_/bias:,this is causing issues for tensorflow.js converters note that the same issue does not occur for non-tf keras"
329849922,19803,https://api.github.com/repos/tensorflow/tensorflow/issues/19803,yongtang,1,0,0,0,0,0,this fix is a follow up on to expose tf.broadcast_to op to r.. note please note this pr is to r feel free to close the pr if not appropriate. signed-off-by yong tang yong.tang.github@outlook.com
329764895,19797,https://api.github.com/repos/tensorflow/tensorflow/issues/19797,awan-10,1,0,0,0,0,0,this pull request is to allow low-level mpi libraries like mvapich-gdr that use ld_preload style interception of cuda calls to optimize performance currently a cuda_invalid_context error appears if cumemalloc is used the change to cudamalloc will allow interception there are no known side effects of this patch
329741795,19795,https://api.github.com/repos/tensorflow/tensorflow/issues/19795,achraf-boussaada,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu macos sierra version tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source) :gcc describe the problemim training an object detection model using the new ssdlite_mobilenet_v_coco and its configuration file ssdlite_mobilenet_v_coco.config and tensorflow installed from source when i launch the training tensorflow starts printing the same info twice this problem didnt happen while training the same network im trying to get with a different model checkpoint ssd_mobilenet_v_coco and the configuration file ssd_mobilenet_v_pets.config and with tensorflow installed from pip i tested with version and note i didnt change the code in both cases and i wonder whats the cause of this source code logs info:tensorflow:global step loss sec/step)info:tensorflow:global step loss sec/step)info:tensorflow:global step loss sec/step)info:tensorflow:global step loss sec/step)info:tensorflow:global step loss sec/step)info:tensorflow:global step loss sec/step)info:tensorflow:recording summary at step info:tensorflow:recording summary at step info:tensorflow:global_step/sec info:tensorflow:global_step/sec
329437002,19774,https://api.github.com/repos/tensorflow/tensorflow/issues/19774,akamaus,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary) :binary installed with pip tensorflow version use command below) :v..--gbce python version describe the problemim trying to load a metagraph file saved with tf-v..-rc--ga loading fails with the following error file home/vyal/.local/lib/python./site-packages/tensorflow/python/training/saver.py line in import_meta_graph kwargs file home/vyal/.local/lib/python./site-packages/tensorflow/python/framework/meta_graph.py line in import_scoped_meta_graph producer_op_list=producer_op_list file home/vyal/.local/lib/python./site-packages/tensorflow/python/util/deprecation.py line in new_func return func(*args kwargs file home/vyal/.local/lib/python./site-packages/tensorflow/python/framework/importer.py line in import_graph_def removedefaultattrs(op_dict producer_op_list graph_def file home/vyal/.local/lib/python./site-packages/tensorflow/python/framework/importer.py line in removedefaultattrs op_def op_dict node.op keyerror imageprojectivetransform i thought its the result of some backward incompability so i retested on python and tf-v..-rc--ga getting very similar message: in tf.train.import_meta_graph(.meta)---------------------------------------------------------------------------valueerror traceback most recent call last)
329291903,19761,https://api.github.com/repos/tensorflow/tensorflow/issues/19761,sxlllslgh,1,0,1,0,0,0,"system information i did not alter any code windows ver tensorflow installed from source tensorflow version python version no bazel microsoft visual studio cuda cudnn nvidia gtx gb build project tf_python_build_pip_package in release configuration describe the problemthe project pywrap_tensorflow_internal failed to build and error was threadpool_device.obj error lnk unresolved external symbol int cdecl tensorflow::port::numhyperthreadspercore(void numhyperthreadspercore@port@tensorflow@@yahxz in function public cdecl tensorflow::threadpooldevice::threadpooldevice(struct tensorflow::sessionoptions const class std::basic_string,class std::allocator,class tensorflow::devicelocality const class tensorflow::allocator threadpooldevice@tensorflow@@qeaa@aebusessionoptions@@aebv?$basic_string@du?$char_traits@d@std@@v?$allocator@d@@@std@@v?$inttype@ubytes_tag_@tensorflow@@_j@gtl@@aebvdevicelocality@@peavallocator@@@z)finally i found that function numhyperthreadspercore was not implemented in tensorflow/core/platform/windows/port.cc while it was implemented in tensorflow/core/platform/posix/port.cc then i copy this function from posix/port.cc to windows/port.cc it worked source code _tensorflow/core/platform/posix/port.cc line int numhyperthreadspercore static const int ht_per_core tensorflow::port::cpuidnumsmt return ht_per_core ht_per_core"
329123439,19753,https://api.github.com/repos/tensorflow/tensorflow/issues/19753,yongtang,1,0,0,0,0,0,this fix is a follow up of to expose tf.broadcast_to previously the op was exposed as tf.contrib.framework.broadcast_to .this fix unhides the broadcastto so that it is exposed in tf.broadcast_to and also removes tf.contrib.framework.broadcast_to .signed-off-by yong tang yong.tang.github@outlook.com
329057322,19749,https://api.github.com/repos/tensorflow/tensorflow/issues/19749,strawhatRick,1,0,0,0,0,0,importerror traceback most recent call last file home/amartya/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file home/amartya/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file home/amartya/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow_internal fp pathname description)importerror libcublas.so cannot open shared object file no such file or directory
329032998,19744,https://api.github.com/repos/tensorflow/tensorflow/issues/19744,jrabary,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source gcc cuda/cudnn version gpu model and memory titan x exact command to reproduce n/awe are trying to use distribute.mirroredstrategy to train a model on multiple gpus on a single machine we have a working implementation without warm start training from scratch and now want to initialise to model using checkpoint from imagenet before the training our first attempt is to add init_from_checkpoint in our model function: def model_fn(features labels mode params tf.train.init_from_checkpoint(params resnet_checkpoint resnet but this gives us the following error .../tensorflow/contrib/distribute/python/values.py line in get_update_device use distributionstrategy.update to modify a mirroredvariable.) is there an example of how to warm start a training with an existing checkpoint
328912053,19736,https://api.github.com/repos/tensorflow/tensorflow/issues/19736,freedomtan,1,0,0,0,0,0,with model mobilenet_v_.__quant.tflite input image(grace_hooper.bmp and labels file labels.txt in tmp.run bazel run config opt tensorflow/contrib/lite/examples/python:label_image we can get results like military uniform windsor tie bow tie mortarboard suit
328900636,19733,https://api.github.com/repos/tensorflow/tensorflow/issues/19733,ytdu,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below v..--gb python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory titan x pascal gb exact command to reproduce describe the problemhi ive found tensorarrayscatterv op to be the bottleneck of my model and it seems like tensorarray.scatter is really slow on gpu.i compared tf.tensorarray.scatter with tf.scatter_update on cpu/gpu with the following code: import timeimport tensorflow as tfwith tf.device(/gpu t tf.variable(tf.random_normal ta tf.tensorarray(tf.float size element_shape perm tf.random_shuffle(tf.range dtype=tf.int for i in range idx perm i i v tf.random_normal t tf.scatter_update(t idx v ta ta.scatter(idx v o tf.gather(t idx o ta.gather(idx : )sess_config tf.configproto()sess_config.allow_soft_placement truesess tf.session(config=sess_config)sess.run( tf.global_variables_initializer() )total_time_tensor_scatter total_time_tensorarray_scatter for i in range start_time time.time sess.run( o total_time_tensor_scatter time.time start_time start_time time.time sess.run( o total_time_tensorarray_scatter time.time start_timeprint(total_time_tensor_scatter total_time_tensor_scatter)print(total_time_tensorarray_scatter total_time_tensorarray_scatter) the results are like:on cpu: total_time_tensor_scatter total_time_tensorarray_scatter on gpu: total_time_tensor_scatter total_time_tensorarray_scatter it seems like tensorarray scatter is slower than tensor scatter on cpu and it is much slower on gpu.by the way the timeline on gpu is like:! image it intrinsic for tensorarray or not?can it be optimized further
328759508,19712,https://api.github.com/repos/tensorflow/tensorflow/issues/19712,yongtang,0,0,0,0,0,1,this fix is an attempt to add kinesis support for tensorflows dataset kinesis is provided by aws as a managed data streaming service it is similar to apache kafka often used in places where maintaining an independent kafka cluster on aws is not desirable or not possible.this fix adds the kinesis support for tensorflow dataset similiar to the kafka integration in tensorflow kinesisdataset outputs tf.string for records.test cases have also been added which could be invoked manually.signed-off-by yong tang yong.tang.github@outlook.com
328365116,19681,https://api.github.com/repos/tensorflow/tensorflow/issues/19681,ArvinSiChuan,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information os ubuntu in nvidia-docker container with image tensorflow/tensorflow:latest-gpu-py describe the problemthe nvidia has upgraded their apt-repo sources lists files to https but the source-list files depended in tensorflow are still old one(with http so when we rebuild the image the issue occurs like this: shellerr release.gpg the following signatures were invalid nodata nodata get xenial-security/main amd packages kb get xenial-backports inrelease kb get xenial/universe sources kb get release.gpg b err release.gpg the following signatures were invalid nodata nodata following the lists file in the up-to-date nvidia/cuda:.-base-ubuntu image i tried to update the sources lists files in etc/apt/sources.list.d and it was solved in cuda.list deb in nvidia-ml.list : deb probably you need to rebuild and update your docker images
327942230,19657,https://api.github.com/repos/tensorflow/tensorflow/issues/19657,samsamoa,9,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos also linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a problem on cpu and gpu exact command to reproduce saver.save(sess,fname describe the problemtf consistently crashes when saving a model after loading datasets of a certain size into memory and creating iterators for them this occurs both on macos and on linux.some additional information if i make the iterators initializable instead of one-shot there is no crash if i only make one iterator not two there is no crash if i reduce the size of the dataset there is no crash source code logsthe most minimal example i could come up with: import numpy as npimport tensorflow as tf the following two lines are necsesary to cause the crash also reduce the size to eliminate the crashdata tf.data.dataset.from_tensor_slices(np.ones( ,, ))iter data.make_one_shot_iterator()data tf.data.dataset.from_tensor_slices(np.ones( ,, ))iter data.make_one_shot_iterator()feat iter.get_next()out tf.layers.dense(inputs=feat units=)saver tf.train.saver()with tf.session as sess sess.run(tf.global_variables_initializer crash here saver.save(sess=sess,save_path=/tmp/crash-example) this causes the following crash: libprotobuf fatal external/protobuf_archive/src/google/protobuf/message_lite.cc check failed byte_size_before_serialization byte_size_after_serialization tensorflow.graphdef was modified concurrently during serialization.terminate called after throwing an instance of google::protobuf::fatalexception what check failed byte_size_before_serialization byte_size_after_serialization tensorflow.graphdef was modified concurrently during serialization.aborted core dumped"
327790195,19649,https://api.github.com/repos/tensorflow/tensorflow/issues/19649,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where there were no normalizer_fn support for sequence_numeric_column unlike numeric_column this fix adds the normalizer_fn support.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
327625636,19640,https://api.github.com/repos/tensorflow/tensorflow/issues/19640,rickragv,2,0,0,0,0,0,im using an aws g.xlarge instance which has gpus.tf serving is able to detect both gpus and initialise them but while running the model it only uses gpu to the maximum.we are on version even though the client sends upto requests in parallel the model server only uses the first gpu facaf--e-c-dbadbec
327320529,19621,https://api.github.com/repos/tensorflow/tensorflow/issues/19621,mBHyuC,3,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below v..--gbce python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory g/quadro mm exact command to reproduce describe the problemthe graph can be executed with adamoptimizer thus the graph input is invalid in the example an out-of-bound embedding index is passed other tested optimizer rmsp,ada yield invalidargumenterror indices is not in source code logs pythonimport tensorflow as tfimport numpy as npdef embed_helper(inputs size dim name=none std np.square dim test_emb tf.variable(tf.random_uniform( size dim std std name=name return tf.nn.embedding_lookup(test_emb inputs)num_factors num_embed graph tf.graph()with graph.as_default x tf.placeholder(tf.int shape=(none x_emb embed_helper(x num_embed num_factors name=none very_complicated_net tf.square(tf.subtract(x_emb tf.constant rmsp and the other tested optimizer yield invalidargumenterror adamoptimizer does not invalidargumenterror see above for traceback indices is not in opt tf.train.rmspropoptimizer(learning_rate opt tf.train.adamoptimizer(learning_rate graph_step opt.minimize(very_complicated_net init tf.global_variables_initializer()session tf.session(config=none graph=graph)session.run(init is not in index_batch_out_of_bound np.array( ).reshape x_feed_dict x index_batch_out_of_bound}session.run(graph_step x_feed_dict"
327252887,19616,https://api.github.com/repos/tensorflow/tensorflow/issues/19616,amundv,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos high sierra tensorflow installed from source or binary) :binary tensorflow version use command below) :v..--gad dev python version bazel version:n/acuda/cudnn version:n/agpu model and memory:n/aexact command to reproduce: pythonfrom tensorflow.python.tools import inspect_checkpoint as chkpchkp.print_tensors_in_checkpoint_file(gs://...,all_tensors=true describe the problemthe function print_tensors_in_checkpoint_file does not seem to handle google cloud bucket addresses of the form gs but rather gives a not found error this does not match the behavior of tf.train.saver.save and restore source code logsn/a"
327211181,19611,https://api.github.com/repos/tensorflow/tensorflow/issues/19611,boeseMilch,3,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow this only concerns a nativ tensorflow function os platform and distribution e.g linux ubuntu tested on arch linux os x and windows tensorflow installed from source or binary binary tensorflow version use command below cpu python version and bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version cpu-only issue gpu model and memory cpu-only issue exact command to reproduce tf.convd_transpose describe the problemtf.convd_transpose is a factor slower than other operations doing the same or an equivalent computation when working with tensorflow on cpu this was previously reported in issue and but none have provided a minimal example and both were closed due to inactivity source code logsminimal example that shows how much slower convd_transposed is compared to einsum doing the same computation and to the convd forward computation: import tensorflow as tfimport timebatch_size inp tf.ones((batch_size,,,,))filter_conv tf.ones((,,,,))filter_fc tf.ones((,,,,))lat tf.ones((batch_size,,,,))sess tf.interactivesession()t time.time takes sec on my machinesess.run(tf.nn.convd_transpose(lat filter_conv inp.shape, ,,,, ,padding=valid))delta_t time.time tprint(time it takes for convd_transpose delta_t)t time.time takes sec on my machinesess.run(tf.einsum(ijclm,abclm->iabcj,lat,filter_fc delta_t time.time tprint(time it takes with einsum to do the same computation delta_t)t time.time takes sec on my machinesess.run(tf.nn.convd(inp filter_conv padding=valid))delta_t time.time tprint(time it takes to apply convd delta_t"
327122935,19602,https://api.github.com/repos/tensorflow/tensorflow/issues/19602,Hoeze,1,0,0,0,0,0,having named dimensions would be a nice improvement to dynamic shapes.im thinking about a similar system like xarrays of current code: images tf.placeholder(tf.float shape=(none none none images_shape tf.shape(images)ones tf.ones(images_shape dtype=images.dtype)images_plus images ones print(images_plus tf.tensor add shape dtype=float>due to the none dimensions its hard to distinguish them especially when using reshaping ops.in my opinion some type of tf.dimension would be better: image_dim tf.dimension(size=none name=image_dim_)image_dim tf.dimension(size=none name=image_dim_)image_dim tf.dimension(size=none name=image_dim_)images tf.placeholder(tf.float shape=(image_dim image_dim image_dim images_shape tf.shape(images)ones tf.ones(images_shape dtype=images.dtype)images_plus images ones print(images_plus tf.tensor add shape=( image_dim image_dim image_dim dtype=float>also this maybe helps to do some shape inference system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source none gcc/compiler version if compiling from source none cuda/cudnn version none gpu model and memory none exact command to reproduce none
326844294,19584,https://api.github.com/repos/tensorflow/tensorflow/issues/19584,gunan,1,0,0,0,0,0,as announced in release notes tensorflow release binaries version and higher are prebuilt with avx instruction sets this means on any cpu that do not have these instruction sets either cpu or gpu version of tf will fail to load with any of the following errors importerror dll load failed a crash with return code our recommendation is to build tf from sources on these systems system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu/windows/macos tensorflow installed from source or binary binary tensorflow version use command below and up python version and any newer bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version any gpu model and memory any exact command to reproduce python c import tensorflow as tf
326751628,19575,https://api.github.com/repos/tensorflow/tensorflow/issues/19575,tonyxty,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below tf.git_version is unknown i compiled from commit dcbbdbeacabfabca python version python bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce bazel build config=opt tensorflow/tools/pip_package:build_pip_package describe the problemwhen compiling tensorflow from source following the instructions here i encountered the following c error: error path/to/tensorflow/tensorflow/python/build c compilation of rule tensorflow/python:cpp_python_util failed exit tensorflow/python/util/util.cc error function in namespace std does not name a template type const std::function
326746091,19573,https://api.github.com/repos/tensorflow/tensorflow/issues/19573,reid-fu,1,0,0,0,0,0,i serialized a tensorflow model with the following code save_path self.saver.save(self.session os.path.join(self.logdir model.ckpt global_step logging.info(model saved in file s save_path and im now trying to restore it from scratch in a separate file using the following code saver tf.train.import_meta_graph(proj_dir logs/default/model.ckpt-.meta session tf.session saver.restore(session proj_dir logs/default/model.ckpt print(model restored)when tf.train.import_meta_graph is called the following exception is thrown libprotobuf error google/protobuf/io/coded_stream.cc a protocol message was rejected because it was too big more than bytes to increase the limit or to disable these warnings see codedinputstream::settotalbyteslimit in google/protobuf/io/coded_stream.h traceback most recent call last file home/reid/projects/research/ccg/taggerflow_modified/test/tf_restore.py line in module saver tf.train.import_meta_graph(proj_dir logs/default/model.ckpt-.meta file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in import_meta_graph read_meta_graph_file(meta_graph_or_file clear_devices file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in read_meta_graph_file text_format.merge(file_content.decode(utf meta_graph_def file usr/lib/python./encodings/utf_.py line in decode return codecs.utf__decode(input errors true unicodedecodeerror utf codec cant decode byte xa in position invalid start bytefor reference heres the first few lines of proj_dir>/logs/default/model.ckpt-.meta a>:^r
326724438,19570,https://api.github.com/repos/tensorflow/tensorflow/issues/19570,yjl9122,1,0,0,0,0,0,it seems in tensorflow the tf.cumsum operation is extremely slow on gpu and takes a huge amount of time of the total time).in pytorch as expected the sort operation is the one that takes the most time cumsum is virtually instant on gpu.i give an issues on others github we get conclusion that is cumsum is very slow aginst pytorch we issue is is lovasz_grad(gt_sorted computes gradient of the lovasz extension w.r.t sorted errors see alg in paper gts tf.reduce_sum(gt_sorted intersection gts tf.cumsum(gt_sorted union gts tf.cumsum gt_sorted jaccard intersection union jaccard tf.concat((jaccard jaccard jaccard return jaccard
326700789,19567,https://api.github.com/repos/tensorflow/tensorflow/issues/19567,simon-moloco,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory x geforce gtx at g exact command to reproduce describe the problemi have a rank tensor say a tensor and i want to apply d max pooling if i run in cpu codes work great however if i run in gpu codes will throw exceptions like the following e tensorflow/stream_executor/cuda/cuda_dnn.cc loaded runtime cudnn library compatibility version but source was compiled with compatibility version if using a binary install upgrade your cudnn library to match if building from sources make sure the library loaded at runtime matches a compatible version specified during compile configuration w tensorflow/stream_executor/stream.h attempting to perform dnn operation using streamexecutor without dnn supporttraceback most recent call last file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train loss self._train_model(input_fn hooks saving_listeners file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model loss mon_sess.run( estimator_spec.train_op estimator_spec.loss file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run raise six.reraise(*original_exc_info file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run return self._sess.run(*args kwargs file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run return self._sess.run(*args kwargs file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run run_metadata file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.internalerror cudnn poolforward launch failed node dnn/input_from_feature_columns_/max_poolingd/maxpool maxpool t=dt_float data_format=nchw ksize padding=same strides device=/job:localhost/replica:/task:/device:gpu: (dnn/input_from_feature_columns_/max_poolingd/maxpool--transposenhwctonchw-layoutoptimizer node dnn/gradients/dnn/input_from_feature_columns_/input_layer/b_media_appbundle_format_embedding/b_media_appbundle_format_embedding_weights_grad/select recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__...d/select tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu: () caused by op udnn/input_from_feature_columns_/max_poolingd/maxpool defined at file usr/lib/python./runpy.py line in run_module_as_main main fname loader pkg_name file usr/lib/python./runpy.py line in run_code exec code in run_globals file data/jenkins/src/marvel/python/moloco/deploy/models/tf_dnn/training/main/task.py line in module tf.app.run file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(argv file data/jenkins/src/marvel/python/moloco/deploy/models/tf_dnn/training/main/task.py line in main t.train_eval file data/jenkins/src/marvel/python/moloco/learn/training/trainer.py line in train_eval self.model.train_eval(config model_dir train_data eval_data file data/jenkins/src/marvel/python/moloco/learn/models/tf_deep.py line in train_eval training.train_and_evaluate(m train_spec eval_spec file data/jenkins/src/marvel/python/moloco/learn/estimators/training.py line in train_and_evaluate executor.run_local file data/jenkins/src/marvel/python/moloco/learn/estimators/training.py line in run_local self._estimator.train(input_fn=self._train_spec.input_fn max_steps=max_steps hooks=train_hooks file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train loss self._train_model(input_fn hooks saving_listeners file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model features labels model_fn_lib.modekeys.train self.config file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in call_model_fn model_fn_results self._model_fn(features=features kwargs file data/jenkins/src/marvel/python/moloco/learn/estimators/dnn.py line in model_fn config=config file data/jenkins/src/marvel/python/moloco/learn/estimators/dnn.py line in dnn_model_fn last_layer_feats=ll_feats.values file data/jenkins/src/marvel/python/moloco/learn/estimators/logit_ops.py line in build_dnn_logits last_layer_feats=last_layer_feats file data/jenkins/src/marvel/python/moloco/learn/estimators/logit_ops.py line in build_hidden_layers net apply_pooling(net pooling pooling_size pooling_stride file data/jenkins/src/marvel/python/moloco/learn/estimators/logit_ops.py line in apply_pooling o tf.layers.max_poolingd(o pooling_size strides=pooling_stride padding=same file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/layers/pooling.py line in max_poolingd return layer.apply(inputs file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/layers/base.py line in apply return self.__call__(inputs args kwargs file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/layers/base.py line in call outputs self.call(inputs args kwargs file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/layers/pooling.py line in call data_format=data_format file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in max_pool name=name file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in max_pool data_format=data_format name=name file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file var/lib/jenkins/venvs/gpumoloco/local/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinternalerror see above for traceback cudnn poolforward launch failed node dnn/input_from_feature_columns_/max_poolingd/maxpool maxpool t=dt_float data_format=nchw ksize padding=same strides device=/job:localhost/replica:/task:/device:gpu: (dnn/input_from_feature_columns_/max_poolingd/maxpool--transposenhwctonchw-layoutoptimizer node dnn/gradients/dnn/input_from_feature_columns_/input_layer/b_media_appbundle_format_embedding/b_media_appbundle_format_embedding_weights_grad/select recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__...d/select tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu: () info:root:return codes source code logshere are my codes: pythondef apply_pooling(inputs pooling pooling_size pooling_stride apply pooling operations on inputs tensor args inputs a tensor with at least rank pooling when not none max means max pooling and average means average pooling pooling_size the pooling size in all spatial dimensions pooling_stride the stride operation size in all spatial dimensions returns a tensors after applying for pooling strategy if pooling is none or pooling pooling.none return inputs expand dimension if inputs has not enough rank ndims len(inputs.get_shape().as_list if ndims raise valueerror(inputs must be at least rank given its format(ndims o inputs if ndims o tf.expand_dims(o todo fix the issue of not being able to run pooling in gpu if pooling pooling.max o tf.layers.max_poolingd(o pooling_size strides=pooling_stride padding=same elif pooling pooling.average o tf.layers.average_poolingd(o pooling_size strides=pooling_stride padding=same else raise valueerror(unsupported pooling strategy format(pooling squeeze dimenion if necessary if ndims o tf.squeeze(o return o
326326336,19545,https://api.github.com/repos/tensorflow/tensorflow/issues/19545,lgeiger,1,0,0,0,0,4,this allows ganestimators to be easily exported via ganestimator::export_saved_model when trying to when trying to export the model export_saved_model raises: python-tracebackvalueerror export_outputs must be a dict and not fixes
326144111,19531,https://api.github.com/repos/tensorflow/tensorflow/issues/19531,smistad,2,0,0,0,0,0,when building tensorflow on windows with cmake using the bit toolchain compiler cl.exe and linker link.exe is needed using the bit toolchain often result in errors such as c compiler out of heap space and c compiler is out of heap space in pass there are several issues reported on this current readme for cmake in tensorflow states that you can fix this by setting up a bunch of environment variables using the visual studio bat script c:\program files x)\microsoft visual studio vc\bin\amd\vcvarsall.bat.however i have discovered that this has no effect if you look in task manager while you are compiling when using this bat script you can see the task microsoft compiler driver bit running the reason why this has no effect is that cmake select which compiler and linker is used it will output this to the console the first time you run configure with cmake you will then see that cmake_cxx_compiler is set to c:/program files x)/microsoft visual studio vc/bin/ x _amd/cl.exe the x part i assume means the bit compiler is used while the amd part means you are building an bit application two very different things the fact that some people report that using the bat script actually helps is most likely due to chance i have observed several times that sometimes this error occurs while other times it doesnt i think this is due to multi-threading during compilation.newer versions of cmake allow you to specify the toolset host architecture to bit using the flag: -thost=x this is documented here adding this flag i observe that cmake_cxx_compiler is set to c:/program files x)/microsoft visual studio vc/bin/ amd /cl.exe instead also while compiling the task manager now shows the task microsoft compiler driver which is the bit version of cl.exe instead of microsoft compiler driver bit) .using this flag i have not experienced compiler out of heap space issues anymore on neither visual studio and on windows i hope others can test this and hopefully verify this solution.i also updated the minimum cmake version required for windows to which supports this host toolset flag
325590125,19491,https://api.github.com/repos/tensorflow/tensorflow/issues/19491,fcacarminati,3,0,0,0,0,0,"hello we are running a not-very-complex d convolution problem had we have extremely poor performance here is the summary of our problemnow the technical part.i am running on an haswell cpu in a mac os running high sierra model name macbook promodel identifier macbookpro,processor name intel core iprocessor speed ghznumber of processors total number of cores l cache per core kbl cache mbmemory gbtensorflow performance memory allocationmemory allocation seems highly unoptimized i see an allocation of gb m allocations out of which we are left with gb persistent corresponding to k permanent allocations the memory churn is enormous and this may affect very seriously performance most of those are very small allocation deallocation which happen hereeigen::nonblockingthreadpooltempl::schedule(std::__::function"
325454613,19478,https://api.github.com/repos/tensorflow/tensorflow/issues/19478,ixime,2,0,0,0,0,0,tensorflow fold has no updates since months and as far as i know it only works with tensorflow v..does anybody know if eager execution in newer tensorflow versions supports dynamic batching like tensorflow fold if not is it likely that future versions would support it?thanks for any answer system information have i written custom code as opposed to using a stock example script provided in tensorflow) :n/a os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary n/a tensorflow version use command below from to the latest python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a
325303242,19461,https://api.github.com/repos/tensorflow/tensorflow/issues/19461,yongtang,0,0,0,0,0,1,apache parquet is a widely used columnar storage format available in the hadoop ecosystem this pr is a preliminary attempt to add apache parquet support for tensorflows dataset api it should help many to working on existing parquet-formatted big data with tesnorflow.the pr may not cover all the use cases though it could be served as a starting point for further improvement in the future.the parquetdataset depends on parquet-cpp apache project as well as other dependencies e.g thrift etc the parquetdataset only builds on linux at the moment this pr also adds the option in configure so that those dependencies could be skipped.signed-off-by yong tang yong.tang.github@outlook.com
324948402,19438,https://api.github.com/repos/tensorflow/tensorflow/issues/19438,venkat-kittu,1,0,0,0,0,0,i am getting below while importing tensorflowmy os is windows cuda cudnn tensorflow tensorflow installed from piptraceback most recent call last file c:\program files\python\lib\site-packages\tensorflow\python\platform\self_check.py line in preload_check ctypes.windll(build_info.cudart_dll_name file c:\program files\python\lib\ctypes\__init__.py line in init self._handle dlopen(self._name mode)oserror winerror the specified module could not be foundduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\program files\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import pywrap_tensorflow pylint disable=unused-import file c:\program files\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module self_check.preload_check file c:\program files\python\lib\site-packages\tensorflow\python\platform\self_check.py line in preload_check build_info.cudart_dll_name build_info.cuda_version_number))importerror could not find cudart_.dll tensorflow requires that this dll be installed in a directory that is named in your path environment variable download and install cuda from this url
324930474,19434,https://api.github.com/repos/tensorflow/tensorflow/issues/19434,kjancke,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below v..--gee python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce bazel build tensorflow/contrib/android:libtensorflow_inference.so crosstool_top=//external:android/crosstool host_crosstool_top=@bazel_tools//tools/cpp:toolchain cpu=xi believe this is a bugwhen i run the above command at some point the compiler linker stops with the following error message:error home/karsten/.cache/bazel/_bazel_karsten/ceebbacf/external/protobuf_archive/build linking of rule protobuf_archive//:js_embed failed exit usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-./libgcc/unwind-dw-fde-dip.c error undefined reference to dl_iterate_phdrclang error linker command failed with exit code use v to see invocation)target tensorflow/contrib/android:libtensorflow_inference.so failed to builduse verbose_failures to see the command lines of failed build steps.i have not injected any custom code
324900370,19433,https://api.github.com/repos/tensorflow/tensorflow/issues/19433,karthee320,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no host os platform and distribution e.g linux ubuntu linux ubuntu target os platform and distribution e.g linux ubuntu linux yocto tensorflow installed from source or binary source cross compiling for arm tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version na gpu model and memory vivante gpu gc opencl exact commands to reproduce configureyou have bazel installed.please specify the location of python default is usr/bin/python found possible python library paths opt/ros/indigo/lib/python./dist-packages usr/local/lib/python./dist-packages usr/lib/python./dist-packagesplease input the desired python library path to use default is opt/ros/indigo/lib/python./dist-packages /usr/local/lib/python./dist-packagesdo you wish to build tensorflow with jemalloc as malloc support y/n yjemalloc as malloc support will be enabled for tensorflow.do you wish to build tensorflow with google cloud platform support y/n nno google cloud platform support will be enabled for tensorflow.do you wish to build tensorflow with hadoop file system support y/n nno hadoop file system support will be enabled for tensorflow.do you wish to build tensorflow with amazon s file system support y/n nno amazon s file system support will be enabled for tensorflow.do you wish to build tensorflow with xla jit support y/n nno xla jit support will be enabled for tensorflow.do you wish to build tensorflow with gdr support y/n nno gdr support will be enabled for tensorflow.do you wish to build tensorflow with verbs support y/n nno verbs support will be enabled for tensorflow.do you wish to build tensorflow with opencl sycl support y/n yopencl sycl support will be enabled for tensorflow.please specify which c compiler should be used as the host c compiler default is usr/bin/g please specify which c compiler should be used as the hostc compiler default is usr/bin/gcc do you wish to build tensorflow with computecpp support y/n ycomputecpp support will be enabled for tensorflow.please specify the location where computecpp for sycl is installed default is usr/local/computecpp relative location>/computecpp-ce-..-ubuntu-.-arm_do you wish to build tensorflow with cuda support y/n nno cuda support will be enabled for tensorflow.do you wish to build tensorflow with mpi support y/n nno mpi support will be enabled for tensorflow.please specify optimization flags to use during compilation when bazel option config=opt is specified default is march=native march=armv-aadd config=mkl to your bazel command to build with mkl support.please note that mkl on macos or windows is still not supported.if you would like to use a local mkl instead of downloading please set the environment variable tf_mkl_root every time before build.would you like to interactively configure workspace for android builds y/n nnot configuring the workspace for android builds.configuration finished bazel build crosstool_top=//arm-compiler:toolchain config=sycl cpu=armeabi-va config=opt s tensorflow/examples/label_image cxxopt=-std=c copt=-mfpu=neoni am currently trying to run a deep learning inference cnn network on embedded platforms in real-time using gpu with improved performance instead of creating my own inference engine i thought of using existing frameworks in gpu mode itself to leverage the performance after various study i found that in the list of frameworks caffe tensorflow mxnet torch there are no resource on how to cross compile a particular framework for my arm architecture except few resources for tensorflow though many frameworks suggest to use native compilation for building from source i have not provided with the resources to build natively for my imx quad cpu x arm cortex-a up to ghz per core gpu vivante gc yocto build leaving cross compilation as the only option.i referred this link cross compiling tf for jetson tk and got successful in cross compiling tensorflow cpu version to benchmark i tried to run inception network and found cpu utilization is nearly and the inference time is around seconds i need to make use of the vivante gpu embedded profile opencl available in the board to reduce this utilization percentage and the inference time since to use gpu a cuda option is ruled out taking into the consideration i have no nvidia gpus.(b i tried to make use of the opencl capability opencl support using the similar way like the cpu build i tried to cross compile with opencl flags support this time but failing miserably during the build forum responses they claim cross compiling tf with computecpp(codeplay sycl is not possible codeplay forum another implementation method with trisycl claims the codes only use cpu the option of using gpu is under research trisycl forum questions there is an another sycl implementation available open source sycl progtx whether tensorflow has an idea to integrate this implementation also is there any pre-requisite my device driver version is only opencl is that enough to run the tensorflow code?anyone attempted this and got success kindly guide me on how to proceed further to achieve my goal
324859721,19431,https://api.github.com/repos/tensorflow/tensorflow/issues/19431,cefengxu,1,0,0,0,0,0,describe the problemfirstly i download the mobilenet_v model from then i used command belown to get a quantized model mobilenet_v_.__frozen_quantized_graph.pb successfully. bazel-bin/tensorflow/tools/graph_transforms/transform_graph in_graph=/tmp mobilenet_v_._/mobilenet_v_.__frozen.pb inputs=input outputs=mobilenetv/predictions/reshape out_graph=/tmp/mobilenet_v_._/mobilenet_v_.__frozen_quantized_graph.pb transforms=add_default_attributes strip_unused_nodes(type=float shape remove_nodes(op=identity op=checknumerics fold_constants(ignore_errors=true fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order however when i used tflite toco command to convert pb to lite format but error was output tflite toco build command: bazel run config=opt tensorflow/contrib/lite/toco:toco input_file=/tmp/mobilenet_v_._/mobilenet_v_.__frozen_quantized_graph.pb output_file=/tmp/mobilenet_v_._/mobilenet_v_.__frozen_quantized_graph.lite input_format=tensorflow_graphdef output_format=tflite input_shapes mean_values std_values input_arrays=input output_arrays=mobilenetv/predictions/reshape inference_type=quantized_uint default_ranges_min default_ranges_max error output f tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc check failed isconstantparameterarray(*model bn_op->inputs isconstantparameterarray(*model bn_op->inputs isconstantparameterarray(*model bn_op->inputs batch normalization resolution requires that mean multiplier and offset arrays be constant error log i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizedconvd i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation requantizationrange i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizedreshape i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizev i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation quantizedreshape i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation dequantize i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before removing unused ops operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before general graph transformations operators arrays quantized f tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc check failed isconstantparameterarray(*model bn_op->inputs isconstantparameterarray(*model bn_op->inputs isconstantparameterarray(*model bn_op->inputs batch normalization resolution requires that mean multiplier and offset arrays be constant
324538383,19398,https://api.github.com/repos/tensorflow/tensorflow/issues/19398,dariocazzani,1,0,0,0,0,0,losses in tf.contrib.losses.metric_learning rely only on euclidean distance.i implemented the cosine distance that can be used to calculate the losses for lifted_struct_loss cluster_loss triplet_semihard_loss i added an extra argument metric with default value euclidean in order to make the api backward compatible
324478763,19390,https://api.github.com/repos/tensorflow/tensorflow/issues/19390,lubomir1,2,0,0,0,0,0,"edit simplified the example added system infoaccording to this thread tensorflow now uses the cudnn accelerations of group convolutions for depthwise_convd_native thank you for working on this however i am having a hard time reproducing any gains from the accelerated version both the native and accelerated versions of depthwise_convd_native is about times slower than doing a full dense convolution.in the example below a dense x convolution with in and out channels should do about times more multiplications compared to a group convolution with the same dimensions and groups xxx vs xxxx the latter can be implemented with depthwise_convd_native with channel_multiplier of followed by a sum so i expect a fully amortized depthwise_convd_native to be times faster than a dense convolution and yet it is about times slower it is also substantially slower than naive slice/convolve/concat implementation of group convolution system information have i written custom code no os platform and distribution linux ubuntu tensorflow installed from source from may commit eebbadadbbdbca python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory geforce gtx ti tf compiled for compute capability and volta v tf compiled for compute capability exact command to reproduce see code belowhere are my results on gtx ti depthwise s depthwise_cudnn s manual_group_conv s dense_conv s here is the code i used to test performance: import numpy as np time osimport tensorflow as tfos.environ tf_cpp_min_log_level def convx(bottom filters return tf.layers.convd(bottom filters kernel_size padding=same use_bias false data_format channels_first this is not even group convolution just the depthwise part without the sumdef depthwise(bottom num_groups input_chans bottom.shape group_size input_chans num_groups w tf.get_variable(name=var shape input_chans group_size return tf.nn.depthwise_convd_native(bottom w strides padding same data_format nchw)def depthwise_cudnn(bottom num_groups with tf.get_default_graph()._kernel_label_map({depthwiseconvdnative cudnn_grouped_convolution return depthwise(bottom num_groups)def manual_group_conv(bottom num_groups input_chans bottom.shape group_size input_chans num_groups slices bottom :,i:(i+group_size for i in range input_chans group_size convs convx(sl group_size for sl in slices return tf.concat(convs axis def dense_conv(bottom num_groups return convx(bottom bottom.shape )input_shape groups dtype tf.floatfor cnv_type in depthwise depthwise_cudnn manual_group_conv dense_conv tf.reset_default_graph cnv cnv_type(tf.constant(np.zeros(input_shape dtype groups n with tf.session as sess sess.run(tf.global_variables_initializer sess.run(cnv initialization run start time.time for i in range(n sess.run(cnv print s fs cnv_type.func_name time.time start) i also tested with nhwc with similar results let me know if i am doing something wrong or whether you can replicate my perf results thanks"
324239037,19368,https://api.github.com/repos/tensorflow/tensorflow/issues/19368,mingyr,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux redhat tensorflow installed from source or binary source tensorflow version use command below r python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory p/g exact command to reproduce n/a describe the problemduring the test of tf.metrics two pieces of code snippets are believed to be equivalent outputting totally different results source code logscode snippet import tensorflow as tfa tf.constant tf.float)mean_a mean_a_uop tf.metrics.mean(a)sess tf.interactivesession()tf.global_variables_initializer().run()tf.local_variables_initializer().run()for in range sess.run(mean_a_uop print(sess.run(mean_a)) outputs: .......... code snippet import tensorflow as tfa tf.constant tf.float)mean_a mean_a_uop tf.metrics.mean(a)with tf.control_dependencies( mean_a_uop mean_a tf.identity(mean_a)sess tf.interactivesession()tf.global_variables_initializer().run()tf.local_variables_initializer().run()for in range print(sess.run(mean_a)) outputs
324216451,19363,https://api.github.com/repos/tensorflow/tensorflow/issues/19363,cancan101,0,0,0,3,0,0,nvidia is now shipping docker images based on ubuntu see it would be great if tensorflow started offering builds against these.ubuntu has added python have i written custom code n/aos platform and distribution dockertensorflow installed from dockertensorflow version n/abazel version n/acuda/cudnn version n/agpu model and memory n/aexact command to reproduce n/a
323976407,19348,https://api.github.com/repos/tensorflow/tensorflow/issues/19348,apischan,1,0,0,0,0,0,in your examples here there is a statement: sess.runner feed(save/const checkpointprefix addtarget(save/control_dependency run(); can you explain somewhere in documentation what is the magic constants used there and list all the variants of them somewhere
323487678,19312,https://api.github.com/repos/tensorflow/tensorflow/issues/19312,xiechao4,1,0,0,0,0,0,i add some code in tflitedemo and will use std::to_string but i failed with error thaterror no member named to_string in namespace stdthe compiling command of tflite indicates it will use gcc include filesexternal/androidndk/ndk/toolchains/llvm/prebuilt/linux-x_/bin/clang isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/./include isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/./libs/x_/include isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/./include/backwardthe android ndk i am using is android-ndk-rbin fact it has to_string definition in home/chao/android/android-ndk-rb/sources/cxx-stl/llvm-libc++/include/stringhow can i make tflite to compile based on llvm-libc not gnu-libstdc++?i update with the system information system information have i written custom code as opposed to using a stock example script provided in tensorflow add std::to_string in kernel/internal/conv.cc as auto str std::to_string(input->dims->size std::cout str os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary sources tensorflow version use command below master python version bazel version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce bazel build crosstool_top=//external:android/crosstool host_crosstool_top=@bazel_tools//tools/cpp:toolchain cxxopt=--std=c tensorflow/contrib/lite/java/demo/app/src/main:tflitecamerademo force_pic cpu=x verbose_failures fat_apk_cpu=x android ndk android-ndk-rc
323306622,19297,https://api.github.com/repos/tensorflow/tensorflow/issues/19297,gunan,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow will be required os platform and distribution e.g linux ubuntu windows all versions tensorflow installed from source or binary source and binary tensorflow version use command below master and all releases python version python bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce install tf on windows and try anything that reads from gcs or s describe the problemcurrently tf has no support for gcs and s on windows because they depend on curl and we have not worked to make curl build and work on windows someone needs to dive in and work through the problems and the rest should be just removing the windows exceptions for gcs and s support in configure.py.this issue is open for community contributions
323227033,19292,https://api.github.com/repos/tensorflow/tensorflow/issues/19292,kirk86,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below rc python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory ti exact command to reproduce any command like tf.keras.layers.timedistributed(tf.layers.convd padding=valid activation=tf.nn.relu)(x where x is any tensor that has keras metadata like keras_shape and/or keras_history describe the problem wrapping any tensor which contains keras metadata such as keras_shape or keras_histroy with tf.layers.layer or passing it through tf.layers.convd does not preserve the keras metadata instead it discards them in addition the tf.layers.layer does not implement any add_inbound_node method which is really confusing and leads to many errors with shape mismatches
323040627,19282,https://api.github.com/repos/tensorflow/tensorflow/issues/19282,DKingCN,1,0,0,0,0,0,im running tensorflow with cuda in a docker container as an instance of tensorflow/tensorflow:latest-gpu-py.when import tensorflow in a python terminal it throws an error because libcuda.so not in library path.these commands fix it cd usr/local/cuda/lib mv stubs/libcuda.so ln s libcuda.so libcuda.so ldconfig please fix that bug in docker image
322949131,19276,https://api.github.com/repos/tensorflow/tensorflow/issues/19276,MarkDaoust,1,0,0,0,0,0,git cherry-pick c updated installation instructions for tensowflow-tensorrt integration minor format changes to clean it up adding the python symlink command for devel packages too forcing the symlink creation updating the sed command for docker parameterized build
322530124,19250,https://api.github.com/repos/tensorflow/tensorflow/issues/19250,hujunxianligong,1,0,0,0,0,0,system information have i written custom code yes os platform and distribution win x tensorflow installed from source or binary binary tensorflow version use command below dev python version bazel version if compiling from source n/a gcc/compiler version if compiling from source):n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a problemi wrote a text cnn in eager model: pythonclass textcnn(tf.keras.model def init__(self self.conv_funcs tf.layers.convd(filter_num filter_size embedding_size activation=tf.nn.relu name=conv_{}.format(filter_size for filter_size filter_num in filter_size_num_list i set the list of convd layers as an instance property(self.conv_funcs and when i restore the model by tfe.checkpoint the weights of the convd layers are not restored.however i add the following code and the tfe.checkpoint successfully restore the weights of the convd layers: python self.conv self.conv_funcs self.conv self.conv_funcs is it a bug that tfe.checkpoint only restore the weights of instance properties of tf.keras.model
322522925,19249,https://api.github.com/repos/tensorflow/tensorflow/issues/19249,karta0807913,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu tx tensorflow installed from source or binary source tensorflow version use command below r r r python version bazel version if compiling from source non-git gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory denver gb exact command to reproduce bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_packageyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem. ./tensorflow/core/kernels/gather_functor_gpu.cu.h error calling a host function(__builtin_expect from a global function(tensorflow::gatheropkernel eigen::half int bool is not allowed./tensorflow/core/kernels/gather_functor_gpu.cu.h error calling a host function(__builtin_expect from a global function(tensorflow::gatheropkernel eigen::half long long bool is not allowed./tensorflow/core/kernels/gather_functor_gpu.cu.h error calling a host function(__builtin_expect from a global function(tensorflow::gatheropkernel eigen::half long long bool is not allowed./tensorflow/core/kernels/gather_functor_gpu.cu.h error calling a host function(__builtin_expect from a global function(tensorflow::gatheropkernel
322450679,19236,https://api.github.com/repos/tensorflow/tensorflow/issues/19236,Burton2000,1,0,0,0,0,0,system information have i written custom code n/a os platform and distribution windows tensorflow installed from binary tensorflow version python version bazel version n/a gcc/compiler version n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemcurrently tf.layers.convd_transpose doesnt have an output_shape parameter like tf.nn.convd_transpose does as a result when using strides the output shape is not defined it be possible to have this added in to the layers api so we can specify output shape
322079790,19209,https://api.github.com/repos/tensorflow/tensorflow/issues/19209,jdgumz,1,0,0,0,0,0,i was experimenting with different activation functions for the final layer of my graph recently when i noticed that the output graph was failing to save because it couldnt find a tensor by a name i had provided e.g my_final_tensor_op ).it worked correctly with an activation function like tf.nn.sigmoid : import tensorflow as tfimport numpy as npsample_values np.array dtype=np.float)sigmoid_tensor tf.nn.sigmoid(sample_values name=my_final_tensor_op)sigmoid_tensor.name my_final_tensor_op: but for tf.nn.leaky_relu i noticed that a maximum gets appended to whatever name value is passed: import tensorflow as tfimport numpy as npsample_values np.array dtype=np.float)leaky_relu_tensor tf.nn.leaky_relu(sample_values name=my_final_tensor_op)leaky_relu_tensor.name my_final_tensor_op/maximum: i suspect the reason why is that the name parameter is not passed in the call to the math_ops.maximum function: return math_ops.maximum(alpha features features) compare this to the case of tf.nn.sigmoid which does pass in the name parameter into the function call that it returns: return gen_math_ops.sigmoid(x name=name) this pr makes the change to have the leaky_relu function pass name to the math_ops.maximum function so that the desired name for the op carries down i also added a unit test that addresses this specific functionality.one potential issue that could come up is if theres a lot of existing code that expects the maximum string to be appended such as in the case where no name is set and the tensor ops name becomes leakyrelu/maximum if thats the case i would at least like to change the methods documentation so that the caller is aware of the maximum string concatenation side effect
322020423,19204,https://api.github.com/repos/tensorflow/tensorflow/issues/19204,jdgumz,2,0,0,0,0,0,i was experimenting with different activation functions for the final layer of my graph recently when i noticed that the output graph was failing to save because it couldnt find a tensor by a name i had provided e.g my_final_tensor_op ).it worked correctly with an activation function like tf.nn.sigmoid : import tensorflow as tfimport numpy as npsample_values np.array dtype=np.float)sigmoid_tensor tf.nn.sigmoid(sample_values name=my_final_tensor_op)sigmoid_tensor.name my_final_tensor_op: but for tf.nn.leaky_relu i noticed that a maximum gets appended to whatever name value is passed: import tensorflow as tfimport numpy as npsample_values np.array dtype=np.float)leaky_relu_tensor tf.nn.leaky_relu(sample_values name=my_final_tensor_op)leaky_relu_tensor.name my_final_tensor_op/maximum: i suspect the reason why is that the name parameter is not passed in the call to the math_ops.maximum function: return math_ops.maximum(alpha features features) compare this to the case of tf.nn.sigmoid which does pass in the name parameter into the function call that it returns: return gen_math_ops.sigmoid(x name=name) this pr makes the change to have the leaky_relu function pass name to the math_ops.maximum function so that the desired name for the op carries down i also added a unit test that addresses this specific functionality.one potential issue that could come up is if theres a lot of existing code that expects the maximum string to be appended such as in the case where no name is set and the tensor ops name becomes leakyrelu/maximum if thats the case i would at least like to change the methods documentation so that the caller is aware of the maximum string concatenation side effect
321998060,19203,https://api.github.com/repos/tensorflow/tensorflow/issues/19203,strin,3,0,0,0,0,0,hi i am trying to build tensorflow r branch with the following settings cuda cudnn gcc but i got the following error: tensorflow/core/kernels/neon/neon_depthwise_conv_op.cc required from here./tensorflow/core/kernels/neon/depthwiseconv_float.h warning variable fixed_input_depth set but not used wunused-but-set-variable int fixed_input_depth info from compiling tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc warning function tensorflow::::identityop::operator was declared but never referencedtensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc warning function tensorflow::::boundedoutputiterator::operator was declared but never referencedtensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc warning function tensorflow::::boundedoutputiterator::operator was declared but never referencedtensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc warning function tensorflow::::boundedoutputiterator::operator was declared but never referenced./tensorflow/core/kernels/gather_functor_gpu.cu.h error calling a host function(__builtin_expect from a global function(tensorflow::gatheropkernel eigen::half int bool is not allowed./tensorflow/core/kernels/gather_functor_gpu.cu.h error calling a host function(__builtin_expect from a global function(tensorflow::gatheropkernel
321944950,19198,https://api.github.com/repos/tensorflow/tensorflow/issues/19198,chaosink,1,0,0,0,0,0,system information have i written custom code i change the cuda capabilities to and os platform and distribution windows tensorflow installed from source or binary im compiling the source code tensorflow version use command below branch r eebdecbccabffa python version bazel version if compiling from source no gcc/compiler version if compiling from source vs v but v for cuda host compiler cuda/cudnn version cuda cudnn gpu model and memory ti titan v exact command to reproduce cmake-gui enable gpu and change cuda host compiler to v describe the problem __hadd is ambiguous when eigen_cuda_arch following is where the ambiguity comes from found in vs image source code logs tf_core_gpu_kernels compilation fails because of this problem: >building nvcc device object cmakefiles/tf_core_gpu_kernels.dir/__/__/core/kernels/release/tf_core_gpu_kernels_generated_check_numerics_op_gpu.cu.cc.obj>check_numerics_op_gpu.cu.cc>e:\program\ml\tensorflow_build_--\external\eigen_archive\eigen\src/core/arch/cuda/half.h error more than one instance of overloaded function hadd matches the argument list function hadd(int int function hadd(__half half argument types are const eigen::half const eigen::half) im confused that nobody has every post such an issue.nobody has ever tried changing cuda capabilities to or is there something wrong with my environment?it seems that this is a pure eigen issue
321447475,19173,https://api.github.com/repos/tensorflow/tensorflow/issues/19173,gr8Adakron,1,0,0,0,0,0,tensorflow version==. tensorhub version==. i am trying to learn tensorflow serving using small tutorial on medium blog as soon as i am running the basic program its returning this error from conv import register_converters as register_converterstraceback most recent call last file stdin line in module file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import pylint disable=redefined-builtin file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python.estimator import estimator_lib as estimator file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/tensorflow/python/estimator/estimator_lib.py line in module from tensorflow.python.estimator.inputs import inputs file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/tensorflow/python/estimator/inputs/inputs.py line in module from tensorflow.python.estimator.inputs.numpy_io import numpy_input_fn file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/tensorflow/python/estimator/inputs/numpy_io.py line in module from tensorflow.python.estimator.inputs.queues import feeding_functions file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py line in module import pandas as pd file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/pandas/__init__.py line in module from pandas.core.api import file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/pandas/core/api.py line in module from pandas.core.categorical import categorical file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/pandas/core/categorical.py line in module from pandas.core.base import pandasobject file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/pandas/core/base.py line in module import pandas.core.nanops as nanops file home/afzal/.virtualenvs/tensorflow_python/lib/python./site-packages/pandas/core/nanops.py line in module ver bn.__version__attributeerror module bottleneck has no attribute version
321432860,19171,https://api.github.com/repos/tensorflow/tensorflow/issues/19171,svjan5,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory geforce gtx ti gb exact command to reproduce code link describe the problemi am getting different results on declaring some unused placeholders in the code even with the same seed after setting the same seed in both the files the results obtained are different although the only difference in the files is an addition of some unused placeholders in placeholder_reproduce_unusedplc.py i am unable to understand the cause of this difference because the input and the computational graph is same for both the files also with different seeds sometimes the code with additional placeholders gives worse results.difference between the two files line placeholder_reproduce_unusedplc.py tf.placeholder(float none timesteps num_input )y tf.placeholder(float none num_classes )x tf.placeholder(float none timesteps num_input )y tf.placeholder(float none num_classes )x tf.placeholder(float none timesteps num_input )y tf.placeholder(float none num_classes )x tf.placeholder(float none timesteps num_input )y tf.placeholder(float none num_classes source code logsrunning placeholder_reproduce.py gives same everytime step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy optimization finished testing accuracy running placeholder_reproduce_unusedplc.py gives same everytime step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy step minibatch loss training accuracy optimization finished testing accuracy
321380466,19163,https://api.github.com/repos/tensorflow/tensorflow/issues/19163,achalshah20,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory titan x geforce exact command to reproduce :code: class temp public temp(){std::unique_ptr
321277326,19157,https://api.github.com/repos/tensorflow/tensorflow/issues/19157,ewilderj,1,0,0,0,0,0,"rfc document for design review pilot estimator head api design.(*not to be merged into tensorflow will be merged into new community repo.)please leave design feedback in this pr comment period will be open until cc ispirmustafa roumposg summary status proposed author(s george roumpos google tensorflow team sponsor mustafa ispir google updated in this doc we discuss the head api which helps users define customizablemodels that follow the tf.estimator api the apiincludes a head interface factory methods to create common heads such as regression head a multi_head method that combines more than one heads for multi-objective learning canned estimators that can use those heads.the api is already exposed under tf.contrib.estimator the code is in tensorflow/contrib/estimator/python/estimator/head.py , tensorflow/contrib/estimator/python/estimator/multi_head.py and tensorflow/python/estimator/canned/head.py and an earlier deprecated)version is exposed under tf.contrib.learn the goal of this design doc is tograduate the api from contrib to core"
321196870,19147,https://api.github.com/repos/tensorflow/tensorflow/issues/19147,sp00ck,1,0,0,0,0,0,is any real data to start oficial ruby port of tensorflow
321065749,19139,https://api.github.com/repos/tensorflow/tensorflow/issues/19139,efeiefei,1,0,0,0,0,0,i have similar errors with when quantize_nodes for tensortensor transformer model.its no problem after freeze_graph its also no problem with quantize_weights but when i qutize_nodes it will: tensorflow.python.framework.errors_impl.invalidargumenterror the node transformer/while/body/parallel_/body/decoder/layer_prepostprocess/layer_norm/add_/eightbit has inputs from different frames the input transformer/while/body/parallel_/body/decoder/layer_prepostprocess/layer_norm/add__eightbit/transformer/while/body/parallel_/body/decoder/layer_prepostprocess/layer_norm/add_/enter/quantize is in frame transformer/while/while_context the input transformer/while/body/parallel_/body/decoder/layer_prepostprocess/layer_norm/mul_/eightbit/requantize is in frame i have tried tf and tf and the error is the same.can anyone help me
320547656,19110,https://api.github.com/repos/tensorflow/tensorflow/issues/19110,jdlamstein,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu raspberry pi b raspian stretch tensorflow installed from source or binary) :what im asking about tensorflow version use command below) :tensorflow version python version python bazel version if compiling from source) :what im asking about gcc/compiler version if compiling from source) :what im asking about cuda/cudnn version :n/a gpu model and memory :broadcom videocore iv mhz bcm d part of gpu mhz video part of gpu mhz exact command to reproduce :addition to documentation installation of tensorflow lite on raspberry pi b raspian stretch describe the problemi would like instructions on how to install tensorflow lite onto raspberry pi b with raspbian stretch os to the best of my knowledge the documentation doesnt yet cover installation for raspbian stretch i posted on the raspberry pi stack exchange and i was directed here my goal is to deploy a tensorflow neural network onto the raspberry pi b with tensorflow lite
320314546,19085,https://api.github.com/repos/tensorflow/tensorflow/issues/19085,voegtlel,1,0,0,0,0,0,this pr fixes the memory leak described in tests for py_func are running.some internals replaced dict in funcregistry._funcs by weakref.weakvaluedictionary python python x replaced the cleanupfunc by a direct reference to the function thus renamed cleanup_py_funcs_used_in_graph to py_funcs_used_in_graph improved tests to cover the leak leak detection is now precise uses gc.collect
320127054,19071,https://api.github.com/repos/tensorflow/tensorflow/issues/19071,jgberg,1,0,0,0,0,0,system informationwe have reproduced this on mac osx a windows box running a linux vm and on a canadian research cluster all with the same result mac was v recompiled to take advantage of cpu v..--gdbfb stock tensorflow code and examples descriptionwe are using tensorflow v we train mobilenet v from tfhub feature version by running the code tensorflow provided in retrain.py from examples when training is done the validation set is run for us about images and it reports say validation accuracy then it saves the model we load the saved model using label_image.py also from tensorflow examples and see how its doing on a small validation set that weve withheld instead of seeing the validation of roughly confirmed we instead see a validation accuracy of about the clincher is that when use the same code and data and instead use mobilenet v we do see that training validation is confirmed by validation reported using label_image.py when calling label_image we change the input_layer from input for mobilenet v to placeholder for mobilenet vcan you confirm this what is going on logs:from retrain.py:...info:tensorflow:initialize variable module/mobilenetv/expanded_conv_/project/weights from checkpoint var/folders/x/fsgcqrthghlwdmgn/t/tfhub_modules/ffebbddaeeea/variables/variables with mobilenetv/expanded_conv_/project/weightsinfo:tensorflow:restoring parameters from tmp/_retrain_checkpointinfo:tensorflow:final test accuracy n=)...then from using label_image.py:it gets is correcthave i written custom code noos platform and distribution macosx latesttensorflow installed from piptensorflow version bazel version build label homebrew build target bazel-out/darwin opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jar build time sun may build timestamp build timestamp as int cuda/cudnn version n/agpu model and memory n/aexact command to reproduce see above
319848917,19049,https://api.github.com/repos/tensorflow/tensorflow/issues/19049,chychen,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gaad python version cuda/cudnn version release v gpu model and memory g describe the feature requestfor current version of tensorflow it will leak memory if we evaluate the sliced tensor as attached code and images which isnt intuitive and hard to debug it seems like the unused part of tensor will leak memory because there is no python object reference on it i am not sure whether it could fixed or not maybe it could show warning at least source code logs import tensorflow as tf from pympler.tracker import summarytracker tracker summarytracker() a tf.zeros( ,, ) sess tf.session() def leak_version(): ____return sess.run(a , ) def safe_version(): ____return sess.run(a for i in range(): ____tracker.print_diff b safe_version() ____b leak_version trace of the leak! image leak version visualize the object reference graph on object b with depth by library objgraph! image safe version visualize the object reference graph on object b with depth by library objgraph! image"
319742840,19041,https://api.github.com/repos/tensorflow/tensorflow/issues/19041,zeyademam,1,0,0,0,0,0,i posted this question here on stackoverflow and someone else posted a similar question earlier yet no satisfying answers when using an estimator in tensorflow and passing the inputs using tf.estimator.inputs.numpy_input_function i would like to find a way to access the input placeholders one for features and one for labels).i train the model using tf.estimator.estimator.train and save the checkpoint files in model_dir later i would like to load the model and variables using the following snippet: sess tf.session() saver tf.train.import_meta_graph(model_dir+/model.ckpt-.meta) saver.restore(sess tf.train.latest_checkpoint(model_dir)) predictions graph.get_tensor_by_name(softmax_tensor:) then do sess.run(predictions but theres no way to pull out the inputs placeholder for the feed_dict parameter of sess.run() .another related issue i have i would also like to add placeholders to an estimator other than the features and labels placeholders for example if i have dropout i would like the dropout_rate or the tf.layers.dropout functions training param to be placeholders this way after loading the model from chkpt as above i can turn off dropout have i written custom code yos platform and distribution ubuntu tensorflow installed from piptensorflow version r..bazel version n/acuda/cudnn version gpu model and memory gtx tiexact command to reproduce n/a
319400781,19015,https://api.github.com/repos/tensorflow/tensorflow/issues/19015,lobachevzky,2,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below master branch commit dfbc python version python bazel version if compiling from source) :build label build target bazel-out/k-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time mon oct build timestamp build timestamp as int gcc/compiler version if compiling from source) :gcc ubuntu ubuntu cuda/cudnn version :cuda cudnn gpu model and memory :geforce gtx titan x gb exact command to reproduce : bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.in configure i use the default settings for everything except i use usr/bin/python and cuda. ~/tensorflow master ethanbro@rldl bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package this raises the following error: starting local bazel server and connecting to it.................warning the following configs were expanded more than once cuda for repeatable flags repeats are counted twice and may lead to unexpected behavior.warning home/ethanbro/.cache/bazel/_bazel_ethanbro/eabccce/external/protobuf_archive/workspace workspace name in home/ethanbro/.cache/bazel/_bazel_ethanbro/eabccce/external/protobuf_archive/workspace com_google_protobuf does not match the name given in the repositorys definition protobuf_archive this will cause a build error in future versionsunhandled exception thrown during build message unrecoverable error while evaluating node package:tensorflow/core/kernels requested by nodes tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key false tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true info elapsed time sinfo processes.failed build did not complete successfully packages loaded currently loading tensorflow/core packages)java.lang.runtimeexception unrecoverable error while evaluating node package:tensorflow/core/kernels requested by nodes tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key false tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true at com.google.devtools.build.skyframe.abstractparallelevaluator$evaluate.run(abstractparallelevaluator.java at com.google.devtools.build.lib.concurrent.abstractqueuevisitor$wrappedrunnable.run(abstractqueuevisitor.java at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java at java.lang.thread.run(thread.java:)caused by java.lang.illegalargumentexception invalid evalexception:java.lang.interruptedexception at com.google.common.util.concurrent.abstractfuture.get(abstractfuture.java at com.google.common.util.concurrent.abstractfuture$trustedfuture.get(abstractfuture.java at com.google.common.util.concurrent.forwardingfuture.get(forwardingfuture.java at com.google.devtools.build.lib.vfs.unixglob$globfuture.get(unixglob.java at com.google.devtools.build.lib.vfs.unixglob$globfuture.get(unixglob.java at com.google.devtools.build.lib.packages.globcache.fromfuture(globcache.java at com.google.devtools.build.lib.packages.globcache.getglobunsorted(globcache.java at com.google.devtools.build.lib.packages.globcache.globunsorted(globcache.java at com.google.devtools.build.lib.packages.packagefactory$legacyglobber.fetch(packagefactory.java at com.google.devtools.build.lib.skyframe.packagefunction$skyframehybridglobber$hybridtoken.resolve(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction$skyframehybridglobber$hybridtoken.access$(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction$skyframehybridglobber.fetch(packagefunction.java at com.google.devtools.build.lib.packages.packagefactory.callglob(packagefactory.java at com.google.devtools.build.lib.packages.skylarknativemodule.glob(skylarknativemodule.java at sun.reflect.generatedmethodaccessor.invoke(unknown source at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java at java.lang.reflect.method.invoke(method.java at com.google.devtools.build.lib.syntax.funcallexpression.callmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.builtincallable.call(builtincallable.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.binaryoperatorexpression.doeval(binaryoperatorexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execassignment(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execstatements(eval.java at com.google.devtools.build.lib.syntax.eval.execifbranch(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execif(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.userdefinedfunction.call(userdefinedfunction.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.callfunction(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.buildfileast.exectoplevelstatement(buildfileast.java at com.google.devtools.build.lib.syntax.buildfileast.exec(buildfileast.java at com.google.devtools.build.lib.packages.packagefactory.evaluatebuildfile(packagefactory.java at com.google.devtools.build.lib.packages.packagefactory.createpackagefromast(packagefactory.java at com.google.devtools.build.lib.skyframe.packagefunction.loadpackage(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction.compute(packagefunction.java at com.google.devtools.build.skyframe.abstractparallelevaluator$evaluate.run(abstractparallelevaluator.java at com.google.devtools.build.lib.concurrent.abstractqueuevisitor$wrappedrunnable.run(abstractqueuevisitor.java at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java at java.lang.thread.run(thread.java at com.google.devtools.build.lib.syntax.evalexception.(evalexception.java at com.google.devtools.build.lib.syntax.evalexception$evalexceptionwithjavacause.(evalexception.java at com.google.devtools.build.lib.syntax.evalexception$evalexceptionwithjavacause.(evalexception.java at com.google.devtools.build.lib.syntax.funcallexpression.callmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.builtincallable.call(builtincallable.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.binaryoperatorexpression.doeval(binaryoperatorexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execassignment(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execstatements(eval.java at com.google.devtools.build.lib.syntax.eval.execifbranch(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execif(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.userdefinedfunction.call(userdefinedfunction.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.callfunction(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.buildfileast.exectoplevelstatement(buildfileast.java at com.google.devtools.build.lib.syntax.buildfileast.exec(buildfileast.java at com.google.devtools.build.lib.packages.packagefactory.evaluatebuildfile(packagefactory.java at com.google.devtools.build.lib.packages.packagefactory.createpackagefromast(packagefactory.java at com.google.devtools.build.lib.skyframe.packagefunction.loadpackage(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction.compute(packagefunction.java at com.google.devtools.build.skyframe.abstractparallelevaluator$evaluate.run(abstractparallelevaluator.java morejava.lang.runtimeexception unrecoverable error while evaluating node package:tensorflow/core/kernels requested by nodes tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key false tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.buildconfigurationvalue$key@edb true at com.google.devtools.build.skyframe.abstractparallelevaluator$evaluate.run(abstractparallelevaluator.java at com.google.devtools.build.lib.concurrent.abstractqueuevisitor$wrappedrunnable.run(abstractqueuevisitor.java at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java at java.lang.thread.run(thread.java:)caused by java.lang.illegalargumentexception invalid evalexception:java.lang.interruptedexception at com.google.common.util.concurrent.abstractfuture.get(abstractfuture.java at com.google.common.util.concurrent.abstractfuture$trustedfuture.get(abstractfuture.java at com.google.common.util.concurrent.forwardingfuture.get(forwardingfuture.java at com.google.devtools.build.lib.vfs.unixglob$globfuture.get(unixglob.java at com.google.devtools.build.lib.vfs.unixglob$globfuture.get(unixglob.java at com.google.devtools.build.lib.packages.globcache.fromfuture(globcache.java at com.google.devtools.build.lib.packages.globcache.getglobunsorted(globcache.java at com.google.devtools.build.lib.packages.globcache.globunsorted(globcache.java at com.google.devtools.build.lib.packages.packagefactory$legacyglobber.fetch(packagefactory.java at com.google.devtools.build.lib.skyframe.packagefunction$skyframehybridglobber$hybridtoken.resolve(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction$skyframehybridglobber$hybridtoken.access$(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction$skyframehybridglobber.fetch(packagefunction.java at com.google.devtools.build.lib.packages.packagefactory.callglob(packagefactory.java at com.google.devtools.build.lib.packages.skylarknativemodule.glob(skylarknativemodule.java at sun.reflect.generatedmethodaccessor.invoke(unknown source at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java at java.lang.reflect.method.invoke(method.java at com.google.devtools.build.lib.syntax.funcallexpression.callmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.builtincallable.call(builtincallable.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.binaryoperatorexpression.doeval(binaryoperatorexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execassignment(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execstatements(eval.java at com.google.devtools.build.lib.syntax.eval.execifbranch(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execif(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.userdefinedfunction.call(userdefinedfunction.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.callfunction(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.buildfileast.exectoplevelstatement(buildfileast.java at com.google.devtools.build.lib.syntax.buildfileast.exec(buildfileast.java at com.google.devtools.build.lib.packages.packagefactory.evaluatebuildfile(packagefactory.java at com.google.devtools.build.lib.packages.packagefactory.createpackagefromast(packagefactory.java at com.google.devtools.build.lib.skyframe.packagefunction.loadpackage(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction.compute(packagefunction.java at com.google.devtools.build.skyframe.abstractparallelevaluator$evaluate.run(abstractparallelevaluator.java at com.google.devtools.build.lib.concurrent.abstractqueuevisitor$wrappedrunnable.run(abstractqueuevisitor.java at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java at java.lang.thread.run(thread.java at com.google.devtools.build.lib.syntax.evalexception.(evalexception.java at com.google.devtools.build.lib.syntax.evalexception$evalexceptionwithjavacause.(evalexception.java at com.google.devtools.build.lib.syntax.evalexception$evalexceptionwithjavacause.(evalexception.java at com.google.devtools.build.lib.syntax.funcallexpression.callmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.builtincallable.call(builtincallable.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.invokeobjectmethod(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.binaryoperatorexpression.doeval(binaryoperatorexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execassignment(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execstatements(eval.java at com.google.devtools.build.lib.syntax.eval.execifbranch(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.eval.execif(eval.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.userdefinedfunction.call(userdefinedfunction.java at com.google.devtools.build.lib.syntax.basefunction.callwithargarray(basefunction.java at com.google.devtools.build.lib.syntax.basefunction.call(basefunction.java at com.google.devtools.build.lib.syntax.funcallexpression.callfunction(funcallexpression.java at com.google.devtools.build.lib.syntax.funcallexpression.doeval(funcallexpression.java at com.google.devtools.build.lib.syntax.expression.eval(expression.java at com.google.devtools.build.lib.syntax.eval.execdispatch(eval.java at com.google.devtools.build.lib.syntax.eval.exec(eval.java at com.google.devtools.build.lib.syntax.buildfileast.exectoplevelstatement(buildfileast.java at com.google.devtools.build.lib.syntax.buildfileast.exec(buildfileast.java at com.google.devtools.build.lib.packages.packagefactory.evaluatebuildfile(packagefactory.java at com.google.devtools.build.lib.packages.packagefactory.createpackagefromast(packagefactory.java at com.google.devtools.build.lib.skyframe.packagefunction.loadpackage(packagefunction.java at com.google.devtools.build.lib.skyframe.packagefunction.compute(packagefunction.java at com.google.devtools.build.skyframe.abstractparallelevaluator$evaluate.run(abstractparallelevaluator.java more source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.here is the tf_configure.bazelrc: build action_env python_bin_path=/usr/bin/python.build action_env python_lib_path=/usr/local/lib/python./dist-packagesbuild python_path=/usr/bin/python.build define with_jemalloc=truebuild:gcp define with_gcp_support=truebuild:hdfs define with_hdfs_support=truebuild define with_s_support=truebuild define with_kafka_support=truebuild:xla define with_xla_support=truebuild:gdr define with_gdr_support=truebuild:verbs define with_verbs_support=truebuild action_env tf_need_opencl_sycl=build action_env tf_need_cuda=build action_env cuda_toolkit_path=/usr/local/cudabuild action_env tf_cuda_version=.build action_env cudnn_install_path=/usr/lib/x_-linux-gnubuild action_env tf_cudnn_version=build action_env tf_nccl_version=build action_env tf_cuda_compute_capabilities=.build action_env ld_library_path=/home/ethanbro/.mujoco/mjpro/bin:/usr/local/cuda-./lib:build action_env tf_cuda_clang=build action_env gcc_host_compiler_path=/usr/bin/gccbuild config=cudatest config=cudabuild define grpc_no_ares=truebuild:opt copt=-march=nativebuild:opt host_copt=-march=nativebuild:opt define with_default_optimizations=truebuild copt=-dgemmlowp_allow_slow_scalar_fallbackbuild host_copt=-dgemmlowp_allow_slow_scalar_fallback here is the content of tf_env.txt cat etc/issue linux rldl generic ubuntu smp mon apr utc x x x gnu/linux are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux rldl generic ubuntu smp mon apr utc x x x gnu/linux check pips numpy protobuf tensorflow-tensorboard check for virtualenv false tensorflow import traceback most recent call last file string line in module file tensorflow/__init__.py line in module from tensorflow.python import pywrap_tensorflow pylint disable=unused-import file tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.platform import self_checkimporterror no module named platform env ld_library_path home/ethanbro/.mujoco/mjpro/bin:/usr/local/cuda-./lib:dyld_library_path is unset nvidia-smi tue may nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx tit off off n/a c p w w mib mib default geforce gtx tit off off n/a c p w w mib mib default geforce gtx tit off off n/a c p w w mib mib default geforce gtx tit off off n/a c p w w mib mib default processes gpu memory gpu pid type process name usage g usr/lib/xorg/xorg mib g usr/lib/xorg/xorg mib g usr/lib/xorg/xorg mib g usr/lib/xorg/xorg mib cuda libs usr/local/cuda-./doc/man/man/libcudart./usr/local/cuda-./doc/man/man/libcudart.so./usr/local/cuda-./targets/x_-linux/lib/libcudart_static.a/usr/local/cuda-./targets/x_-linux/lib/libcudart.so.../usr/local/matlab/ra/bin/glnxa/libcudart.so
319379342,19013,https://api.github.com/repos/tensorflow/tensorflow/issues/19013,damienpontifex,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary binary tensorflow version use command below v..--gaecf python version bazel version if compiling from source homebrew gcc/compiler version if compiling from source apple llvm version clang cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemin the libcurl programming tutorial under the section persistence is the way to happiness it notes that re-cycling the same easy handle can give several benefits this could lead to performance improvements when using remote filesystems that utilise this class e.g gcs file system and ive been working on azure blob storage currently it seems that a single instance of the curlhttprequest and hence curl handle can only be used once and must be instantiated per request.is this worth investigating to integrate some sort of reset method to this class along with curl_easy_reset plus extract defaults set in the constructor into a setup defaults method plus some profiling to ensure the change is better
318877433,18980,https://api.github.com/repos/tensorflow/tensorflow/issues/18980,kerolos,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution:linux debian tensorflow installed from source or binary bazel tensorflow version use command below v python version bazel version if compiling from source gcc/compiler version if compiling from source and cuda/cudnn version gpu model and memory gtx ti gb gb exact command to reproduce gcc describe the problemi could not able to install tensorflow v on my machine i used different gcc and g versions and in debian furthermore i got the same errors error:error home/pm/local/cpp/tensor_flow_/tensorflow/tensorflow/core/kernels/build output tensorflow/core/kernels/_objs/eye_functor_gpu/tensorflow/core/kernels/eye_functor_gpu.cu.pic.o was not createderror home/pm/local/cpp/tensor_flow_/tensorflow/tensorflow/core/kernels/build not all outputs were created or validtarget tensorflow:libtensorflow_cc.so failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path sfailed build did not complete successfully
318818130,18976,https://api.github.com/repos/tensorflow/tensorflow/issues/18976,dbarta,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary source tensorflow version use command below v..-rc--gacd rc python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemim trying to convert my custom model to tflite using tf.contrib.lite.toco_convert the model has some transposeconv in it i expect these to convert as looking at commit there does seem to be quite a bit of code in place to support transposeconv including the actual implementation and graph transformations however when i run the conversion it tells me its not supported see below output).it seems like this op is supported but simply unregistered for some reason which might be a bug or maybe there is some mysterious reason it cannot be registered?thanks source code logs i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before removing unused ops operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before general graph transformations operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc after general graph transformations pass operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc after general graph transformations pass operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc after general graph transformations pass operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before dequantization graph transformations operators arrays quantized i tensorflow/contrib/lite/toco/allocate_transient_arrays.cc total transient array allocated size bytes theoretical optimalvalue bytes f tensorflow/contrib/lite/toco/tflite/export.cc some of the operators in the model are not supported by the standard tensorflow literuntime if you have a custom implementation for them you can disable this error with allow_custom_ops here is a list of operators for which you will need customimplementations transposeconv
318596569,18942,https://api.github.com/repos/tensorflow/tensorflow/issues/18942,pvaneck,1,0,0,0,0,0,this commit allows users to continuously scroll the screen when the mouse is held down on the scroll bar when using the curses ui for tfdbg
318455681,18931,https://api.github.com/repos/tensorflow/tensorflow/issues/18931,Skydes,1,0,0,0,0,0,system information os linux ubuntu tensorflow installed from source master branch (acddabfdeedbfbebadb gcc cmake no gpu problemthe cmake build fails on ubuntu when grpc support is disabled command cmake dcmake_install_prefix=../test dcmake_build_type=release dtensorflow_build_shared_lib=on dtensorflow_build_all_kernels=on dtensorflow_build_contrib_kernels=off dtensorflow_build_cc_example=off dtensorflow_build_python_bindings=off dtensorflow_enable_grpc_support=off dtensorflow_enable_ssl_support=off dtensorflow_build_cc_tests=off dtensorflow_build_python_tests=off dtensorflow_enable_gpu=off make logs ... make no rule to make target grpc needed by tensorflow/core/debug/debug_service.grpc.pb.cc.stop.make cmakefiles/tf_protos_cc.dir/all error
318398943,18925,https://api.github.com/repos/tensorflow/tensorflow/issues/18925,jackyko1991,2,0,0,0,0,0,in the latest eager core update i see that eager_operation is moved to tensorflow/core/common_runtime/eager however this file require c_api as one of the dependency.the c_api is suppose to be built after cpu_core_lib is generated i am now a bit confuse as i am trying to rewrite the cmake files in order to fit the new eager structure.any hel
318359847,18920,https://api.github.com/repos/tensorflow/tensorflow/issues/18920,AlexHarn,2,0,0,0,0,0,system information not relevant. have i written custom code yes os platform and distribution different linux systems tensorflow installed from source and pip on different systems)tensorflow version different versions all bazel version different versions on different systems cuda/cudnn version different versions all gpu model and memory gtx gtx tesla p potentially many others on our cluster but not tested yet)exact command to reproduce run bug_test.py describe the problem motivation(you might want to skip this)tensorflow is a powerful framework that can potentially be used for much more than static computations like those needed to train neural networks we a couple of physicists are trying to use tensorflow to get a gradient on some physical parameters to fit those parameters to measured data by propagating the gradient through the entire physics simulation to replicate the data this is extremely relevant for a lot of people and the idea is not new instead of performing extremely time consuming in terms of computational and human work time grid searches on high dimensional parameter spaces we would like to be able to directly perform gradient descent on such spaces in an automated fashion by using tensorflows automatic differentiation you can find our work in progress repository here the problemlike explicitly stated here on page this means that we assume that pred is not trainable the number of loop iterations is just a constant while backpropagating the gradient this leads to some extremely unexpected behavior like described in this issue at least unexpected for someone who is a tensorflow newbie like i am for our plans to work it is mandatory for the gradient to include information on how the number of loop iterations would have changed if the trainable variables changed example(you might want to skip this)in our project we have a trainable variable which describes the mean distance between scattering events of photons tensorflow is able to propagate the gradient through the simulation while loop but the gradient only thinks that the photons will reach further points when the scattering length is longer but is not aware of the fact that there will be less iterations requesti do not know if this is even possible with how tensorflow works but we would love to see a way to make this work tensorflow would be a really great tool for us to use so is this something you might work on in the future also does anyone maybe have any idea of how to solve or get around this right now with the current implementation we have put quite some thought into this ourselves already but until now we have not found a working solution source codefor a work in progress example of how this would be useful you can take a look at the master and experimental branches of our repo for a stripped down minimal one dimensional example of the simulation you can take a look at minimal_d.py this in its current state however actually converges for some more information on the unexpected behavior take a look at my issue where i reference commits which demonstrate whats happening.a minimal example which does not include any physics and purely demonstrates the unexpected behavior is given here of the code includes detailed comments if you are interested in the background and general idea you can take a look at my slides even though i am not sure how much of a help those will be for someone from outside slides to might be helpful for everyone though especially slide which shows how we fit the data using simulations for the general concept testing right now the data is also given by the same simulation with true parameters.we really hope to get some form of feedback here and i am sorry if this is a trivial or stupid request in the eyes of someone more experienced with tensorflow
318164312,18906,https://api.github.com/repos/tensorflow/tensorflow/issues/18906,tfboyd,18,0,0,0,0,0,updated june-i have done some testing with cuda cudnn nccl x with tf_cnn_benchmarks.below you will see that if you upgrade to cudnn and a newer device driver you get some pretty nice gains and no need to compile from source as well as using cuda compiling from source and using nccl x hierarchal copy was almost the same i was able to get xvs resnetv fp with synthetic data and about real data sustained over a full run for under hours i suspect many of you saw nvidia announce k and k for xv resnet that was with unreleased libraries and we are working to ensure we can hit those numbers when the libraries are available or with our own tricks.i apologize for any short hand i use below in describing the runs i am happy to answer questions or make clarifications this testing was slightly informal but recent full scale testing yielded similar results cuda recommended driver cuda v..--gdc hierarchical copy cuda v..--gdc nccl cuda v..--gfeba nccl cuda v..--gfeba hierarchical copy cuda dev hierarchical copy cuda dev hierarchical copy cuda v..--gdc hierarchical copy cuda recommended driver cuda v..--gfeba hierarchical copy cuda v..--gfeba hierarchical copy cuda v..--gfeba nccl cuda recommended driver cuda v..--gfeba nccl cuda v..--gfeba hierarchical copy cuda v..--gfeba nccl sgd full test runs with cuda top ranges between and and does not seem to be based on the hyper parameters i was focused on testing nccl vs hierarchical copy i have a minor concern about my validation command but this is still good info.hierarchical copy accuracy accuracy examples nccl repacking accuracy accuracy examples nccl repacking accuracy accuracy examples my first hierarchical copy run was exactly note i would like to move to cuda as the default but the driver is not widely available for easy apt-get install i am working long-term to get the build team to support a secondary cuda build i am also very open to feedback as i do not see any easy path forward to moving to newer cuda versions faster you can always usually install a new cudnn we are compiling with but you see gains by installing as you see above if you upgrade your driver
317895277,18890,https://api.github.com/repos/tensorflow/tensorflow/issues/18890,guy6656,2,0,0,0,0,0,system informationi minimize the model to very simple model with one input convolution layer and pool is the output.i work on ubuntu i compile tensorflow lite from source with bazeltensorflow version is python bazel version gcc/compiler version gcc ubuntu ubuntu working on cpu describe the problemi am trying to run my tensorflow model on samsung galaxy s i trained model with tensorflow freeze it and convert it to tflite model file with toco.i am running cpp program on the phone according to documentation and load the tflite model file std::unique_ptrerror_reporter tflite::ops::builtin::builtinopresolver resolver std::unique_ptrusennapi(false if interpreter->allocatetensors ktfliteok printf(failed to allocate tensors!\n i succeed to initialize the model but failed to run interpreter->allocatetensors() there is no print of why it failed to allocate the tensors just my print that the function failed if interpreter->allocatetensors ktfliteok printf(failed to allocate tensors!\n i minimized my model to include only reshape convolution layer and pool and still failed to run allocatetensorsoutput with debug information:resolved reportertensors size nodes size inputs input name patchesinput name null)reshape reshape/shape conv/relu conv/convolution_bias conv/kernel input/patches-input output patches failed to allocate tensors
317821563,18880,https://api.github.com/repos/tensorflow/tensorflow/issues/18880,make1980,1,0,0,0,0,0,currently tensorflow cluster is created statically with host name/port information defined in cluster spec the host name and port information for a worker cant be changed during run-time in a multi-tenancy environment where host name/port is assigned to worker by a job manager after worker failover happens this static cluster spec prevents other workers to communicate with this new worker using the latest host name and port.with dynamic rpc address resolution feature the host name of each tf server is resolved against an rpc address registry before initiating each rpc call this allows changing the host name and port of each tf server at run time which is useful in the following cases a stateless tf worker fails over and gets assigned with new host name/port by an external job scheduler a stateful tf worker fails over and all workers get restarted without changing the cluster spec the failed worker can be assigned with new host name/port.i have composed a design document here a prototype has been done as well based on this design we hope to contribute this feature to the upstream any feedbacks for the design are welcome
317403817,18841,https://api.github.com/repos/tensorflow/tensorflow/issues/18841,naibaf7,6,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu fedora tensorflow installed from source or binary source tensorflow version use command below v..-rc--gaddadfd rc python version m bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory gtx gb exact command to reproduce : include makefile.configtf_inc python w ignore c import tensorflow as tf print(tf.sysconfig.get_include()) tf_cflags python w ignore c import tensorflow as tf print join(tf.sysconfig.get_compile_flags())) tf_lflags python w ignore c import tensorflow as tf print join(tf.sysconfig.get_link_flags())) cuda_inc cuda_home)/../gpucc nvcc ccbin=$(cxx)cflags std=c tf_cflags i i$(cuda_inc i$(tf_inc)gpucflags clflags odflags pthread fopenmp shared fpic tf_lflags)gpulflags x cu xcompiler cflags lflags expt-relaxed-constexprgpudef dgoogle_cuda=cgpuflags lcudasrc cart_hex_interpol.ccgpusrc cart_hex_interpol.cu.ccsrc_o cart_hex_interpol.ogpusrc_o cart_hex_interpol.cu.olib cart_hex_interpol.soall gpudefault gpucpu cxx cflags src lflags o lib)gpu gpucc gpudef cflags gpucflags gpusrc gpulflags o gpusrc_o cxx gpudef cflags src gpusrc_o lflags cgpuflags o lib)clean rm f src_o gpusrc_o lib describe the problemwhen compiling a custom tensorflow operator using a makefile and including cuda/gpu code the compilation fails due to missing fatal error tensorflow/compiler/xla/statusor.h no such file or directory the files are missing in usr/local/lib/python./site-packages/tensorflow because the headers do not get installed during installation of the wheel package for tensorflow.a temporary fix is to copy the headers from source: sudo mkdir usr/local/lib/python./site-packages/tensorflow/include/tensorflow/compiler/sudo mkdir usr/local/lib/python./site-packages/tensorflow/include/tensorflow/compiler/xlasudo cp tensorflow/compiler/xla/*.h usr/local/lib/python./site-packages/tensorflow/include/tensorflow/compiler/xla/ this issue was not present in tensorflow but must have been introduced since then compiling with or without xla makes no difference source code logs make gpu jnvcc ccbin=/usr/local/gcc-./bin/g dgoogle_cuda std=c python w ignore c import tensorflow as tf print join(tf.sysconfig.get_compile_flags i i/usr/local/cuda i python w ignore c import tensorflow as tf print(tf.sysconfig.get_include c cart_hex_interpol.cu.cc x cu xcompiler std=c python w ignore c import tensorflow as tf print join(tf.sysconfig.get_compile_flags i i/usr/local/cuda i python w ignore c import tensorflow as tf print(tf.sysconfig.get_include o pthread fopenmp shared fpic python w ignore c import tensorflow as tf print join(tf.sysconfig.get_link_flags expt-relaxed-constexpr o cart_hex_interpol.cu.oin file included from usr/local/lib/python./site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h from usr/local/lib/python./site-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h from usr/local/lib/python./site-packages/tensorflow/include/tensorflow/core/util/cuda_launch_config.h from usr/local/lib/python./site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h from cart_hex_interpol.cu.cc::/usr/local/lib/python./site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h fatal error tensorflow/compiler/xla/statusor.h no such file or directory include tensorflow/compiler/xla/statusor.h
317220781,18831,https://api.github.com/repos/tensorflow/tensorflow/issues/18831,selcouthlyBlue,1,0,0,0,0,0,this can replace validationmonitor which is now deprecated it doesnt have the fancy stuff validationmonitor has like early-stopping and whatnot but its a start
317146694,18825,https://api.github.com/repos/tensorflow/tensorflow/issues/18825,saginadir,1,0,0,0,0,0,awesome and details doc!but i woulda avoid calling it an awkward package path
317090537,18820,https://api.github.com/repos/tensorflow/tensorflow/issues/18820,formath,2,0,0,0,0,0,add op to tensorflow/contrib/makefile/tf_op_files.txt to fix problem of op not registered fix header and source not found problems when using build_all_linux.sh
316586222,18775,https://api.github.com/repos/tensorflow/tensorflow/issues/18775,yongtang,1,0,0,0,0,0,this fix is an attempt to fix the build issues with gpu on linux.previously cmake on linux only works for cpu build.this fix addresses multiple issues on cmake file so that gpu build could work on linux as well.the build is performed on ubuntu cuda nccl v with the following command mkdir build cd build cmake dtensorflow_enable_gpu=on dcuda_toolkit_root_dir=/usr/local/cuda dtensorflow_cudnn_include=/usr/local/cuda-./include dcmake_build_type=release tensorflow/contrib/cmake make j all this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
316512119,18759,https://api.github.com/repos/tensorflow/tensorflow/issues/18759,filipefilardi,2,0,0,0,0,0,create pr template to make easier to contribute to this repo and easier to check if the pull request is consistent and followed all guidelines.if this repo had my pull request this commit would be like this tensorflow pull request template pull request checklist before sending your pull requests make sure you followed this list and tick all items x read contributing guideline (contributing.md x read code of conduct (code_of_conduct.md x fill contributor license agreement cla x check if my changes are consistent with the guidelines x changes are consistent with the coding style x run unit tests issue fix does this pull request fix any issue yes x nofixed issue none description detailed description of what youve done add pull request template
316475814,18751,https://api.github.com/repos/tensorflow/tensorflow/issues/18751,1icas,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory describe the problemi download the tensorflow use the git clone today.and when i open the android studio and open the tensorflow_lite demo(contrib/lite/java/demo).some errors occur source code logssome errors occured on my android studio console error:could not find tensorflow-lite.jar org.tensorflow:tensorflow-lite:..-rc).searched in the following locations ide log for more details help show log
316474501,18750,https://api.github.com/repos/tensorflow/tensorflow/issues/18750,ccyyy,2,0,0,0,0,0,"describe the problemi ran tensorflow debugger using the command python py debug but got the following error and python stops working.tfdbg run i c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\debug\debug_graph_utils.cc for debugging tfdbg has set the parallel_iterations attribute of all scheduled enter/refenter nodes to this does not affect subsequent non-debug runs f c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\debug\debug_io_utils.cc non-ok-status env->newwritablefile(file_path f status not found failed to create a newwriteablefile c:\users\chengc~\appdata\local\temp\tfdbg_zheiebg/_tfdbg_device_,job_localhost,replica_,task_,device_cpu_/bboxes_matching_batch_dict/bboxes_matching_batch_/map/while/bboxes_matching_single/while/bboxes_jaccard/strided_slice_/stack__debugidentity no such process"
316298960,18736,https://api.github.com/repos/tensorflow/tensorflow/issues/18736,reinaldocmendes,14,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below python version python bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version no gpu model and memory no exact command to reproduce describe the problemmy server has gb of ram but tensorflow uses only of that memory.it returns the message:<>is it possible to increase this percentage source code logs source code:import kerasfrom keras import regularizersfrom keras.layers import input convd maxpoolingd upsamplingd zeropaddingd addfrom keras.models import modelfrom keras.layers.core import layer dense dropout activation flatten reshapefrom keras.regularizers import lfrom keras.utils import np_utilsfrom keras.callbacks import tensorboardfrom sklearn.model_selection import train_test_splitimport numpy as npimport hpyimport timefile_train features/files_train_test/train_inceptionv_docvec.npyfile_test features/files_train_test/test_inceptionv_docvec.npyx_train np.load(file_train)x_test np.load(file_test)print(shape train x_train.shape,shape test x_test.shape)print(size train len(x_train size test len(x_test))print(prod train np.prod(x_train.shape prod test np.prod(x_test.shape : ))x_train x_train.reshape((len(x_train np.prod(x_train.shape : )))x_test x_test.reshape((len(x_test np.prod(x_test.shape : )))print(shape train x_train.shape,shape test x_test.shape)epochs batch_size input_size x_train.shape output_size x_train.shape hidden_size x_train.shape file_name features/files_reduce/sparse/autoencoder_inceptionv_docvec_x input(shape=(input_size,))h dense(hidden_size activation=relu activity_regularizer=regularizers.l(e-))(x)r dense(output_size activation=sigmoid)(h)autoencoder model(inputs=x outputs=r)autoencoder.compile(optimizer=adam loss=mse)autoencoder.summary()autoencoder.fit(x_train x_train batch_size=batch_size epochs=epochs verbose validation_data=(x_test x_test))autoencoder.save(file_name+name_full)print(save encode file_name+name_full)===============================================================================output using tensorflow backend.shape train shape test size train size test prod train prod test shape train shape test layer type output shape param input inputlayer none dense dense none dense dense none total params trainable params non-trainable params train on samples validate on samplesepoch w tensorflow/core/framework/allocator.cc allocation of exceeds of system memory w tensorflow/core/framework/allocator.cc allocation of exceeds of system memory"
316229028,18731,https://api.github.com/repos/tensorflow/tensorflow/issues/18731,Kayoku,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu beta tensorflow installed from source or binary) :source tensorflow version use command below) :master python version bazel version if compiling from source gcc/compiler version if compiling from source) :gcc cuda/cudnn version gpu model and memory :gtx ti go go ram exact command to reproduce :bazel-bin/tensorflow/contrib/lite/toco/toco input_file=frozen_graph.pb input_format=tensorflow_graphdef output_format=tflite output_file=/tmp/test.tflite inference_input_type=float--input_arrays=input_image--output_arrays=output_corner,output_mask--input_shapes describe the problemim working on a fcn i want to export my frozen graph to tflite graph but its not working im wondering why but i guess its just due to the lack of maturity of tflite and probably some operator compatibility in my case its about the operator merge someone can explain me if there is a way/trick to make it work source code logs i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation logicalor i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before removing unused ops operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc after removing unused ops pass operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before general graph transformations operators arrays quantized f tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc check failed other_op->type operatortype::ktensorflowmerge aborted core dumped) i also make a summarize_graph for you maybe it help found possible inputs name=input_image type=int shape no variables spotted.found possible outputs name=output_corner op=tile name=output_mask op=tile found m const parameters variable parameters and control_edgesop types used const switch fusedbatchnorm add identity merge convd relu stridedslice shape mul cast maxpool pack realdiv slice convdbackpropinput tile reshape randomuniform placeholderwithdefault greater floor concatv placeholder maximum logicalor resizebilinear sigmoid softmax squeeze expanddims sub argmax"
316078996,18711,https://api.github.com/repos/tensorflow/tensorflow/issues/18711,alexlee-gk,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gaecf python version bazel version if compiling from source n/a gcc/compiler version if compiling from source a cuda/cudnn version cuda cudnn gpu model and memory titan xp gb exact command to reproduce cuda_visible_devices python test_conv_upsample_pool_ops.py describe the problemtldr upsampling+convd which is very expensive could be implemented such that it has the same time and memory complexity as strided convd_transposed similarly for convd+average_pooling and strided convd an efficient implementation could be s faster where s is the stride typically s=).strided convd_transposed have been traditionally used in decoders for dense prediction problems e.g image generation video prediction semantic segmentation however this op often produces outputs with checkerboard artifacts e.g see and papers citing it an alternative is to use bilinear upsampling followed by a standard convolution with no strides this is widely used in several recent works and it mitigates the checkerboard artifacts but at a cost its computationally and memory expensive e.g see upsampling+convd does s more computation than the strided counterpart where s is the stride factor furthermore the intermediate tensor after upsampling could be very large if the input has a large number of channels.under certain conditions which happens to be the most common use case upsampling+convd could be rewritten as convolving the upsampling kernel with the given kernel followed by convd_transposed of the given input and the combined kernels this follows from commutative and associative properties of linearity in convolutions taking proper care of flipping the filters so that the ops are actual convolutions).a similar reasoning applies for an efficient implementation of convd+average_pooling.implementations of the mentioned ops are here see upsample_convd and conv_poold script that tests for equivalence and timings is here report some timings below of hundreds of evaluations the naive op should be about x slower than the strided counterpart since s in theory the optimized op should be as fast as the strided counterpart but my implementation is about x slower but still much faster than the naive version and i think there is room for improvement upsample convd optimized upsample convd naive strided convd_transpose forward pass s s s forward and backward pass s s s convd pool optimized convd pool naive strided convd forward pass s s s forward and backward pass s s s
315932774,18696,https://api.github.com/repos/tensorflow/tensorflow/issues/18696,dlsmith,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemive run into two issues when using a warm start with the estimator api please let me know if youd like me to file these as separate issues if checkpoints exist in the model_dir then the warm start is overridden when the session is created the warm start is performed on every train call which means warm starting cant be used in the context of train_and_evaluate or in a manual loop unless reinitializing the estimator ).id be willing pending corporate cla to submit a patch if you can clarify the desired behavior i would personally expect that checkpoint restore is skipped on session creation if a warm start was performed the warm start settings would be cleared after the first train call however if this is the desired behavior looking longer term it seems like warm start settings shouldnt be a property of the estimator instance
315837895,18689,https://api.github.com/repos/tensorflow/tensorflow/issues/18689,tranhungnghiep,4,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow na os platform and distribution e.g linux ubuntu debian gnu/linux stretch cpu model intel(r xeon(r cpu e ghz and any old cpu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source na gcc/compiler version if compiling from source na cuda/cudnn version na gpu model and memory na exact command to reproduce import tensorflow describe the problemsince version prebuilt binaries use avx instructions thus tf cannot be used on old cpus however many of them are not that old in fact i was surprise that my server cannot run tf i believe many people run into the same problem because those cpus are popular in many labs at universities.this is bad because it blocks many potential users/researchers from trying tf and starting using tf user should not build tf themselves because building by each user is daunting and resource-wasteful.so if it is not technically impossible please provide alternative prebuilt binaries that support old cpus
315551817,18664,https://api.github.com/repos/tensorflow/tensorflow/issues/18664,yongtang,1,0,0,0,0,0,with the most recent master the following test fails: bazel test s config=opt cache_test_results=no tensorflow/python/kernel_tests:init_ops_test eye linalg_ops.eye(n dtype=self.dtype)nameerror global name linalg_ops is not defined this fix fixes the test failure.signed-off-by yong tang yong.tang.github@outlook.com
315400729,18640,https://api.github.com/repos/tensorflow/tensorflow/issues/18640,alyaxey,11,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary from pip tensorflow version use command below rc python version anaconda python bazel version if compiling from source no gcc/compiler version if compiling from source no cuda/cudnn version no gpu model and memory no exact command to reproduce : pythonimport tensorflow as tfv tf.get_variable name=a shape dtype=tf.float initializer=tf.zeros_initializer(),)saver tf.train.saver()with tf.session as s s.run(tf.global_variables_initializer saver.save(s tmp.tf/a saver.restore(s tmp.tf/a describe the problemwhen saving and then restoring variable gb on macos tensorflow throws invalidargumenterror source code logs i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use avx fmatmp.tf/ainfo:tensorflow:restoring parameters from tmp.tf/a w tensorflow/core/framework/op_kernel.cc op_requires failed at save_restore_v_ops.cc invalid argument tmp.tf/a.data--of invalid argumenttraceback most recent call last file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn options feed_dict fetch_list target_list run_metadata file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in call_tf_sessionrun status run_metadata file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in exit c_api.tf_getcode(self.status.status))tensorflow.python.framework.errors_impl.invalidargumenterror tmp.tf/a.data--of invalid argument node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev/tensor_names save/restorev/shape_and_slices) during handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in restore self.saver_def.filename_tensor_name save_path file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in do_run run_metadata file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror tmp.tf/a.data--of invalid argument node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev/tensor_names save/restorev/shape_and_slices) caused by op save/restorev defined at file stdin line in module file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in init self.build file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in build self._build(self._filename build_save=true build_restore=true file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in build build_save=build_save build_restore=build_restore file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in build_internal restore_sequentially reshape file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in addrestoreops restore_sequentially file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/training/saver.py line in bulk_restore return io_ops.restore_v(filename_tensor names slices dtypes file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/ops/gen_io_ops.py line in restore_v shape_and_slices=shape_and_slices dtypes=dtypes name=name file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file users/alyaxey/anaconda/envs/myenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinvalidargumenterror see above for traceback tmp.tf/a.data--of invalid argument node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev/tensor_names save/restorev/shape_and_slices"
315245228,18618,https://api.github.com/repos/tensorflow/tensorflow/issues/18618,yongtang,1,0,0,0,0,0,wrote a script to scan throught the python files in the repo and found the remaining duplicate imports like from tensorflow.python.ops import random_ops-from tensorflow.python.ops import random_ops from tensorflow.python.util.deprecation import deprecated this fix removed all of them for duplicate imports in the repo.signed-off-by yong tang yong.tang.github@outlook.com
315219726,18615,https://api.github.com/repos/tensorflow/tensorflow/issues/18615,yongtang,1,0,0,0,0,0,the inputs of tfrecorddataset have the requirements for shapes.however the check was not done in the shape function this fix adds shape checks whenever possible.signed-off-by yong tang yong.tang.github@outlook.com
314576737,18552,https://api.github.com/repos/tensorflow/tensorflow/issues/18552,OleRoel,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :*no os platform and distribution e.g linux ubuntu dockerfile nvidia/cuda:.-base-ubuntu.my system fedora tensorflow installed from source or binary) :source tensorflow version use command below) :v..-rc python version bazel version n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce ./parameterized_docker_build.sh describe the problembuild the docker image from a fresh clone of tensorflow shellgit clone fetch all tags prunegit checkout b v v..-rccd tensorflow/tensorflow/tools/dockerexport tf_docker_build_is_devel=noexport tf_docker_build_type=gpuexport tf_docker_build_python_version=python./parameterized_docker_build.sh after some time the build fails with: shellattributeerror namespacepath object has no attribute sorttarget tensorflow/tools/pip_package:build_pip_package failed to build source code logs shellerror workspace/tensorflow/tools/api/generator/build executing genrule tensorflow/tools/api/generator:python_api_gen failed exit bash failed error executing command cd home/oroel/projects/picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eabdabedbdaffbe/execroot/org_tensorflow exec env cuda_toolkit_path=/usr/local/cuda cudnn_install_path=/usr/local/cuda gcc_host_compiler_path=/usr/bin/gcc ld_library_path=/usr/local/cuda/extras/cupti/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib path=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin python_bin_path=/usr/bin/python python_lib_path=/usr/local/lib/python./dist-packages tf_cuda_clang tf_cuda_compute_capabilities tf_cuda_version tf_cudnn_version tf_nccl_version tf_need_cuda tf_need_opencl_sycl bin/bash c source external/bazel_tools/tools/genrule/genrule-setup.sh bazel-out/k-py-opt/bin/tensorflow/tools/api/generator/create_python_api bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/app/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/bitwise/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/compat/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/contrib/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/contrib/stat_summarizer/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/data/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/distributions/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/distributions/bijectors/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/errors/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/estimator/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/estimator/export/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/estimator/inputs/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/feature_column/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/gfile/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/graph_util/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/image/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/initializers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/activations/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/densenet/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_resnet_v/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_v/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/mobilenet/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/nasnet/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/resnet/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/xception/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/backend/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/callbacks/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/constraints/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/boston_housing/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/fashion_mnist/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/imdb/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/mnist/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/reuters/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/estimator/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/initializers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/layers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/losses/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/metrics/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/models/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/optimizers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/image/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/sequence/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/text/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/regularizers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/utils/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/scikit_learn/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/layers/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/linalg/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/logging/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/losses/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/manip/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/math/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/metrics/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/nn/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/nn/rnn_cell/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/profiler/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/python_io/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/resource_loader/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/builder/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/constants/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/loader/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/main_op/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_constants/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_def_utils/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/tag_constants/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/utils/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/sets/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/spectral/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/summary/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/sysconfig/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/test/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/train/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/train/queue_runner/__init__.py bazel-out/k-py-opt/genfiles/tensorflow/tools/api/generator/api/user_ops/__init__.py)traceback most recent call last file home/oroel/projects/picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eabdabedbdaffbe/execroot/org_tensorflow/bazel-out/k-py-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py line in module from tensorflow.python.util import tf_decorator file home/oroel/projects/picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eabdabedbdaffbe/execroot/org_tensorflow/bazel-out/k-py-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py line in module from tensorflow.core.framework.graph_pb import file home/oroel/projects/picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eabdabedbdaffbe/execroot/org_tensorflow/bazel-out/k-py-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb.py line in module from google.protobuf import descriptor as descriptor file usr/local/lib/python./dist-packages/google/protobuf/__init__.py line in module import__(pkg_resources).declare_namespace(__name file usr/lib/python/dist-packages/pkg_resources/__init__.py line in module call_aside file usr/lib/python/dist-packages/pkg_resources/__init__.py line in call_aside f(*args kwargs file usr/lib/python/dist-packages/pkg_resources/__init__.py line in initialize_master_working_set add_activation_listener(lambda dist dist.activate file usr/lib/python/dist-packages/pkg_resources/__init__.py line in subscribe callback(dist file usr/lib/python/dist-packages/pkg_resources/__init__.py line in lambda add_activation_listener(lambda dist dist.activate file usr/lib/python/dist-packages/pkg_resources/__init__.py line in activate declare_namespace(pkg file usr/lib/python/dist-packages/pkg_resources/__init__.py line in declare_namespace handle_ns(packagename path_item file usr/lib/python/dist-packages/pkg_resources/__init__.py line in handle_ns rebuild_mod_path(path packagename module file usr/lib/python/dist-packages/pkg_resources/__init__.py line in rebuild_mod_path orig_path.sort(key=position_in_sys_path)attributeerror namespacepath object has no attribute sorttarget tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path sfailed build did not complete successfullybuild failed
314337008,18518,https://api.github.com/repos/tensorflow/tensorflow/issues/18518,several27,4,0,0,0,0,0,system informationcolab.research.google.com using python and gpu runtime describe the problemgetting the module from the hub doesnt work. keyerror the name global_step refers to a tensor which does not exist the operation global_step does not exist in the graph source code logs hub.module
314166948,18496,https://api.github.com/repos/tensorflow/tensorflow/issues/18496,loretoparisi,1,0,0,0,0,0,"i was trying the colab example with tensorflow hub here install the latest tensorflow version.!pip install quiet upgrade pre tensorflow install tf-hub.!pip install tensorflow-hub!pip install tf-nightly to have the nightly build but when i try to install pythonimport tensorflow as tfimport tensorflow_hub as hubimport matplotlib.pyplot as pltimport numpy as npimport osimport pandas as pdimport reimport seaborn as sns i get this version error runtimeerrortraceback most recent call last)=..a from tf-nightly downloading tb_nightly-..a-py-none-any.whl mb mb kb/s requirement already satisfied six in usr/local/lib/python./dist-packages from tf-nightly)requirement already satisfied setuptools in usr/local/lib/python./dist-packages from protobuf>=..->tf-nightly)requirement already satisfied htmllib in usr/local/lib/python./dist-packages from tb-nightly<..a,>=..a->tf-nightly)requirement already satisfied bleach in usr/local/lib/python./dist-packages from tb-nightly<..a,>=..a->tf-nightly)requirement already satisfied markdown in usr/local/lib/python./dist-packages from tb-nightly<..a,>=..a->tf-nightly)requirement already satisfied werkzeug in usr/local/lib/python./dist-packages from tb-nightly<..a,>=..a->tf-nightly)installing collected packages tb-nightly tf-nightlysuccessfully installed tb-nightly-..a tf-nightly-...dev"
314127331,18494,https://api.github.com/repos/tensorflow/tensorflow/issues/18494,yongtang,1,0,0,0,0,0,the softmax_cross_entropy_with_logits has been deprecated and replaced with softmax_cross_entropy_with_logits_v .this causes nn.sampled_softmax_loss to always generate a wanring whenever called this fix replaces softmax_cross_entropy_with_logits with softmax_cross_entropy_with_logits_v and maintains the existing behavior to fix the warning.signed-off-by yong tang yong.tang.github@outlook.com
314079712,18491,https://api.github.com/repos/tensorflow/tensorflow/issues/18491,SimpleJian,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu centos linux release tensorflow installed from source or binary pip install tensorflow-gpu pip install tensorflow-gpu tensorflow version use command below and python version bazel version if compiling from source n/a not compiled from source gcc/compiler version if compiling from source n/a not compiled from source cuda/cudnn version cuda cudnn for tensorflow cuda cudnn for tensorflow gpu model and memory geforce gtx ti mib exact command to reproduce python profile_crf.py describe the problemafter upgrade tensorflow from to crf become x slower.when run in tf it cost about seconds every steps but about seconds every steps in tf the source code are the same see below source code logsthe source code are modified from numpy as npimport tensorflow as tfimport time data settings.num_examples num_words num_features num_tags random features.x np.random.rand(num_examples num_words num_features).astype(np.float random tag indices representing the gold sequence.y np.random.randint(num_tags size= num_examples num_words ).astype(np.int all sequences in this example have the same length but they can be variable in a real model sequence_lengths np.full(num_examples num_words dtype=np.int)sequence_lengths np.full(num_examples num_words dtype=np.int train and evaluate the model.with tf.graph().as_default with tf.session as session add the data to the tensorflow graph x_t tf.constant(x y_t tf.constant(y sequence_lengths_t tf.constant(sequence_lengths compute unary scores from a linear layer weights tf.get_variable(weights num_features num_tags matricized_x_t tf.reshape(x_t num_features matricized_unary_scores tf.matmul(matricized_x_t weights unary_scores tf.reshape(matricized_unary_scores num_examples num_words num_tags compute the log-likelihood of the gold sequences and keep the transition params for inference at test time log_likelihood transition_params tf.contrib.crf.crf_log_likelihood unary_scores y_t sequence_lengths_t compute the viterbi sequence and score viterbi_sequence viterbi_score tf.contrib.crf.crf_decode unary_scores transition_params sequence_lengths_t add a training op to tune the parameters loss tf.reduce_mean(-log_likelihood train_op tf.train.gradientdescentoptimizer(.).minimize(loss session.run(tf.global_variables_initializer mask np.expand_dims(np.arange(num_words axis np.expand_dims(sequence_lengths axis total_labels np.sum(sequence_lengths train for a fixed number of iterations seconds for i in range start_time time.time tf_viterbi_sequence session.run( viterbi_sequence train_op seconds time.time start_time if i and i print(time elapsed format(seconds seconds correct_labels np.sum((y tf_viterbi_sequence mask accuracy correct_labels float(total_labels print(accuracy f accuracy
313898995,18473,https://api.github.com/repos/tensorflow/tensorflow/issues/18473,advaza,2,0,0,0,0,0,system information os platform and distribution linux ubuntu tensorflow installed from source tensorflow version python version bazel version gcc/compiler version cuda/cudnn version gpu model and memory titan x pascal mb memoryim trying to add multi-gpu support to my tensorflow training code using tf.contrib.distribute.mirroredstrategy as a parameter to tf.estimator.runconfig i get the following error message language python traceback most recent call last file python./site-packages/tensorflow/python/training/coordinator.py line in stop_on_exception yield file python./site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py line in call_for_each_tower self merge_args merge_kwargs file python./site-packages/tensorflow/python/training/optimizer.py line in distributed_apply reduced_grads distribution.batch_reduce(sum grads_and_vars file python./site-packages/tensorflow/python/training/distribute.py line in batch_reduce return self._batch_reduce(method_string value_destination_pairs file python./site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py line in batch_reduce value_destination_pairs file python./site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py line in batch_reduce raise valueerror( value_destination_pairs must be a list or a tuple of valueerror value_destination_pairs must be a list or a tuple of tuples of perdevice objects and destinations the following code produces the error i omitted the code for parsing the tfrecord to image tensor as i dont believe this code effects the error but i can add it if necessary language python import glob os import tensorflow as tf slim tf.contrib.slim initialization of args arguments parser def init pass def input_fn dataset tf.data.tfrecorddataset(glob.glob(os.path.join(args.train_data_dir train dataset dataset.map lambda x parse_and_preprocess_image(x args.image_size num_parallel_calls dataset dataset.repeat dataset dataset.batch(batch_size dataset dataset.prefetch return dataset def model_fn(features labels=none mode=tf.estimator.modekeys.train params=none train_images_batch features res slim.convd(inputs=train_images_batch kernel_size stride num_outputs scope=conv loss tf.reduce_mean((train_images_batch res optimizer tf.train.adamoptimizer train_op slim.learning.create_train_op(loss optimizer return tf.estimator.estimatorspec mode=tf.estimator.modekeys.train loss=loss train_op=train_op def train init distribution tf.contrib.distribute.mirroredstrategy(num_gpus=args.num_gpus config tf.estimator.runconfig model_dir=args.log_dir train_distribute=distribution estimator tf.estimator.estimator(model_fn=model_fn config=config estimator.train input_fn=input_fn max_steps=args.train_steps def main train if name main main()thank you!adva
313602622,18450,https://api.github.com/repos/tensorflow/tensorflow/issues/18450,Moocow9m,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu arch linux tensorflow installed from source or binary source from branch r did not compile tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce configure bazel build config=opt tensorflow/tools/pip_package:build_pip_package describe the problemwhile compiling tensorflow build failed i just pulled the latest commit(ecefccabfdaeaabeffa from the r branch i also updated bazel from to and some other programs(should be irrelevant to tensorflow beforehand source code logs error home/userhome/.cache/bazel/_bazel_userhome/abceeaacedb/external/jpeg/build illegal ambiguous match on configurable attribute deps in jpeg//:jpeg:@jpeg//:k@jpeg//:armeabi-vamultiple matches are not allowed unless one is unambiguously more specialized.error analysis of target tensorflow/tools/pip_package:build_pip_package failed build aborted:/home/userhome/.cache/bazel/_bazel_userhome/abceeaacedb/external/jpeg/build illegal ambiguous match on configurable attribute deps in jpeg//:jpeg:@jpeg//:k@jpeg//:armeabi-vamultiple matches are not allowed unless one is unambiguously more specialized.info elapsed time sfailed build did not complete successfully packages loaded) update the r branch compiles successfully the branch might have a fix or perhaps a dependency was updated and that fixed it
313507845,18434,https://api.github.com/repos/tensorflow/tensorflow/issues/18434,samikama,1,0,0,0,0,0,tuple
313127891,18408,https://api.github.com/repos/tensorflow/tensorflow/issues/18408,chiachunfu,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory ti gb exact command to reproduce : import tensorflow as tffrom tensorflow.python.platform import gfilewith tf.session as sess model_filename test_tftrt.pb with gfile.fastgfile(model_filename rb as f graph_def tf.graphdef graph_def.parsefromstring(f.read g_in tf.import_graph_def(graph_def)logdir=/check_graphtrain_writer tf.summary.filewriter(logdir)train_writer.add_graph(sess.graph describe the problemi was trying to write an event file for a tf-trt model so i can visualize it after the graph was optimized by tensorrt when running the snippet above i got error message source code logsg_in tf.import_graph_def(graph_def file home/user/.virtualenvs/buildtf_./local/lib/python./site-packages/tensorflow/python/util/deprecation.py line in new_func return func(*args kwargs file home/user/.virtualenvs/buildtf_./local/lib/python./site-packages/tensorflow/python/framework/importer.py line in import_graph_def raise valueerror(no op named s in defined operations node.op)valueerror no op named trtengineop in defined operations
313076782,18394,https://api.github.com/repos/tensorflow/tensorflow/issues/18394,ankitvgupta,19,0,0,0,0,5,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary pip tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory nvidia k exact command to reproduce n/ayou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.i would love for there to be a sessionrunhook implementation in tensorflow that implements early stopping now that tf.contrib.learn is being officially deprecated the existing way that i did early stopping using a validationmonitor is no longer an option this seems like a super important feature to have.the docs indicate in several places that you can simply extend a sessionrunhook to do this and that seems reasonable i think that having a standard way to do this would be super useful to lots of users perhaps even directly built into trainspec source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.n/a
312674992,18360,https://api.github.com/repos/tensorflow/tensorflow/issues/18360,peastman,8,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu any tensorflow installed from source or binary see below tensorflow version use command below any python version any bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :when you install a precompiled tensorflow binary with pip or conda it doesnt include the c and c interfaces to use it from those languages you need to compile tensorflow for source and build a special version of it that includes the needed api see i suggest including them in the standard binaries this would have multiple benefits.first it would make development in other languages much easier you could just pip install tensorflow and link against the library it installed having to build a special version of tensorflow from source adds an unnecessary barrier to getting started.second it would eliminate a problem that so far ive been unable to resolve i want to create a c library that uses tensorflow that library will be usable directly from c but ill also use swig to create a python wrapper for it for example a user should be able to write python code to build a graph then pass it directly to my library.that wont work right now because there are two separate versions of tensorflow involved one that python is linked against and a different one that my library is linked against when the user is building a graph in python theyre working with one tensorflow library but when they try to pass it to my library theyre suddenly switching to a completely different copy of tensorflow so it doesnt have access to memory allocated by the first copy i havent found any solution to this problem
312663955,18356,https://api.github.com/repos/tensorflow/tensorflow/issues/18356,sadlerjw,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos high sierra tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source xcode apple llvm version clang cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce tensorflow/contrib/makefile/build_all_ios.sh g path/to/model.pb describe the problemas of xcode was working fine on compiling tf for ios using build_all_ios.sh fails complaining that thread-local storage is not supported for the current target this is related to which introduced the thread_local attribute for ios builds source code logs tensorflow/contrib/makefile/build_all_ios.sh g path/to/model.pb ... gcc std=c dis_slim_build fno-exceptions dndebug o dandroid_types=__android_types_full dselective_registration dsupport_selective_registration mios-simulator-version-min arch i mno-sse fembed-bitcode d__thread=thread_local duse_gemm_for_conv wno-c++-narrowing dtf_lean_binary d__android_types_full fno-exceptions isysroot applications/xcode.app/contents/developer/platforms/iphonesimulator.platform/developer/sdks/iphonesimulator..sdk mt users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_i/tensorflow/core/common_runtime/local_device.o mmd mp mf users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/dep/ios_i//tensorflow/core/common_runtime/local_device.td i i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/fftd i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/proto i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/proto_text i/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include i/usr/local/include c tensorflow/core/common_runtime/local_device.cc o users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_i/tensorflow/core/common_runtime/local_device.oin file included from tensorflow/core/common_runtime/local_device.cc::in file included from tensorflow/core/common_runtime/local_device.h::in file included from tensorflow/core/common_runtime/device.h::in file included from tensorflow/core/framework/allocator.h::in file included from tensorflow/core/framework/numeric_types.h::in file included from third_party/eigen/unsupported/eigen/cxx/tensor::in file included from users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/eigen/cxx/tensor::in file included from users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/eigen/cxx/threadpool::/users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/eigen/cxx/src/threadpool/simplethreadpool.h error thread-local storage is not supported for the current target eigen_thread_local perthread per_thread users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/eigen/cxx/src/threadpool/threadlocal.h note expanded from macro eigen_thread_local#define eigen_thread_local static thread command line note expanded from here#define thread thread_local
312605062,18349,https://api.github.com/repos/tensorflow/tensorflow/issues/18349,sibyjackgrove,0,0,0,0,0,2,model_to_dot is not currently accessible from tensorflow keras so visualizing the graph directly using commands like svg(model_to_dot(model).create(prog=dot format=svg is not possible add tf_export(keras.utils.model_to_dot in vis_utils.pyadd from tensorflow.python.keras._impl.keras.utils.vis_utils import model_to_dot in tensorflow/python/keras/_impl/keras/utils/__init__.pyadd from tensorflow.python.keras._impl.keras.utils.vis_utils import model_to_dot in tensorflow/python/keras/utils//__init__.py
312504055,18345,https://api.github.com/repos/tensorflow/tensorflow/issues/18345,TBastiani,1,0,0,0,0,0,system information have i written custom code yes os platform and distribution archlinux/linux tensorflow installed from source will segfault before fb tensorflow version bunknown latest master python version bazel version gcc/compiler version cuda/cudnn version gpu model and memory titan xp/gb exact command to reproduce see below describe the problemwhen trying to run the source code below i get an error saying theres a cycle in my graph this only seems to happen with xla enabled and does not happen if i dont include the additional required control dependency for batch normalisation indeed this seems to occur whenever i use a dynamic rnn in combination with batch normalisation and xla jit support sample code to reproduce pythonimport tensorflow as tf with tf.device(/cpu xin tf.placeholder(tf.float none name=input rnn_cell tf.contrib.rnn.lstmcell out tf.nn.dynamic_rnn(rnn_cell xin dtype=tf.float out tf.layers.batch_normalization(out training=true out tf.identity(out name=output optimiser tf.train.adamoptimizer update_ops tf.get_collection(tf.graphkeys.update_ops with tf.control_dependencies(update_ops out optimiser.minimize(out global_step=tf.variable dtype=tf.float name=train_op)config tf.configproto(allow_soft_placement false)sess tf.session(config=config)sess.run(tf.global_variables_initializer())sample_in sess.run(out feed_dict={xin sample_in output log i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name titan xp major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc adding visible gpu devices i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name titan xp pci bus id compute capability traceback most recent call last file home/thom/.python/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file home/thom/.python/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn target_list status run_metadata file home/thom/.python/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in exit c_api.tf_getcode(self.status.status))tensorflow.python.framework.errors_impl.internalerror cycle detected when adding enter->frame edge edge from gradients/f_count to null would create a cycle null rnn/tensorarraystack/tensorarraygatherv rnn/transpose batch_normalization/moments/mean batch_normalization/moments/squeeze batch_normalization/assignmovingavg/sub batch_normalization/assignmovingavg/mul batch_normalization/assignmovingavg gradients/f_count workaroundi seem to be able to workaround the issue by relying on tf.contrib.layers.batch_norm instead of tf.layers.batch_normalization by setting the updates_collections parameter to none in order to force inlining of the update operation sample code with workaround pythonimport tensorflow as tf with tf.device(/cpu xin tf.placeholder(tf.float none name=input rnn_cell tf.contrib.rnn.lstmcell out tf.nn.dynamic_rnn(rnn_cell xin dtype=tf.float out tf.contrib.layers.batch_norm(out is_training=true updates_collections=none out tf.identity(out name=output optimiser tf.train.adamoptimizer out optimiser.minimize(out global_step=tf.variable dtype=tf.float name=train_op)config tf.configproto(allow_soft_placement false)sess tf.session(config=config)sess.run(tf.global_variables_initializer())sample_in sess.run(out feed_dict={xin sample_in additional information i have already submitted a question on stack overflow a week ago and have had no answers so far make sure you try and reproduce the issue on the tip of master or the sample will cause a segmentation fault
312455869,18342,https://api.github.com/repos/tensorflow/tensorflow/issues/18342,coutner,1,0,0,1,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow) :y os platform and distribution e.g linux ubuntu tensorflow installed from source or binary) :source tensorflow version use command below) :(v..--gdce python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory titan x pascal describe the problem hi,i am trying to quantize ssd_mobilenet_v using tensorflow object detection api.first,i replace all the graph_hook_fn with tf.contrib.quantize.create_training_graph and enable fused batch norm in after training i manage to get the mobilenet_ssd.tflite with the commands below.then,i deploy the tflite model file in a samsung galaxy note but failed with the following exception,which happens in java.lang.runtimeexception java.lang.nullpointerexception can not allocate memory for the interpreter at org.tensorflow.demo.tfliteobjectdetectionapimodel.create(tfliteobjectdetectionapimodel.java at org.tensorflow.demo.detectoractivity.onpreviewsizechosen(detectoractivity.java at org.tensorflow.demo.cameraactivity.onpreviewframe(cameraactivity.java at android.hardware.camera$eventhandler.handlemessage(camera.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at java.lang.reflect.method.invoke(method.java at com.android.internal.os.zygoteinit$methodandargscaller.run(zygoteinit.java at com.android.internal.os.zygoteinit.main(zygoteinit.java caused by java.lang.nullpointerexception can not allocate memory for the interpreter at org.tensorflow.lite.nativeinterpreterwrapper.createinterpreter(native method at org.tensorflow.lite.nativeinterpreterwrapper.(nativeinterpreterwrapper.java at org.tensorflow.lite.interpreter.(interpreter.java at org.tensorflow.demo.tfliteobjectdetectionapimodel.create(tfliteobjectdetectionapimodel.java at org.tensorflow.demo.detectoractivity.onpreviewsizechosen(detectoractivity.java at org.tensorflow.demo.cameraactivity.onpreviewframe(cameraactivity.java at android.hardware.camera$eventhandler.handlemessage(camera.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at java.lang.reflect.method.invoke(method.java at com.android.internal.os.zygoteinit$methodandargscaller.run(zygoteinit.java at com.android.internal.os.zygoteinit.main(zygoteinit.java when i convert tflite back to pb format and check the inference graph in tensorboard,i find some relu_unfused nodes are not quantized properly.what is the problem? before toco convert before toco convert relu from the way,if i use dummy-quantization to try out quantized inference on a float graph,the tflite model works fine in android,and the structure of graph in graphviz dot format just looks the same as my quantized version source code logs strip out problematic nodes before even letting toco see the graphdefbazel run c opt tensorflow/python/tools/optimize_for_inference input=$detect_pb output=$stripped_pb frozen_graph=true input_names=preprocessor/sub output_names=concat,concat alsologtostderr run toco conversion.image_size=bazel run tensorflow/contrib/lite/toco:toco input_file=$stripped_pb output_file=$detect_fb input_format=tensorflow_graphdef output_format=tflite input_shapes=,${image_size},${image_size input_arrays=preprocessor/sub output_arrays=concat,concat inference_type=quantized_uint mean_values std_values dump_graphviz=/tmp"
312255830,18316,https://api.github.com/repos/tensorflow/tensorflow/issues/18316,NightCrawler96,1,0,0,0,0,0,"system information custom code simple q learning model using keras with tf and open ai gym windows tensorflow installed with pip tensorflow-gpu python cuda cudnn nvidia geforce gtx ti gddr gb describtionwhen im running a script randomly two types of errors happen blue screen with driver power state failure tensorflow cannot locate gpu failed call to cuinit cuda_error_no_devicethis happens approximately once per runs geforce is my secondary gpu card so i assume that it is not related to watchdog timer.result od devicequery device geforce gtx ti cuda driver version runtime version cuda capability major/minor version number total amount of global memory mbytes bytes multiprocessors cuda cores/mp cuda cores gpu max clock rate mhz ghz memory clock rate mhz memory bus width bit l cache size bytes maximum texture dimension size x,y,z d d d maximum layered d texture size num layers d layers maximum layered d texture size num layers d layers total amount of constant memory bytes total amount of shared memory per block bytes total number of registers available per block warp size maximum number of threads per multiprocessor maximum number of threads per block max dimension size of a thread block x,y,z max dimension size of a grid size x,y,z maximum memory pitch bytes texture alignment bytes concurrent copy and kernel execution yes with copy engine(s run time limit on kernels yes integrated gpu sharing host memory no support host page-locked memory mapping yes alignment requirement for surfaces yes device has ecc support disabled cuda device driver mode tcc or wddm wddm windows display driver model device supports unified addressing uva yes supports cooperative kernel launch no supports multidevice co-op kernel launch no device pci domain id bus id location id compute mode default multiple host threads can use cudasetdevice with device simultaneously devicequery cuda driver cudart cuda driver version cuda runtime version numdevs result pass"
312185240,18307,https://api.github.com/repos/tensorflow/tensorflow/issues/18307,oscarbg,2,0,0,0,0,0,windows only but this should support amd and intel gpus using dd backend or even directml which with custom metacommands seems even able to use tensor cores on volta gpus..hope to hear news at least by build conference in may
312146647,18302,https://api.github.com/repos/tensorflow/tensorflow/issues/18302,thomasquintana,3,0,0,0,0,0,jsimsa please help thxplease go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory titanx gb exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request. traceback most recent call last file trainer.py line in module main(args file trainer.py line in main train(source meta args.destination file trainer.py line in train dataset dataset.padded_batch(batch_size padded_shapes none none file usr/local/lib/python./dist-packages/tensorflow/python/data/ops/dataset_ops.py line in padded_batch return paddedbatchdataset(self batch_size padded_shapes padding_values file usr/local/lib/python./dist-packages/tensorflow/python/data/ops/dataset_ops.py line in init batching of padded sparse tensors is not currently supported)typeerror batching of padded sparse tensors is not currently supported source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem. pythonfrom argparse import argumentparserimport pickleimport tensorflow as tfdef main(args generate the complete source and meta paths source args.source if not source.endswith(.tfrecords source tfrecords.format(source meta args.source if not source.endswith(.tfrecords meta meta.pkl.format(meta else meta meta.pkl.format(meta.rsplit train the model train(source meta args.destination)def parse(example_proto features bucket tf.fixedlenfeature tf.int coefficients tf.fixedlenfeature tf.string coefficients_length tf.fixedlenfeature tf.int label tf.varlenfeature(tf.int parsed_features tf.parse_single_example(example_proto features bucket tf.cast(parsed_features bucket tf.int coefficients tf.decode_raw(parsed_features coefficients tf.float coefficients_length tf.cast(parsed_features coefficients_length tf.int label tf.cast(parsed_features label tf.int return bucket coefficients coefficients_length labeldef train(source meta destination batch_size epochs load the training meta data file open(meta rb meta pickle.load(file file.close create a tf.data input pipe line dataset tf.data.tfrecorddataset( source dataset dataset.map(parse dataset dataset.padded_batch(batch_size padded_shapes none none dataset dataset.repeat(epochs iterator dataset.make_initializable_iterator bucket coefficients coefficients_length label iterator.get_next with tf.session as session session.run(iterator.initializer print(session.run( bucket coefficients coefficients_length label ))if name main parser argumentparser(description=trains a model parser.add_argument(--destination required=true type=str help=the path where the trained model should be stored parser.add_argument(--source required=true type=str help=the path to the training data args parser.parse_args main(args
311918915,18292,https://api.github.com/repos/tensorflow/tensorflow/issues/18292,voegtlel,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes see below os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bit bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory geforce ti exact command to reproduce see below issue py_func which has a python function referencing the graph might be an unobvious reference see below results in the graph never being garbage collected if you know about this issue usually it can be worked around but in general its a pitfall which should be fixed in the code see below for suggestion source code examplethe following source code produces the leak if you comment out py_func.graph graph you will see that deleted mypyfunc will be printed. pyimport gcimport tensorflow as tfclass mypyfunc def init__(self self.graph none def call__(self inputs return inputs def del__(self print(deleted mypyfunc)def run_graph graph tf.graph with graph.as_default inputs tf.constant py_func mypyfunc op tf.py_func(py_func inputs inputs.dtype comment this out to fix the leak py_func.graph graph with tf.session(graph=graph as sess print(result sess.run(op))run_graph()gc.collect()run_graph()gc.collect analysisthe issue originates from script_ops.py here all py_func s get registered globally i.e these will only be deleted when cleanupfunc is garbage collected it is instantiated at script_ops.py and stored in the graph when py_func is called the idea is that the cleanupfunc.__del method is called when the graph is garbage collected which in turn will delete the reference to the python function so it can get garbage collected as well.in the example mypyfunc contains a reference to the graph by calling py_func the reference to mypyfunc is stored globally in py_funcs funcregistry that means the graph is referenced globally so it will never get garbage collected which in turn means that the cleanupfunc.__del will never be called so mypyfunc will never be deleted from py_funcs importanceconsider the following: pyclass myop def init__(self inputs self.op tf.py_func(self._py_func inputs inputs.dtype def py_func(self inputs return inputs this looks like valid code but having myop.op in your graph will make the graph leak the problem lies in the fact that py_func has is bound to the myop instance which contains a reference to a tensorflow op which in turn has a reference to the graph for the remaining inference see above this is a big issue because tf.estimator.train_and_evaluate creates a lot of graphs during training i.e when it switches between training and evaluation a new graph is created suggestioninstead of globally storing references to the functions and keys to them they might be better stored in the graph directly this also prevents them from being deleted while the graph exists thus they are only cyclic references in the graph and can be garbage collected with the graph instead of storing graph._cleanup_py_funcs_used_in_graph.append(cleanup the function should be stored directly like graph._py_funcs_used_in_graph.append(func
311722412,18271,https://api.github.com/repos/tensorflow/tensorflow/issues/18271,nxphi47,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu macos ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :no gpu model and memory exact command to reproduce describe the problem tf.string_split is documented as if delimiter contains multiple bytes it is treated as a set of delimiters with each considered a potential split point hence each character in a multi-byte string delimiter will become a separate delimiter which is very non-intuitive and difficult to implement.so for example i want to split by to the token eos> tf.string_split( hello world eos tensorflow delimiter=).values equal h ll w rld t n rfl w should be hello world tensorflow can tf.string_split be reimplemented so that it consider the whole delimiter string whether single or multiple character to be its true delimiter instead making every single character in it delimiter?thank you
311277447,18238,https://api.github.com/repos/tensorflow/tensorflow/issues/18238,datwelk,2,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu n/a tensorflow installed from source or binary n/a tensorflow version use command below v..--gdeb python version python bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problem tf.boolean_mask currently grabs individual elements from the input tensor according to the shape and content of the boolean mask argument see the returning tensors dimensionality is always lower than or equal to the input tensor it would be very useful if tf.boolean_mask included an additional optional boolean argument keepdims if keepdims=false the behavior of the method is unchanged if keepdims=true the output tensor would have the same dimensionality as the input tensor this would allow filtering out of certain values from the input while keeping its overall shape as much as possible see example below source code logs tensor mask np.array( true false , false true , false true )boolean_mask(tensor mask keepdims=false boolean_mask(tensor mask keepdims=true mask np.array( true false , false true , false false )boolean_mask(tensor mask keepdims=false boolean_mask(tensor mask keepdims=true"
311090797,18229,https://api.github.com/repos/tensorflow/tensorflow/issues/18229,ShivVM,2,0,0,0,0,0,system informationhave i written custom code as opposed to using a stock example script provided in tensorflow noos platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binarytensorflow version use command below release python version bazel version if compiling from source gcc/compiler version if compiling from source n/acuda/cudnn version using dell laptopgpu model and memory n/a exact command to reproduce python freeze_graph.py input_binary=true input_graph=c:/tmp/output_graph.pb input_checkpoint=c:/tmp/_retrain_checkpoint output_graph=c:/tmp/frozen_graph.pb output_node_names=output_nodei have trained tesnsorflow to identify classes(my own classes of images which has resulted in output_graph.pb retrain_checkpoint.data--of retrain_checkpoint.index retrain_checkpoint.meta checkpoint files being generated.i want to convert this graph file to mobile version so i used below command as mentioned in tutorials..python freeze_graph.py input_binary=true input_graph=c:/tmp/output_graph.pb input_checkpoint=c:/tmp/_retrain_checkpoint output_graph=c:/tmp/frozen_graph.pb output_node_names=output_node i get this error: _saveables self._validateandsliceinputs(names_to_saveables file c:\users\svm\appdata\local\continuum\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py line in validateandsliceinputs variable)typeerror names_to_saveables must be a dict mapping string names to tensors/variables not a variable tensor(final_retrain_ops/biases/final_biases shape dtype=float
311070048,18227,https://api.github.com/repos/tensorflow/tensorflow/issues/18227,ed-alertedh,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below tensorflow-gpu python version bit bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version cuda cudnn gpu model and memory nvidia geforce gtx gb exact command to reproduce import tensorflow as tf import numpy as np import itertools net_inputs np.array estimator tf.estimator.estimator custom model_fn not all that relevant feed a large numpy array in as a single batch my_input_fn tf.estimator.inputs.numpy_input_fn x={inputs net_inputs y=none shuffle=false batch_size=net_inputs.shape this generator only gives one result at a time predictions_generator estimator.predict(input_fn=my_input_fn predict_keys= raw_predictions aggregate the whole batch back together again slow predictions np.reshape np.fromiter(itertools.chain.from_iterable(x raw_predictions for x in predictions_generator input_vects.dtype net_inputs.shape describe the problemthe way the generator in tf.estimator.estimator.predict is currently implemented to yield individual predictions from batched predictions adds quite a bit of overhead specifically the loop here hacked together my own copy of predict that directly yields preds_evaluated and the total time taken on my test-set of a few million examples with a batch size of k was of the time taken when i used the method above to re-aggregate back to a numpy array is there any chance of adding an optional parameter to predict to change the behaviour to this i realise that estimators are more intended for ease-of-use but this would only be a few lines to change and by keeping it as an optional parameter backwards-compatibility would be maintained happy to prepare a pr if theres a chance it would be accepted
311021401,18222,https://api.github.com/repos/tensorflow/tensorflow/issues/18222,ppwwyyxx,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu n/a tensorflow installed from source or binary n/a tensorflow version use command below n/a python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/afollowing the discussion with isaprykin last friday there should be an option for batchnorm to compute the mean variance over the entire batch across gpus when running with distribution strategy.for typical image classification models its not needed however this is essential to other applications where batch-per-gpu is highly limited such as object detection.some related papers
311013276,18219,https://api.github.com/repos/tensorflow/tensorflow/issues/18219,Novak3,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu tensorflow docker under centos tensorflow installed from source or binary tensorflow docker tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory quadro m exact command to reproduce n/a feature request not bug report describe the problemfeature request outputprojectionwrappers for use with tf.nn.bidirectional_dynamic_rnn i dont think this is a bug report per se because i dont think the wrapper was designed for bidirectional rnns tf.nn.bidirectional_dynamic_rnn wants a forward and backward rnn cell and provides a tuple of tuples forward and backward outputs forward and backward output_states the outputprojectionwrapper just wants a cell as input and returns another cell as an output.so while we can wrap both the forward and backward layers in their own outputprojectionwrappers and send those on to tf.nn.bidirectional_dynamic_rnn syntactically it works its not at all what we want conceptually all weve done is project the forward and backward layers independently
310962298,18213,https://api.github.com/repos/tensorflow/tensorflow/issues/18213,ppwwyyxx,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu n/a tensorflow installed from source or binary n/a tensorflow version use command below n/a python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/aaccording to the current padding mode same will depend on the input size to determine how many pixels to pad for example:input kernel stride padding input kernel stride padding however in most other cnn implementations and also historically padding does not depend on the input size for kernel and stride padding usually will be which is actually equivalent to when input=).potential issues inconsistent with models trained in other frameworks its not the first time i have to manually fix the padding when loading a model released by others e.g here this also causes pain for multi-backend framework such as keras because same does not mean the same thing for each backend one example keras issue here also keras has to explicit pad the image in its resnet model here due to how padding is computed by left=total_padding right=total_padding-left the number of pixels padded on left or top of the image may change with different input size as shown by the example above this is not a good default and in particular harmful for pixel-level tasks such as detection&segmentation where all the annotations have an offset starting from the top-left corner of the image.in fact many of tf teams own code has to fix this manually by tf.pad for example tensorflow/benchmarks has a new mode called same_resnet the recent tpu training code has a function called convd_fixed_padding slim has a function called convd_same all these i think its reasonable to add a new mode to make things easier
310946657,18212,https://api.github.com/repos/tensorflow/tensorflow/issues/18212,yongtang,1,0,0,0,0,0,this fix tries to fix the issue raised in where typeerror dict_keys object does not support indexing was thrown when using contrib.distribute in python the issue is that distributedvalues.devices returned self._index.keys which is a dict_keys and is not a list in python this fix converts the dict_keys to list for python to fix the issue.this fix fixes this fix als fixes signed-off-by yong tang yong.tang.github@outlook.com
310806555,18202,https://api.github.com/repos/tensorflow/tensorflow/issues/18202,bennuttall,3,0,0,0,0,0,i maintain the piwheels project where we build arm platform python wheels on raspberry pi and provide them to raspbian users pre-configured in pip.conf for speedy installs as well as automating building most packages were able to manually import wheels built elsewhere.its great that youre now providing raspberry pi builds on jenkins but the current undocumented installation process is to pip install from the url of the file on jenkins if you were able to upload these wheels to piwheels users would just be able to pip install tensorflow and get it.however you seem to provide wheels for python and but not which is the version which ships with raspbian stretch would you be able to build for python as well?also you give your wheels a platform tag of any which is naughty the platform reported on pi zero is linux_armvl and on pi is linux_armvl .if you can provide the following wheels for v and future releases submitted to pypi no need for nightlies python armv python armv python armv python armv python armv python armvthen i can upload them to piwheels and raspberry pi users will have them available no hassle.thanks!p.s if you want to get in touch im ben at raspberrypi dot org
310719110,18193,https://api.github.com/repos/tensorflow/tensorflow/issues/18193,FlashTek,5,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows x tensorflow installed from source or binary binary pip tensorflow version use command below cpu python version bazel version if compiling from source n.a gcc/compiler version if compiling from source n.a cuda/cudnn version n.a gpu model and memory n.a exact command to reproduce describe the problemfollowing the example code for creating estimators from keras models i encountered a problem the example code runs fine when weights=none is used in the initialization of the inceptionv object but when i change it to weights=imagenet the call to model_to_estimator fails failedpreconditionerror attempting to use uninitialized value batch_normalization_/beta node retval_batch_normalization_/beta retval t=dt_float index device=/job:localhost/replica:/task:/device:cpu: (batch_normalization_/beta this looks like the weights of the model are not correctly initialized but strangely i can use the keras model to obtain predictions which should mean that the model is correctly initialized.as far as i know there are no information in the documentation that pre-trained models i.e models with weights set to anything different than none are not supported by this function or that one has to change the workflow somehow therefore this is either a bug in tensorflow or a case of a misleading documentation source code logs python instantiate a keras inception v model.keras_inception_v tf.keras.applications.inception_v.inceptionv(weights=imagenet compile model with the optimizer loss and metrics youd like to train with.keras_inception_v.compile(optimizer=tf.keras.optimizers.sgd(lr momentum loss=categorical_crossentropy metric=accuracy create an estimator from the compiled keras model note the initial model state of the keras model is preserved in the created estimator.est_inception_v tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v treat the derived estimator as you would with any other estimator first recover the input name(s of keras model so we can use them as the feature column name(s of the estimator input function:keras_inception_v.input_names print out input once we have the input name(s we can create the input function for example for input(s in the format of numpy ndarray:train_input_fn tf.estimator.inputs.numpy_input_fn x={input train_data y=train_labels num_epochs shuffle=false to train we call estimators train function:est_inception_v.train(input_fn=train_input_fn steps
310335584,18161,https://api.github.com/repos/tensorflow/tensorflow/issues/18161,rajendraarora16,1,0,0,0,0,0,there was not tensorboard link in readme file since added a nice one to introduce
310255189,18147,https://api.github.com/repos/tensorflow/tensorflow/issues/18147,JoeBanks13,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu arch linux tensorflow installed from source or binary binary wheel pip install tensorflow tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory intel graphics gb ram exact command to reproduce import tensorflow describe the problemive installed tensorflow v via pip and i am having issues working with it upon entering a repl and attempting to import tensorflow the repl silently exits with an exit code of ive run pdb and tried importing it and it appears it is exiting around the from tensorflow.python import section of the tensorflow code looking at the pdb output it appears that code is exiting with error code after running importlib._bootstrap._find_and_load which is a python internal source code logspdb output from trying to import tensorflow around the section that it exits frozen importlib._bootstrap>()_get_module_lock()->_modulelock(...(pdb n frozen importlib._bootstrap>()__enter__()(pdb n--return frozen importlib._bootstrap>()__enter__()->none(pdb n frozen importlib._bootstrap>()_find_and_load()(pdb n frozen importlib._bootstrap>()_find_and_load()(pdb n frozen importlib._bootstrap>()_find_and_load()(pdb n what im trying to run: py python.python default jan gcc on linuxtype help copyright credits or license for more information import tensorflow as tf echo
310145485,18130,https://api.github.com/repos/tensorflow/tensorflow/issues/18130,annarev,1,0,0,0,0,0,everything in contrib/learn/python/learn/datasets/base.py has been deprecated one of the function in there is a decorator retry because another function in that file is decorated with retry the function is called upon import which prints a warning.i have fixed this by adding a private function internal_retry which is used internally and redefining retry to simply call this that way using retry in user-code will still print the deprecated warning but its not printed upon every import.i also cleaned up the docstrings slightly.piperorigin-revid
309977859,18111,https://api.github.com/repos/tensorflow/tensorflow/issues/18111,yaohualibin,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows and linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory exact command to reproduce :pythonimport tensorflow.contrib describe the problemusing tensorflow.contrib produces a warning in the warning iswarning:tensorflow:from envs\tf-.\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py retry from tensorflow.contrib.learn.python.learn.datasets.base is deprecated and will be removed in a future version.instructions for updating:use the retry module or similar alternatives
309911395,18104,https://api.github.com/repos/tensorflow/tensorflow/issues/18104,Dref360,0,0,0,0,0,2,fixes this allows users to have inputs which are not numeric like booleans or strings
309853231,18096,https://api.github.com/repos/tensorflow/tensorflow/issues/18096,yoavz,3,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory gpu exact command to reproduce n/ayou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problem documentation indicates that there are several routine options for cudnnconvolutionbackwardfilter cudnnconvolutionbackwarddata and cudnnpoolingbackward operations they default to non-deterministic atomic operations but have the option to run in a deterministic mode to achieve determinism on tensorflow gpu i would like to be able to make this performance trade-off but currently cannot find a way to enable these options in tensorflow.can a user-facing option be added perhaps in tf.configproto to configurate these cudnn routines this could be configured in a similar way as inter_op_parallelism_threads and intra_op_parallelism_threads are set to to achieve determinism on cpu source code logsn/a
309853029,18094,https://api.github.com/repos/tensorflow/tensorflow/issues/18094,zmjjmz,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu debian tensorflow installed from source or binary installed via pip tensorflow version use command below v..--gdeb python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce requires significant code let me know if necessary describe the problemthis is kind of a simple issue with using keras models as tensorflow estimators i unfortunately need to do this awkward conversion in order to use sagemaker which is even more awkwardly behind by two versions of tensorflow which is fun.basically i have a keras model that expects a tf.string input dtype which is then passed through to a lookup layer for some text embeddings this works fine as a keras model and works fine if i extract the input layers myself and connect them into an estimator however if i go to create an estimator from the model using model_to_estimator i run into this code path conversion then causes the model to break further down the line im not sure why this float cast occurs but this commit seems to imply that keras models are meant to only take floatx input which doesnt really seem right.would not doing this cast break anything if so is there a way to use a non-float input with keras models that need to be converted to estimators?thanks source code logsheres the exact traceback for the issue: /home/u/zach/proj/dataplayground/local/lib/python./site-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters warning:tensorflow:using temporary folder as model directory tmp/tmpwogzk i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use avx fma warning:tensorflow:output final_representation missing from loss dictionary we assume this was done on purpose and we will not be expecting any data to be passed to final_representation during training warning:tensorflow:output oov_code missing from loss dictionary we assume this was done on purpose and we will not be expecting any data to be passed to oov_code during training testing common_estimator_fns.py locally making estimator model dir tmp/tmpwogzk training estimator float tensor(random_shuffle_queue_dequeuemany shape dtype=string device=/device:cpu traceback most recent call last file common_estimator_fns.py line in module hooks= tf_debug.localclidebughook file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train loss self._train_model(input_fn hooks saving_listeners file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in train_model features labels model_fn_lib.modekeys.train self.config file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/estimator/estimator.py line in call_model_fn model_fn_results self._model_fn(features=features kwargs file common_estimator_fns.py line in model_fn return keras_model_fn(features labels mode file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/keras/_impl/keras/estimator.py line in model_fn labels file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/keras/_impl/keras/estimator.py line in clone_and_build_model model models.clone_model(keras_model input_tensors=input_tensors file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/keras/_impl/keras/models.py line in clone_model return clone_functional_model(model input_tensors=input_tensors file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/keras/_impl/keras/models.py line in clone_functional_model output_tensors topology._to_list(layer(computed_tensor kwargs file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in call output super(layer self).__call__(inputs kwargs file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/layers/base.py line in call self._assert_input_compatibility(inputs file home/u/zach/proj/dataplayground/local/lib/python./site-packages/tensorflow/python/layers/base.py line in assert_input_compatibility found dtype str(x.dtype valueerror input of layer lookedup is incompatible with the layer expected dtype=
309428344,18053,https://api.github.com/repos/tensorflow/tensorflow/issues/18053,girving,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu n/a tensorflow installed from source or binary n/a tensorflow version use command below n/a python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemsince tensorflow must be compatible with python tf.print is uppercase however tf.print would work fine for python users and python users with from future import print_function theres no difficulty adding this to the source since all tensorflow source files have from future import print_function and it wouldnt interfere with any other users.objections to me adding
308994208,18016,https://api.github.com/repos/tensorflow/tensorflow/issues/18016,cvanweelden,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu fedora release twenty five tensorflow installed from source or binary binary for cpu tensorflow version use command below v..--gdeb python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemwe have several datasets on which we want to track performance during training corresponding to multiple data splits or different types of data that are processed by the model to do this we run estimator.evaluate for each dataset after each epoch of training.the problem with this is that estimator.evaluate reconstructs the graph and loads the variables from checkpoint each time that it is called in our case reconstructing the large graph takes longer than doing the evaluation on the relatively small datasets id propose a feature to allow evaluating on multiple datasets without reloading the graph.ideas add an estimator.evaluate_multiple method that takes a mapping from name to input_fn add functionality to explicitly start an evaluation session for an estimator and allow passing this as input to estimator.evaluate source code logs model estimator(my_model_fn)for epoch in range(n_epochs estimator.train(train_input_fn steps=n_steps_per_epoch for name eval_input_fn in eval_datasets eval_input_fn create one shot iterators estimator.evaluate(eval_input_fn name=name
308852236,18011,https://api.github.com/repos/tensorflow/tensorflow/issues/18011,nickwhsu,2,0,0,0,0,0,system informationhave i written custom code as opposed to using a stock example script provided in tensorflow noos platform and distribution e.g linux ubuntu ubuntu ltstensorflow installed from source or binary piptensorflow version use command below python version bazel version if compiling from source):gcc/compiler version if compiling from source):cuda/cudnn version:gpu model and memory:exact command to reproduce:describe the problemcurrently tensorflow lite provide c++/java api i would know about any plans for these operations can be made available in the python api
308414616,17995,https://api.github.com/repos/tensorflow/tensorflow/issues/17995,erikbern,6,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary pypi tensorflow version use command below v..--gdeb python version bazel version using a precompiled version not sure cuda/cudnn version not using a gpu gpu model and memory not using a gpu exact command to reproduce python igamma_test.py describe the problem tf.igamma which is the lower regularized incomplete gamma function returns an incorrect derivative with respect to a this is probably very low down on the list of things to fix but i wanted to highlight it since i spent something like hours trying to understand why my model wasnt converging i was fitting a gamma distribution and deep into the code it turns out that tf.igamma doesnt return the right derivative.i suspect the derivative wrt a isnt supported but i would have much rather seen an exception being thrown my workaround ended up being not fitting a with gradient descent but instead just perturbing it by epsilon luckily i only had one single value that i tried to fit)filing this issue mostly in the hope that anyone in the future doesnt waste the same amount of time that i spent source code logs a tf.placeholder(dtype=tf.float shape= none )x tf.placeholder(dtype=tf.float shape= none )y tf.igamma(a x)y_grad_a tf.gradients(y a returns none should return a tensory_grad_x tf.gradients(y x returns a tensor
308403948,17994,https://api.github.com/repos/tensorflow/tensorflow/issues/17994,Hunterwolf88,1,0,0,0,0,0,hello there!sorry for the noob question im new in the world of deep learning especially with gpu processing.im trying to run tensorflow-gpu but probably im doing something wrong.i tried many scripts with always the same result without any solution.i think i installed cuda and cudnn properly as well tensorflow.this is my system:ubuntu gpu nvidia geforce gtx ram gbcuda cudnn conda environment in the example reported below:python==.pathlib==..scandir==.hpy==..keras==..opencv-python==...tensorflow-gpu==..scikit-imagedlibface_recognitiontqdmand basically this is what i get everytime i try to start some kind of training in this example faceswap.py training with cant see any change on nvidia-smi while this is going on so i think the gpu is not actually used.thank you in advance for any kind of help im going mad about this loading trainer from model_original plugin...starting press enter to stop training and save model i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use sse sse avx avx fma i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device device:gpu device name geforce gtx pci bus id compute capability w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate gib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate gib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate gib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib current allocation summary follows i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib in use in bin kib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks mib in use in bin mib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks mib in use in bin mib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks mib in use in bin mib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks mib in use in bin mib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks mib in use in bin mib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks mib in use in bin mib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use gib allocated for chunks gib in use in bin gib client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin for mib was mib chunk state i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xe of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xe of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xe of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xcc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xccc of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xed of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xedaed of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xee of size i tensorflow/core/common_runtime/bfc_allocator.cc summary of in-use chunks by size i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mib i tensorflow/core/common_runtime/bfc_allocator.cc sum total of in-use chunks gib i tensorflow/core/common_runtime/bfc_allocator.cc stats limit inuse maxinuse numallocs maxallocsize w tensorflow/core/common_runtime/bfc_allocator.cc w tensorflow/core/framework/op_kernel.cc resource exhausted oom when allocating tensor with shape and type float on job:localhost/replica:/task:/device:gpu by allocator gpu__bfc exception in thread thread-:traceback most recent call last file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn status run_metadata file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in exit c_api.tf_getcode(self.status.status))tensorflow.python.framework.errors_impl.resourceexhaustederror oom when allocating tensor with shape and type float on job:localhost/replica:/task:/device:gpu by allocator gpu__bfc node model_/convd_/convolution convd t=dt_float data_format=nhwc dilations padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/device:gpu: (model_/pixel_shuffler_/reshape convd_/kernel/read) hint if you want to see a list of allocated tensors when oom happens add report_tensor_allocations_upon_oom to runoptions for current allocation info node loss/mul recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__loss/mul tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu: () hint if you want to see a list of allocated tensors when oom happens add report_tensor_allocations_upon_oom to runoptions for current allocation info.during handling of the above exception another exception occurred:traceback most recent call last file home/hunterwolf/anaconda/envs/faceswap/lib/python./threading.py line in bootstrap_inner self.run file home/hunterwolf/anaconda/envs/faceswap/lib/python./threading.py line in run self._target(*self._args self._kwargs file home/hunterwolf/anacondaprojects/deepfakes-faceswap/scripts/train.py line in processthread raise e file home/hunterwolf/anacondaprojects/deepfakes-faceswap/scripts/train.py line in processthread trainer.train_one_step(epoch self.show if save_iteration or self.save_now else none file home/hunterwolf/anacondaprojects/deepfakes-faceswap/plugins/model_original/trainer.py line in train_one_step loss_a self.model.autoencoder_a.train_on_batch(warped_a target_a file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/engine/training.py line in train_on_batch outputs self.train_function(ins file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/backend/tensorflow_backend.py line in call self.session_kwargs file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/client/session.py line in do_run options run_metadata file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.resourceexhaustederror oom when allocating tensor with shape and type float on job:localhost/replica:/task:/device:gpu by allocator gpu__bfc node model_/convd_/convolution convd t=dt_float data_format=nhwc dilations padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/device:gpu: (model_/pixel_shuffler_/reshape convd_/kernel/read) hint if you want to see a list of allocated tensors when oom happens add report_tensor_allocations_upon_oom to runoptions for current allocation info node loss/mul recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__loss/mul tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu: () hint if you want to see a list of allocated tensors when oom happens add report_tensor_allocations_upon_oom to runoptions for current allocation info.caused by op model_/convd_/convolution defined at file home/hunterwolf/anaconda/envs/faceswap/lib/python./threading.py line in bootstrap self._bootstrap_inner file home/hunterwolf/anaconda/envs/faceswap/lib/python./threading.py line in bootstrap_inner self.run file home/hunterwolf/anaconda/envs/faceswap/lib/python./threading.py line in run self._target(*self._args self._kwargs file home/hunterwolf/anacondaprojects/deepfakes-faceswap/scripts/train.py line in processthread model pluginloader.get_model(trainer)(get_folder(self.arguments.model_dir self.arguments.gpus file home/hunterwolf/anacondaprojects/deepfakes-faceswap/plugins/model_original/autoencoder.py line in init self.initmodel file home/hunterwolf/anacondaprojects/deepfakes-faceswap/plugins/model_original/model.py line in initmodel self.autoencoder_a kerasmodel(x self.decoder_a(self.encoder(x file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/engine/topology.py line in call output self.call(inputs kwargs file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/engine/topology.py line in call output_tensors self.run_internal_graph(inputs masks file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/engine/topology.py line in run_internal_graph output_tensors to_list(layer.call(computed_tensor kwargs file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/layers/convolutional.py line in call dilation_rate=self.dilation_rate file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/keras/backend/tensorflow_backend.py line in convd data_format=tf_data_format file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in convolution return op(input filter file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in call return self.conv_op(inp filter file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in call return self.call(inp filter file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in call name=self.name file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in convd data_format=data_format dilations=dilations name=name file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file home/hunterwolf/anaconda/envs/faceswap/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessresourceexhaustederror see above for traceback oom when allocating tensor with shape and type float on job:localhost/replica:/task:/device:gpu by allocator gpu__bfc node model_/convd_/convolution convd t=dt_float data_format=nhwc dilations padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/device:gpu: (model_/pixel_shuffler_/reshape convd_/kernel/read) hint if you want to see a list of allocated tensors when oom happens add report_tensor_allocations_upon_oom to runoptions for current allocation info. node loss/mul recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge__loss/mul tensor_type=dt_float device=/job:localhost/replica:/task:/device:cpu: () hint if you want to see a list of allocated tensors when oom happens add report_tensor_allocations_upon_oom to runoptions for current allocation info
308331538,17982,https://api.github.com/repos/tensorflow/tensorflow/issues/17982,tamimaddari82,1,0,0,0,0,0,used work_sharder to shard the computation of each batch according to
308186516,17965,https://api.github.com/repos/tensorflow/tensorflow/issues/17965,iganichev,0,0,0,0,1,0,have i written custom code as opposed to using a stock example script provided in tensorflow custom code os platform and distribution e.g linux ubuntu linux tensorflow installed from source or binary binary tensorflow version use command below v..--gaa python version exact command to reproduce : python import tensorflow as tf def body(v m tf.constant( v v ta tf.tensorarray(dtype=tf.float size t ta.write m return t cond tf.constant(false t tf.cond(cond true_fn=lambda body false_fn=lambda body with tf.session as ss print(ss.run(t.stack())) the code above leads to the following error:traceback most recent call last file array_cond.py line in module print(ss.run(t.stack file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror retval does not have valueinterestingly if false is replace with true it works as expected
308122876,17961,https://api.github.com/repos/tensorflow/tensorflow/issues/17961,wendazhou,2,0,0,4,0,0,this pull request implements grouped convolutions backed by the cudnn convolution groups feature.the feature is exposed in the dnn support class and the convd ops launchers but no api operations are created to instantiate grouped convolutions directly.this pull request also implements dispatching the depthwisenativeconvd and the corresponding backpropagation operations to these new grouped convolution kernels this feature is gated behind a feature flag tf_depthwise_conv_use_grouped_conv which is disabled by default this increases performance slightly for training mobilenet in single precision about on volta but also paves the way for float training which is very slow with the current implementation).todo need advice directions add tests for the new depthwisenativeconvd path probably duplicate existing tests but will also need to set the environment variable how to do this the feature flag may not be completely satisfactory at the moment users compiling on cudnn and setting the feature flag will face a somewhat cryptic error that no algorithms are found as the shapes will be incorrect for the convd kernels
308024416,17950,https://api.github.com/repos/tensorflow/tensorflow/issues/17950,dhgrs,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :i use docker tensorflow/tensorflow:..-rc-devel-gpu-py tensorflow version use command below) :..-rc python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :cuda gpu model and memory :ti(gb exact command to reproduce :see my gist below describe the problemi used networks in tf.keras.applications and tf.keras.model_to_estimator i noticed that training loss gets low but validation loss doesnt when i dont use pretrained model and train from scratch i doubted overfitting so i tried evalutating on training dataset and get large validation loss althogh traing loss gets low inspite of same dataset i think parameters of batchnormalization are not updated when use model_to_estimator isnt it a bug?! loss source code logs
307808207,17933,https://api.github.com/repos/tensorflow/tensorflow/issues/17933,rvfischione,6,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes see below os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see below describe the problemincidentally this is both a bug report and a feature request to reproduce the bug run the following: pythonimport tensorflow as tffrom tensorflow.python import debug as tf_debugsession tf.session()session tf_debug.tensorboarddebugwrappersession(session localhost:)current_epoch tf.variable trainable=false name=current_epoch)increment_epoch tf.assign(current_epoch current_epoch start_epoch current_epoch.eval(session)print(start_epoch start_epoch)for cur_epoch in range(start_epoch session.run(increment_epoch) the following is the output: traceback most recent call last file c:/workspace/test.py line in module for cur_epoch in range(start_epoch typeerror unimplementederror object cannot be interpreted as an integerstart_epoch grpc debug url scheme is not implemented on windows yet. its clear that the error is being assigned to a variable instead of being raised furthermore the behavior is only present when evaluating a variable this is unexpected as it has nothing to do with the session itself.i came across this while trying to enable debugging via tensorboard after some digging it appears that the error message originates from tensorflow core code so thats why this issue is here and not in the tensorboard project is there any status on implementation of this feature if not could there be a log warning or some sort of feedback to reflect this the tensorboard modal at gives no indication that it is platform-dependent
307781735,17932,https://api.github.com/repos/tensorflow/tensorflow/issues/17932,lhlmgr,2,0,0,0,0,0,"hello everyone,i just tried the new function to group variable length inputs for the dataset api namely tf.contrib.data.bucket_by_sequence_length for a small estimator-model.i implemented the input_fn such that it returns a dataset where each element is a tuple feature-dict label however when i run it i get following exception traceback most recent call last file home/leo/anaconda/lib/python./site-packages/tensorflow/python/data/ops/dataset_ops.py line in apply dataset transformation_func(self file home/leo/anaconda/lib/python./site-packages/tensorflow/contrib/data/python/ops/grouping.py line in apply_fn window_size_func=window_size_fn file home/leo/anaconda/lib/python./site-packages/tensorflow/python/data/ops/dataset_ops.py line in apply dataset transformation_func(self file home/leo/anaconda/lib/python./site-packages/tensorflow/contrib/data/python/ops/grouping.py line in apply_fn window_size_func file home/leo/anaconda/lib/python./site-packages/tensorflow/contrib/data/python/ops/grouping.py line in init self._make_key_func(key_func input_dataset file home/leo/anaconda/lib/python./site-packages/tensorflow/contrib/data/python/ops/grouping.py line in make_key_func self._key_func.add_to_graph(ops.get_default_graph file home/leo/anaconda/lib/python./site-packages/tensorflow/python/framework/function.py line in add_to_graph self._create_definition_if_needed file home/leo/anaconda/lib/python./site-packages/tensorflow/python/framework/function.py line in create_definition_if_needed self._create_definition_if_needed_impl file home/leo/anaconda/lib/python./site-packages/tensorflow/python/framework/function.py line in create_definition_if_needed_impl outputs self._func(*inputs file home/leo/anaconda/lib/python./site-packages/tensorflow/contrib/data/python/ops/grouping.py line in tf_key_func ret key_func(*nested_args typeerror element_to_bucket_id takes positional argument but were givenhere is a link to the function.here is a code snipped to reproduce the error: pythonimport tensorflow as tfdef input_fn def generator text label for x y in zip(text label yield x y dataset tf.data.dataset.from_generator(generator=generator output_shapes=(tf.tensorshape( none tf.tensorshape output_types=(tf.int tf.int dataset dataset.map(parse_example dataset dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=element_length_fn bucket_batch_sizes bucket_boundaries pad_to_bucket_boundary=false return datasetdef parse_example(x y return dict x=x ydef element_length_fn(element features label element return tf.shape(features x ) if name main with tf.session as sess dataset input_fn iter dataset.make_one_shot_iterator print(sess.run(iter.get_next())) my env-specs are logged in tf_env.txt in advance"
307436541,17908,https://api.github.com/repos/tensorflow/tensorflow/issues/17908,benbarsdell,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary docker image tensorflow/tensorflow:nightly-devel-gpu tensorflow version use command below nightly also rc python version bazel version if compiling from source na gcc/compiler version if compiling from source na cuda/cudnn version gpu model and memory na exact command to reproduce :copying repro from tensorflow as tfimport tensorflow.contrib.nccl as ncclif name main devices gpu gpu with tf.device(devices data tf.constant received_tensor nccl.broadcast(data received_tensors for device in devices with tf.device(device received_tensors.append(tf.identity(received_tensor sess tf.session sess.run(received_tensors describe the problem traceback most recent call last file stdin line in module file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.unimplementederror this op should be replaced during graph optimization node ncclbroadcast ncclbroadcast t=dt_float shape device=/job:localhost/replica:/task:/device:gpu: (const) caused by op uncclbroadcast defined at file stdin line in module file usr/local/lib/python./dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py line in broadcast return gen_nccl_ops.nccl_broadcast(input=tensor shape=tensor.shape file usr/local/lib/python./dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py line in nccl_broadcast ncclbroadcast input=input shape=shape name=name file usr/local/lib/python./dist-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessunimplementederror see above for traceback this op should be replaced during graph optimization node ncclbroadcast ncclbroadcast t=dt_float shape device=/job:localhost/replica:/task:/device:gpu: (const
306717552,17851,https://api.github.com/repos/tensorflow/tensorflow/issues/17851,Purpleslz,2,0,0,0,0,0,im a starter in eager execution and not familiar in dl framework however in practice multiple gpus training is an important feature pytorch has nn.dataparallel and distributed package to support distributed training recently im working on training model using eager execution with multiple gpus and i have noticed that in alextp said were still fairly early in the project so for now threading is the only supported way_.i have two questions about it if there is an example about using threading to train with multiple gpus in eager execution it will help a lot to starters i have concerns about performance using threading(only one thread can run python at one time in cython implementation could threading speed up the training process for example if i have some python operations betweens tf operations(eg denselayer some operations using numpy python list etc denselayer those tf operations(denselayer in different threads could be parallelized but those numpy operations in different threads are not going to be parallelizable?sorry for my poor english please correct me if im wrong thank you
306716096,17850,https://api.github.com/repos/tensorflow/tensorflow/issues/17850,jessebenson,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux debian:stretch armv and armv tensorflow installed from source or binary n/a tensorflow version use command below python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemthere has been significant community effort to build tensorflow from source for arm e.g and it would be great to get these as part of the official/nightly builds python wheel/libtensorflow.so/libtensorflow_framework.so binaries source code logsn/a
306469957,17824,https://api.github.com/repos/tensorflow/tensorflow/issues/17824,lhlmgr,6,0,0,0,0,0,"hello everyone,as mentioned in the getting started with tensorflow custom estimators one has to know the expected label for the data since the labels will be discarded during the predict function. python generate predictions from the modelexpected setosa versicolor virginica predict_x sepallength sepalwidth petallength petalwidth predictions classifier.predict input_fn=lambda:iris_data.eval_input_fn(predict_x batch_size=args.batch_size)) which can be seen here features input_hooks self._get_features_from_input_fn input_fn model_fn_lib.modekeys.predict estimator_spec self._call_model_fn features none model_fn_lib.modekeys.predict self.config) i totally agree to discard the labels and dont pass them to the model_fn function however it would be much easier to return them also from the input_fn -function if they are provided a simplified solution without the case distinction of given/not given labels could be: python def predict(self input_fn predict_keys=none hooks=none checkpoint_path=none yields predictions for given features args input_fn a function that constructs the features prediction continues until input_fn raises an end-of-input exception outofrangeerror or stopiteration see get_started/premade_estimators#create_input_functions for more information the function should construct and return one of the following a tf.data.dataset object outputs of dataset object must have same constraints as below features a tensor or a dictionary of string feature name to tensor features are consumed by model_fn they should satisfy the expectation of model_fn from inputs a tuple in which case the first item is extracted as features predict_keys list of str name of the keys to predict it is used if the estimatorspec.predictions is a dict if predict_keys is used then rest of the predictions will be filtered from the dictionary if none returns all hooks list of sessionrunhook subclass instances used for callbacks inside the prediction call checkpoint_path path of a specific checkpoint to predict if none the latest checkpoint in model_dir is used yields evaluated values of predictions tensors raises valueerror could not find a trained model in model_dir valueerror if batch length of predictions are not same valueerror if there is a conflict between predict_keys and predictions for example if predict_keys is not none but estimatorspec.predictions is not a dict hooks check_hooks_type(hooks check that model has been trained if not checkpoint_path checkpoint_path saver.latest_checkpoint(self._model_dir if not checkpoint_path raise valueerror(could not find trained model in model_dir format self._model_dir with ops.graph().as_default as g random_seed.set_random_seed(self._config.tf_random_seed self._create_and_assert_global_step(g features labels input_hooks self._get_features_and_labels_from_input_fn input_fn model_fn_lib.modekeys.predict estimator_spec self._call_model_fn features none model_fn_lib.modekeys.predict self.config predictions self._extract_keys(estimator_spec.predictions predict_keys all_hooks list(input_hooks all_hooks.extend(hooks all_hooks.extend(list(estimator_spec.prediction_hooks or with training.monitoredsession session_creator=training.chiefsessioncreator checkpoint_filename_with_path=checkpoint_path master=self._config.master scaffold=estimator_spec.scaffold config=self._session_config hooks=all_hooks as mon_sess while not mon_sess.should_stop preds_evaluated gt_labels mon_sess.run( predictions labels if not isinstance(predictions dict for pred true_label in zip(preds_evaluated gt_labels yield pred true_label else for i in range(self._extract_batch_length(preds_evaluated yield key value i for key value in six.iteritems(preds_evaluated gt_labels i os platform and distribution ubuntu ltstensorflow installed from piptensorflow version tensorflow-gpu bazel version n/acuda/cudnn version n/agpu model and memory n/aexact command to reproduce n/a"
306420697,17822,https://api.github.com/repos/tensorflow/tensorflow/issues/17822,GPhilo,5,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu tested on raspberry pi raspbian ubuntu cross-compile to arm docker tensorflow/tensorflow:nightly-devel cross-compile to arm tensorflow installed from source or binary source tensorflow version use command below latest current master on git python version bazel version if compiling from source build is done via makefiles gcc/compiler version if compiling from source tested with on raspbian on raspbian on ubuntu and whatever is in the docker image cuda/cudnn version gpu model and memory exact command to reproduce follow instructions given in describe the problembuild fails with error set e dirname tensorflow/contrib/lite/build_rpi_lib.sh cd tensorflow/contrib/lite pwd script_dir=/tensorflow/tensorflow/contrib/lite cd tensorflow/tensorflow/contrib/lite cc_prefix=arm-linux-gnueabihf make j f tensorflow/contrib/lite/makefile target=rpi target_arch=armv/bin/sh not foundarm-linux-gnueabihf-gcc std=c o dndebug march=armv-a mfpu=neon-vfpv funsafe-math-optimizations ftree-vectorize i i/tensorflow/tensorflow/contrib/lite i/tensorflow/tensorflow/contrib/lite/downloads i/tensorflow/tensorflow/contrib/lite/downloads/eigen i/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp i/tensorflow/tensorflow/contrib/lite/downloads/neon__sse i/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src i/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include i/tensorflow/tensorflow/contrib/lite/../../../bazel-genfiles/tensorflow/core/framework i/tensorflow/tensorflow/contrib/lite/gen/obj i/usr/local/include c tensorflow/contrib/lite/tools/benchmark_model.cc o tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv/tensorflow/contrib/lite/tools/benchmark_model.oin file included from tensorflow/core/lib/core/errors.h from tensorflow/core/platform/env.h from tensorflow/contrib/lite/tools/benchmark_model.cc::./tensorflow/core/lib/core/status.h fatal error tensorflow/core/lib/core/error_codes.pb.h no such file or directorycompilation terminated.tensorflow/contrib/lite/makefile recipe for target tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv/tensorflow/contrib/lite/tools/benchmark_model.o failedmake tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv/tensorflow/contrib/lite/tools/benchmark_model.o error note the build for the tflite static library target completes its the benchmark program that fails
306246018,17811,https://api.github.com/repos/tensorflow/tensorflow/issues/17811,philippnormann1337,16,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux debian buster/sid tensorflow installed from source or binary binary tensorflow version use command below rc python version rc bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see source code below describe the problemfeature columns offer easy and reproducible feature encodings it would be nice if they could be made compatible with sequential data as well i currently would like to use them for processing inputs in a custom rnn estimator and sadly discovered that the current input_layer api doesnt support sequential inputs it always returns a tensor shaped batch_size first_layer_dimension which makes it unusable in combination with the dynamic_rnn wrapper which expects inputs of shape batch_size max_time first_layer_dimension source code logsmy first attempt at a workaround for this shortcoming was mapping the input_layer function across the sequences of the inputs pythondef model_fn(features labels mode params config sequence_lengths features.pop(session_length rnn_cell state_size initializer params rnn_cell .values encoded_features tf.map_fn(lambda input_features tf.feature_column.input_layer(input_features params feature_columns features dtype=tf.float encoded_labels tf.tile(tf.map_fn(lambda seqeunce_labels tf.feature_column.input_layer(seqeunce_labels params label_columns labels dtype=tf.float sequence_lengths cell rnn_cell(num_units=state_size initializer=initializer outputs state tf.nn.dynamic_rnn(cell=cell inputs=encoded_features dtype=tf.float sequence_length=sequence_lengths logits tf.contrib.layers.fully_connected(outputs num_outputs activation_fn=none predictions tf.nn.softmax(logits loss tf.losses.softmax_cross_entropy(encoded_labels logits optimizer tf.train.adamoptimizer(learning_rate=params learning_rate train_op optimizer.minimize(loss=loss global_step=tf.train.get_global_step return tf.estimator.estimatorspec(mode predictions=predictions train_op=train_op loss=loss) due to some kind of frame error this sadly is also not possible maybe i am missing something here and any tips on why this is failing would be highly appreciated. bashinvalidargumenterror the node group_deps has inputs from different frames the input map/while/input_layer/url_indicator/url_lookup/hash_table/table_init is in frame map/while/while_context the input map_/while/input_layer/label_indicator/label_lookup/hash_table/table_init is in frame map_/while/while_context. in my opinion a native higher-level solution to this problem is needed nonetheless
306196252,17801,https://api.github.com/repos/tensorflow/tensorflow/issues/17801,roman-orekhov,10,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu lts gnu/linux rt-tegra aarch tensorflow installed from source or binary source tensorflow version use command below rc r master python version bazel version if compiling from source gcc/compiler version if compiling from source ubuntu cuda/cudnn version gpu model and memory name gp major minor totalmemory gib as reported by some previous version of tensorflow on the other part of the same drive px exact command to reproduce : git checkout v..-rc ./configure bazel build tensorflow:libtensorflow_cc.so describe the problemsame problem for all branches listed under tensorflow version .build fails immediately with cuda configuration error cannot find libdevice..bc under usr/local/cuda-. its strange because i explicitly answered versions and libdevice..bc exists only in afaik source code logs error.log
306068048,17783,https://api.github.com/repos/tensorflow/tensorflow/issues/17783,qtdaniel,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow i have written custom code a simple reproduction script is included below os platform and distribution e.g linux ubuntu linux ubuntu also occurring on linux ubuntu tensorflow installed from source or binary tensorflow installed from binary also occurring after building from source tensorflow version use command below tensorflow v also occurring on v v v.rc python version python ubuntu base and also occurring on anaconda python bazel version if compiling from source bazel version gcc/compiler version if compiling from source gcc cuda/cudnn version cuda not used cpu only gpu model and memory gpu not used cpu only intel(r xeon(r cpu e and intel(r xeon(r platinum m exact command to reproduce command to reproduce using script given below python sfi.py describe the problema segmentation fault is occurring with the following gdb backtrace when a logits tensor of sufficient size is passed to sparse_softmax_cross_entropy_with_logits the single argument to the demonstration code below adjusts the size i have found that there is a point below which the segfault does not seem to ever occur and above which the segfault always seems to occur but around that point e.g within the segfault behaviour is intermittent right on the change point i can run the same code with the same argument and it will sometimes generate a segfault and sometimes not though the random data generated in the demo code may be causing this randomness). program received signal sigsegv segmentation fault. switching to thread xfffcdffb lwp xfffefa in eigen::internal::innermostdimreducer::reduce(eigen::tensorevaluator/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(gdb bt xfffefa in eigen::internal::innermostdimreducer::reduce(eigen::tensorevaluator/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffefaf in eigen::internal::evalrange::run(eigen::tensorevaluator/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffedcd in eigen::threadpooldevice::parallelfor(long eigen::tensoropcost const std::function/.conda/envs/research/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfffebcecb in eigen::nonblockingthreadpooltempl::workerloop(int from home_dir>/.conda/envs/research/lib/python./site-packages/tensorflow/python/../libtensorflow_framework.so xfffebcebe in std::_function_handler::_m_invoke(std::_any_data const from home_dir>/.conda/envs/research/lib/python./site-packages/tensorflow/python/../libtensorflow_framework.so xfffeb in from usr/lib/x_-linux-gnu/libstdc++.so xffffe in start_thread arg=xfffcdffb at pthread_create.c xffffed in clone at sysdeps/unix/sysv/linux/x_/clone.s: the problem occurs in all of these configurations binary cpu-only install of tf v v v and v build from source code v v building with mkl and without mkl ubuntu python and anaconda python but only python in both cases in clean virtual/conda envs with only the minimum tf dependencies installed source code logsive been able to distil the problem down to the following code which reliably reproduces the problem for me on my local hardware and also on a m.xlarge ec instance running ubuntu server or amazon linux the following code has no external data or code dependencies other than tensorflow.the script has a single argument which sets the vocabulary size this was originally an rnn lm if this value is large enough a segfault occurs the only operation of note is the sparse_softmax_cross_entropy_with_logits . #!/usr/bin/env pythonfrom future import absolute_import division print_function unicode_literalsimport sysimport tensorflow as tfdef main vocabulary_size int(sys.argv batch_size step_size print(vocabulary size vocabulary_size labels tf.get_variable(labels shape= batch_size step_size dtype=tf.int logits tf.get_variable(logits shape= batch_size step_size vocabulary_size costs tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels logits=logits with tf.session as session session.run(tf.global_variables_initializer print(executing session.run(costs print(success!)if name main main
306066913,17782,https://api.github.com/repos/tensorflow/tensorflow/issues/17782,waschbaer00,3,0,0,0,0,0,"hi there,as in the release notes said it is compatible to and you said you are going to fix the bug till februray so is tf now compatible to cuda"
306049832,17778,https://api.github.com/repos/tensorflow/tensorflow/issues/17778,jtavrisov,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary source tensorflow version use command below most recent pulled from github gcc/compiler version if compiling from source vs cuda/cudnn version cpu only bazel version na gpu model and memory na exact command to reproduce practically any tensorflow command in debugi have a large project c that uses a cnn for a small part of the code i have built tensorflow from sources as a shared library using the cmake instructions i have linked against this built library and integrated it in our code the project uses qtcreator and qmake.in currently works perfect in release mode however as it is a larger project there is needs to occasionally build in debug when doing a debug build i get an exception thrown on basically the first instance of tensorflow currently a call to readbinaryproto if you comment that line out it will break on the next tensorflow call.i have built a standalone project in visual studio that isolates the tensorflow part of the project and it behaves exactly the same way.the exception is microsoft c exception std::bad_alloc at memory location xfafc.i know there is no supported way to build a debug version of the library but i need to use this library like this i have no pressing need to debug tensorflow related code just the rest of it.thank you
305793205,17753,https://api.github.com/repos/tensorflow/tensorflow/issues/17753,huismiling,2,0,0,0,0,0,cat etc/issue linux xxxx generic ubuntu smp fri feb utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux xxxxx generic ubuntu smp fri feb utc x x x gnu/linux check pips numpy numpydoc protobuf tensorflow rc)tensorflow-tensorboard check for virtualenv false tensorflow import tf.version rctf.git_version bv..-rc--gafbtf.compiler_version bv..-rc--gafbsanity check array dtype=int env ld_library_path home/user/software_install/tensorrt-../lib:/usr/local/cuda/lib:dyld_library_path is unset nvidia-smi fri mar nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx off on n/a c p w w mib mib default processes gpu memory gpu pid type process name usage g usr/lib/xorg/xorg mib g compiz mib cuda libs usr/local/cuda-./lib/libcudart_static.a/usr/local/cuda-./lib/libcudart.so.../usr/local/cuda-./doc/man/man/libcudart./usr/local/cuda-./doc/man/man/libcudart.so.++++++++++problem+++++++++++i try to quantize my model computation is as follow:pointwise_convd(relu(depthwise_convd(x)))then i use tf.contrib.quantize.create_eval_graph to quantize graph however there is an error when i use lite.toco_convert to convert model into tflite.i want to konw how to change the graph to contain min/max information for relu layer.msg:array conv_depthwise_relu which is an input to the conv operator producing the output array conv is lacking min/max data which is necessary for quantization either target a non-quantized output format or change the input graph to contain min/max information or pass default_ranges_min and default_ranges_max if you do not care about the accuracy of results. quant.zip
305579751,17738,https://api.github.com/repos/tensorflow/tensorflow/issues/17738,seungjooli,3,0,0,0,0,0,folding batch normalization which has parameters like is_training=false fuse=false scale=false raises an error because its batch_mean_tensor is none i suggest using moving_mean_tensor instead
305523597,17736,https://api.github.com/repos/tensorflow/tensorflow/issues/17736,sandflee,1,0,0,0,0,0,"system information have i written custom code yes os platform and distribution centos linux release tensorflow installed from binary tensorflow version v..--gdeb python version cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemworker task will crashed after s if ps task or other worker task not started not sure its a bug or misused the api source code logslogs i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use avx fma i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps bjlt-h.sy i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost bjlt-h.sy i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost:variables initialized warning:tensorflow:from demo.py init from tensorflow.python.training.supervisor is deprecated and will be removed in a future version.instructions for updating:please switch to tf.train.monitoredtrainingsession e tensorflow/core/distributed_runtime/master.cc master init unavailable os errortraceback most recent call last file demo.py line in module tf.app.run(main=main file usr/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(argv file demo.py line in main with sv.prepare_or_wait_for_session(server.target config tf.configproto(gpu_options=gpu_options allow_soft_placement true log_device_placement true as sess file usr/lib/python./site-packages/tensorflow/python/training/supervisor.py line in prepare_or_wait_for_session init_feed_dict=self._init_feed_dict init_fn=self._init_fn file usr/lib/python./site-packages/tensorflow/python/training/session_manager.py line in prepare_session sess.run(init_op feed_dict=init_feed_dict file usr/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file usr/lib/python./site-packages/tensorflow/python/client/session.py line in do_run options run_metadata file usr/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.unavailableerror os errorfull code see pythondef main cluster specification flags.task_index int(os.environ tf_index flags.job_name os.environ tf_role cluster_def json.loads(os.environ tf_cluster_def cluster tf.train.clusterspec(cluster_def print(clusterspec cluster_def print(current task id flags.task_index role flags.job_name gpu_options tf.gpuoptions(allow_growth true server tf.train.server(cluster job_name=flags.job_name,task_index=flags.task_index,config tf.configproto(gpu_options=gpu_options,allow_soft_placement true if flags.job_name ps server.join elif flags.job_name worker set the train parameters learning_rate flags.learning_rate training_epochs flags.training_epochs batch_size flags.batch_size iterdata traindata(flags.data_path batch_size with tf.device(tf.train.replica_device_setter(worker_device=(/job:worker/task:%d%(flags.task_index)),cluster=cluster count the number of updates global_step tf.get_variable(global_step initializer tf.constant_initializer trainable false input with tf.name_scope(input x tf.placeholder(tf.float shape= none name=x-input y tf.placeholder(tf.float shape= none name=y-input model parameters tf.set_random_seed with tf.name_scope(weights w tf.variable(tf.random_normal w tf.variable(tf.random_normal bias with tf.name_scope(biases b tf.variable(tf.zeros b tf.variable(tf.zeros implement model with tf.name_scope(softmax y is our prediction z tf.add(tf.matmul(x,w),b a tf.nn.softmax(z z tf.add(tf.matmul(a,w),b y tf.nn.softmax(z specify cost function with tf.name_scope(cross_entropy this is our cost cross_entropy tf.reduce_mean(-tf.reduce_sum(y tf.log(y reduction_indices specify optimizer with tf.name_scope(train optimizer is an operation which we can execute in a session grad_op tf.train.gradientdescentoptimizer(learning_rate train_op grad_op.minimize(cross_entropy global_step=global_step init_op tf.summary.scalar(cross_entropy cross_entropy merged tf.summary.merge_all init_op tf.global_variables_initializer saver tf.train.saver print(variables initialized sv tf.train.supervisor(is_chief flags.task_index global_step global_step init_op init_op with sv.prepare_or_wait_for_session(server.target config tf.configproto(gpu_options=gpu_options allow_soft_placement true log_device_placement true as sess perform training cycles start_time time.time if(flags.task_index train_writer tf.summary.filewriter(flags.log_dir sess.graph for epoch in range(training_epochs number of batches in one epoch sys.stderr.write(reporter progress:%.f\n%(float(epoch)/(training_epochs totalstep iterdata.batchcount for step in range(totalstep iterator_curr iterdata.nextbatch flag for iter in iterator_curr if flag train_x iter .reshape train_y onehot(iter ).reshape else train_x np.concatenate((train_x iter .reshape train_y np.concatenate((train_y onehot(iter ).reshape flag summary cost gstep sess.run train_op merged cross_entropy global_step feed_dict={x train_x y train_y elapsed_time time.time start_time start_time time.time if(flags.task_index train_writer.add_summary(summary gstep print(step d gstep epoch d epoch cost f cost time fms float(elapsed_time sys.stderr.write(reporter progress:%.f\n%(float(epoch+)/(training_epochs print(train completed if(flags.task_index train_writer.close print(saving model saver.save(sess flags.save_path+/model.ckpt print(done"
305453901,17734,https://api.github.com/repos/tensorflow/tensorflow/issues/17734,nagachika,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu google cloud ml engine maybe ubuntu tensorflow installed from source or binary pre installed on google cloud ml engine tensorflow version use command below python version bazel version if compiling from source) :na gcc/compiler version if compiling from source) :na cuda/cudnn version :na gpu model and memory :na exact command to reproduce describe the problemaccording to the documentation of google cloud ml engine the ml engine pass the option job-dir to the python process and the program should be able to handle job-dir option.the snippet shown in the next section can run successfully with tensorflow but failed with rc.as a result i cannot use runtime version on cloud ml engine with my package using tf.flags to handle option job-dir .the support of tensorflow on cloud ml engine training was released officially just few days before source code logshere is a snippet to reproduce the issue. import tensorflow as tftf.flags.define_string(job-dir default job dir)def main(argv print(tf.__version print(tf.flags.flags.job_dir)tf.app.run() here is the backtrace from python snippent.py job-dir=foo..traceback most recent call last file snippet.py line in module tf.app.run file users/chikanaga/.anyenv/envs/pyenv/versions/../lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(argv file hoge.py line in main print(tf.flags.flags.job_dir file users/chikanaga/.anyenv/envs/pyenv/versions/../lib/python./site-packages/tensorflow/python/platform/flags.py line in getattr return wrapped.__getattr__(name file users/chikanaga/.anyenv/envs/pyenv/versions/../lib/python./site-packages/absl/flags/_flagvalues.py line in getattr raise attributeerror(name)attributeerror job_dir
305270622,17717,https://api.github.com/repos/tensorflow/tensorflow/issues/17717,avigyan1009,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux redhat tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n.a gcc/compiler version if compiling from source n.a cuda/cudnn version n.a gpu model and memory n.a exact command to reproduce n.a describe the problemi am using standard library-provided k-means with estimator api for distributed tensorflow i have a cluster of three machine and i have updated the tf_config env variable on all three machine i am using hdfs to store the model directory so that all machine can access that it but when i execute the python file from the master machine the grpc server gets created and then ps server and worker server waiting for response from master message is repeated after every seconds or so if sample code for using this estimator-api based k-means would had been present it would have helped better source code logsimport tensorflow as tfimport numpy as npimport pandas as pdk n variables points np.random.uniform n variables )input_fn=lambda tf.train.limit_epochs(tf.convert_to_tensor(points dtype=tf.float num_epochs=)kmeans=tf.contrib.factorization.kmeansclustering(num_clusters=k use_mini_batch=false,model_dir=my_hdfs_path)train_spec tf.estimator.trainspec(input_fn=input_fn max_steps=)eval_spec tf.estimator.evalspec(input_fn=input_fn)tf.estimator.train_and_evaluate(kmeans train_spec eval_spec)list(kmeans.predict(input_fn=input_fn tf_config cluster chief master ps slave worker slave task type:chief index error message:createsession still waiting for response from worker job:ps/replica:/task:createsession still waiting for response from worker job:worker/replica:/task"
304832519,17684,https://api.github.com/repos/tensorflow/tensorflow/issues/17684,Neargye,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below r commit cbcaeffafedaa python version bazel version if compiling from source gcc/compiler version if compiling from source gcc version ubuntu ubuntu cuda/cudnn version n/a build without support cuda gpu model and memory n/a build without support cuda exact command to reproduce describe the problemtrained model successfully froze it works on the tensorflow android using tensorflowinferenceinterface.i try to convert this into a tf lite format but i get an error source code logs bazel-bin/tensorflow/contrib/lite/toco/toco input_file=./test_model/frozen_graph.pb input_format=tensorflow_graphdef output_file=./test_model/unet.tflite output_format=tflite input_array=input input_data_type=float input_shape inference_type=float inference_input_type=float output_array=final/sigmoid v i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before removing unused ops operators arrays quantized i tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc before general graph transformations operators arrays quantized f tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc check failed mean_shape.dims multiplier_shape.dims()aborted core dumped
304452589,17650,https://api.github.com/repos/tensorflow/tensorflow/issues/17650,skycoop,36,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gdeb python version python bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory gtx ti p exact command to reproduce n/a describe the problemwere using tf.estimator.train_and_evaluate for running our training but were running into an issue with getting it to run evaluation at the correct frequency because the training input pipeline is fully reset after evaluation were attempting to follow the docs recommendation of running evaluation after an epoch or two this is a problem for us because we can only figure out how to set the evaluation frequency using tf.estimator.evalspec.throttle_secs which runs evaluation every throttle_secs seconds since our evaluation takes less time than throttle_secs we run on a few different hardware platforms and configurations that all alter the training speed so the only way to ensure that we perform evaluation after finishing an epoch is to calculate a value for throttle_secs that incorporates that trainings training speed this is obviously suboptimal compared to setting the evaluation frequency in steps rather than seconds.here are the approaches to solving this problem that ive been able to find after a little poking around prevent evaluations triggered by throttle_secs passing from saving a new checkpoint and only run if there is a new checkpoint this lets the user specify tf.estimator.runconfig.save_checkpoints_steps to set the evaluation frequency this is actually how i thought throttle_secs worked based on my reading of the documentation allow the user to set throttle_steps as a part of the evalspec this value would could be used by the secondorsteptimer to run the evaluation based on how many steps have elapsed instead of seconds.id be willing to submit a pr with either fix but im not sure which one would be correct/best so id appreciate any feedback or alternate solutions smile
304339051,17642,https://api.github.com/repos/tensorflow/tensorflow/issues/17642,scottcjt,0,0,0,0,0,1,"this issue is caused by a breaking change in ndk clang r in thoserecent ndks clang assumes neon vectors are aligned to bit boundary,and thus generates bit address hints however on old androidversions heap-allocated memory are not always aligned to bit.after consulting ndk maintainers here the breaking change of compiler will not be fixed so tensorflow should fix it instead.p.s it also seems to be responsible for secondly it looks to me that existing code here allocates more than it actually needs the element count should not be multiplied by sizeof(floatx_t again please help verify this part"
304155179,17629,https://api.github.com/repos/tensorflow/tensorflow/issues/17629,cubetastic33,6,0,0,0,0,0,os platform and distribution:linux ubuntu tensorflow installed using piptensorflow version with gpu supportpython version cuda version gpu model and memory nvidia geforce mx gbcommand to reproduce python import tensorflow as tf(basically run any tensorflow program to reproduce)problem:whenever you run a tensorflow program you get a huge error log but the main problem is this: importerror libcublas.so cannot open shared object file no such file or directory so the reason this is happening is because tensorflow wants cuda but i have cuda this problem can be fixed by installing cuda but i have a few requests seeing that a couple of people have this problem see i think that tensorflow could be updated so that it works with cuda but i think this issue is only with ubuntu or the following could be done:update the tensorflow documentation saying that you specifically need cuda for tensorflow and cuda for tensorflow and so onand also include this in the errors list at if a pull request is required to update the documentation i am fine with doing that
304152012,17628,https://api.github.com/repos/tensorflow/tensorflow/issues/17628,hanspond,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary pip tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version none gpu model and memory :none exact command to reproduce describe the problemi was just trying to make a classifier ios app by simply replacing the inception pb file to my pb file for my classifier.the ios app example i was using is camera it worked well with the original inception.but when i changed the pb file i got an error as listed below the pb file was generated with tensorflow-for-poest then the model doesnt work the camera is working but no classification message.i assume there is some mismatch between versions of tf source code logs e users/apple/onedrive/p/tensorflow/tensorflow/examples/ios/camera/cameraexampleviewcontroller.mm running model failed:invalid argument nodedef mentions attr dilations not in op
303653832,17573,https://api.github.com/repos/tensorflow/tensorflow/issues/17573,deltheil,1,0,0,0,0,0,tensorflow/contrib/lite/build_ios_universal_lib.sh fails with error use of undeclared identifier assert
303577664,17566,https://api.github.com/repos/tensorflow/tensorflow/issues/17566,rongou,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary pip install tensorflow-gpu tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :cuda cudnn gpu model and memory exact command to reproduce :i was trying to build a horovod image but this would affect anyone using the nvidia/cuda:.-cudnn-devel-ubuntu base image: shelldocker build t horovod run it rm horovod python tensorflow_mnist.py describe the problemwhen building a docker image based on nvidia/cuda:.-cudnn-devel-ubuntu and doing a pip install tensorflow-gpu the resulting image causes a crash because the base image contains cudnn while the tensorflow-gpu pip package was built against cudnn source code logserror messages: shell e tensorflow/stream_executor/cuda/cuda_dnn.cc loaded runtime cudnn library compatibility version but source was compiled with compatibility version if using a binary install upgrade your cudnn library to match if building from sources make sure the library loaded at runtime matches a compatible version specified during compile configuration f tensorflow/core/kernels/conv_ops.cc check failed stream->parent()->getconvolvealgorithms conv_parameters.shouldincludewinogradnonfusedalgo
303276766,17519,https://api.github.com/repos/tensorflow/tensorflow/issues/17519,rishabhmalhotra,1,0,0,0,0,0,running popular forks fasterrcnn maskrcnn one finds they cannot be serialized as a graphdef because of the usage of py_funcs ref official tf documentation)
303205292,17514,https://api.github.com/repos/tensorflow/tensorflow/issues/17514,bhagyashripachkor,1,0,0,0,0,0,"i am trying to create exe file for python application built on tkinter python keras tensorflow cx_freeze all the package installs were done through pip.windows bitim using command python setup.py buildbelow is my setup.py file import cx_freeze import sys import matplotlib import os import pandas import tkinter.filedialog import keras import sklearn import numpy import tensorflow import openpyxl import datetime base none os.environ tcl_library rc:\users\bpachkor\appdata\local\programs\python\python\tcl\tcl os.environ tk_library rc:\users\bpachkor\appdata\local\programs\python\python\tcl\tk if sys.platform win base wingui from glob import glob data_files microsoft.vc.crt glob(rc:\program files x)\microsoft visual studio vc\redist\x\microsoft.vc.crt executables cx_freeze.executable(app-.py,base=base cx_freeze.setup name foreacast options build_exe packages: tkinter,matplotlib ,include_files: numpy options build_exe packages: cx_freeze,datetime,openpyxl,tkinter,numpy,matplotlib,pandas,tkinter.filedialog,keras,sklearn,tensorflow version description test data_files=data_files executables executables)below is input file app-.py#from tkinter import tk label button frameimport tkinter as tkfrom tkinter.filedialog import askopenfilenameimport osimport tkinter.messageboximport numpy as npimport xlrdimport datetimeimport matplotlib.pyplot as pltimport pandas as pdimport kerasfrom keras.models import sequentialfrom keras.layers import lstmfrom keras.layers import densefrom keras.layers import dropoutfrom keras import optimizersfrom sklearn.preprocessing import minmaxscalerfrom numpy.random import seedimport tensorflow as tffrom tensorflow import set_random_seed from openpyxl import load_workbook here we are creating our class window and inheriting from the frame class frame is a class from the tkinter module see lib/tkinter/__init__)class window(tk.frame define settings upon initialization here you can specify def init__(self master=none parameters that you want to send through the frame class tk.frame.__init__(self master,background=#bff reference to the master widget which is the tk window self.master master with that we want to then run init_window which doesnt yet exist self.init_window self.master.minsize(width height self.master.maxsize(width height self.master.configure(background=#cbd creation of init_window def close_window root.destroy def init_window(self changing the title of our master widget self.master.title(call volume forecasting tk.label(root text=call volume forecasting,font helvetica bold italic,bg ff,fg white,width=).grid(row=,column=,columnspan=,padx pady=,sticky=tk.w self.checkadvisor=tk.intvar self.checkshareholder=tk.intvar self.checkretirement=tk.intvar self.checkfast=tk.intvar self.checkvip=tk.intvar self.checkcgf=tk.intvar self.checkwt=tk.intvar self.checkiobrp=tk.intvar self.checkft=tk.intvar self.checknjbest=tk.intvar self.checkselectall=tk.intvar self.checktype tk.intvar self.input_file tk.button(root,text=choose input file,command=self.choose_input_file,font helvetica bold italic,bg ff,fg white,width=,height=).grid(row column columnspan padx=,pady=,sticky=tk.w self.input_file tk.text(root,width=,height self.input_file.grid(row column=,columnspan=,padx=,pady sticky=tk.w self.advisorstate tk.checkbutton(root text=advisor variable self.checkadvisor,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.advisorstate.grid(row column padx pady=,sticky=tk.w self.shareholderstate tk.checkbutton(root text=shareholder,variable self.checkshareholder font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.shareholderstate.grid(row column padx pady=,sticky=tk.w self.retirementstate tk.checkbutton(root text=retirement,variable self.checkretirement,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.retirementstate.grid(row column padx pady=,sticky=tk.w self.faststate tk.checkbutton(root text=fast,variable=self.checkfast,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.faststate.grid(row column padx pady=,sticky=tk.w self.vipstate tk.checkbutton(root text=vip,variable=self.checkvip,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.vipstate.grid(row column padx pady=,sticky=tk.w self.cgf tk.checkbutton(root text=cgf,variable=self.checkcgf,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.cgf.grid(row column padx pady=,sticky=tk.w self.wt tk.checkbutton(root text=wt,variable=self.checkwt,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.wt.grid(row column padx pady=,sticky=tk.w self.iobrp tk.checkbutton(root text=iobrp,variable=self.checkiobrp,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.iobrp.grid(row column padx pady=,sticky=tk.w self.ft tk.checkbutton(root text=ft variable=self.checkft,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.ft.grid(row column padx pady=,sticky=tk.w self.njbest tk.checkbutton(root text=njbest,variable=self.checknjbest,font helvetica bold italic,bg light grey,fg black,width=,anchor=tk.w self.njbest.grid(row column padx pady=,sticky=tk.w self.selectall tk.checkbutton(root text=select all skills,variable=self.checkselectall,command=self.cb_check,font helvetica bold italic,bg light grey,fg black self.selectall.grid(row column padx=,pady=,sticky=tk.w self.daily tk.radiobutton(root text=daily,variable=self.checktype,value=,font helvetica bold italic,width=,bg=light grey,anchor=tk.w self.daily.grid(row column pady=,sticky=tk.w self.monthly tk.radiobutton(root text=monthly,variable=self.checktype,value=,font helvetica bold italic,width=,bg=light grey,anchor=tk.w self.monthly.grid(row column pady=,sticky=tk.w self.both tk.radiobutton(root text=both,variable=self.checktype,value=,font helvetica bold italic,width=,bg=light grey,anchor=tk.w self.both.grid(row column pady=,sticky=tk.w tk.button(root text=build,font helvetica bold italic,bg ff,fg white,width=,height=,command=self.build_models).grid(row column padx=,pady=,sticky=tk.w tk.button(root text=forecast,font helvetica bold italic,bg ff,fg white,width=,height=,command=self.forecast_models).grid(row column padx=,pady=,sticky=tk.w tk.button(root text=clear,font helvetica bold italic,bg ff,fg white,width=,height=,command self.clear).grid(row column padx=,pady=,sticky=tk.w tk.button(root text=exit,font helvetica bold italic,bg ff,fg white,width=,height=,command self.close).grid(row column padx=,pady=,sticky=tk.w def cb_check(self if self.checkselectall.get self.advisorstate.config(state=tk.disabled self.checkadvisor.set self.shareholderstate.config(state=tk.disabled self.checkshareholder.set self.retirementstate.config(state=tk.disabled self.checkretirement.set self.faststate.config(state=tk.disabled self.checkfast.set self.vipstate.config(state=tk.disabled self.checkvip.set self.cgf.config(state=tk.disabled self.checkcgf.set self.wt.config(state=tk.disabled self.checkwt.set self.iobrp.config(state=tk.disabled self.checkiobrp.set self.ft.config(state=tk.disabled self.checkft.set self.njbest.config(state=tk.disabled self.checknjbest.set else self.advisorstate.config(state=tk.normal self.shareholderstate.config(state=tk.normal self.retirementstate.config(state=tk.normal self.faststate.config(state=tk.normal self.vipstate.config(state=tk.normal self.cgf.config(state=tk.normal self.wt.config(state=tk.normal self.iobrp.config(state=tk.normal self.ft.config(state=tk.normal self.njbest.config(state=tk.normal def close(self root.destroy def clear(self print(in claer self.checkadvisor.set self.checkshareholder.set self.checkretirement.set self.checkfast.set self.checkvip.set self.checkcgf.set self.checkwt.set self.checkiobrp.set self.checkft.set self.checknjbest.set self.checkselectall.set self.checktype.set self.input_file.delete tk.end self.advisorstate.config(state=tk.normal self.shareholderstate.config(state=tk.normal self.retirementstate.config(state=tk.normal self.faststate.config(state=tk.normal self.vipstate.config(state=tk.normal self.cgf.config(state=tk.normal self.wt.config(state=tk.normal self.iobrp.config(state=tk.normal self.ft.config(state=tk.normal self.njbest.config(state=tk.normal self.input_file.config(state=tk.normal def choose_input_file(self self.filename askopenfilename if os.path.isfile(self.filename self.input_file.configure(state=tk.normal self.input_file.insert(tk.insert,self.filename self.input_file.configure(state=tk.disabled fname self.input_file.get end-c workbook xlrd.open_workbook(fname,w print(self.input_file else print(no file chosen tkinter.messagebox.showinfo(title message=please select input file def validation(self if len(self.input_file.get end-c tkinter.messagebox.showinfo(title message=please select input file else print(file selected self.validate_chbox def validate_chbox(self if self.checkadvisor.get and self.checkshareholder.get and self.checkretirement.get and self.checkfast.get and self.checkvip.get and self.checkcgf.get and self.checkwt.get and self.checkiobrp.get and self.checkft.get and self.checknjbest.get and self.checkselectall.get tkinter.messagebox.showinfo(title message=please select the skill else self.validate_radiobtn def validate_radiobtn(self if self.checktype.get tkinter.messagebox.showinfo(title message=please select the forecast type def build_models(self self.validation self.build def build(self if self.checkselectall.get self.buildallmodels else self.build_advisor_selected def buildallmodels(self print(in buildallmodels def forecast_models(self self.validation self.forecast def forecast(self if self.checkselectall.get self.forecastallmodels else self.forecast_advisor_selected def build_advisor_selected(self if self.checkadvisor.get if self.checktype.get print(daily self.build_advisor_daily elif self.checktype.get print(monthly self.advisor_monthly else print(both self.advisor_both def forecast_advisor_selected(self if self.checkadvisor.get if self.checktype.get print(daily self.forecast_advisor_daily elif self.checktype.get print(monthly self.forecast_advisor_monthly else print(both self.forecast_advisor_both def build_advisor_daily(self required_data self.read_advisor_daily print(required_data global advisor_build if global advisor_build self.build_advisor_daily_model(required_data else print(advisor model already built.please forecast def read_advisor_daily(self fname self.input_file.get end-c workbook xlrd.open_workbook(fname,w sheets workbook.sheet_names date_col required_data sh workbook.sheet_by_name(daily_data for rownum in range(,sh.nrows row_valaues sh.row_values(rownum date_col.append(datetime.datetime(*xlrd.xldate_as_tuple(row_valaues ,workbook.datemode required_data.append(row_valaues required_data pd.dataframe(required_data required_data.replace np.nan inplace=true required_data.dropna(inplace=true return required_data def build_advisor_daily_model(self,required_data print(in build_advisor_daily_model self.destroy print(required_data np.random.seed tf.set_random_seed tkinter.messagebox.showinfo(title message=advisor model building started print(type(required_data data_set required_data tkinter.messagebox.showinfo(title message=data set read model_data_set data_set.iloc model_data_set model_data_set model_data_set print(len(model_data_set a=len(model_data_set print(type(model_data_set tkinter.messagebox.showinfo(title message=a modified_data model_data_set modified_data modified_data.values.reshape print(modified_data.shape print(modified_data global sc sc minmaxscaler(feature_range train sc.fit_transform(modified_data print(train print(train.shape print(train.shape print(train.shape global y_train x_train y_train l len(modified_data print(l for i in range(,l x_train.append(train i-:i y_train.append(train i print(len(x_train print(len(y_train x_train np.array(x_train y_train np.array(y_train print(x_train.shape print(x_train print(y_train.shape x_train x_train.reshape(x_train.shape print(x_train.shape tkinter.messagebox.showinfo(title message=train data built total columns lag global regressor regressor sequential regressor.add(lstm(units=,return_sequences=true,input_shape=(x_train.shape regressor.add(dropout regressor.add(lstm(units=,return_sequences=true regressor.add(dropout regressor.add(lstm(units=,return_sequences=true regressor.add(dropout regressor.add(lstm(units=,return_sequences=true regressor.add(dropout regressor.add(lstm(units=,return_sequences=true regressor.add(dropout regressor.add(lstm(units regressor.add(dropout regressor.add(dense(units rmsprop keras.optimizers.rmsprop(lr rho epsilon decay tkinter.messagebox.showinfo(title message=compiling model regressor.compile(optimizer=adam,loss=mean_squared_error,metrics= accuracy tkinter.messagebox.showinfo(title message=model compilation complete regressor.fit(x_train,y_train,epochs=,batch_size=,shuffle=false global advisor_build advisor_build print(y_train tkinter.messagebox.showinfo(title message=advisor model building completed return self.regressor,advisor_build,y_train def forecast_advisor_daily(self print(in forecast global advisor_build print(advisor_build global y_train global sc global regressor advisor_build if advisor_build y_test y_train print(y_test print(len(y_test x_test prediciton_list y_test y_test.reshape print(y_test.shape y_test sc.transform(y_test x_test_.append(y_test print(x_test x_test np.array(x_test print(x_test print(x_test_.shape for x in range y_pred regressor.predict(x_test a sc.inverse_transform(y_pred b a x_test np.delete(x_test_,,axis a y_pred x_test np.concatenate((x_test_,np.zeros((,,))),axis x_test np.insert(x_test_,,a,axis x_test np.delete(x_test_,,axis prediciton_list.append(b print(prediciton_list fname self.input_file.get end-c workbook xlrd.open_workbook(fname,w date_col required_data sh workbook.sheet_by_name(daily_data for rownum in range(,sh.nrows row_valaues sh.row_values(rownum date_col.append(datetime.datetime(*xlrd.xldate_as_tuple(row_valaues ,workbook.datemode required_data.append(row_valaues required_data pd.dataframe(required_data print(required_data len len(required_data required_data.replace np.nan inplace=true z required_data.last_valid_index print(z print(type(z ind z ind ind y required_data.loc required_data.last_valid_index print(y required_data.dropna(inplace=true print(required_data date_col pd.dataframe(date_col print(date_col end_date date_col.iloc z z z y z print(z start_date date_col.iloc z print(start_date.date df pd.dataframe(prediciton_list print(df df for i in range(ind,ind df.append(date_col.iloc i df pd.dataframe(df,columns date print(df k df.loc df date .isin print(k print(type(k print(len(k index_list if len(k for z in range(len(k index_list.append(k.index z print(index_list for l in range(len(index_list m index_list l df.xs(m else print(no holidays print(df book load_workbook(fname writer pd.excelwriter(fname engine=openpyxl writer.book book writer.sheets dict((ws.title ws for ws in book.worksheets print(y df.to_excel(writer daily_output startcol=,startrow=y,header=none,index=false writer.save tkinter.messagebox.showinfo(title message=advisor forecasting completed else tkinter.messagebox.showinfo(title message=please build first and then forecast def destroy(self global advisor_build global regressor regressor none global y_train y_train none global sc sc none advisor_build def mean_absolute_percentage_error(y_true y_pred perc if x else np.abs((x-y)/x for x,y in zip(y_true,y_pred return np.mean(perc root tk.tk()#creation of an instance#holiday_list global advisor_buildglobal regressorglobal y_trainglobal scapp window(root)#mainloop root.mainloop below is the error c:\users\bpachkor\appdata\local\programs\python\python>python setup.py buildusing tensorflow backend.running buildrunning build_exetraceback most recent call last file setup.py line in module executables executables file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\dist.py line in setup distutils.core.setup( attrs file c:\users\bpachkor\appdata\local\programs\python\python\lib\distutils\core.py line in setup dist.run_commands file c:\users\bpachkor\appdata\local\programs\python\python\lib\distutils\dist.py line in run_commands self.run_command(cmd file c:\users\bpachkor\appdata\local\programs\python\python\lib\distutils\dist.py line in run_command cmd_obj.run file c:\users\bpachkor\appdata\local\programs\python\python\lib\distutils\command\build.py line in run self.run_command(cmd_name file c:\users\bpachkor\appdata\local\programs\python\python\lib\distutils\cmd.py line in run_command self.distribution.run_command(command file c:\users\bpachkor\appdata\local\programs\python\python\lib\distutils\dist.py line in run_command cmd_obj.run file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\dist.py line in run freezer.freeze file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\freezer.py line in freeze self.finder self._getmodulefinder file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\freezer.py line in getmodulefinder finder.includepackage(name file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\finder.py line in includepackage self._importallsubmodules(module deferredimports file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\finder.py line in importallsubmodules recursive file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\finder.py line in importallsubmodules recursive file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\finder.py line in importallsubmodules recursive file c:\users\bpachkor\appdata\local\programs\python\python\lib\site-packages\cx_freeze\finder.py line in importallsubmodules raise importerror(no module named r submodulename)importerror no module named tensorflow.contrib.ios_examples.benchmark.benchmark.xcodeproji have tried doing same steps for python but with no luck"
303088216,17508,https://api.github.com/repos/tensorflow/tensorflow/issues/17508,meteorcloudy,0,0,0,4,0,0,error var/lib/buildkite-agent/.cache/bazel/_bazel_buildkite-agent/febdfdafaa/external/jpeg/build illegal ambiguous match on configurable attribute deps in jpeg//:jpeg jpeg//:k jpeg//:armeabi-va tf build is broken with bazel@head see reason is we recently change default value of android_cpu to armeabi-va results two config_setting enabled at the same time in jpeg.build.we should better use cpu instead of android_cpu to select for architecture.@jart gregestrenfyi buchgr
302994318,17501,https://api.github.com/repos/tensorflow/tensorflow/issues/17501,ChiFang,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary by pip install upgrade tensorflow-gpu tensorflow version use command below) :tensorflow-gpu python version python bazel version if compiling from source gcc/compiler version if compiling from source) :gcc ubuntu ubuntu cuda/cudnn version cuda cudnn gpu model and memory null exact command to reproduce describe the problem i save my pre-trained model into pb file according to freeze_graph.py and optimize_for_inference.py i covert to tflite by command-line with following cmd tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco input_file=rd_net_transconv_fuse.pb output_file=./rd_net_transconv_fuse.tflite input_format=tensorflow_graphdef output_format=tflite inference_type=float input_data_types float input_arrays=placeholder output_arrays=seq_/conv/biasadd input_shapes logtostderrthe error show that transposeconv is not supported f tensorflow/contrib/lite/toco/tflite/export.cc some of the operators in the model are not supported by the standard tensorflow lite runtime if you have a custom implementation for them you can disable this error with allow_custom_ops here is a list of operators for which you will need custom implementations transposeconv.if i use python_api to convert my tensor into tflite it work fine without any error msg.but the tflite can not be used in android it will crash when i want to getoutputname or runerror msg in android cant get output names model error:invalid handle to interpreter.does tensorflow lite support transposeconv i dont see any information in compatibility guide suggestion thanks
302902460,17494,https://api.github.com/repos/tensorflow/tensorflow/issues/17494,mas-dse-greina,4,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux centos tensorflow installed from source or binary binary intel mkl wheel tensorflow version use command below bunknown python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce :u-net model graph compiles but error during training from concatenation op.the code is here can collect some of this information using our environment capture script cat etc/issue linux lancelot el.x smp thu jan utc x x x gnu/linuxversion core)version_id=centos_mantisbt_project_version=redhat_support_product_version are we in docker no compiler c gcc red hat copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux lancelot el.x smp thu jan utc x x x gnu/linux check pips numpy protobuf tensorflow check for virtualenv false tensorflow import tf.version tf.git_version v..--gdebtf.compiler_version v..--gdebsanity check array dtype=int)/home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters env ld_library_path is unsetdyld_library_path is unset nvidia-smi p.sh line nvidia-smi command not found cuda libs can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.this is a bug in the tensorflow i am using the intel-supplied pip wheel for mkl optimization i ran the same code with the tf mkl wheel and my code worked just fine the error messages indicate that there is a problem with the concatenation operation i am not seeing this error with the generic tensorflow i.e pip install tensorflow i believe it is limited to the mkl op for concatenation source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.warning:tensorflow:variable will be deprecated use variable.assign_mul if you want assignment to the variable value or x x y if you want a new python tensor object.train on samples validate on samplesepoch traceback most recent call last file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn target_list status run_metadata file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in exit c_api.tf_getcode(self.status.status))tensorflow.python.framework.errors_impl.abortederror operation received an exception:status message could not create a concat primitive descriptor in file tensorflow/core/kernels/mkl_concat_op.cc node concatenate/concat mklconcatv n t=dt_float tidx=dt_int kernel=mklop device=/job:localhost/replica:/task:/device:cpu: (transconv/biasadd convb/relu concatenate/concat/axis dmt convb/relu dmt/_) during handling of the above exception another exception occurred:traceback most recent call last file train.py line in module settings.out_channel_no settings.model_fn settings.mode args file train.py line in train_and_predict callbacks= model_checkpoint tensorboard_checkpoint file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/training.py line in fit validation_steps=validation_steps file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/training.py line in fit_loop outs f(ins_batch file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/backend.py line in call fetches=fetches feed_dict=feed_dict self.session_kwargs file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in do_run options run_metadata file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.abortederror operation received an exception:status message could not create a concat primitive descriptor in file tensorflow/core/kernels/mkl_concat_op.cc node concatenate/concat mklconcatv n t=dt_float tidx=dt_int kernel=mklop device=/job:localhost/replica:/task:/device:cpu: (transconv/biasadd convb/relu concatenate/concat/axis dmt convb/relu dmt/_) caused by op concatenate/concat defined at file train.py line in module settings.out_channel_no settings.model_fn settings.mode args file train.py line in train_and_predict model model_multilayer(args false false img_rows img_cols input_no output_no print_summary=true file train.py line in model_multilayer kernel_size strides padding=same)(conv conv axis=concat_axis file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/layers/merge.py line in concatenate return concatenate(axis=axis kwargs)(inputs file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in call output super(layer self).__call__(inputs kwargs file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/layers/base.py line in call outputs self.call(inputs args kwargs file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/layers/merge.py line in call return self._merge_function(inputs file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/layers/merge.py line in merge_function return k.concatenate(inputs axis=self.axis file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/keras/_impl/keras/backend.py line in concatenate return array_ops.concat( to_dense(x for x in tensors axis file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/array_ops.py line in concat return gen_array_ops._concat_v(values=values axis=axis name=name file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in concat_v concatv values=values axis=axis name=name file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file home/bduser/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessabortederror see above for traceback operation received an exception:status message could not create a concat primitive descriptor in file tensorflow/core/kernels/mkl_concat_op.cc node concatenate/concat mklconcatv n t=dt_float tidx=dt_int kernel=mklop device=/job:localhost/replica:/task:/device:cpu: (transconv/biasadd convb/relu concatenate/concat/axis dmt convb/relu dmt
302749773,17485,https://api.github.com/repos/tensorflow/tensorflow/issues/17485,hrschubert,9,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes a minimal example reproducing the bug is provided below os platform and distribution e.g linux ubuntu windows x tensorflow installed from source or binary binary installed using pip install tensorflow-gpu tensorflow version use command below bunknown also tested on bunknown python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version cuda cudnn cuda cudnn gpu model and memory :(device name geforce gtx m pci bus id compute capability memory gib)(device name quadro m pci bus id compute capability memory gib exact command to reproduce python example.py see below describe the problemthe function tf.contrib.image.transform crashes when cuda is enabled under windows it produces the following errors cuda_error_illegal_instruction on tensorflow or cuda_error_launch_failed on tensorflow however it functions correctly when cuda is disabled by setting cuda_visible_devices to i tested a variation of different parameters such as varying the batch size image sizes and number of channels but the behavior stays the same in addition i reproduced the same error on a different machine with an older tensorflow version source code logs code : import numpy as npimport tensorflow as tfbatch_size image_size channels data np.zeros shape=(batch_size image_size image_size channels dtype=np.float)data_node tf.placeholder shape=(batch_size image_size image_size channels dtype=tf.float)identity tf.constant dtype=tf.float)transform tf.tile(tf.expand_dims(identity batch_size data_node_transformed tf.contrib.image.transform(data_node transform)data_t tf.session().run( data_node_transformed feed_dict={data_node data console output output with tensorflow with geforce gtx m i c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties:name geforce gtx m major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc adding visible gpu devices i c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name geforce gtx m pci bus id compute capability e c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\stream_executor\cuda\cuda_event.cc error polling for event status failed to query event cuda_error_illegal_instruction e c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\stream_executor\cuda\cuda_driver.cc could not synchronize on cuda context cuda_error_illegal_instruction f c:\tf_jenkins\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc unexpected event status output with tensorflow with quadro m i c:\tf_jenkins\home\workspace\relwin\m\windowsgpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties name quadro m major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i c:\tf_jenkins\home\workspace\relwin\m\windowsgpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device device:gpu device name quadro m pci bus id compute capability e c:\tf_jenkins\home\workspace\relwin\m\windowsgpu\py\\tensorflow\stream_executor\cuda\cuda_driver.cc could not synchronize on cuda context cuda_error_launch_failed no stack trace available e c:\tf_jenkins\home\workspace\relwin\m\windowsgpu\py\\tensorflow\stream_executor\cuda\cuda_event.cc error polling for event status failed to query event cuda_error_launch_failed f c:\tf_jenkins\home\workspace\relwin\m\windowsgpu\py\\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc unexpected event status
302290508,17438,https://api.github.com/repos/tensorflow/tensorflow/issues/17438,PhilJd,8,0,0,0,0,0,this pull request implements decoupled weight decay as described in fixing weight decay regularization by loshchilov hutter paper shows that for adaptive gradient algorithms the implemented method regularizes variables with large gradients more than l regularization would and that this yields better training loss and generalization error.for sgd variants this simplifies hyperparameter search since it decouples the settings of weight decay and learning rate which is nicely visualized in fig in the paper adamw implementation explicitly adds the optimizers described in the paper adamw and momentumw to tf.contrib.opt and provides a factory function extend_with_decoupled_weight_decay that can be used to create a new optimizer class with decoupled weight decay.closes
302174337,17424,https://api.github.com/repos/tensorflow/tensorflow/issues/17424,rjt10,2,0,0,0,0,0,i am having a hard time finding documentation on how to use a tensorflow model trained using the estimator dataset apis on an android app.having spent hours searching for it and turned out nothing and maybe theres a many others having the same issue and the answer could be just a two-minutes thing for people who knows i am raising it as an issue here can someone point me to a good reference and/or tutorial i looked at the tensorflowinferenceinterface but my understanding is it need you to specify which operator you want to feed the input to but the estimator/dataset abstraction is at another level so i am somewhat lost here.thanks jerrythe version of tensorflow i am using is v..--gdeb
302173340,17423,https://api.github.com/repos/tensorflow/tensorflow/issues/17423,tushuhei,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary binary tensorflow version use command below v..--gaad python version exact command to reproduce python tensorflow/examples/image_retraining/retrain.py image_dir resources/tf-retrain-images learning_rate testing_percentage validation_percentage train_batch_size validation_batch_size flip_left_right true random_scale random_brightness eval_step_interval how_many_training_steps architecture mobilenet describe the problem retrain.py fails with the error below when it starts to create bottleneck files for testing datasets after training is done looks like something wrong with making bottleneck files for test images.fyi it works when i checkout retrain.py back to commit dceac source code logs...info:tensorflow step validation accuracy n=)info:tensorflow step train accuracy info:tensorflow step cross entropy info:tensorflow step validation accuracy n=)model path tmp/imagenet/mobilenet_v_.__frozen.pbinfo:tensorflow:restoring parameters from tmp/_retrain_checkpointinfo:tensorflow:creating bottleneck at tmp/bottleneck/cat/cat..jpg_mobilenet_._.txttraceback most recent call last file users/tushuhei/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in run allow_operation=false file users/tushuhei/pyenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in as_graph_element return self._as_graph_element_locked(obj allow_tensor allow_operation file users/tushuhei/pyenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in as_graph_element_locked raise valueerror(tensor s is not an element of this graph obj)valueerror tensor tensor(decodejpginput dtype=string is not an element of this graph.during handling of the above exception another exception occurred:traceback most recent call last file tensorflow/examples/image_retraining/retrain.py line in create_bottleneck_file resized_input_tensor bottleneck_tensor file tensorflow/examples/image_retraining/retrain.py line in run_bottleneck_on_image image_data_tensor image_data file users/tushuhei/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file users/tushuhei/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in run e.args )typeerror cannot interpret feed_dict key as tensor tensor tensor(decodejpginput dtype=string is not an element of this graph.during handling of the above exception another exception occurred:traceback most recent call last file tensorflow/examples/image_retraining/retrain.py line in module tf.app.run(main=main argv= sys.argv unparsed file users/tushuhei/pyenv/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(argv file tensorflow/examples/image_retraining/retrain.py line in main bottleneck_tensor file tensorflow/examples/image_retraining/retrain.py line in run_final_eval bottleneck_tensor flags.architecture file tensorflow/examples/image_retraining/retrain.py line in get_random_cached_bottlenecks resized_input_tensor bottleneck_tensor architecture file tensorflow/examples/image_retraining/retrain.py line in get_or_create_bottleneck bottleneck_tensor file tensorflow/examples/image_retraining/retrain.py line in create_bottleneck_file str(e)))runtimeerror error during processing file users/tushuhei/resources/tf-retrain-images/cat/cat..jpg cannot interpret feed_dict key as tensor tensor tensor(decodejpginput dtype=string is not an element of this graph
302143823,17420,https://api.github.com/repos/tensorflow/tensorflow/issues/17420,fmannan,2,0,0,0,0,0,fixes cc drpngx gpapan
302075513,17411,https://api.github.com/repos/tensorflow/tensorflow/issues/17411,konnerthg,30,0,0,2,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below cp-cpmu-manylinux_x can only guess since python c import tensorflow as tf print(tf.git_version tf.version gives me an error already python version python exact command to reproduce import tensorflow i created a fresh virtual environment virtualenv p python test_venv/ and installed tensorflow pip install upgrade no-cache-dir tensorflow import tensorflow gives me illegal instruction core dumped) please help me understand whats going on and how i can fix it thank you.cpu information: -cpu description cpu product intel(r core(tm i cpu m ghz bus info cpu version cpu version capabilities x fpu fpu_exception wp vme de pse tsc msr pae mce cx apic sep mtrr pge mca cmov pat pse clflush dts acpi mmx fxsr sse sse ss ht tm pbe syscall nx rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes monitor ds_cpl vmx est tm ssse cx xtpr pdcm sse sse popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat cpufreq *edit*stacktrace obtained with gdb xfffe in std::pair::unaryvariantdecoderegistration(std::string const from media/gerry/hdd_/ws_hdd/test_venv/local/lib/python./site-packages/tensorflow/python/../libtensorflow_framework.so xfffeea in global__sub_i_tensor.cc from media/gerry/hdd_/ws_hdd/test_venv/local/lib/python./site-packages/tensorflow/python/../libtensorflow_framework.so xffffdeba in call_init l=
301942171,17391,https://api.github.com/repos/tensorflow/tensorflow/issues/17391,samikama,2,0,0,0,0,0,this pr improves the tf-tensorrt integration and provides fp and int tensorrt engine support calibration mechanism for int engines expanded op support user configurable graph partition size threshold
301624864,17369,https://api.github.com/repos/tensorflow/tensorflow/issues/17369,feranick,8,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary both tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :on macos type: sudo pip install tensorflow pip will start installing grpcio but an error during the installation of grpcio see below will prevent the complete installation of tensorflow.(this does not happen on tf which doesnt seem to use grpcio source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem. macbook-pro:grpc feranick sudo pip install upgrade tensorflowthe directory users/feranick/library/caches/pip/http or its parent directory is not owned by the current user and the cache has been disabled please check the permissions and owner of that directory if executing pip with sudo you may want sudos h flag.the directory users/feranick/library/caches/pip or its parent directory is not owned by the current user and caching wheels has been disabled check the permissions and owner of that directory if executing pip with sudo you may want sudos h flag.collecting tensorflow downloading tensorflow-..-cp-cpm-macosx___x_.whl mb mb kb/s requirement already up-to-date absl-py in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date wheel in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date termcolor in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date astor in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date protobuf in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date six in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date tensorboard in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date gast in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)requirement already up-to-date numpy in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow)collecting grpcio from tensorflow downloading grpcio-...tar.gz mb mb kb/s requirement already up-to-date setuptools in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from protobuf>=..->tensorflow)requirement already up-to-date htmllib in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorboard<..,>=..->tensorflow)requirement already up-to-date bleach in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorboard<..,>=..->tensorflow)requirement already up-to-date werkzeug in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorboard<..,>=..->tensorflow)requirement already up-to-date markdown in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorboard<..,>=..->tensorflow)installing collected packages grpcio tensorflow running setup.py install for grpcio error complete output from command opt/local/library/frameworks/python.framework/versions/./bin/python u c import setuptools tokenize;__file__=/private/tmp/pip-build-dadh/grpcio/setup.py;f=getattr(tokenize open open)(__file__);code=f.read().replace(\r\n n);f.close();exec(compile(code file exec install record tmp/pip-mopqn_-record/install-record.txt single-version-externally-managed compile found cython-generated files running install running build running build_py running build_project_metadata creating python_build creating python_build/lib.macosx-.-x creating python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_channel.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_common.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/__init__.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_utilities.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_plugin_wrapping.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_interceptor.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_grpcio_metadata.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_server.py python_build/lib.macosx-.-x_-./grpc copying src/python/grpcio/grpc/_auth.py python_build/lib.macosx-.-x_-./grpc creating python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/_server_adaptations.py python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/interfaces.py python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/_metadata.py python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/__init__.py python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/utilities.py python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/implementations.py python_build/lib.macosx-.-x_-./grpc/beta copying src/python/grpcio/grpc/beta/_client_adaptations.py python_build/lib.macosx-.-x_-./grpc/beta creating python_build/lib.macosx-.-x_-./grpc/framework copying src/python/grpcio/grpc/framework/__init__.py python_build/lib.macosx-.-x_-./grpc/framework creating python_build/lib.macosx-.-x_-./grpc/_cython copying src/python/grpcio/grpc/_cython/__init__.py python_build/lib.macosx-.-x_-./grpc/_cython creating python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/callable_util.py python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/abandonment.py python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/__init__.py python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/stream.py python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/stream_util.py python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/future.py python_build/lib.macosx-.-x_-./grpc/framework/foundation copying src/python/grpcio/grpc/framework/foundation/logging_pool.py python_build/lib.macosx-.-x_-./grpc/framework/foundation creating python_build/lib.macosx-.-x_-./grpc/framework/common copying src/python/grpcio/grpc/framework/common/style.py python_build/lib.macosx-.-x_-./grpc/framework/common copying src/python/grpcio/grpc/framework/common/cardinality.py python_build/lib.macosx-.-x_-./grpc/framework/common copying src/python/grpcio/grpc/framework/common/__init__.py python_build/lib.macosx-.-x_-./grpc/framework/common creating python_build/lib.macosx-.-x_-./grpc/framework/interfaces copying src/python/grpcio/grpc/framework/interfaces/__init__.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces creating python_build/lib.macosx-.-x_-./grpc/framework/interfaces/face copying src/python/grpcio/grpc/framework/interfaces/face/__init__.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces/face copying src/python/grpcio/grpc/framework/interfaces/face/utilities.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces/face copying src/python/grpcio/grpc/framework/interfaces/face/face.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces/face creating python_build/lib.macosx-.-x_-./grpc/framework/interfaces/base copying src/python/grpcio/grpc/framework/interfaces/base/__init__.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces/base copying src/python/grpcio/grpc/framework/interfaces/base/utilities.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces/base copying src/python/grpcio/grpc/framework/interfaces/base/base.py python_build/lib.macosx-.-x_-./grpc/framework/interfaces/base creating python_build/lib.macosx-.-x_-./grpc/_cython/_cygrpc copying src/python/grpcio/grpc/_cython/_cygrpc/__init__.py python_build/lib.macosx-.-x_-./grpc/_cython/_cygrpc creating python_build/lib.macosx-.-x_-./grpc/_cython/_credentials copying src/python/grpcio/grpc/_cython/_credentials/roots.pem python_build/lib.macosx-.-x_-./grpc/_cython/_credentials running build_ext b c compiling third_party/cares/cares/ares__close_sockets.c\n c compiling third_party/cares/cares/ares__get_hostent.c\n c compiling third_party/cares/cares/ares__read_line.c\n c compiling third_party/cares/cares/ares__timeval.c\n c compiling third_party/cares/cares/ares_cancel.c\n c compiling third_party/cares/cares/ares_create_query.c\n c compiling third_party/cares/cares/ares_data.c\n c compiling third_party/cares/cares/ares_destroy.c\n c compiling third_party/cares/cares/ares_expand_name.c\n c compiling third_party/cares/cares/ares_expand_string.c\n c compiling third_party/cares/cares/ares_fds.c\n c compiling third_party/cares/cares/ares_free_hostent.c\n c compiling third_party/cares/cares/ares_free_string.c\n c compiling third_party/cares/cares/ares_getenv.c\n c compiling third_party/cares/cares/ares_gethostbyaddr.c\n c compiling third_party/cares/cares/ares_gethostbyname.c\n c compiling third_party/cares/cares/ares_getnameinfo.c\n c compiling third_party/cares/cares/ares_getopt.c\n c compiling third_party/cares/cares/ares_getsock.c\n c compiling third_party/cares/cares/ares_init.c\n c compiling third_party/cares/cares/ares_library_init.c\n c compiling third_party/cares/cares/ares_llist.c\n c compiling third_party/cares/cares/ares_mkquery.c\n c compiling third_party/cares/cares/ares_nowarn.c\n c compiling third_party/cares/cares/ares_options.c\n c compiling third_party/cares/cares/ares_parse_a_reply.c\n c compiling third_party/cares/cares/ares_parse_aaaa_reply.c\n c compiling third_party/cares/cares/ares_parse_mx_reply.c\n c compiling third_party/cares/cares/ares_parse_naptr_reply.c\n c compiling third_party/cares/cares/ares_parse_ns_reply.c\n c compiling third_party/cares/cares/ares_parse_ptr_reply.c\n c compiling third_party/cares/cares/ares_parse_soa_reply.c\n c compiling third_party/cares/cares/ares_parse_srv_reply.c\n c compiling third_party/cares/cares/ares_parse_txt_reply.c\n c compiling third_party/cares/cares/ares_platform.c\n c compiling third_party/cares/cares/ares_process.c\n c compiling third_party/cares/cares/ares_query.c\n c compiling third_party/cares/cares/ares_search.c\n c compiling third_party/cares/cares/ares_send.c\n c compiling third_party/cares/cares/ares_strcasecmp.c\n c compiling third_party/cares/cares/ares_strdup.c\n c compiling third_party/cares/cares/ares_strerror.c\n c compiling third_party/cares/cares/ares_timeout.c\n c compiling third_party/cares/cares/ares_version.c\n c compiling third_party/cares/cares/ares_writev.c\n c compiling third_party/cares/cares/bitncmp.c\n c compiling third_party/cares/cares/inet_net_pton.c\n c compiling third_party/cares/cares/inet_ntop.c\n c compiling third_party/cares/cares/windows_port.c\n ar creating private/tmp/pip-build-dadh/grpcio/libs/opt/libares.a\n c compiling src/boringssl/err_data.c\n bin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/e_os.h error openssl_export macro redefined werror,-wmacro-redefined \n define openssl_export extern\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\n#define openssl_export\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_integer is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_integer;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_integer;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_enumerated is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_enumerated;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_enumerated;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_bit_string is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_bit_string;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_bit_string;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_octet_string is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_octet_string;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_octet_string;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_printablestring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_printablestring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_printablestring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_tstring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_tstring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_tstring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_iastring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_iastring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_iastring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_generalstring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_generalstring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_generalstring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_universalstring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_universalstring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_universalstring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_bmpstring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_bmpstring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_bmpstring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_utctime is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_utctime;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_utctime;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_time is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_time;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_time;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_generalizedtime is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_generalizedtime;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_generalizedtime;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_visiblestring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_visiblestring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_visiblestring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_utfstring is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_utfstring;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_utfstring;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_string is a c feature werror,-wtypedef-redefinition \ntypedef struct asn_string_st asn_string;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef struct asn_string_st asn_string;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_boolean is a c feature werror,-wtypedef-redefinition \ntypedef int asn_boolean;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef int asn_boolean;\n nin file included from src/boringssl/err_data.c::\nin file included from opt/local/include/openssl/err.h::\n/opt/local/include/openssl/ossl_typ.h error redefinition of typedef asn_null is a c feature werror,-wtypedef-redefinition \ntypedef int asn_null;\n nthird_party/boringssl/include/openssl/base.h note previous definition is here\ntypedef int asn_null;\n nfatal error too many errors emitted stopping now ferror-limit= \n errors generated.\nmake private/tmp/pip-build-dadh/grpcio/objs/opt/src/boringssl/err_data.o error n traceback most recent call last file string line in module file private/tmp/pip-build-dadh/grpcio/setup.py line in module cmdclass=command_class file opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/setuptools/__init__.py line in setup return distutils.core.setup( attrs file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/core.py line in setup dist.run_commands file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/dist.py line in run_commands self.run_command(cmd file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/dist.py line in run_command cmd_obj.run file opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/setuptools/command/install.py line in run return orig.install.run(self file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/command/install.py line in run self.run_command(build file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/cmd.py line in run_command self.distribution.run_command(command file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/dist.py line in run_command cmd_obj.run file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/command/build.py line in run self.run_command(cmd_name file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/cmd.py line in run_command self.distribution.run_command(command file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/dist.py line in run_command cmd_obj.run file opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/setuptools/command/build_ext.py line in run build_ext.run(self file opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/cython/distutils/old_build_ext.py line in run build_ext.build_ext.run(self file opt/local/library/frameworks/python.framework/versions/./lib/python./distutils/command/build_ext.py line in run self.build_extensions file private/tmp/pip-build-dadh/grpcio/src/python/grpcio/commands.py line in build_extensions raise exception(make command failed exception make command failed command opt/local/library/frameworks/python.framework/versions/./bin/python u c import setuptools tokenize;__file__=/private/tmp/pip-build-dadh/grpcio/setup.py;f=getattr(tokenize open open)(__file__);code=f.read().replace(\r\n n);f.close();exec(compile(code file exec install record tmp/pip-mopqn_-record/install-record.txt single-version-externally-managed compile failed with error code in private/tmp/pip-build-dadh/grpcio"
301533277,17367,https://api.github.com/repos/tensorflow/tensorflow/issues/17367,benbarsdell,1,0,0,0,0,0,speeds up cudnn rnns with fp input/output when possible on supported gpus computations will fall back to pseudo-fp if tensor op math is not supported enabled by default but can be disabled by setting the environment variable tf_disable_cudnn_rnn_tensor_op_math
301382585,17353,https://api.github.com/repos/tensorflow/tensorflow/issues/17353,valiok98,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory tesla k major minor memoryclockrate(ghz gb exact command to reproduce floyd run gpu env tensorflow data valiok/datasets/raspberrypi/:/data data valiok/datasets/raspberrypi/:/mobilenet data valiok/datasets/raspberrypi/:/images data valiok/datasets/raspberrypi/:test_ckpt python train.py logtostderr train_dir=training pipeline_config_path=training/ssd_mobilenet_v_pets.configfor the record i tried with previous tf versions and it did not make a diffrence.so i am currently attempting to train a custom object-detection model on tensorflow to recognize images of a raspberrypi everything is already set up and running on my hardware,but due to limitations of my gpu i settled for the cloud i have uploaded my data(train test records ans csv-files and my checkpoint model that is what i get from the logs:tensorflow:restoring parameters from mobilenet/model.ckpttensorflow:starting session.tensorflow:saving checkpoint to path training/model.ckpttensorflow:starting queues.tensorflow:error reported to coordinator class tensorflow.python.framework.errors_impl.invalidargumenterror>,indices is not in i also have a folder called images with the actual jpg files and it is also on the cloud but for some reason i must specify every directory with a preceeding forward slash and that might be a problem as i currently do not know whether some of the files are trying to import these images but could not find the path because of the missing if any of you happens to share a solution i would be really thankful.and so here are my full logs:pstinfo:tensorflow:restoring parameters from mobilenet/model.ckptpstinfo:tensorflow:starting session.pstinfo:tensorflow:saving checkpoint to path training/model.ckptpstinfo:tensorflow:starting queues.pstinfo:tensorflow:error reported to coordinator class tensorflow.python.framework.errors_impl.invalidargumenterror indices is not in pst node cond_/randomcropimage/gather gather tindices=dt_int tparams=dt_bool validate_indices=true device=/job:localhost/replica:/task:/device:cpu: (cond_/switch cond_/randomcropimage/prunecompleteleyoutsidewindow/reshape) pstinfo:tensorflow:global_step/sec pstinfo:tensorflow:caught outofrangeerror stopping training.pstinfo:tensorflow:finished training saving model to disk.psttraceback most recent call last):pstfile train.py line in module>psttf.app.run()pstfile usr/local/lib/python./site-packages/tensorflow/python/platform/app.py line in runpst_sys.exit(main(argv))pstfile train.py line in mainpstworker_job_name is_chief flags.train_dir)pstfile code/trainer.py line in trainpstsaver=saver)pstfile usr/local/lib/python./site-packages/tensorflow/contrib/slim/python/slim/learning.py line in trainpstignore_live_threads=ignore_live_threads)pstfile usr/local/lib/python./site-packages/tensorflow/python/training/supervisor.py line in stoppstignore_live_threads=ignore_live_threads)pstfile usr/local/lib/python./site-packages/tensorflow/python/training/coordinator.py line in joinpstsix.reraise(*self._exc_info_to_raise"
301210630,17334,https://api.github.com/repos/tensorflow/tensorflow/issues/17334,cancan101,4,0,0,0,0,0,system information cat etc/issue linux node-jupyter aws ubuntu smp tue jun utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker yes compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux node-jupyter aws ubuntu smp tue jun utc x x x gnu/linux check pips numpy protobuf tensorflow-gpu check for virtualenv false tensorflow import tf.version tf.git_version v..--gcdfctf.compiler_version v..--gcdfcsanity check array dtype=int env ld_library_path usr/local/cuda/extras/cupti/lib:/usr/local/nvidia/lib:/usr/local/nvidia/libdyld_library_path is unset nvidia-smi tue aug nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m tesla k off e off n/a c p w w mib mib e process processes gpu memory gpu pid type process name usage no running processes found cuda libs usr/local/cuda-./targets/x_-linux/lib/libcudart_static.a/usr/local/cuda-./targets/x_-linux/lib/libcudart.so cat etc/issue linux node-jupyter aws ubuntu smp tue jan utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker yes compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux node-jupyter aws ubuntu smp tue jan utc x x x gnu/linux check pips numpy protobuf post)tensorflow tensorflow-tensorboard check for virtualenv false tensorflow import tf.version tf.git_version v..--gdebtf.compiler_version v..--gdebsanity check array dtype=int)/usr/local/lib/python./dist-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters env ld_library_path usr/local/cuda/extras/cupti/lib:/usr/local/nvidia/lib:/usr/local/nvidia/libdyld_library_path is unset nvidia-smi wed feb nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m tesla m on e off n/a c p w w mib mib e process processes gpu memory gpu pid type process name usage no running processes found cuda libs usr/local/cuda-./targets/x_-linux/lib/libcudart.so.../usr/local/cuda-./targets/x_-linux/lib/libcudart_static.a cat etc/issue linux node-jupyter aws ubuntu smp tue jan utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker yes compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux node-jupyter aws ubuntu smp tue jan utc x x x gnu/linux check pips numpy protobuf post)tensorflow tensorflow-tensorboard check for virtualenv false tensorflow import tf.version tf.git_version v..--gdebtf.compiler_version v..--gdebsanity check array dtype=int)/usr/local/lib/python./dist-packages/hpy/__init__.py futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters env ld_library_path usr/local/cuda/extras/cupti/lib:/usr/local/nvidia/lib:/usr/local/nvidia/libdyld_library_path is unset nvidia-smi wed feb nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m tesla m on e off n/a c p w w mib mib e process processes gpu memory gpu pid type process name usage no running processes found cuda libs usr/local/cuda-./targets/x_-linux/lib/libcudart.so.../usr/local/cuda-./targets/x_-linux/lib/libcudart_static.a v..--gdeb image tensorflow/tensorflow:..-devel-gpu-py describe the problemi am seeing this warning message even though i thought that avx instructions should be used in i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use sse sse avx avx fma this is the same warning i saw on
301055681,17328,https://api.github.com/repos/tensorflow/tensorflow/issues/17328,senthilki,2,0,0,0,0,0,"hi tensorflow,i would like to know whether tensorflow does support on-device training i.e creation of model on android device right now tensorflow provides python script to generate model in the native system and only inference we can execute on android device obviously i cannot execute the python to create the model in android does tensorflow provides any example/feature in c to train the model on-device android feature request a standalone c application which can be cross compiled for android to train the model from dataset training the model might be time consuming process if done on android but at least some minimal support for smaller data set would be helpful.regards,senthil"
300964019,17320,https://api.github.com/repos/tensorflow/tensorflow/issues/17320,BKZero,1,0,0,0,0,0,because i have my own native and c code if i want to use tensorflow lite feature and put it to my android project i have to write tensorflow lite code in c++.but it is too hard to find out how to get the input and output.i have look up the ios example because part of the code is written in c but it crashed when i have:float output interpreter->typed_output_tensor();for(int i i output_size i logd(bkzero jni result f output i );i believe it is not a bug under ios platform.this part of code in android example is written in java i cannot figure out how to write an implemetation of the tensorhandle like:long outputshandles run(interpreterhandle errorhandle sizes datatypes numsofbytes inputs if outputshandles null outputshandles.length throw new illegalstateexception(interpreter has no outputs tensor outputs new tensor outputshandles.length for int i i outputshandles.length i outputs i tensor.fromhandle(outputshandles i log.i(bkzero java outputs outputs i
300869991,17316,https://api.github.com/repos/tensorflow/tensorflow/issues/17316,mrry,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below tf-nightly python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see above describe the problemfor performance the implementation of refcounted comprises a set of inline methods furthermore it contains a cunning optimization that avoids updating the refcount when the caller to unref is the only owner also includes some dcheck macros to ensure that various invariants hold that that second dcheck performs a side-effect to make the first one succeed the release build is built with ndebug defined so the dcheck code doesnt execute however the default flags you get when following the adding an new op tutorial to build an extension do not include dndebug recall that the methods are marked inline this means that some code in the release binary might call an inlined version of unref and the store to ref will be elided but some code in an extension e.g one that creates a custom resourcebase which inherits from refcounted might call refcounted and the dcheck will be performed leading to a failure.the workaround is to add dndebug to the compiler flags when adding a new op should we update the documentation update the tf.sysconfig.get_compile_flags implementation or modify how refcounted is implemented to avoid this problem altogether
300845304,17315,https://api.github.com/repos/tensorflow/tensorflow/issues/17315,yaroslavvb,1,0,0,0,0,0,theres a significant slowdown when feeding numpy arrays which are read-only looking at cpu profile it looks like time is spent in memcpy incorrectly reported as nss_passwd_lookup this memory copy seems unnnecessaryie arr=-byte aligned numpy arraysess.run(some_op feed_dict={a:arr gb/secarr.flags writeable =falsesess.run(some_op feed_dict={a:arr gb/sec tf_numpy_benchmark.py n tf_numpy_benchmark.py benchmark=feed_cpu_tensor allocator=tf num-iters=feed_cpu_tensor gb/sec min median mean python tf_numpy_benchmark.py benchmark=feed_cpu_tensor allocator=tf_readonly num-iters=feed_cpu_tensor gb/sec min median mean cc alextp
300309173,17276,https://api.github.com/repos/tensorflow/tensorflow/issues/17276,imsheridan,1,0,0,0,0,0,this is to fix the current linear model tutorial told how to train and evaluate a linear model but it doesnt seem that there are final steps to complete the tutorial to take a set of given values and predict income_bracket this fix of the tutorial is to provide a simple code example for newbies on how to extract final predictions after training the model
300157723,17269,https://api.github.com/repos/tensorflow/tensorflow/issues/17269,frkhit,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu env from google colab tensorflow installed from source or binary env from google colab tensorflow version use command below rc python version python describe the problemi have code like this: slim.assign_from_checkpoint_fn pretrained_model_file variables_to_restore ignore_missing_vars=true) when pretrained_model_file vgg_.ckpt where vgg_.ckpt is a checkpoint file from pre-trained model it raise exception: ---------------------------------------------------------------------------valueerror traceback most recent call last)
300008357,17253,https://api.github.com/repos/tensorflow/tensorflow/issues/17253,guxiaobo,1,0,0,0,0,0,"hi,i seems the c api only have frontend api to access tensorflow core is there a plan to make higher level apis such as various algorithms and the estimator api thanks.i am sorry if this is not the right place to post this"
299885264,17233,https://api.github.com/repos/tensorflow/tensorflow/issues/17233,yaroslavvb,2,0,0,0,0,0,upgrading to pip install tf-nightly-gpu from pip install tensorflow slows down feeding about xfeeding mb array used to take ms and it takes ms after the change i think this is due to change in alignment requirements for avx-compiled binary i want to start the thread to figure out how to regain the performance before these changes make it into official releasebenchmark align_feed_bug.py version python align_feed_bug.pyfeed-cpu-variable min median mean after upgrading to tf nightly version devpython align_feed_bug.pyfeed-cpu-variable min median mean ive tried using eamartin s recipe to make sure numpy array are byte aligned but that didnt make any difference in speed python align_feed_bug.py align=feed-cpu-variable min median mean cc martinwicke
299572469,17204,https://api.github.com/repos/tensorflow/tensorflow/issues/17204,yaroslavvb,1,0,0,0,0,0,im noticing that htod copies are taking significantly longer than dtoh.ie doing sess.run(gpu_var vs sess.run(assign_gpu_var feed_dict={gpu_var.initial_value arr}) mb tensor takes ms on v to fetch gb/sec but ms to feed gb/sec benchmark there a way to make it faster something to do with memory pinning?)
299479451,17200,https://api.github.com/repos/tensorflow/tensorflow/issues/17200,adamjones1,1,0,0,0,0,0,the following lines worked with keras and tensorflow pre input_tensor input((input_epochs_per_output_epoch samples_per_epoch features_per_epoch)) inner_intermediate timedistributed(batchnormalization(axis name=bn_input)(input_tensor) many lines... inner_out timedistributed(bidirectional(gru(intermediate_features return_sequences=false recurrent_dropout=recurrent_dropout dropout=normal_dropout kernel_initializer=kernel_initializer kernel_regularizer=l(l_lambda implementation=lstm_implementation merge_mode=concat name=inner_out)(inner_intermediate) however with tensorflow using the keras within tensorflow the final line fails with valueerror as_list is not defined on an unknown tensorshape.because there are a lot of lines between and which have various layers wrapped by timedistributed ive figured out that it is only gru and lstm layers that cause this error ive tried specifying the input_shape for both the the wrappers and/or the layers themselves to no avail
299279939,17193,https://api.github.com/repos/tensorflow/tensorflow/issues/17193,sanxiyn,1,0,0,0,0,0,it should be host c compiler not hostc compiler compare set_host_cxx_compiler
299066524,17172,https://api.github.com/repos/tensorflow/tensorflow/issues/17172,case540,1,0,0,0,0,0,to run from external workspace you should now be able to invokescript like the following this will generate some tensorflow specficbazel options and import them into your projects bazelrc.$(bazel info output_base)/external/org_tensorflow/configure.py workspace=$(pwd
298567973,17150,https://api.github.com/repos/tensorflow/tensorflow/issues/17150,Hvass-Labs,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary pip install tensorflow version use command below keras tf python version bazel version if compiling from source na gcc/compiler version if compiling from source na cuda/cudnn version cant remember gpu model and memory gtx exact command to reproduce see below backgroundthis issue seems to be specifically about keras with tensorflow so i have posted it here.i have a keras model for doing machine translation of human languages it has an encoder and decoder each of which use the embedding and gru layers from keras the output of the decoder is a one-hot encoded array.my data-set is from europarl so it is very large already and converting the target-data from integer-tokens to one-hot-encoded labels would be extremely wasteful and take many gb of memory.one solution would be to write my own data-generator and only convert integer-tokens to one-hot-labels for a batch at a time but thats not a very elegant solution.the correct solution is of course to use a sparse version of the crossentropy-loss which automatically converts the integer-tokens to a one-hot-encoded label for comparison to the models output keras has a built-in loss-function for doing exactly this called sparse_categorical_crossentropy however it doesnt seem to work as intended errorthe following shows the essential parts of the code omitted code for building neural network output of the decoder-part of the neural network decoder_dense dense(num_words activation=softmax name=decoder_output decoder_output decoder_dense(decoder_gru_output model model(inputs= encoder_input decoder_input outputs= decoder_output model.compile(optimizer=adam loss=sparse_categorical_crossentropy model.fit(x=x_data y=y_data batch_size epochs=)everything runs fine except model.fit at the end which gives this error valueerror error when checking target expected decoder_output to have dimensions but got array with shape this is the shape of the models output decoder_output.get_shape tensorshape( dimension(none dimension(none dimension() )this is the shape of the target-data which is a dim array of integer-values y_data decoder_output .shape note that i only allow sequences of length for the decoders output working solutionwe can use tensorflows implementation of sparse cross-entropy which seems to work as intended.first we need to have a linear activation on the output of the decoder decoder_dense dense(num_words activation=linear note changed from softmax name=decoder_output)then we need a wrapper-function for the loss that is compatible with keras def sparse_loss(y_true y_pred return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true logits=y_pred)then we need to create a placeholder variable for the batch of target-values once again i only allow sequences of length this is of course a variable in my own code decoder_target tf.placeholder(dtype=int shape=(none model_train.compile(optimizer=adam loss=sparse_loss target_tensors= decoder_target )this works fine and we can train it by calling model.fit(x=x_data y=y_data batch_size epochs=)maybe keras should use tensorflows sparse-cross-entropy more directly because it seems to handle higher-dim data better documentationlooking at the implementation of sparse_categorical_crossentropy in keras there is actually some reshaping going on there but the doc-string doesnt make clear what is assumed of the input/output dims and when/how reshaping is supposed to be done so its impossible to know whether it is a bug or a feature i am experiencing and how to deal with it properly.the doc-string needs to be made more clear by someone who understands the intention of this code.furthermore the doc-string needs to be exported somehow to the online docs because it is not shown here
298393097,17137,https://api.github.com/repos/tensorflow/tensorflow/issues/17137,christian-rauch,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below python version python bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory geforce gtx gb vram exact command to reproduce bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package describe the problemi am trying to build tensorflow with bazel from the google apt repo build immediately fails with a misleading error message: current bazel version is expected at least because the build script is comparing strings instead of integers of the version numbers.backtrace bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_packageerror home/christian/development/tensorflow/workspace traceback most recent call last file home/christian/development/tensorflow/workspace line closure_repositories file home/christian/.cache/bazel/_bazel_christian/deabbbbebcacfcb/external/io_bazel_rules_closure/closure/repositories.bzl line in closure_repositories check_bazel_version(closure rules file home/christian/.cache/bazel/_bazel_christian/deabbbbebcacfcb/external/io_bazel_rules_closure/closure/repositories.bzl line in check_bazel_version fail((%s requires bazel s but was...)))closure rules requires bazel but was error error evaluating workspace fileerror home/christian/development/tensorflow/workspace traceback most recent call last file home/christian/development/tensorflow/workspace line tf_workspace file home/christian/development/tensorflow/tensorflow/workspace.bzl line in tf_workspace check_version file home/christian/development/tensorflow/tensorflow/workspace.bzl line in check_version fail(\ncurrent bazel version is current bazel version is expected at least error error evaluating workspace file
298048818,17101,https://api.github.com/repos/tensorflow/tensorflow/issues/17101,JoanRessing,6,0,0,0,0,0,installed tensorflow on windows education version using c pip install upgrade tensorflow-gpuinstalled cuda from cudnn for cuda from have python and nvidia gtx gbexact command to reproduce python neural.pybazel version n/a describe the problemtried creating a simple network but running into the error described in title i checked the directory where the cuda_path refers to and the file is there source code logs from keras.models import sequential from keras.layers import dense import pandas as pd def load_mnist(path train pd.read_csv(path train.csv y train.ix train train.drop(label test pd.read_csv(path test.csv return train y test train y test load_mnist(data model sequential model.add(dense input_dim activation=relu model.add(dense activation=relu model.add(dense activation=sigmoid model.compile(loss=binary_crossentropy optimizer=adam model.fit(train y epochs batch_size predictions model.predict(test)! importerror
297979088,17083,https://api.github.com/repos/tensorflow/tensorflow/issues/17083,facaiy,3,0,0,0,0,0,fix
297925798,17076,https://api.github.com/repos/tensorflow/tensorflow/issues/17076,Yagun,1,0,0,0,0,0,this is a feature request.please add some example to the docs describing how to use report_tensor_allocations_upon_oom and other options of runoptionsall i could find is this file it is not obvious for example it contains from tensorflow.core.protobuf import config_pband then with session.session as sess sess.run(c options=config_pb.runoptions report_tensor_allocations_upon_oom=true))and more questions arise like what is config_pb etc.thanks
297752052,17064,https://api.github.com/repos/tensorflow/tensorflow/issues/17064,MiguelMonteiro,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary no tensorflow version python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a feature request feature requested the ability to allocate memory in a new op as c types and not only as tensors;i have been implementing a new op on tensorflow following the guide and have noticed what could be a useful feature for people implementing new ops in c++/cuda.currently using opkernelconstruction context it is only possible to allocate memory cpu or gpu in the form of tensors for basic c types you can obtain a pointer for that type easily for example: tensor tensor;op_requires_ok(context context->allocate_temp(dt_float tensorshape tensor));float ptr tensor.flat.data(); however for more complex types like structs this is not possible or at least not direct why not have something like: struct a int a int b;};a a nullptr;op_requires_ok(context context->allocate_bytes(n_bytes=sizeof(a address a)); i think this could be useful when porting c code from elsewhere and simplifies memory allocation of non-tensor types in c++.could this be a useful feature or is there a good reason its not implemented note:one possible work-around with the current system is to allocate a tensor of type uint_ with the number of bytes required and then used reinterpret_cast struct a int a int b;};a a nullptr;tensor tensor;op_requires_ok(context context->allocate_temp(dt_uint tensorshape({sizeof(a tensor));a reinterpret_cast(tensor.flat().data()); this kind of feels like cheating and over complex just for allocating memory
297488935,17047,https://api.github.com/repos/tensorflow/tensorflow/issues/17047,abiro,5,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu lts tensorflow installed from source or binary) :source tensorflow version use command below) :v..-rc--gececd python version python bazel version if compiling from source) :bazel gcc/compiler version if compiling from source) :gcc version ubuntu ubuntu cuda/cudnn version :none gpu model and memory :none exact command to reproduce get the dockerfile from here build with the mkl library docker build t tf_mkl f dockerfile.devel-cpu-mkl build without the mkl library docker build t tf_nomkl f dockerfile.devel-cpu-mkl build-arg mkl_flag run image built with the mkl library docker run tf_mkl result error see output below run image built without the mkl library docker run tf_nomkl result no error describe the problemwhen building tensorflow with the mkl library inference with the xception model results in a failing assertion and the program exits see the error message and the code below please the dockerfile linked above contains the source code and can be used to easily reproduce the problem source code logs source code pythonimport tensorflow as tfimport numpy as npx tf.keras.applications.xception.xception(weights=xception_weights.h)x.predict(np.zeros batch_size logsstep from above outputs the following f tensorflow/core/kernels/mkl_input_conversion_op.cc check failed tf_input.checkreordertoopmem memory::primitive_desc(output_mkl_md cpu_engine tensor_out net true vs aborted core dumped miscthe host machine used to build and run the docker containers was an ec c.xlarge instance with an intel(r xeon(r cpu e v ghz processor.related to
297286683,17021,https://api.github.com/repos/tensorflow/tensorflow/issues/17021,Sarasra,1,0,0,0,0,0,describe the problemtf.losses.sigmoid_cross_entory parameter description of label indicates it has to be integer and in range there are no integers between so that seems really difficult to abide by it seems that in the code it does not need to be integer and the range should be why does softmax_cross_entropy assume one-hot encoding for instance in distillation your labels are softmax outputs floats in
297230834,17017,https://api.github.com/repos/tensorflow/tensorflow/issues/17017,PhilJd,5,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu not relevant tensorflow installed from source or binary not relevant tensorflow version use command below not relevant python version not relevant bazel version if compiling from source not relevant gcc/compiler version if compiling from source not relevant cuda/cudnn version not relevant gpu model and memory not relevant exact command to reproduce not relevant describe the problemfair just released an initial version of their tensor comprehension framework which i think is a really clever concept the tensor comprehension library allows to define functions with a syntax similar to einstein-notation and then compiles these functions into fast gpu code via evolutionary search is this something you would consider including into the core or would you rather favor an integration as a separate framework?cheers,phil"
297103980,17011,https://api.github.com/repos/tensorflow/tensorflow/issues/17011,voegtlel,6,0,0,0,0,0,status quoright now the output size of tf.contrib.image.transform is equal to the input size requestspecify output size for tf.contrib.image.transform by argument usage example:this would be very useful for custom augmentations where the input images do not have a fixed size but the output images shall have a constant size this operation can be simulated by padding the input images such that no information is lost when e.g rotation the image then transforming them and finally cropping/padding them to the desired size but itd be much more efficient and a lot easier if the op could support setting the desired output size.thanks
296991474,17002,https://api.github.com/repos/tensorflow/tensorflow/issues/17002,sandeepkumar8713,3,0,0,0,0,0,describe the problemhi i have been trying to use tf.contrib.factorization.kmeansclustering for clustering i got the documentation in link but it did not specify how to give input there is no input argument here init num_clusters model_dir=none initial_clusters=random_init distance_metric=squared_euclidean_distance random_seed use_mini_batch=true mini_batch_steps_per_iteration kmeans_plus_plus_num_retries relative_tolerance=none config=none)so i request you to please add a working example to train and predict using kmeansclustring in tutorial example.thank you
296189869,16935,https://api.github.com/repos/tensorflow/tensorflow/issues/16935,jhljx,4,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory :geforce m(gb exact command to reproduce describe the problemi run the function cifar.maybe_download_and_extract of tensorflow/models/tutorials/image/cifar.py file in python command and jupyter notebook the code runs successfully in python command but it causes absl.flags._exceptions.unparsedflagaccesserror from cifar.flags.data_dir in jupyter notebook.the jupyter notebook has already configured the tensorflow kernel other tensorflow examples could run smoothly on jupyter notebook.significant code in cifar.py file is import tensorflow as tf import cifar_input flags tf.app.flags.flags basic model parameters tf.app.flags.define_integer(batch_size number of images to process in a batch tf.app.flags.define_string(data_dir tmp/cifar_data path to the cifar data directory tf.app.flags.define_boolean(use_fp false train the model using fp def maybe_download_and_extract download and extract the tarball from alexs website dest_directory flags.data_dir if not os.path.exists(dest_directory os.makedirs(dest_directory the exception happened from the code statement dest_directory flags.data_dir .the successful output in python command is as follows cifar.maybe_download_and_extract downloading cifar--binary.tar.gz successfully downloaded cifar--binary.tar.gz bytes.the detailed error in jupyter notebook is as follows unrecognizedflagerror traceback most recent call last ipython-input--ddaeebfd in module cifar.maybe_download_and_extract models/tutorials/image/cifar/cifar.py in maybe_download_and_extract def maybe_download_and_extract download and extract the tarball from alexs website dest_directory flags.data_dir if not os.path.exists(dest_directory os.makedirs(dest_directory anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/platform/flags.py in getattr__(self name a flag if not wrapped.is_parsed wrapped(_sys.argv return wrapped.__getattr__(name anaconda/envs/tensorflow/lib/python./site-packages/absl/flags/_flagvalues.py in call__(self argv known_only suggestions helpers.get_flag_suggestions(name list(self raise exceptions.unrecognizedflagerror name value suggestions=suggestions self.mark_as_parsed unrecognizedflagerror unknown command line flag f source code logs git clone cd models/tutorials/image/cifar import cifar cifar.maybe_download_and_extract
296186918,16933,https://api.github.com/repos/tensorflow/tensorflow/issues/16933,lxkarthi,3,0,0,0,0,0,floatlist and feature is slow for numpy array.saving numpy arrays with np.load and np.save is much faster than converting to tfrecord and reading it back.while profiling the code i found that half of the time is spent in floats_feature.tf.train.floatlist is taking of the time.how to speed this up system information below snippet of code to convert numpy array is much slow compared np.save np.load os platform and distribution linux ubuntu tensorflow version use command below python version source code logs import tensorflow as tfimport numpy as npdef floatme(value return tf.train.floatlist(value=value)def floats_feature(value return tf.train.feature(float_list=floatme(value))tfr_filename deleteme.tfrdata join(np.random.randint size=).astype(str for i in range() with tf.python_io.tfrecordwriter(tfr_filename as writer print(converting to vectors vectors np.fromstring(line dtype=int sep count for line in data print(converting to examples for i vec in enumerate(vectors create an example protocol buffer example tf.train.example(features=tf.train.features(feature label floats_feature( vec vec data floats_feature(vec writer.write(example.serializetostring()) ncalls tottime percall cumtime percall filename:lineno(function convert_train_dataset_tfrecord.py:(floatme numpy.core.multiarray.fromstring convert_train_dataset_tfrecord.py:(_floats_feature
296175671,16930,https://api.github.com/repos/tensorflow/tensorflow/issues/16930,augusterodin,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problemi found an issue when i use a tile operator followed by a gather operator it seems that there is a isinstance(grad ops.tensor assert in the gradient function of tile operator while gather operator will result in a sparse gradient with the type of indexedslices.please see below example for details source code logshere is a small example to re-produce this issue: pythonx tf.variable name=x)x tf.tile(x i tf.variable(list(range name=i)x tf.gather(x i)s tf.reduce_sum(x)optimizer tf.train.gradientdescentoptimizer(learning_rate=.)train_op optimizer.minimize(s) you will get below errors: valueerror no attr named xlacompile in name tileop tileinput x/readinput tile/multiplesattr key t value type dt_float attr key tmultiples value type dt_int during handling of the above exception another exception occurred:traceback most recent call last file d:/test/test_gather.py line in module train_op optimizer.minimize(s file d:\pyenv\tf.\lib\site-packages\tensorflow\python\training\optimizer.py line in minimize grad_loss=grad_loss file d:\pyenv\tf.\lib\site-packages\tensorflow\python\training\optimizer.py line in compute_gradients colocate_gradients_with_ops=colocate_gradients_with_ops file d:\pyenv\tf.\lib\site-packages\tensorflow\python\ops\gradients_impl.py line in gradients grad_scope op func_call lambda grad_fn(op out_grads file d:\pyenv\tf.\lib\site-packages\tensorflow\python\ops\gradients_impl.py line in maybecompile return grad_fn exit early file d:\pyenv\tf.\lib\site-packages\tensorflow\python\ops\gradients_impl.py line in lambda grad_scope op func_call lambda grad_fn(op out_grads file d:\pyenv\tf.\lib\site-packages\tensorflow\python\ops\array_grad.py line in tilegrad assert isinstance(grad ops.tensor)assertionerror i think this is caused by the assertion in gradient function of tile operator: python@ops.registergradient(tile)def tilegrad(op grad sum reduces grad along the tiled dimensions assert isinstance(grad ops.tensor input_shape array_ops.shape(op.inputs ) could we remove this assert or change the type to ops._tensorlike
296148100,16926,https://api.github.com/repos/tensorflow/tensorflow/issues/16926,cgarciae,5,0,0,0,0,2,the dataset api is excellent i am even using it to perform data augmentation before training however i notice that when converting my dataset to tfrecord i follow a pattern that might easily be automated as a to_records method in fact i think this is somewhat similar to using the cache method anyway it would make it very straight forward for dataset users to create tfrecords
295942905,16894,https://api.github.com/repos/tensorflow/tensorflow/issues/16894,jooojo,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary from binary tensorflow version use command below python version bazel version if compiling from source none gcc/compiler version if compiling from source) :gcc version ubuntu ubuntu for compiling custom ops cuda/cudnn version do not matter gpu model and memory do not matter exact command to reproduce as following describe the problemfor the code below the complete sample follows later) pythonfirst_op custom_ops_module.custom_first(features_v)with tf.control_dependencies( first_op second_op tf.reshape(custom_ops_module.custom_second for i in range sess.run(second_op) according to the doc of control dependencies second_op will only run after first_op has executed.however this is not true for tensorflow when the ops are customized cpp codes tensorflow the code runs exactly what i want a first_op - second_op loop tensorflow second_op runs first followed the first_op - second_op loop tensorflow second_op runs first and only once globally then first_op runs again and again source code logsto reproduce a customized op is need c source and compile script attached the python part loads the compiled custom_ops.so and behaves as described above for tensorflow and each with log provided. python#!/usr/bin/env python coding utf from future import print_functionimport osos.environ tf_cpp_min_log_level =import tensorflow as tfcustom_ops_module tf.load_op_library(./custom_ops.so)def run features_v tf.variable tf.zeros(shape name=features_v trainable=false collections= tf.graphkeys.local_variables first_op custom_ops_module.custom_first(features_v with tf.control_dependencies( first_op second_op tf.reshape(custom_ops_module.custom_second sess_config tf.configproto sess_config.allow_soft_placement true sess tf.session(config=sess_config sess.run(tf.global_variables_initializer sess.run(tf.local_variables_initializer print(init vars done print(start loop for i in range print(loop i sess.run(second_op)if name main print(tensorflow tf.__version run() the custom ops cpp#include tensorflow/core/framework/op.h#include tensorflow/core/framework/shape_inference.h#include tensorflow/core/framework/op_kernel.h#include glog/logging.h namespace using tensorflow::device_cpu;using tensorflow::tensor customfirstop register_op(customfirst input(features float class customfirstop public tensorflow::opkernel public explicit customfirstop(tensorflow::opkernelconstruction context tensorflow::opkernel(context void compute(tensorflow::opkernelcontext context override const tensor input_tensor context->input log(info firstop enter with input_tensor.debugstring log(info firstop leave register_kernel_builder(name(customfirst).device(device_cpu customfirstop customsecondop register_op(customsecond output(images uint class customsecondop public tensorflow::opkernel public explicit customsecondop(tensorflow::opkernelconstruction context tensorflow::opkernel(context void compute(tensorflow::opkernelcontext context override log(info secondop enter tensorflow::tensor output_tensor nullptr op_requires_ok(context context->allocate_output tensorflow::tensorshape output_tensor auto output_tensor_buffer output_tensor->shapeddebugstring register_kernel_builder(name(customsecond).device(device_cpu customsecondop op end compile scripts for tensorflow and bin/bashtf_inc python c import tensorflow as tf print(tf.sysconfig.get_include g std=c shared custom_ops.cc o custom_ops.so fpic i tf_inc lglog compile scripts for tensorflow bin/bashtf_cflags python c import tensorflow as tf print join(tf.sysconfig.get_compile_flags tf_lflags python c import tensorflow as tf print join(tf.sysconfig.get_link_flags g std=c shared custom_ops.cc o custom_ops.so fpic tf_cflags tf_lflags lglog log with tensorflow tensorflow init vars donestart looploop warning logging before initgooglelogging is written to stderri custom_ops.cc firstop enter with tensor
295939432,16893,https://api.github.com/repos/tensorflow/tensorflow/issues/16893,dmak,3,0,0,0,0,0,it looks like build script uses strings for version comparison i.e tuple hence bazel version is considered less then required version user@server:~/tensorflow bazel build config opt tensorflow/java:tensorflow tensorflow/java:libtensorflow_jnierror home/user/tensorflow-../workspace traceback most recent call last file home/user/tensorflow-../workspace line closure_repositories file home/user/.cache/bazel/_bazel_user/dcebdaddecbeec/external/io_bazel_rules_closure/closure/repositories.bzl line in closure_repositories check_bazel_version(closure rules file home/user/.cache/bazel/_bazel_user/dcebdaddecbeec/external/io_bazel_rules_closure/closure/repositories.bzl line in check_bazel_version fail((%s requires bazel s but was...)))closure rules requires bazel but was non-git) os platform and distribution suse linux tensorflow installed from this source version bazel version compiled from this source version n/a was not enabledgpu model and memory n/a was not enabledexact command to reproduce bazel build config opt tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni
295812216,16887,https://api.github.com/repos/tensorflow/tensorflow/issues/16887,daisylab,1,0,0,0,0,0,"please go to stack overflow for help and support you open a github issue here is our policy it must be a bug a feature request or a significant problem with documentation for small docs fixes please send a pr instead the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu slackware linux bit tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory :ti gb exact command to reproduce :bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_packageyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request gcc does not compile tf source code.first of all sorry for that im not using ubuntu linux for tf i know that ubuntu linux is the only supported linux system for tf instead i use an ancient distribution i.e slackware linux recently i got a security update for spectre as a side-effect i also got an updated gcc i usually compile the tensorflow source code for optimization however it can not be compiled with updated gcc like this; info from compiling tensorflow/contrib/resampler/kernels/resampler_ops_gpu.cu.cc:/usr/lib/gcc/x_-slackware-linux/../include/avxfintrin.h error argument of type const void is incompatible with parameter of type const float usr/lib/gcc/x_-slackware-linux/../include/avxfintrin.h error argument of type const void is incompatible with parameter of type const float usr/lib/gcc/x_-slackware-linux/../include/avxfintrin.h error argument of type const void is incompatible with parameter of type const double usr/lib/gcc/x_-slackware-linux/../include/avxfintrin.h error argument of type const void is incompatible with parameter of type const double so i googled a little bit and found the following issue in the middle of the thread i saw i think the problem here is that gcc shipped with avx*intrin.h headers that switched to using void and const void but without switching the builtins to do the same this is why works but breaks so i tracked down the matter that i could see all of the scatter/gather intrinsics in avxintrin.h use int/float/double pointers which is incorrect so at first i thought this problem is about gcc but someone suggested that its maybe related to cuda its the gpu/cuda code that doesnt support the new compiler so if you want to build that your only option is downgrade the compiler until nvidia releases a new cuda sdk.from wrap up i was able to compile tf successfully with gcc with gcc i get error messages here and there and yet i do not know what makes this errors i suspect the combination of gcc and cuda and also tensorflow does not work well but i still can not figure out which of them makes this fault.thank you for your help.best regards,sungjin source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem"
295765757,16886,https://api.github.com/repos/tensorflow/tensorflow/issues/16886,philipstephens,4,0,0,0,0,0,i tried cloning the alpha zero chess from and almost got everything to work however when i try python src/chess_zero/run.py self i get the message importerror could not find cudart_.dll tensorflow requires that this dll be installed in a directory that is named in your path environment variable download and install cuda from this url the problem is that i have cuda and cannot seem to get cuda on the website would it be hard to make a version of tensorflow-gpu that works with cuda system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary) :installed from tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory :nvidia geforce gtx ti gb ram exact command to reproduce :python src/chess_zero/run.py selfyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version)e:\edownloads>python c import tensorflow as tf print(tf.git_version tf.version)traceback most recent call last file c:\users\user\anaconda\lib\site-packages\tensorflow\python\platform\self_check.py line in preload_check ctypes.windll(build_info.cudart_dll_name file c:\users\user\anaconda\lib\ctypes\__init__.py line in init self._handle dlopen(self._name mode)oserror winerror the specified module could not be foundduring handling of the above exception another exception occurred:traceback most recent call last file string line in module file c:\users\user\anaconda\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\users\user\anaconda\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\users\user\anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module self_check.preload_check file c:\users\user\anaconda\lib\site-packages\tensorflow\python\platform\self_check.py line in preload_check build_info.cudart_dll_name build_info.cuda_version_number))importerror could not find cudart_.dll tensorflow requires that this dll be installed in a directory that is named in your path environment variable download and install cuda from this url describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request i am requesting an update because i have cuda and cant get cuda source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
295750983,16885,https://api.github.com/repos/tensorflow/tensorflow/issues/16885,rayanelleuch,3,0,0,0,0,0,this pull request follow the propositions from and try to reproduce the layer implemented in integrated the code as a layer in tf.contrib.layersthere is most probably additional changes that are needed.i saw that there should be a testing unit added but i dont know exactly how it should be done
295723524,16883,https://api.github.com/repos/tensorflow/tensorflow/issues/16883,selcouthlyBlue,1,0,0,0,0,0,i would like to contribute a multidimensional rnn feature in the contrib directory based on the implementation mentioned here right now its possible to implement various types of multidimensional rnns by feeding in your data as time being one direction say x taking the output of the rnn transposing it and feeding it into a second rnn etc alternatively feed your data its transpose into separate rnns possibly with tied weights and depth-concatenate the results and maybe feed the result into another rnn.by any chance is it related to the main paper for it
295491137,16862,https://api.github.com/repos/tensorflow/tensorflow/issues/16862,NguyenAnhTien,4,0,0,0,0,0,no such package nasm java.io.ioexception error downloading
295178229,16835,https://api.github.com/repos/tensorflow/tensorflow/issues/16835,Bonnevie,2,0,0,0,0,0,in numpy its my understanding that the np.einsum function has been extended with the functionality of opt_einsum which computes an optimal or near-optimal way to perform the tensor contractions.as far as ive heard the tensorflow einsum is a lot more basic but extremely convenient and cannot be relied upon for performance also even though opt_einsum can give allegedly perfect decomposition paths these optimizations appear to not always work well in tensorflow which might be down to memory order available numerical routines views vs reshapes etc as a feature request i think it would be hugely beneficial to have a high-performing version of tf.einsum in tensorflow as it makes it easy to handle many complicated linear algebra tasks compactly system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu lts tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a
295042824,16824,https://api.github.com/repos/tensorflow/tensorflow/issues/16824,zhoub,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows x tensorflow installed from source or binary source tensorflow version use command below branch r python version bazel version if compiling from source gcc/compiler version if compiling from source visual studio msvc cuda/cudnn version only cpu gpu model and memory only cpu exact command to reproduce cd tensorflow/contrib/cmake cmake-gui hi,im using cmake to build the library on windows everything works well but id like to make a very tiny library with c api only is that possible to do this from cmake thanks very much"
294945666,16810,https://api.github.com/repos/tensorflow/tensorflow/issues/16810,kr-ish,1,0,0,0,0,0,the documentation for the tf.dataset.data.shuffle function states the following reshuffle_each_iteration optional a boolean which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over defaults to true.)however the default value in the function is none def shuffle(self buffer_size seed=none reshuffle_each_iteration=none):the function calls the shuffledataset class whose init function also sets the same argument to none by default and uses the following logic to set the default value of the argument to true if reshuffle_each_iteration is none self._reshuffle_each_iteration true else self._reshuffle_each_iteration reshuffle_each_iterationthis commit sets the argument to true by default in both the function and the class making the above code block redundant and replacing it with only self._reshuffle_each_iteration reshuffle_each_iteration
294491341,16779,https://api.github.com/repos/tensorflow/tensorflow/issues/16779,ljanyst,0,0,0,0,0,1,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source gcc version cuda/cudnn version gpu model and memory irrelevant exact command to reproduce install gcc cross-compiler sudo apt-get install gcc-aarch-linux-gnu g++-aarch-linux-gnu install the cuda cross-aarch packages build python for the target i wrote a short blog post with the details clone tensorflowgit checkout v..-cross-jetson-txcd third_party/toolchains/cpus/aarch./configure.pycd configurebazel build config=opt config=cuda crosstool_top=//third_party/toolchains/cpus/aarch:toolchain cpu=arm compiler=cuda tensorflow/tools/pip_package:build_pip_package describe the problemi have succeeded but i encountered a bunch of issues on the way due to my unfamiliarity with bazel my solution is rather hacky it looks to me like properly fixing it is a rather low hanging fruit for a person familiar with bazel so i document what i have discovered here configuring cuda for the targeti could not find a way in the to check cuda configuration script whether the source is supposed to be configured for a cross build so i ended up changing the hardcoded paths if there was a way to check if were building for a platform that is different from the one of the build host this could be easily turned into some sort of an if statement specifying target python installationi could not find an easy way to patch that through so i ended up putting it in the crosstool file: cxx_flag isystemcxx_flag target_python_includes code generators depend on libtensorflow_framework.so which in turn depends on cudathis means that i needed to have the cuda and cudnn libraries for the build host and needed to pass the relevant library paths to the compiler in a wrapper script linking of the code generators fails on the build-host sidebazel builds both the code generators and libtensorflow_framework.so for the host but does not link the binaries against the framework library.i have worked around the two above issues by writing a compiler wrapper scripts doing the following: pythonif ofile is not none is_gen ofile.endswith(py_wrappers_cc or ofile.endswith(gen_cc if is_cuda yes and ofile.endswith(libtensorflow_framework.so or is_gen cuda_libdirs l targets/x_-linux/lib.format(cuda_dir l targets/x_-linux/lib/stubs.format(cuda_dir l lib.format(cudnn_dir if is_gen tf_libs l bazel-out/host/bin/tensorflow ltensorflow_framework call( find_executable(gcc cuda_libdirs args tf_libs incomplete rpath in the target-side libtensorflow_framwork.so the library gets linked using a bunch of rpath parameters but one seems to be missing it causes linking errors down the road i have added the following to the original crosstool_wrapper_driver_is_not_gcc script to fix the problem: python ofile getoptionvalue(sys.argv o if ofile and ofile .endswith(libtensorflow_framework.so cpu_compiler_flags wl,-rpath,+os.getcwd()+/bazel-out/arm-py-opt/genfiles/external/local_config_cuda/cuda/cuda/lib platform metadata of the resulting wheel packagesince the build host platform is linux_x this ends up being written in the wheel metadata the issue can be fixed by manually passing the plat-name parameter to distutils i have therefore added to build_pip_package.sh please let me know if youd like a pull request.it hope its helpful"
294437476,16774,https://api.github.com/repos/tensorflow/tensorflow/issues/16774,Hvass-Labs,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below keras tf python version bazel version if compiling from source na gcc/compiler version if compiling from source na cuda/cudnn version gpu model and memory gtx gb exact command to reproduce na describe the problemthe tensorboard-callback in keras supports the writing of embeddings to the log but this is not supported in the tensorflow version of keras even though the doc-string of the tensorflow version actually lists these parameters these are missing from the init is the original keras implementation which has e.g embeddings_freq seems to be a difference between the original keras implementation and the version in tensorflow i am wondering if these are somehow developed in parallel so it is actually not the original keras that is included with tensorflow?thanks
294153301,16736,https://api.github.com/repos/tensorflow/tensorflow/issues/16736,hannw,1,0,0,0,0,0,the pr implement the rnn cell in the following paper
294100631,16723,https://api.github.com/repos/tensorflow/tensorflow/issues/16723,YanzheL,0,0,0,0,1,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu linux ubuntu aarch tensorflow installed from source or binary) :source on branch r tensorflow version use command below) :v..--gece python version :python bazel version if compiling from source and both tried with clean installation gcc/compiler version if compiling from source) :gcc ubuntu/linaro ubuntu cuda/cudnn version :cuda cudnn gpu model and memory :nvidia tegra x major pascal architecture g jdk version : root@tegra-ubuntu:/usr/src java versionopenjdk version openjdk runtime environment build u-b-ubuntu...-b)openjdk bit server vm build b mixed mode)root@tegra-ubuntu:/usr/src javac versionjavac exact command to reproduce : shellroot@tegra-ubuntu:/usr/src/tensorflow configureextracting bazel installation...you have bazel non-git installed.please specify the location of python default is usr/bin/python usr/bin/pythonfound possible python library paths usr/local/lib/python./dist-packages usr/lib/python/dist-packagesplease input the desired python library path to use default is usr/local/lib/python./dist-packages do you wish to build tensorflow with jemalloc as malloc support y/n jemalloc as malloc support will be enabled for tensorflow.do you wish to build tensorflow with google cloud platform support y/n nno google cloud platform support will be enabled for tensorflow.do you wish to build tensorflow with hadoop file system support y/n hadoop file system support will be enabled for tensorflow.do you wish to build tensorflow with amazon s file system support y/n nno amazon s file system support will be enabled for tensorflow.do you wish to build tensorflow with apache kafka platform support y/n no apache kafka platform support will be enabled for tensorflow.do you wish to build tensorflow with xla jit support y/n no xla jit support will be enabled for tensorflow.do you wish to build tensorflow with gdr support y/n no gdr support will be enabled for tensorflow.do you wish to build tensorflow with verbs support y/n no verbs support will be enabled for tensorflow.do you wish to build tensorflow with opencl sycl support y/n no opencl sycl support will be enabled for tensorflow.do you wish to build tensorflow with cuda support y/n ycuda support will be enabled for tensorflow.please specify the cuda sdk version you want to use e.g leave empty to default to cuda please specify the location where cuda toolkit is installed refer to readme.md for more details default is usr/local/cuda please specify the cudnn version you want to use leave empty to default to cudnn please specify the location where cudnn library is installed refer to readme.md for more details default is usr/local/cuda :do you wish to build tensorflow with tensorrt support y/n no tensorrt support will be enabled for tensorflow.please specify a list of comma-separated cuda compute capabilities you want to build with.you can find the compute capability of your device at note that each additional compute capability significantly increases your build time and binary size default is do you want to use clang as cuda compiler y/n nvcc will be used as cuda compiler.please specify which gcc should be used by nvcc as the host compiler default is usr/bin/gcc do you wish to build tensorflow with mpi support y/n no mpi support will be enabled for tensorflow.please specify optimization flags to use during compilation when bazel option config=opt is specified default is march=native would you like to interactively configure workspace for android builds y/n not configuring the workspace for android builds.preconfigured bazel build configs you can use any of the below by adding config to your build command see tools/bazel.rc for more details config=mkl build with mkl support config=monolithic config for mostly static monolithic build config=tensorrt build with tensorrt support.configuration finished the output from compile procedure is shellroot@tegra-ubuntu:/usr/src/tensorflow bazel build config=opt config=cuda tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni.........................error root/.cache/bazel/_bazel_root/cbebbabedf/external/bazel_tools/tools/jdk/build in singlejar attribute of java_toolchain rule bazel_tools//tools/jdk:toolchain bazel_tools//tools/jdk:singlejar must produce a single fileerror analysis of target tensorflow/java:tensorflow failed build aborted analysis of target bazel_tools//tools/jdk:toolchain failed build abortedinfo elapsed time sfailed build did not complete successfully packages loaded currently loading tensorflow you can collect some of this information using our environment capture script tensorflow/tools/tf_env_collect.shcollecting system information i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc arm does not support numa returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name nvidia tegra x major minor memoryclockrate(ghz pcibusid totalmemory gib freememory mib i tensorflow/core/common_runtime/gpu/gpu_device.cc adding visible gpu devices i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name nvidia tegra x pci bus id compute capability wrote environment to tf_env.txt you can review the contents of that file.and use it to populate the fields in the github issue template.cat tf_env.txt root@tegra-ubuntu:/usr/src cat tf_env.txt cat etc/issue linux tegra-ubuntu tegra smp preempt fri dec pst aarch aarch aarch gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker no compiler c ubuntu/linaro ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux tegra-ubuntu tegra smp preempt fri dec pst aarch aarch aarch gnu/linux check pips numpy protobuf tensorflow tensorflow-tensorboard check for virtualenv false tensorflow import tf.version tf.git_version v..--gecetf.compiler_version v..--gecesanity check array dtype=int env ld_library_path is unsetdyld_library_path is unset nvidia-smi tensorflow/tools/tf_env_collect.sh line nvidia-smi command not found cuda libs usr/local/cuda-./targets/aarch-linux/lib/libcudart_static.a/usr/local/cuda-./targets/aarch-linux/lib/libcudart.so.../usr/local/cuda-./doc/man/man/libcudart./usr/local/cuda-./doc/man/man/libcudart.so. you can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version) root@tegra-ubuntu:/usr/src python c import tensorflow as tf print(tf.git_version tf.version)v..--gece describe the problemi have successfully compiled tensorflow python from source using same configure procedure as above with the following command: root@tegra-ubuntu:/usr/src/tensorflow bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package there is no error in output and i can install the whl file with pip.after that i tried to compile the java native library without the configure step because i already configured it when compiling python version using the command: root@tegra-ubuntu:/usr/src/tensorflow bazel build config=opt config=cuda tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni it failed with errors: shellroot@tegra-ubuntu:/usr/src/tensorflow bazel build config=opt config=cuda tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni.........................error root/.cache/bazel/_bazel_root/cbebbabedf/external/bazel_tools/tools/jdk/build in singlejar attribute of java_toolchain rule bazel_tools//tools/jdk:toolchain bazel_tools//tools/jdk:singlejar must produce a single fileerror analysis of target tensorflow/java:tensorflow failed build aborted analysis of target bazel_tools//tools/jdk:toolchain failed build abortedinfo elapsed time sfailed build did not complete successfully packages loaded currently loading tensorflow here are some ways i tried but faild with same error configure again with same configure settings and compile remove the directory cache and do the step remove the directory cache and tensorflow source directory git clone tensorflow from r branch then do step remove cache and the bazel binary compile and install bazel from latest source release without error then i use bazel to compile tensorflow java this produced same error
293612568,16665,https://api.github.com/repos/tensorflow/tensorflow/issues/16665,GauthierChan,3,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes see below os platform and distribution e.g linux ubuntu macos high sierra tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source homebrew gcc/compiler version if compiling from source describe the problemwhen running a custom tensorflow library built with selective_registration and running our quantized model on android we see this crash log: ruby java.lang.illegalargumentexception no opkernel was registered to support op min with these attrs registered devices cpu registered kernels no registered kernels node mul__eightbit/mul_/y/min min t=dt_float tidx=dt_int keep_dims=false (mul__eightbit/mul_/y/reshape mul__eightbit/mul_/y/reduction_dims at org.tensorflow.session.run(native method at org.tensorflow.session.access$(session.java at org.tensorflow.session$runner.runhelper(session.java at org.tensorflow.session$runner.run(session.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.run(tensorflowinferenceinterface.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.run(tensorflowinferenceinterface.java at io.cometapp.tensortest.models.yolo.yoloclassifier.predict(yoloclassifier.java at io.cometapp.tensortest.classifieractivity$.run(classifieractivity.java at java.lang.thread.run(thread.java:) here is how we build the custom tensorflow library bazel build c opt copt=-dselective_registration copt=-dsupport_selective_registration copt=-dtensorflow_disable_meta copt=-d__android_types_full tensorflow/contrib/android:libtensorflow_inference.so crosstool_top=//external:android/crosstool host_crosstool_top=@bazel_tools//tools/cpp:toolchain cpu=armeabi-va if we do not use copt=-dselective_registration copt=-dsupport_selective_registration we are able to run the model successfully.here is the ops_to_register.h that we use c this file was autogenerated by print_selective_registration_header.py#ifndef ops_to_register#define ops_to_register namespace constexpr const char skip(const char x return x x skip(x x x constexpr bool isequal(const char x const char y return skip(x skip(y skip(x skip(y isequal(skip(x skip(y skip(x skip(y template,constantop,dequantizeop,identityop,reductionop>,binaryop cpudevice functor::maximum>,reductionop>,noop,padop,placeholderop,quantizevop,quantizedbiasaddop,quantizedconvdop,quantizedmaxpoolingop,quantizedmulop,binaryop cpudevice functor::div>,requantizationrangeop,requantizeop,reshapeop,binaryop cpudevice functor::sub>,recvop,sendop,};#define should_register_op_kernel(clz find_in::f(clz knecessaryopkernelclasses))constexpr inline bool shouldregisterop(const char op return false isequal(op concatv isequal(op const isequal(op dequantize isequal(op identity isequal(op max isequal(op maximum isequal(op min isequal(op noop isequal(op pad isequal(op placeholder isequal(op quantizev isequal(op quantizedbiasadd isequal(op quantizedconvd isequal(op quantizedmaxpool isequal(op quantizedmul isequal(op realdiv isequal(op requantizationrange isequal(op requantize isequal(op reshape isequal(op sub isequal(op recv isequal(op send define should_register_op(op shouldregisterop(op)#define should_register_op_gradient false#endif"
293581173,16662,https://api.github.com/repos/tensorflow/tensorflow/issues/16662,jitoledo,6,0,0,0,0,0,i get this error message when trying to build from source r with the new bazel version published today.current bazel version is expected at least i guess the version check is wrong
293567670,16660,https://api.github.com/repos/tensorflow/tensorflow/issues/16660,jedakiah13,4,0,0,0,0,0,i am working with tensorflow in java using the maven dependency.i would like to use the gpu version in my java application but i notice there is only a supported maven repository for linux.will there be support for tensorflow with java in windows?if so then what is the timeline for this?otherwise is there another way round it?thanks
293541456,16659,https://api.github.com/repos/tensorflow/tensorflow/issues/16659,meteorcloudy,0,0,0,3,0,0,this change requires upgrading bazel to because we need cc_import rule.i disabled all failing tests on windows i will send issues to their owner later and write some tips about how to reproduce and test them.@gunan mrry martinwicke fyi dslomov laszlocsomor
293480784,16654,https://api.github.com/repos/tensorflow/tensorflow/issues/16654,jendrikjoe,10,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no just commented lines in the bzl files out os platform and distribution on jetson tx tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory tx gpu gb not sure exact command to reproduce bazel build c opt local_resources verbose_failures config=cuda tensorflow/tools/pip_package:build_pip_package describe the problemi cannot build tensorflow using bazel it seems like the version checks in repositories.bzl and wokspace.bzl fail commenting them out solves the issue even though i know that is no persistent solution i think it is simply that bazel thinks that is smaller due to its string comparison but i am no bazel expert source code logserror home/nvidia/git/tensorflow/workspace traceback most recent call last file home/nvidia/git/tensorflow/workspace line closure_repositories file home/nvidia/.cache/bazel/_bazel_nvidia/ccbbcaafcd/external/io_bazel_rules_closure/closure/repositories.bzl line in closure_repositories check_bazel_version(closure rules file home/nvidia/.cache/bazel/_bazel_nvidia/ccbbcaafcd/external/io_bazel_rules_closure/closure/repositories.bzl line in check_bazel_version fail((%s requires bazel s but was...)))closure rules requires bazel but was non-git)error error evaluating workspace fileerror home/nvidia/git/tensorflow/workspace traceback most recent call last file home/nvidia/git/tensorflow/workspace line tf_workspace file home/nvidia/git/tensorflow/tensorflow/workspace.bzl line in tf_workspace check_version file home/nvidia/git/tensorflow/tensorflow/workspace.bzl line in check_version fail(\ncurrent bazel version is current bazel version is non-git expected at least error error evaluating workspace fileerror skipping tensorflow/tools/pip_package:build_pip_package error loading package external package external contains errorswarning target pattern parsing failed.error error loading package external package external contains errorsinfo elapsed time sfailed build did not complete successfully packages loaded
293312351,16629,https://api.github.com/repos/tensorflow/tensorflow/issues/16629,sleighsoft,0,0,0,0,0,1,tensorflow version pip)i am interested in the runtime of ops when using the tf.data.dataset api.information like how long shuffle repeat or batch took.when running the code below and visualizing it in tensoboard the intersting ops are marked as unused substructure.is my approach the right one?i find it a little confusing that they show up but are not traceable.example code pythonimport numpy as npimport tensorflow as tftf.set_random_seed()data np.array astype(np.float)with tf.graph().as_default dataset tf.data.dataset.from_tensor_slices(data sess tf.session dataset dataset.cache dataset dataset.shuffle seed dataset dataset.repeat dataset dataset.batch iterator tf.data.iterator.from_structure output_types=tf.float batch iterator.get_next init iterator.make_initializer(dataset sess.run(init writer tf.summary.filewriter(tmp/datasettest sess.graph run_options tf.runoptions(trace_level=tf.runoptions.full_trace i while true try run_metadata tf.runmetadata sess.run(batch options=run_options run_metadata=run_metadata writer.add_run_metadata(run_metadata step{}.format(i i except tf.errors.outofrangeerror as ex break except exception as ex raise ex this will result in the following visualization in tensorboard.! image
293141437,16622,https://api.github.com/repos/tensorflow/tensorflow/issues/16622,Tracy6465,1,0,0,0,0,0,i update tensorflow to and reinstall cuda to now i run my program get the error:traceback most recent call last file home/chris/tensorflowdemo/__wordvec.py line in module import tensorflow as tf file home/chris/.local/lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import file home/chris/.local/lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file home/chris/.local/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file home/chris/.local/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file home/chris/.local/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file home/chris/.local/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow_internal fp pathname description file usr/lib/python./imp.py line in load_module return load_dynamic(name filename file file usr/lib/python./imp.py line in load_dynamic return load(spec)importerror libcublas.so cannot open shared object file no such file or directory
293075404,16617,https://api.github.com/repos/tensorflow/tensorflow/issues/16617,xinmei9322,1,0,0,0,0,0,hi i came across a problem when using eager execution with gpu system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version python cuda/cudnn version cuda gpu model and memory geforce gtx ti gb ram exact command to reproduce : pythonfrom future import divisionimport tensorflow as tfimport tensorflow.contrib.eager as tfetfe.enable_eager_execution(device_policy=tfe.device_placement_silent)with tf.device(/gpu a tf.ones b tf.ones c tf.tensordot(a b axes=) i get the error tensorflow.python.framework.errors_impl.notfounderror no registered listdiff opkernel for gpu devices compatible with node listdiff listdiff t=dt_int out_idx=dt_int (dummy_input dummy_input if i remove device_policy=tfe.device_placement_silent there is still an error tensorflow.python.framework.errors_impl.invalidargumenterror tensors on conflicting devices cannot compute cast as input was expected to be on job:localhost/replica:/task:/device:gpu but is actually on job:localhost/replica:/task:/device:cpu operation running on job:localhost/replica:/task:/device:gpu tensors can be copied explicitly using gpu or cpu or transparently copied by using tfe.enable_eager_execution(tfe.device_placement_silent copying tensors between devices may slow down your model op:cast name tensordot/cast
293014783,16611,https://api.github.com/repos/tensorflow/tensorflow/issues/16611,chesschi,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gacd python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory gtx gb chief and m gb slave exact command to reproduce :cuda_visible_devices python test.py job-name ps task-index cuda_visible_devices python test.py job-name worker task-index cuda_visible_devices python test.py job-name ps task-index cuda_visible_devices python test.py job-name worker task-index describe the problemi am trying to run the distributed training and use tf.train.monitoredtrainingsession with pcs pc has one gtx gb and is set as chief while pc has one m gb and is set as non-chief they are connected back-to-back without switch/router the chief worker was running okay but the non-chief worker was stuck at tf.train.monitoredtrainingsession and did not proceed to execute the code within the session source code logs import argparseimport tensorflow as tfdef parse_command parser argparse.argumentparser(description=monitor training session test parser.add_argument(--job-name dest=job_name default=worker nargs help=job name worker|ps parser.add_argument(--task-index dest=task_index type=int default help=task index return parser.parse_args()if name main print(test started cluster ps worker options parse_command cluster_spec tf.train.clusterspec(cluster server tf.train.server(server_or_cluster_def=cluster_spec job_name=options.job_name task_index=options.task_index if options.job_name ps server.join sys.exit is_chief options.task_index config tf.configproto config.gpu_options.allow_growth true config.log_device_placement true step_size print(running options.job_name str(options.task_index with tf.device(tf.train.replica_device_setter worker_device job:worker/task:%d options.task_index cluster cluster_spec global_step tf.train.get_or_create_global_step learning_rate tf.train.exponential_decay global_step step_size staircase=true with tf.train.monitoredtrainingsession(master=server.target is_chief=is_chief as session print(monitoredtrainingsession started for i in range for j in range(step_size lr gstep session.run( learning_rate global_step print(learning rate str(lr global step str(gstep)) pc gtx logs ...running worker i tensorflow/core/distributed_runtime/master_session.cc start master session abbadd with config:monitoredtrainingsession startedlearning rate global step=learning rate global step=learning rate global step=... pc m logs i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost:running worker:(...wait for secs)monitoredtrainingsession session was not ready after waiting secs i was using the tf.train.syncreplicasoptimizer example to implement between-graph and synchronous training but found that the non-chief worker has never printed monitoredtrainingsession started then i slowly remove all the unnecessary code which becomes the code provided above and found that tr.train.monitoredtrainingsession does not seem to work for the bare minimum configuration please can you kindly have a look many thanks
292942889,16603,https://api.github.com/repos/tensorflow/tensorflow/issues/16603,majgis,1,0,0,0,0,0,any chance a rebuild of the py docker image is easy enough to pick up security patches it would save a team a lot of work thank you
292832754,16587,https://api.github.com/repos/tensorflow/tensorflow/issues/16587,feranick,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux os x tensorflow installed from source or binary source and binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory gtx gtx ti exact command to reproduce : sudo pip install hpy run python from there type: import tensorflow as tf describe the problema feature of hpy used in tf is deprecated in particular conversion of the second argument of issubdtype from float to np.floating is deprecated source code logswarning message futurewarning conversion of the second argument of issubdtype from float to np.floating is deprecated in future it will be treated as np.float np.dtype(float).type from conv import register_converters as register_converters
292710497,16576,https://api.github.com/repos/tensorflow/tensorflow/issues/16576,carlthome,6,0,0,0,0,0,im making an issue here because im sure this is being worked on somewhere in this huge repo and ive failed to find it by search.almost all models have hyperparameters that cannot be set by gradient descent number of layers for example these needs to be tuned preferably programatically with a smart strategy. whats the canonical way of doing hyperparameter tuning with the tf.estimator api? (also how can we do early stopping with tf.estimator?)im currently wrapping around scikit-optimize which is ok but then ill never be able to run parallel experiments across workers and its a bit tricky to know if the hyperparameters will lead to oom aside from using tf.profile. pythonimport osfrom skopt import gp_minimizefrom skopt.space import real categorical integerfrom skopt.utils import use_named_argslogdir tensorboard/space real name=learning_rate categorical( true false name=skip_connections integer name=layers) @use_named_args(space)def score( params model_dir os.path.join(logdir str(params estimator tf.estimator.estimator(model_fn model_dir params=params trainspec tf.estimator.trainspec(train_input_fn evalspec tf.estimator.evalspec(eval_input_fn try tf.estimator.train_and_evaluate(estimator trainspec evalspec metrics estimator.evaluate(test_input_fn return metrics loss except tf.errors.resourceexhaustederror tf.train.nanlossduringtrainingerror return egp_minimize(score space
292458835,16545,https://api.github.com/repos/tensorflow/tensorflow/issues/16545,LucasMahieu,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux centos tensorflow installed from source or binary with pip tensorflow version use command below vs python version bazel version if compiling from source no gcc/compiler version if compiling from source no cuda/cudnn version no gpu model and memory no exact command to reproduce :clone the tensorflow/models repo.then cd models/research/slim/python export_inference_graph.py model_name=inception_v image_size output_file=/tmp/inception_v.pb then freeze the graph cd tensorflow/tensorflow/python/tools/python freeze_graph.py input_graph tmp/inception_v.pb input_checkpoint tmp/inception_v.ckpt output_graph tmp/inception_v_frozen.pb input_binary true output_node_names inceptionv/logits/predictions/reshape_ then in python do import tensorflow as tffrom tensorflow.core.framework import graph_pbfrom tensorflow.python.platform import gfileimport tensorflow.tools.graph_transforms as graph_transformsgraph graph_pb.graphdef()with open(/tmp/inception_v_frozen.pb rb as f s f.read graph.parsefromstring(s)graph graph_transforms.transformgraph(graph input inputs nodes inceptionv/logits/predictions/reshape outputs nodes fold_constants() )with gfile.fastgfile(/tmp/inception_v_frozen+_optimized.pbtxt w as f f.write(str(graph describe the problemi am using the graph_transform to fold constants in by graph saved as pb.when i use the fold_constants transformation some inputs of some nodes are renamed but not the corresponding nodes in the whole graph so the graph is no longer valid i have an input placeholder in the graph.and the node connected to this placeholder as an input name input instead of input.with the version of tensorflow i didnt have this issue.to reproduce follow the instructions below and take a look to the tmp/inception_v_frozen_optimized.pbtxt graph and search le node named input it is the input placeholder then search node which has an input named input this name input is node a node of the graph
292227744,16523,https://api.github.com/repos/tensorflow/tensorflow/issues/16523,nikhilk,2,0,0,0,0,0,system information have i written custom code no os platform and distribution linux ubuntu tensorflow installed from binary tensorflow version python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a feature requestcurrently tf.print produces info logs.if log level is set to get rid of info logs given the noise level it renders tf.print as non-functional.it would be nice to have tf.print work either by making it actually work like print i.e it is not a mechanism to generate logs or if it should remain that way then have it accept a log level optional parameter
292194117,16513,https://api.github.com/repos/tensorflow/tensorflow/issues/16513,Paula15,1,0,0,0,0,0,after upgrading to tf when i import tensorflow it raises: importerror libcublas.so cannot open shared object file no such file or directory system ubuntu lts bit python tensorflow tensorflow-gpu gpu geforce gtx titan cuda
292148753,16504,https://api.github.com/repos/tensorflow/tensorflow/issues/16504,fjanoos,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu ubuntu wheezy tensorflow installed from source or binary) :pip tensorflow version use command below) :tf.version tf.git_version v..--gtf.compiler_version v..--gsanity check array dtype=int python version describe the problemive found a few issues when trying to propagate gradients through tf.while_loops.one issue is ops like this inside the loop body break gradients k tf.print k k eta loss w_n chg_w g_inf eg k eta loss(w chg_w g_inf but more concerningly conjoined conditions such as this: tf.logical_and k max_its chg_w tol or even this tf.cast max_its-k dtype chg_w tol) breaks the differentiablity across the while loop source code logs pythontf.reset_default_graph()sess tf.interactivesession()g tf.graph().as_default()max_its tol e-c tf.constant np.arange dtype=dtype)w tf.variable initial_value=np.ones dtype=dtype)/k tf.variable dtype=tf.int chg_w tf.constant np.inf dtype=dtype def eg_step k w chg_w grad tf.gradients tf.reduce_sum w c w w_n w tf.exp grad w_n w_n tf.reduce_sum w_n chg_w tf.reduce_sum tf.abs w_n w tf.reduce_sum tf.abs w k k this busts the differentiablity k tf.print k k eta loss w_n chg_w g_inf eg k eta loss(w chg_w g_inf return k w_n chg_wdef continue_cond k w chg_w args note either of this conjoined conditions tf.logical_and k max_its chg_w tol or tf.cast max_its-k dtype chg_w tol do no propagate gradients correctly return k max_its tf.logical_and k max_its chg_w tol k w chg_w tf.while_loop cond=_continue_cond body=_eg_step loop_vars= k w chg_w name=while_loop parallel_iterations see if the gradient is propagatedtf.gradients w c
292114390,16488,https://api.github.com/repos/tensorflow/tensorflow/issues/16488,albertz,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu travis trusty ubuntu tensorflow installed from source or binary binary pip tensorflow version use command below latest pip i guess python version bazel version if compiling from source gcc/compiler version if compiling from source gcc cuda/cudnn version gpu model and memory exact command to reproduce describe the problem python c import tensorflow as tf print(tf.git_version tf.version)runtimeerror module compiled against api version xc but this version of numpy is xbimporterror numpy.core.multiarray failed to importimporterror numpy.core.umath failed to importimporterror numpy.core.umath failed to import f tensorflow/python/lib/core/bfloat.cc check failed pybfloat_type.tp_base nullptr home/travis/.travis/job_stages line aborted core dumped python c import tensorflow as tf print(tf.git_version tf.version source code logssee the travis log
292102474,16481,https://api.github.com/repos/tensorflow/tensorflow/issues/16481,eaplatanios,1,0,0,0,0,0,"hi,i upgraded from rc to the current master branch and i started receiving the following error w tensorflow/core/framework/op_kernel.cc op_requires failed at lookup_table_op.cc not found container localhost does not exist could not find resource localhost/hash_table_/users/anthony/development/github/symphony-mt/temp/data/iwslt-/vocab.vi_whole_line_line_number w tensorflow/core/framework/op_kernel.cc op_requires failed at iterator_ops.cc not found container localhost does not exist could not find resource localhost/hash_table_/users/anthony/development/github/symphony-mt/temp/data/iwslt-/vocab.vi_whole_line_line_number node lookup_/lookuptablefind lookuptablefindv tin=dt_string tout=dt_int (lookup__placeholder input lookup__placeholder_) exception in thread main org.platanios.tensorflow.jni.notfoundexception container localhost does not exist could not find resource localhost/hash_table_/users/anthony/development/github/symphony-mt/temp/data/iwslt-/vocab.vi_whole_line_line_number node lookup_/lookuptablefind lookuptablefindv tin=dt_string tout=dt_int (lookup__placeholder input lookup__placeholder node model/model/iterator/next iteratorgetnext output_shapes output_types= dt_int dt_int dt_int dt_int dt_int device=/job:localhost/replica:/task:/device:cpu: (model/model/iterator) its hard to reproduce this error but a summary of the context is that i have a lookup table op inside a dataset map operator and i get this error when i try to execute the corresponding iterator getnext op im looking for information in how to parse and debug this error i never explicitly set any containers for my variables or lookup tables i.e leave them to the default value an empty string were there any changes introduced recently that could result in this error note that this happens with my scala api but not with the python api and so it may be that i havent updated something in my code i just dont really know where to look for this.thanks"
292081878,16479,https://api.github.com/repos/tensorflow/tensorflow/issues/16479,tutan0558,6,0,0,0,0,0,i installed and tying to import tensorflow but it said that cannot find cudart_.dll then i installed the cuda and then everything works fine so i want to make sure than does not support cuda or i have something installed wrong
291987395,16465,https://api.github.com/repos/tensorflow/tensorflow/issues/16465,memo,4,0,0,0,0,0,system information based on example linux ubuntu installed from binary v..--gacd also python anaconda custom bit default oct ipython cuda release v cudnn also cuda release v cudnn geforce gtx m also gtx driver version describe the problema when i create frames from a signal with frame_length and frame_step i.e hop size overlap using a hann window also tried hamming and then i reconstruct with overlap_and_add id expect the signal to be reconstructed correctly because of cola etc but instead it comes out exactly double the amplitude i need to divide the resulting signal by two for it to be correct b if i use stft to create a series of overlapping spectrograms and then reconstruct with inverse stft again with frame_length and frame_step the signal is again reconstructed at double amplitude i realise why these might be the case unity gain at overlap for hann so overlap will double the signal but is it not normal for the reconstruction function to take this into account e.g librosa istft does return signal with correct amplitude while tensorflow returns double.c at any other frame_step there is severe amplitude modulation going on see images below this doesnt seem right at all update if i explicitly set window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step in inverse_stft the output is correct so it seems the frame_step parameter in inverse_stft is not being passed into the window function which is also what the results hint at source code logsoriginal data orig output from frames overlap_and_add:! tensorflow frame l s frame l s frame l s frame l s output from stft+istft:! tensorflow stft l s stft l s stft l s stft l s output from stft+istft:! librosa stft l s stft l s stft l s stft l s code from future import print_function from future import division import numpy as np import scipy.io.wavfile import math import random import matplotlib.pyplot as plt import tensorflow as tf out_prefix tensorflow def plot(data title do_save=true plt.figure(figsize plt.plot(data :*frame_length plt.ylim plt.title(title plt.grid if do_save plt.savefig(title png plt.show def reconstruct_from_frames(x frame_length frame_step name frame frames_t tf.contrib.signal.frame(x frame_length=frame_length frame_step=frame_step windowed_frames_t frames_t tf.contrib.signal.hann_window(frame_length periodic=true output_t tf.contrib.signal.overlap_and_add(windowed_frames_t frame_step=frame_step return name output_t def reconstruct_from_stft(x frame_length frame_step name stft spectrograms_t tf.contrib.signal.stft(x frame_length frame_step output_t tf.contrib.signal.inverse_stft(spectrograms_t frame_length frame_step return name output_t def test(fn input_data print tf.reset_default_graph input_t tf.placeholder(tf.float none name output_t fn(input_t frame_length frame_step title l{}.s{}.format(out_prefix sample_rate name frame_length frame_step print(title with tf.session output_data output_t.eval({input_t:input_data output_data frame_length/frame_step tensorflow needs this to normalise amp plot(output_data title scipy.io.wavfile.write(title+.wav sample_rate output_data def generate_data(duration_secs sample_rate num_sin min_freq max_freq rnd_seed max_val generate signal from multiple random sin waves if rnd_seed random.seed(rnd_seed data np.zeros( duration_secs*sample_rate np.float for i in range(num_sin w np.float(np.sin(np.linspace math.pi random.randrange(min_freq max_freq num=duration_secs*sample_rate data random.random w if max_val data max_val np.max(np.abs(data return data frame_length sample_rate input_data generate_data(duration_secs sample_rate=sample_rate num_sin rnd_seed max_val title orig.format(sample_rate plot(input_data title scipy.io.wavfile.write(title+.wav sample_rate input_data for frame_step in test(reconstruct_from_frames input_data test(reconstruct_from_stft input_data print(done.)librosa code from future import print_function from future import division import numpy as np import scipy.io.wavfile import math import random import matplotlib.pyplot as plt import librosa.core as lc out_prefix librosa def plot(data title do_save=true plt.figure(figsize plt.plot(data :*frame_length plt.ylim plt.title(title plt.grid if do_save plt.savefig(title png plt.show def reconstruct_from_stft(x frame_length frame_step name stft stft lc.stft(x n_fft=frame_length hop_length=frame_step istft lc.istft(stft frame_step return name istft def test(fn input_data print name output_data fn(input_data frame_length frame_step title l{}.s{}.format(out_prefix sample_rate name frame_length frame_step print(title output_data frame_length/frame_step tensorflow needs this to normalise amp plot(output_data title scipy.io.wavfile.write(title+.wav sample_rate output_data def generate_data(duration_secs sample_rate num_sin min_freq max_freq rnd_seed max_val generate signal from multiple random sin waves if rnd_seed random.seed(rnd_seed data np.zeros( duration_secs*sample_rate np.float for i in range(num_sin w np.float(np.sin(np.linspace math.pi random.randrange(min_freq max_freq num=duration_secs*sample_rate data random.random w if max_val data max_val np.max(np.abs(data return data frame_length sample_rate input_data generate_data(duration_secs sample_rate=sample_rate num_sin rnd_seed max_val title orig.format(sample_rate plot(input_data title scipy.io.wavfile.write(title+.wav sample_rate input_data for frame_step in test(reconstruct_from_stft input_data print(done
291863532,16452,https://api.github.com/repos/tensorflow/tensorflow/issues/16452,RomRoc,2,0,0,0,0,0,hello is there any plan to include semantic segmentation api in future tensorflow releases similar to tensorflow object detection api there are other semantic segmentation repositories in github an awesome list is here but i would like to see the implementation from tensorflow organization.thanks
291781993,16431,https://api.github.com/repos/tensorflow/tensorflow/issues/16431,chenzhiwo,1,0,0,1,0,0,now we can cross compiling or native compiling libtensorflow-lite.a for rpi fix string does not name a type error by adding its namespace remove unnecessary space between cc_prefix and gcc adding o dndebug cflags same as cxxflags remove redundant lpthread link flag add makefile for rpi
291750550,16424,https://api.github.com/repos/tensorflow/tensorflow/issues/16424,jmsdnns,7,0,0,0,0,0,bleach came out nov th and this is old enough to cause dependency issues for projects that stayed up to date with bleach.in particular this causes issues for jupyter users
291459669,16391,https://api.github.com/repos/tensorflow/tensorflow/issues/16391,ankitvgupta,1,0,0,0,0,0,addresses the issue at
291446478,16390,https://api.github.com/repos/tensorflow/tensorflow/issues/16390,orshi,1,0,0,0,0,0,in line of quantize_graph.py pythondef quantize_nodes_recursively(self current_node the entry point for quantizing nodes to eight bit and back if self.already_visited current_node.name return the initial value of dic self.already_visited is empty so the statement pythonif self.already_visited current_node.name : may raise exception.so i think it would be better to update to pythonif self.already_visited.has_key(current_node.name same issue also in pythondef round_nodes_recursively(self current_node) please take a look thanks
291402215,16381,https://api.github.com/repos/tensorflow/tensorflow/issues/16381,mas-dse-greina,2,0,0,0,0,0,it seems like there is an inherent assumption in distributed tensorflow that all nodes must share a common file system such as google cloud or nfs ive found in testing that models will train just fine without a common file system but the final trained model doesnt save properly when you try something like builder tf.saved_model_builder.savedmodelbuilder(export_dir builder.save() the issue seems to be that the parameter server has the variables and the chief node has the graph itd be great if tensorflow added a function to allow us to consolidate the graph and variables at the end of training onto the chief node in order to save the trained model right now there doesnt seem to be an easy way to do this.thanks
291329984,16374,https://api.github.com/repos/tensorflow/tensorflow/issues/16374,Azureum,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows build and windows tensorflow installed from source or binary binary tensorflow version use command below rc and tf-nightly dev python version and bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory nvidia gt m gb exact command to reproduce toco help describe the problemi am trying to run the codelab tutorial of tensorflow lite after installing tf-nightly when i try to run the command toco help i get the error modulenotfounderror no module named tensorflow.contrib.lite.toco.python.i have tried this on computers all windows and the same problem persists source code logsc:\users\hp\downloads>toco helptraceback most recent call last file c:\programdata\anaconda\lib\runpy.py line in run_module_as_main main mod_spec file c:\programdata\anaconda\lib\runpy.py line in run_code exec(code run_globals file c:\programdata\anaconda\scripts\toco.exe\__main__.py line in module>modulenotfounderror no module named tensorflow.contrib.lite.toco.python
291200010,16363,https://api.github.com/repos/tensorflow/tensorflow/issues/16363,sleighsoft,2,0,0,0,0,0,python tensorflow rc pipos windows no cuda just cpui get this when creating a lookup table with tf.contrib.lookup.index_table_from_file .as i have multiple graphs i create that table in each graph from the same file i need it in this results in the warning from the title of this issue are tables shared across graphs how to figure out if a table from a specific file is already existing/initialized?this also occurs with tensorflow/nmt
291068466,16348,https://api.github.com/repos/tensorflow/tensorflow/issues/16348,Anjum48,1,0,0,0,0,0,the release.md states that prebuilt binaries are now built against cuda and cudnn says that cuda is not supported.could we change the release.md so that it says prebuilt binaries are now built against cuda and cudnn until later versions are supported
290735786,16315,https://api.github.com/repos/tensorflow/tensorflow/issues/16315,xldrx,1,0,0,0,0,0,"i have a cluster of long-lived tensorflow servers tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server).my problem is how to reset variables on these server there is a behavior in distributed tensorflow in which a variable defined on a worker e.g ps outlives the session which defines it i understand this behavior is intentional to support between graph model-replica.however in my use case this behavior causes unexpected problem i have not found a mechanism to override this it there is i believe it is helpful to better reflect it in the documentation if there is not i hope i can make a case to motivate its existence.in my use case different training jobs are ran sequentially i.e one training job at a time on this cluster each using one client which connects to only one master the problem i have is if a variable is defined in two training job with a same name but different shape sizes the latter client gets the following error on session creation: invalidargumenterror see above for traceback assign requires shapes of both tensors to match lhs shape rhs shape a node a/assign assign t=dt_float class= loc:@a use_locking=true validate_shape=true device=/job:worker/replica:/task:/device:gpu: (a a/initializer/random_uniform) tf.reset_default_graph does not help the solution to this problem could be a mechanism similar tf.reset_default_graph that resets variables in all the workers.to replicate this problem let say we have two workers one ps and on worker)worker bash#worker bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server cluster_spec=ps|localhost:,worker|localhost job_name=worker task_id worker bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server cluster_spec=ps|localhost:,worker|localhost job_name=worker task_id= (same result with tf.train.server workers.)then run the simple code: pythonimport tensorflow as tfvar tf.get_variable(a shape=(,))with tf.train.monitoredtrainingsession(master=localhost as sess pass it should work just fine.then when this code which is identical except the variable shape is ran: pythonimport tensorflow as tfvar tf.get_variable(a shape=(,))with tf.train.monitoredtrainingsession(master=localhost as sess pass this example fails"
290515004,16291,https://api.github.com/repos/tensorflow/tensorflow/issues/16291,mattdingmeng,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes session_options.config.mutable_gpu_options()->set_visible_device_list();session->reset(tensorflow::newsession(session_options error in this linestatus session_create_status session)->create(graph_def os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :cuda cudnn gpu model and memory :quadro p g gpu memory exact command to reproduce describe the problemi built the tensorflow c api from the source using bazel when i link the shared library libtensorflow_cc.so my code works fine i do not need to link libtensorflow_framework.so since i used config=monolithic when i build tensorflow using bazel.)however i want to specify the gpu device in my code using this function to set gpu options:session_options.config.mutable_gpu_options()->set_visible_device_list();session->reset(tensorflow::newsession(session_options error in this line source code logslogs i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name quadro p major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc adding visible gpu device i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name quadro p pci bus id compute capability e tensorflow/core/common_runtime/gpu/process_state.cc invalid allocator type segmentation fault core dumped)source codes reads a model graph definition from disk and creates a session object you can use to run it.status loadgraph(const string graph_file_name std::unique_ptrvisible_device_list std::cout<<list gpu done<set_visible_device_list std::cout<<gpu assign is done<reset(tensorflow::newsession(session_options std::cout<<new session is created std::endl status session_create_status session)->create(graph_def std::cout<<graph is loaded std::endl if session_create_status.ok return session_create_status return status::ok
290177222,16257,https://api.github.com/repos/tensorflow/tensorflow/issues/16257,GrifisJP,1,0,0,0,0,0,"i am hoping that tf.contrib.distributions module is expanded so that we can calculate kl divergence between multivariate gaussian mixture models(gmm with its paramter list such as weight mean covariance given as tensor array because i think there is going to be a more need for that for many applications thank you.with current version either we can calculate kl divergence for a single gauss or create gmm object but not kl for gmm tried as shown below but it didnt work import tensorflow as tf print(tensorflow tf.__version for python import numpy as np import matplotlib.pyplot as plt ds tf.contrib.distributions kl_divergence=tf.contrib.distributions.kl_divergence gaussian mixure mix weight bimix_gauss ds.mixture cat=ds.categorical(probs= mix mix ),#weight components ds.normal(loc scale ds.normal(loc scale gaussian mixture mix weight bimix_gauss ds.mixture cat=ds.categorical(probs= mix mix ),#weight components ds.normal(loc scale ds.normal(loc scale kl between gm and gm kl_value=kl_divergence distribution_a=bimix_gauss distribution_b=bimix_gauss allow_nan_stats=true name=none sess tf.session with sess.as_default x tf.linspace int(e)).eval plt.plot(x bimix_gauss.prob(x).eval(),r plt.plot(x bimix_gauss.prob(x).eval(),b plt.show print(kl_value=,kl_value.eval()) then i got this error notimplementederror no kl(distribution_a distribution_b registered for distribution_a type mixture and distribution_b type mixture i know that with python sklearn without tensorflow we can calculate kl for gmm as shown below system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory exact command to reproduce"
290134557,16253,https://api.github.com/repos/tensorflow/tensorflow/issues/16253,samikama,5,0,0,1,0,0,this pr introduces a new op that wraps around an highly optimized tensorrt engine and provides a seamless integration between tensorrt and tensorflow add a trtengineop that encapsulates a tensorrt executable add createinferencegraph to contract a tensorrt-compilable subgraph to a trtengineop update build files to include new contrib package add tensorflow.contrib.tensorrt python package to expose api to python
290045864,16246,https://api.github.com/repos/tensorflow/tensorflow/issues/16246,pfc,3,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu arch tensorflow installed from source or binary git python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version exact command to reproduce :./configurebazel build config=opt config=cuda jobs tensorflow/tools/pip_package:build_pip_package describe the problemfailed to build source code logs usr/lib/gcc/x_-pc-linux-gnu/../include/c++/tuple error mismatched argument pack lengths while expanding std::is_convertible<_uelements elements return and_...>::value usr/lib/gcc/x_-pc-linux-gnu/../include/c++/tuple error body of constexpr function static constexpr bool std::_tc<::_implicitlymoveconvertibletuple with uelements const std::tuple
289194443,16187,https://api.github.com/repos/tensorflow/tensorflow/issues/16187,JesperChristensen89,2,0,0,0,0,0,i am trying to deploy the pretrained faster-rcnn inception v from the object detection api on a jetson tx i am running cuda cudnn and have tested with both tf and in a jupyter notebook environment when i monitor the gpu memory it starts out by having gb free and when launching these fills up immediately when i run on my gtx gb gpu i have effectively the same amount of memory free but are having no issues running.smaller models as ssd mobilenet runs without problems.from tests performed today i can supply the following dumps.jupyter notebook terminal output i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc arm does not support numa returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name nvidia tegra x major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc adding visible gpu device i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device job:localhost/replica:/task:/device:gpu with mb memory physical gpu device name nvidia tegra x pci bus id compute capability e tensorflow/stream_executor/cuda/cuda_driver.cc failed to synchronize the stop event cuda_error_launch_failed e tensorflow/stream_executor/cuda/cuda_timer.cc internal error destroying cuda event in context xfb cuda_error_launch_failed e tensorflow/stream_executor/cuda/cuda_timer.cc internal error destroying cuda event in context xfb cuda_error_launch_failed e tensorflow/stream_executor/cuda/cuda_dnn.cc failed to enqueue convolution on stream cudnn_status_execution_failed e tensorflow/stream_executor/event.cc error destroying cuda event in context xfb cuda_error_launch_failed e tensorflow/stream_executor/event.cc error destroying cuda event in context xfb cuda_error_launch_failed e tensorflow/stream_executor/event.cc error destroying cuda event in context xfb cuda_error_launch_failed e tensorflow/stream_executor/event.cc error destroying cuda event in context xfb cuda_error_launch_failed e tensorflow/stream_executor/event.cc error destroying cuda event in context xfb cuda_error_launch_failed error dump from printout inside the notebook: exception in thread thread-:traceback most recent call last file usr/lib/python./threading.py line in bootstrap_inner self.run file usr/lib/python./threading.py line in run self.__target(*self.__args self.__kwargs file ipython-input--acdd line in worker im t_elapsed detect_objects(frame_rgb sess detection_graph file ipython-input--cdae line in detect_objects feed_dict={image_tensor image_np_expanded file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)internalerror cudnn launch failure input shape filter shape node firststagefeatureextractor/inceptionv/inceptionv/convd_c_x/convd convd t=dt_float data_format=nhwc dilations padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/device:gpu: (firststagefeatureextractor/inceptionv/inceptionv/convd_b_x/relu firststagefeatureextractor/inceptionv/convd_c_x/weights/read/___cf node batchmulticlassnonmaxsuppression/map/while/multiclassnonmaxsuppression/sortbyfield/equal recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge_...ield/equal tensor_type=dt_bool device=/job:localhost/replica:/task:/device:cpu: (^_cloopbatchmulticlassnonmaxsuppression/map/while/multiclassnonmaxsuppression/non_max_suppression/iou_threshold/_) caused by op ufirststagefeatureextractor/inceptionv/inceptionv/convd_c_x/convd defined at file usr/lib/python./threading.py line in bootstrap self.__bootstrap_inner file usr/lib/python./threading.py line in bootstrap_inner self.run file usr/lib/python./threading.py line in run self.__target(*self.__args self.__kwargs file ipython-input--acdd line in worker tf.import_graph_def(od_graph_def name file usr/local/lib/python./dist-packages/tensorflow/python/util/deprecation.py line in new_func return func(*args kwargs file usr/local/lib/python./dist-packages/tensorflow/python/framework/importer.py line in import_graph_def op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinternalerror see above for traceback cudnn launch failure input shape filter shape node firststagefeatureextractor/inceptionv/inceptionv/convd_c_x/convd convd t=dt_float data_format=nhwc dilations padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/device:gpu: (firststagefeatureextractor/inceptionv/inceptionv/convd_b_x/relu firststagefeatureextractor/inceptionv/convd_c_x/weights/read/___cf node batchmulticlassnonmaxsuppression/map/while/multiclassnonmaxsuppression/sortbyfield/equal recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:cpu send_device=/job:localhost/replica:/task:/device:gpu send_device_incarnation tensor_name=edge_...ield/equal tensor_type=dt_bool device=/job:localhost/replica:/task:/device:cpu: (^_cloopbatchmulticlassnonmaxsuppression/map/while/multiclassnonmaxsuppression/non_max_suppression/iou_threshold/_) output of tegrastats at the point of error: ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd ram mb lfb xmb cpu emc ape grd as you can see the ram are nowhere near full at the moment of the error.can anybody suggest a solution to this
289171688,16182,https://api.github.com/repos/tensorflow/tensorflow/issues/16182,civilman628,14,0,0,0,0,0,i am very confused what are the relationships between tf.slim tf high level api and keras i just want to know which one has the long term evolution fragmentation like android os is a very bad and dangerous thing at least for me i am not comfortable with tf.slim at all why tf cannot have a unified and standardized api the benefits are so obvious it should not become different political parties fight each other
288803715,16147,https://api.github.com/repos/tensorflow/tensorflow/issues/16147,louisquinn,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source and virtual env problem doesnt change tensorflow version use command below rc makes no difference on python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version cuda gpu model and memory v gb exact command to reproduce describe the problem inference using the v is very slow for example performing object detection with ssd mobilenet is achieving a max frame rate of compared to on a gtx initialization with a warm up image is extremely slow up to mins for the first image i have tried model quantization using graph_transforms/transform_graph in an attempt to use the fp mode and various combinations of cuda cudnn and tensorflow versions with no difference is there some recommended environment setup for the v?i am successfully running darknet with a massive increase of speed
288802244,16146,https://api.github.com/repos/tensorflow/tensorflow/issues/16146,gcampax,2,0,0,0,0,0,describe the problemthe documentation for tf.contrib.rnn.gridlstmcell cites the paper grid long short-term memory by kalchbrenner et al.the paper describes an architecture called the d grid lstm to replace a stack of lstm cells in a d grid lstm state components are passed from one layer to the next vertically.in tensorflow rnn parlance one would expect both the state and the output of the cell to be an lstmstatetuple which would allow seamless integration with a multirnncell.in the current implementation instead it appears that the vertical unrolling is done internally to the gridlstmcell.i say it appears because i cant quite make sense of the arguments and their documentation specifically there is a required num_frequency_block argument whose meaning is quite obscure.looking at the implementation also did not help me understand what value is actually expected in that parameter and the related parameters.note that the above mentioned paper does not talk about frequencies anywhere.would it be possible to expand on the documentation for the cell as well as provide a code example on how to replicate the d grid lstm from the paper
288494711,16123,https://api.github.com/repos/tensorflow/tensorflow/issues/16123,facaiy,0,0,0,0,0,1,resolve implementationthe pr proposes a slide method for dataset groups elements in fixed size blocks by passing a sliding window over dataset it behaves like batch in fact batch(n slide(n n) .~i failed to move c implementation from core to contrib any help will be appreciated how to test x add test case x pass all tests
288348776,16104,https://api.github.com/repos/tensorflow/tensorflow/issues/16104,svenstaro,2,0,0,0,0,0,describe the problemcurrently its impossible to use the system-installed protobuf library because the tensorflow build always uses the protobuf_archive version there should be an option to use the one installed in the system.background i package tensorflow for arch linux and we run into symbol conflicts if a user wants to use protobuf and tensorflow together in a binary because tensorflows protobuf symbols conflict with the one installed in the system already.original arch bug report
288192039,16076,https://api.github.com/repos/tensorflow/tensorflow/issues/16076,mxh,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes see below os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary from binary with pip tensorflow version use command below v python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory ti exact command to reproduce python mwe.py describe the problem you use tf.test.testcase.test_session in setup there will be one skipped test for which setup is run but not teardown i believe this has to do with tf.testcase.test_session trying to take care of automatic test discovery on the line above but im not sure source code logsin the following mwe mwe.py after running this test there will be a tmp.txt file left in the current directory.mwe import tensorflow as tf import os import unittest class footest(tf.test.testcase def setup(self with open(tmp.txt w as f f.write(hello with self.test_session as sess pass def teardown(self os.unlink(tmp.txt def testexample(self self.assertequal if name__==__main unittest.main
288090352,16069,https://api.github.com/repos/tensorflow/tensorflow/issues/16069,vhagier,1,0,0,0,0,0,im using python win bit and tensorflow and now im working on project and part export model im taking this error:notfounderror see above for traceback key generator/encoder_/conv/filter not found in checkpoint how can i solve this problem what i run c:\users\hajum>python c:\users\hajum\desktop\faceface-demo-master\reduce_model.py model-input c:\users\hajum\desktop\faceface-model model-output c:\users\hajum\desktop\faceface-reduced-model same folder names with project but i have my own modelswhat it shows w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/decoder_/deconv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/offset not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/batchnorm/scale not found in checkpoint w c:\l\tensorflow_\work\tensorflow-..\tensorflow\core\framework\op_kernel.cc not found key generator/encoder_/conv/filter not found in checkpointtraceback most recent call last file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_call return fn(*args file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run_fn status run_metadata file c:\users\hajum\anaconda\lib\contextlib.py line in exit next(self.gen file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\framework\errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.notfounderror key generator/encoder_/conv/filter not found in checkpoint node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/cpu: (_arg_save/const save/restorev_/tensor_names save/restorev_/shape_and_slices) during handling of the above exception another exception occurred:traceback most recent call last file c:\users\hajum\desktop\faceface-demo-master\reduce_model.py line in module saver.restore(sess checkpoint file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\training\saver.py line in restore self.saver_def.filename_tensor_name save_path file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run run_metadata_ptr file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run feed_dict_string options run_metadata file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_run target_list options run_metadata file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.notfounderror key generator/encoder_/conv/filter not found in checkpoint node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/cpu: (_arg_save/const save/restorev_/tensor_names save/restorev_/shape_and_slices) caused by op save/restorev defined at file c:\users\hajum\desktop\faceface-demo-master\reduce_model.py line in module saver tf.train.saver file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\training\saver.py line in init self.build file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\training\saver.py line in build restore_sequentially=self._restore_sequentially file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\training\saver.py line in build restore_sequentially reshape file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\training\saver.py line in addrestoreops tensors self.restore_op(filename_tensor saveable preferred_shard file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\training\saver.py line in restore_op spec.tensor.dtype file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\ops\gen_io_ops.py line in restore_v dtypes=dtypes name=name file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\framework\op_def_library.py line in apply_op op_def=op_def file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in create_op original_op=self._default_original_op op_def=op_def file c:\users\hajum\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in init self._traceback extract_stack()notfounderror see above for traceback key generator/encoder_/conv/filter not found in checkpoint node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/cpu: (_arg_save/const save/restorev_/tensor_names save/restorev_/shape_and_slices
287981881,16063,https://api.github.com/repos/tensorflow/tensorflow/issues/16063,JGuillaumin,1,0,0,0,0,0,hi im trying to build tensorflow for rasperrypi from my computer ubuntu when im running tensorflow/tools/ci_builds/pi/build_raspberry_pi.sh from the docker container build from tensorflow/tools/ci_builds/dockerfile.pi-python some compilation commands from bazel build failed my goal is to use tensorflow on my raspberrypi with a picamera with the c api compiled with bazel or makefile i started to run some code on my rpi but the compilation step is so long hours to compile/install tensorflow from sources with bazel i would like to save time by compiling my code on my laptop then sending it to the rpi for execution here my commands bash get tensorflow git clone cd tensorflow git checkout r.cd tensorflow/tools/ci_build this docker file was added with r. !docker build t tf_ci_buid/pi-py f dockerfile.pi-python cd run the docker imagedocker run it v pwd:/workspace w workspace tf_ci_buid/pi-py:latest then from the docker container run the sh code this script contains the bazel build command root@dbdff:/workspace tensorflow/tools/ci_build/pi/build_raspberry_pi.sh but during the execution of bazel build the compilation of code from external/highwayhash/highwayhash seems to fail see error message bellow)someone has already encountered this issue or its due to the new version r error message error root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/highwayhash/build c compilation of rule highwayhash//:sip_hash failed exit arm-linux-gnueabihf-gcc failed error executing command cd root/.cache/bazel/_bazel_root/eabdabedbdaffbe/execroot/org_tensorflow exec env path=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin pwd=/proc/self/cwd python_bin_path=/usr/bin/python python_lib_path=/usr/local/lib/python./dist-packages tf_need_cuda tf_need_opencl_sycl root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/arm-linux-gnueabihf-gcc u_fortify_source d_fortify_source fstack-protector draspberry_pi g o dndebug ffunction-sections fdata-sections dgemmlowp_allow_slow_scalar_fallback march=armv-a mfpu=neon-vfpv std=gnu ds_iread=s_irusr ds_iwrite=s_iwusr o u__gcc_have_sync_compare_and_swap u__gcc_have_sync_compare_and_swap u__gcc_have_sync_compare_and_swap funsafe-math-optimizations ftree-vectorize fomit-frame-pointer std=c isystem usr/include/arm-linux-gnueabihf isystem usr/include/python isystem usr/include md mf bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d frandom-seed=bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o iquote external/highwayhash iquote bazel-out/armeabi-opt/genfiles/external/highwayhash iquote external/bazel_tools iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools isystem external/bazel_tools/tools/cpp/gcc wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted no-canonical-prefixes fno-canonical-system-headers c external/highwayhash/highwayhash/sip_hash.cc o bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o)ccplus warning command line option std=gnu is valid for c/objc but not for c++in file included from root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/../../../../../arm-linux-gnueabihf/include/c++/../arm-linux-gnueabihf/bits/gthr-default.h from root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/../../../../../arm-linux-gnueabihf/include/c++/../arm-linux-gnueabihf/bits/gthr.h from root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/../../../../../arm-linux-gnueabihf/include/c++/../ext/atomicity.h from root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/../../../../../arm-linux-gnueabihf/include/c++/../memory from external/highwayhash/highwayhash/state_helpers.h from external/highwayhash/highwayhash/sip_hash.h from external/highwayhash/highwayhash/sip_hash.cc::/root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/../../../../../arm-linux-gnueabihf/include/c++/../ext/concurrence.h error pthread_spins was not declared in this scope gthread_mutex_t m_mutex gthread_mutex_init root/.cache/bazel/_bazel_root/eabdabedbdaffbe/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/../../../../../arm-linux-gnueabihf/include/c++/../ext/concurrence.h error pthread_spins was not declared in this scope gthread_recursive_mutex_t m_mutex gthread_recursive_mutex_init info elapsed time s critical path sfailed build did not complete successfully
287936599,16052,https://api.github.com/repos/tensorflow/tensorflow/issues/16052,ankitvgupta,6,0,0,0,0,0,hello i have really liked the new tf.data.dataset api and had a feature request i need to often make data transformations that require third-party libraries and use dataset.map along with a tf.py_func command as shown in the importing data tutorial in the process of doing this tensorflow is not able to infer the shape of the numpy arrays that are returned by the py_func-based functions and so the output_shapes attribute of the dataset returns something like tensorshape(none tensorshape(none tensorshape(none tensorshape(none tensorshape(none)) to address this i have been adding a new map function after that calls set_shape on each tensor to enforce the shape requirement for example i have code that looks something like this: dataset dataset.map(lambda strings labels tuple(tf.py_func(_featurize strs labels tf.int tf.float tf.int tf.int labels.dtype )))dataset dataset.map(_set_shapes) where def set_shapes(af pf split atp labels af.set_shape( none pf.set_shape( none split.set_shape( none atp.set_shape( none labels.set_shape( none return af pf split atp labels could this be simplified by adding a new tf.data.dataset member function called set_dataset_shape which essentially just implements the above set_shapes method system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary aws deep learning ami tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version aws deep learning ami-based gpu model and memory nvidia k exact command to reproduce n/a
287796396,16040,https://api.github.com/repos/tensorflow/tensorflow/issues/16040,eddy-ilg,0,1,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary) :pip install user tensorflow-gpu tensorflow version use command below python version bazel version if compiling from source) :na gcc/compiler version if compiling from source) :gcc ubuntu ubuntu cuda/cudnn version :cuda-..-cudnn-v gpu model and memory :any exact command to reproduce :see description belowyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemboolean tensors seem to have a bug in c i define an op with a boolean input tensor like so register_op(spatialaugmentationpossible input(input float output(output bool input(mirror bool input(angle float input(dx float input(dy float input(sx float input(sy float attr(crop_width int attr(crop_height int note that mirror is a boolean tensor the op compiles fine when calling like this op ops.spatial_augmentation_possible input=test_input mirror=tf.cast(mirror dtype=tf.bool angle=tf.convert_to_tensor(angle dtype=tf.float dx=tf.convert_to_tensor(dx dtype=tf.float dy=tf.convert_to_tensor(dy dtype=tf.float sx=tf.convert_to_tensor(sx dtype=tf.float sy=tf.convert_to_tensor(sy dtype=tf.float crop_width crop_height possible_tensor sess.run(op)i get f tensorflow/core/framework/tensor.cc check failed dtype expected_dtype vs i checked the proto corresponds to boolean and to float if i change the line with the mirror argument like this mirror=tf.cast(mirror dtype=tf.float),i get valueerror tensor conversion requested dtype bool for tensor with dtype float tensor(cast shape dtype=float device=/device:gpu:)that means now it is complaining that i dont input a bool but the message above is saying that i shouldnt input a bool for this reason there seems to be a bug best eddy"
287623951,16025,https://api.github.com/repos/tensorflow/tensorflow/issues/16025,nbro,1,0,0,0,0,0,system information no os platform and distribution mac os x v tensorflow installed from binary tensorflow version v..-rc--gabfcac dev python version python bazel version n/a gcc/compiler version n/a cuda/cudnn version n/a gpu model and memory intel iris pro mb exact command to reproduce n/a have i written custom code n/a describe the problemthe parameter max_to_keep of the saver class does not seem to have effect once a model and its training variables are restored in other words the first time i train my model the saver is keeping only max_to_keep checkpoints then i interrupt the training later when i resume it the number of checkpoints keeps going without any apparent limit related issues
287212480,15981,https://api.github.com/repos/tensorflow/tensorflow/issues/15981,blake-varden,2,0,0,0,0,0,os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below python version describe the problemwhen using the tf.estimator the summary files save out summaries for the loss variable evaluated every checkpoint the summary for the training is saved as loss i got this tensorboard by running the ciphar_estimator code located makes it difficult to compare the eval/train loss on the same graph in tensorboard what causes this naming issue and what can be done to fix it?thanks!! screen shot at pm
287151811,15978,https://api.github.com/repos/tensorflow/tensorflow/issues/15978,nbro,2,0,0,0,0,0,system informationnot necessary in this case describe the problemno documentation for the configproto class in the tf website specifically in neither of the following pages possible solutionsthe following article contains info about configproto either the docs for configproto can be written based on that info or at least a link to that article should be added to the configproto docs
287145709,15977,https://api.github.com/repos/tensorflow/tensorflow/issues/15977,tomrunia,1,0,0,0,0,0,"i am building a video input pipeline for deepminds kinetics dataset using tfrecord files since the dataset is large k videos my tfrecord files store the frames as compressed jpg images otherwise it would require too much space on disk each tf.train.example has the following structure: example num_frames tf.int label tf.int frames tf.string frames tf.string where all the frames store a jpg image as compressed bytes using tf.data.tfrecorddataset and tf.image.decode_jpg i am able to load the images and decode from jpg into tf.uint tensors full code can be found here decode(serialized_example prepare feature list read encoded jpg images as bytes features dict features class_label tf.fixedlenfeature tf.int for i in range features frames/{:d}.format(i tf.fixedlenfeature tf.string parse into tensors parsed_features tf.parse_single_example(serialized_example features decode the encoded jpg images images for i in range images.append(tf.image.decode_jpeg(parsed_features frames/{:d}.format(i pack the frames into one big tensor of shape n,h,w images tf.stack(images label tf.cast(parsed_features class_label tf.int return images label two things currently seem impossible with the current features of tfrecord files there seems to be no way to take a random sample of frames the code example now takes the first frames from the tfrecord but what is often preferred is taking a random sample of consecutive frames in one of my failed attempts i have tried to accomplish this along the lines of: num_frames tf.cast(parsed_features num_frames tf.int)offset tf.random_uniform(shape minval maxval=label dtype=tf.int the number of frames in the video example seems impossible to access in tensorflow it can be obtained using tf.train.example.fromstring as given here but that does not help me in this case if this was possible i could just load all the video frames into a tensor at increased cost and than use tf.random_crop to sample a random number of frames from the video my overall question is whether the input pipeline for videos using tfrecord files can be improved this needs to consider speed of reading data and compression options to limit file size for enormous datasets it would be convenient to directly use mp streams with tfrecord files however decoding this is problably much slower than decoding jpg images edit this pull request is related that there are many ways to setup the data pipeline for videos i have described some of them in this post on stackoverflow and motivated why i chose for tfrecord files this post also describes the problem described here so it may be informative i written custom code n/aos platform and distribution n/atensorflow installed from n/atensorflow version n/abazel version n/acuda/cudnn version n/agpu model and memory n/aexact command to reproduce n/a"
286816461,15953,https://api.github.com/repos/tensorflow/tensorflow/issues/15953,shakedel,2,0,0,0,0,0,feature request system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu n/a tensorflow installed from source or binary n/a tensorflow version use command below latest rc python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problem would be nice to be able to direct the string to a log file instead of stderr following the todo in the code
286567899,15927,https://api.github.com/repos/tensorflow/tensorflow/issues/15927,eladweiss,2,0,0,0,0,0,verbs implementation to use direct tensor writes copies motivation:following hkust research on the use of gpu direct and their gdr implementation we wish to adopt the copies approach and apply it to the current verbs implementation while keeping the current implementation advantages such as configurability and the use of rdma for control messages performance:compared with the current grpc verbs and gdr implementation the result implementation gave the best performance for every model with any number of nodes for vgg on nodes with p gpus each the prototype beat the second place by over implementation requirements tensor writes need to be done directly from the source tensor to the destination tensor with no memory copies in between this should be done for all dmable tensors which are located either on cpu or on a rdma compatible gpu device gpu direct non dmable tensors canmemcopy false will be serialized to proto on the sender side rdma written to a registered buffer on the receiver side and then deserialized by the receiver tensors which are located on a non-rdma-compatible gpu will be rdma written to a registered cpu proxy buffer on the receiver side and then copied to gpu by the receiver implementation constrains:for best stability and proof of correctness we will divide the implementation to two stages at first stage we will keep changes to the current implementation to the minimum possible the expense will be that we may have unused or unnecessary code leftovers which may also affect performance at second stage we will re-iterate over the code and remove irrelevant code parts.the design of the solution aims that we will achieve both stages with relative ease design guidelines since we do not want to do any unnecessary memory copying we will no longer allocate a fixed cpu buffer as the destination for the rdma write instead we will do the writing directly to the result tensor or if the result tensor is on a device which does not support rdma we will do the writing to a proxy cpu tensor and then copy its content to the result tensor the address of the destination tensor needs to be sent to the sender side for writing meaning that the result/proxy tensor should be pre-allocated on the receiver side prior to sending the tensor request in order to do that we need to know its meta-data i.e shape and data-type for dmable tensors and proto-size for serialized tensors unfortunately this information is only available on the sender side which complicates manners in order to avoid sending extra messages for querying the meta-data on each step we store a local meta-data cache per tensor based on the assumption that the meta-data of a tensor rarely changes between steps we expect that on most times the cache will only be updated once when the sender receives a request for a tensor if it is the first time this tensor is requested or in the rare case that the meta-data did change the sender will first send a meta-data response on which the receiver will update the local cache and reallocate the result/proxy tensors if required when the receiver sends the tensor request it will contain also the meta-data currently stored in its local cache so the sender can compare it to see if there was a change when the sender writes the tensor content to the result tensor no additional data is being written with it that means we need to reside on ibverbs immediate uint_t to indicate which request we are responding to in order to trigger the receive callback the easiest and most elegant way is to key the recv callback with a unique request_index uint_t instead of the current key_with_step_id string since the sender no longer writes the tensor from/to fixed buffers we no longer need to schedule the writes using the local/remote status in addition we no longer rely on the rmdatensorbuffer members as the source/destination addresses and rkey/lkey instead each rdmatensorbuffer will hold multiple response objects one per step-id from which we derive destination address and rkey the source address and lkey are always the ones of the source tensor with the addition of tensor pre-allocation we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes after implementing a common method for tensor pre-allocation it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending meta-data response callback and content response callback all in a single request class the request class holds all the relevant request information which reduces excessive parameter passing and lambda capturing this decision is purely for elegance and code simplicity and we decided to implement it in first stage because it makes the implementation much easier new types/classes enum rdmaimmdatatype immediate types to distinguish between different rdma writes on the remote side ack writes and control-message writes have a fixed immediate value the rest of the writes are tensor writes and the immediate value is the relevant request index enum rdmawriteidtype types to distinguish between different rdma write-complete events ack control message tensor dma write and tensor proto write class rdmawriteid context for rdma write complete events holds the rdmawriteidtype and additional data class remoteaddresscontext remote address information address mr will be passed as write context for tensor proto writes class rdmatensormetadata meta-data for a tensor type shape is_dead proto_size class rdmamemorymgr manages the meta-data cache and the registered memory regions class rdmatensorrequest holds and manages information for a single tensor request throughout the entire receive cycle api start start the request recvtensormetadata receive meta-data from the remote side recvtensorcontent receive tensor content from the remote side and invoke the done callback class rdmatensorresponse holds information for a single tensor response such as destination address and rkey protocol changes:the protocol messages themselves will remain mostly unchanged at the first stage but will be used differently as described below the current messages structures already have most of the required fields for the new implementation the only change is the buffer_size field which is no longer used since we are no longer sending additional information with the tensor and thus it is now always equal to the tensor_bytes field instead we use that field to pass the request_index message structure type name_size name step_id request_index remote_addr rkey is_dead data_type tensor_shape tensor_bytes b b b b b b b xb xb b rdma_message_tensor_request receiver sender the original tensor request type the message type name name_size name of the requested tensor step_id step id request_index request index remote_addr/rkey address/rkey of the result/proxy tensor irrelevant for first-time request is_dead/data_type/tensor_shape/tensor_bytes the current meta-data as stored in the receiver local cache the sender will use that information to know if the receivers cache requires updating rdma_message_buffer_request sender receiver the meta-data update message in case meta-data had changed or if it is the first time the tensor is requested type the message type request_index request index is_dead/data_type/tensor_shape/tensor_bytes the up-to-date meta-data rdma_message_buffer_response receiver sender tensor re-requset after meta-data update and reallocation of result/proxy tensors type the message type name name_size name of the requested tensor step_id step id request_index request index remote_addr/rkey address/rkey of the reallocated result/proxy tensor is_dead/data_type/tensor_shape/tensor_bytes the new meta-data will be removed in the next phase rdma_message_tensor_write sender receiver no longer sent there is only a direct write of the tensor content to the result/proxy tensor request index passed as the immediate value of the write rdma_message_tensor_idle receiver sender no longer sent.! alt text phase message protocol second stage optimizations remove unused code leftovers remove the ack buffer completely since we can rely completely on its immediate value future optimizations map the tensor names to indexes to significantly reduce the request message size understand the purpose of empty tensors and if we can skip remote fetching for them consider concatenating multiple requests and/or using multiple message buffers consider a no-request architecture
286302443,15880,https://api.github.com/repos/tensorflow/tensorflow/issues/15880,dtegunov,2,0,0,0,0,0,when using the tf c library inside an application that also uses gpus for other tasks not implemented in tf it would be useful to be able to deallocate all the gpu memory tf has allocated once the session is closed and no further tf calls are expected for the time being gpu_options.allow_growth keeps tfs allocated pool small but it still can grow to several gb even after the session is deleted the pool doesnt shrink to free it up the whole application must be restarted if im not mistaken.being able to destroy the processstate singleton seems to solve it without breaking anything however its destructor is protected alternatively getting the allocator for each gpu from processstate and manually destroying them does the trick but renders tf unusable for all future operations because processstate still thinks the allocators exist and doesnt recreate them when they are required again.i think making the processstate destructor public or adding a public method to invoke similar code would be the best solution but maybe im missing an obvious solution that already exists
286230464,15874,https://api.github.com/repos/tensorflow/tensorflow/issues/15874,matthew-z,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gacd python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version cuda v cudnn gpu model and memory km exact command to reproduce describe the problemthe source code is a minimal one to use dynamic rnn to predict token tag.i tried to use tf.control_dependencies to ensure loss will be evaluated before train_op however i mistakenly evaluated loss in session.run( train loss then i found that if the input length example_length in the example code is larger than or equal to the program will hang without any notification if i set cuda_visible_devices to use cpu only the program will output an error code however if the input length is smaller than it will run without any problem i am not sure if it is a bug or an intentional behavior source code logs pythonimport tensorflow as tffrom tensorflow.contrib.rnn import stack_bidirectional_dynamic_rnnfrom tensorflow.python.ops import rnn_cellexample_length with tf.graph().as_default x tf.random_uniform(maxval minval shape example_length dtype=tf.float lengths tf.constant( example_length y tf.random_uniform(maxval minval shape example_length dtype=tf.int cell rnn_cell.basicrnncell output tf.nn.dynamic_rnn(cell x dtype=float logits tf.layers.dense(output units loss tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits labels=y with tf.control_dependencies( loss opt tf.train.adamoptimizer train_op opt.minimize(loss sess tf.interactivesession sess.run(tf.global_variables_initializer for i in range l sess.run( train_op loss print(i l traceback only output error with cpu ---------------------------------------------------------------------------invalidargumenterror traceback most recent call last)/usr/local/var/pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/client/session.py in do_call(self fn args try return fn(*args except errors.operror as e:/usr/local/var/pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/client/session.py in run_fn(session feed_dict fetch_list target_list options run_metadata feed_dict fetch_list target_list status run_metadata usr/local/var/pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/framework/errors_impl.py in exit__(self type_arg value_arg traceback_arg compat.as_text(c_api.tf_message(self.status.status c_api.tf_getcode(self.status.status delete the underlying status object from memory otherwise it stays aliveinvalidargumenterror retval does not have valueduring handling of the above exception another exception occurred:invalidargumenterror traceback most recent call last)
286186262,15869,https://api.github.com/repos/tensorflow/tensorflow/issues/15869,moorage,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary either tensorflow version use command below python version n/a bazel version if compiling from source n/a gcc/compiler version if compiling from source mingw cuda/cudnn version none gpu model and memory none exact command to reproduce visit can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemfeature request doesnt have windows instructions i am curious if the tensorflow/go will be installable for windows source code logsi currently get an error on the ld.exe phase of the go get for github.com/tensorflow/tensorflow/tensorflow/go cannot find ltensorflow im not sure how to specify that the tensorflow.dll should be used
286070142,15855,https://api.github.com/repos/tensorflow/tensorflow/issues/15855,ksindi,2,0,0,1,0,0,the are many issues and stackoverflow posts asking how to export a retrained inception model it would be nice if retrain.py did this so that its easier for newcomers to use tensorflow serving.this pr exports the model after retrain is finished ive also added a comment on how to serve the retrained model.confirmed works for tensorflow version
285891437,15836,https://api.github.com/repos/tensorflow/tensorflow/issues/15836,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where it was not possible to specify an axis for tf.nn.crelu by default axis was used for concatenation implicitly.this fix adds the support of axis for tf.nn.crelu and adds test cases for it.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
285718076,15818,https://api.github.com/repos/tensorflow/tensorflow/issues/15818,sekhar989,3,0,0,0,0,0,i am trying to use a custom model in the tf-detect android demo ssd_mobilenet_v_coco the model is trained on classes after exporting the model ive optimised it using tensorflow-mobile. bazel-bin/tensorflow/tools/graph_transforms/transform_graph in_graph=frozen_inference_graph.pb out_graph=optimized_inf_graph.pb inputs=image_tensor outputs=detection_boxes detection_scores detection_classes num_detections transforms=fold_batch_norms fold_old_batch_norms quantize_weights the optimised graph is giving proper output in my local system but when its integrated in the application there is no output shown in the screen but when im using the unoptimised graph frozen_inference_graph.pb in the application its working fine im getting outputs.what am i doing wrong here?have i written custom code noos platform and distribution mac os sierratensorflow installed from virtualenv installationtensorflow version bazel version build label homebrewcuda/cudnn version nagpu model and memory naexact command to reproduce trained a ssd_mobilenet_v_coco model using google cloud ml for classes exported the frozen graph from checkpoints using the below command set: python export_inference_graph.py input_type image_tensor pipeline_config_path training/ssd_inception_v_coco.config trained_checkpoint_prefix training/model.ckpt output_directory frozen_graph/ export_inference_graph.py is the python script provided in here tested the frozen_inference_graph.py in my local system its working fine used the below command to convert the frozen_inference_graph.py to optimized graph: bazel-bin/tensorflow/tools/graph_transforms/transform_graph in_graph=
285427677,15785,https://api.github.com/repos/tensorflow/tensorflow/issues/15785,Maheshsmartdata,5,0,0,0,0,0,i follow the instructions on the tensorflow official website on how to import android samples i did exactly as they said but when i try to run the app it shows the following error.error:execution failed for task compiledebugjavawithjavac unable to find source java class users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/java/src/main/java/org/tensorflow/op/core/constant.java because it does not belong to any of the source dirs users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/src/main/java users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/src users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/build-types/debug/java users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/gradlebuild/generated/source/r/debug users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/gradlebuild/generated/source/buildconfig/debug users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/gradlebuild/generated/source/aidl/debug users/vinay.garg/androidstudioprojects/tensorflow/tensorflow/examples/android/gradlebuild/generated/source/rs/debug the problem is constants.java file is in the parent directory and not in the sample project directory i tried to find the usage of the constants.java file but cant find its use anywhere in the sample project what am i missing here
285337649,15778,https://api.github.com/repos/tensorflow/tensorflow/issues/15778,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where it was not possible to specify the colors for boxes in tf.image.draw_bounding_boxes instead a predefined fixed color table was used to cycle through colors.this fix adds colors input to drawboundingboxexv so that it is possible to specify the color in case no color is specified the default color table will be used.since there is an api change the op is labeled as v.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
285320935,15773,https://api.github.com/repos/tensorflow/tensorflow/issues/15773,developer-mayuan,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below v python version cuda/cudnn version gpu model and memory ti describe the problemim currently implementing a pose estimation system and i defined my network with loss and train_op in each of degree yaw pitch and roll and im current using your tf.estimator api which i think is pretty convenient to monitor the system however i found that i may only be able to define one loss and train_op using this set of api i would like to know if there is any solution to train and monitor the system with multiple loss and train_op at the same time thanks source code logs return tf.estimator.estimatorspec mode=mode predictions=predictions loss= yaw_total_loss pitch_total_loss roll_total_loss error train_op= yaw_train_op pitch_train_op roll_train_op error eval_metric_ops=none
285147190,15720,https://api.github.com/repos/tensorflow/tensorflow/issues/15720,prithagupta,1,0,0,0,0,0,system information i am using tensorflow version os platform and distribution linux tensorflow installed from pip version python anaconda custom bit cuda gpu model and memory nvidia k kepler describe the problemi am using adam optimizer with a normal sequential network created via keras using tensorflow as backend i get the following logs repeatedly for fitting and creating the network i also applied batch-normalization for the dense layer/job:localhost/replica:/task:/device:gpu device name tesla kxm pci bus id compute capability adam_/decay variablev job:localhost/replica:/task:/device:gpu:isvariableinitialized isvariableinitialized job:localhost/replica:/task:/device:gpu:adam_/decay/read identity job:localhost/replica:/task:/device:gpu:adam_/decay/assign assign job:localhost/replica:/task:/device:gpu:adam_/beta variablev job:localhost/replica:/task:/device:gpu:isvariableinitialized isvariableinitialized job:localhost/replica:/task:/device:gpu:training/adam/gradients/batch_normalization__/batchnorm/mul_grad/shape const job:localhost/replica:/task:/device:gpu:training/adam/gradients/batch_normalization__/batchnorm/mul_grad/shape const job:localhost/replica:/task:/device:gpu:training/adam/gradients/batch_normalization__/moments/squeeze_grad/shape const job:localhost/replica:/task:/device:gpu:training/adam/gradients/zeros_/const const job:localhost/replica:/task:/device:gpu:how to turn off these unwanted logs i have already applied the following solution for switching off the warning logs from the tensorflow source code from keras.layers.normalization import batchnormalizationmodel sequential()model.add(dense input_dim init=relu))model.add(batchnormalization())model.add(activation(relu))model.add(dense init=uniform))model.add(batchnormalization())model.add(activation(relu))model.add(dense init=uniform))model.add(batchnormalization())model.add(activation(softmax))model.compile(loss=binary_crossentropy optimizer=adam())model.fit(x_train y_train
284937584,15697,https://api.github.com/repos/tensorflow/tensorflow/issues/15697,Bihaqo,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu tested on linux ubuntu and mac os tensorflow installed from source or binary both affected tensorflow version use command below v..--gacd python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problemtensorflow transpose is slower than numpy transpose on my example source code logs import numpy as npa np.random.randn timeit np.transpose(a ns on my machineb tf.variable(tf.random_normal variable to avoid generating random numbers while measuring timesess tf.session()sess.run(tf.global_variables_initializer())op tf.transpose(b op%timeit sess.run(op s
284730573,15656,https://api.github.com/repos/tensorflow/tensorflow/issues/15656,harimaruthachalam,3,0,0,0,0,0,i am using nvidia geforce gtx and installed nvidia i installed cudnn and cuda as of my understanding i know that tensorflow is not supported in cuda my question is when i can expect the next build/release of tf to support cuda for the time being shall i make a link from cuda to cuda and expect to work or is there any better way to solve the problem
284509792,15636,https://api.github.com/repos/tensorflow/tensorflow/issues/15636,llyyun,1,0,0,0,0,0,hii tried the examples on ios according to tf lite guide but failed when assign data to tflite because address out is null probably my tflite file is incorrect but i am not sure can anybody give some help my test step is as follows try the following code get the tflite file converteds_model.tflite import tensorflow as tfimg tf.placeholder(name=img dtype=tf.float shape val img tf.constant tf.constant out tf.identity(val name=out)with tf.session as sess tflite_model tf.contrib.lite.toco_convert(sess.graph_def img out open(converteds_model.tflite wb).write(tflite_model integrated the tflite into my app which is from ios sample code simple(/users/sensteer/software/tensorflowclone/tensorflow/tensorflow/contrib/lite/examples/ios/simple).but exception happed because address out is null int input interpreter->inputs input is float out interpreter->typed_tensor(input out is null so my questions are the tflite created above is right or not the reading tflite code is right or not if the tflite file is not right do i must create tflite with pb ckpt and frozengraphdef mentioned in guide?thanks
284509471,15635,https://api.github.com/repos/tensorflow/tensorflow/issues/15635,warriorZH,2,0,0,0,0,0,"system information os platform and distribution linux ubuntu tensorflow installed from source tensorflow version python bazel version bazel release gcc/compiler version gcc version gcc cuda/cudnn only for cpu describe the problemhi i have tried to load the rnn model inside android that i generated from python for machine translation when i want to use beam_search in decode i meet an cant find op error at android i have tried below solutions but the error is continue solution use python script tensorflow/python/tools/print_selective_registration_header.py to create ops_to_register.h then copy it into tensorflow/core/framework then generate tensorflow/contrib/android:libtensorflow_inference.so i change print_selective_registration_header.py: parser.add_argument(--default_ops,type=str,#default=noop:noop,_recv:recvop,_send:sendop default=all cmd like below: bazel build tensorflow/python/tools:print_selective_registration_header bazel-bin/tensorflow/python/tools/print_selective_registration_header graphs=/path/to/my/model/decode-model__real_model_with_beam.pb ops_to_register.h bazel build c opt copt=-dselective_registration copt=-dsupport_selective_registration tensorflow/contrib/android:libtensorflow_inference.so host_crosstool_top=@bazel_tools//tools/cpp:toolchain crosstool_top=//external:android/crosstool cpu=armeabi-va solution i met the same type of mistake before and with op type not registered listdiff error i find listdiff_op.cc is at tensorflow/core/kernels then i add a line at tensorflow/core/kernels/build and re-generate libtensorflow_inference.so to solve that error but the beam_search_ops.cc is at tensorflow/contrib/seqseq/kernels when i add it like listdiff its still reporting that error at android studio add line like below filegroup(name android_extended_ops_group,srcs listdiff_op.cc,#//tensorflow/contrib/seqseq/kernels/beam_search_ops.cc,... ) i need some help for solving this problem thanks source code logs error log at android studio: e/androidruntime fatal exception main process com.example.phua.mt pid java.lang.runtimeexception unable to start activity componentinfo{com.example.phua.mt_/com.example.phua.mt_.mainactivity org.tensorflow.tensorflowexception op type not registered gathertree in binary running on localhost make sure the op and kernel are registered in the binary running in this process at android.app.activitythread.performlaunchactivity(activitythread.java at android.app.activitythread.handlelaunchactivity(activitythread.java at android.app.activitythread.-wrap(unknown source at android.app.activitythread$h.handlemessage(activitythread.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at com.android.internal.os.zygote$methodandargscaller.run(zygote.java at com.android.internal.os.zygoteinit.main(zygoteinit.java caused by org.tensorflow.tensorflowexception op type not registered gathertree in binary running on localhost make sure the op and kernel are registered in the binary running in this process at org.tensorflow.graph.importgraphdef(native method at org.tensorflow.graph.importgraphdef(graph.java at org.tensorflow.graph.importgraphdef(graph.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.loadgraph(tensorflowinferenceinterface.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.(tensorflowinferenceinterface.java at com.example.phua.mt_.tfonandroid.loadmodel(tfonandroid.java at com.example.phua.mt_.mainactivity.oncreate(mainactivity.java at android.app.activity.performcreate(activity.java at android.app.instrumentation.callactivityoncreate(instrumentation.java at android.app.activitythread.performlaunchactivity(activitythread.java at android.app.activitythread.handlelaunchactivity(activitythread.java at android.app.activitythread.-wrap(unknown source at android.app.activitythread$h.handlemessage(activitythread.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java"
284452301,15626,https://api.github.com/repos/tensorflow/tensorflow/issues/15626,theyonibomber,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu compiled on macos observed on android tensorflow installed from source or binary source tensorflow version use command below commit abfcaceda on master between release of and compared to python version n/a java android code bazel version if compiling from source homebrew gcc/compiler version if compiling from source apple llvm version clang cuda/cudnn version n/a gpu model and memory observed on a huawei nexus p exact command to reproduce n/a java android code describe the problemfollowing the move to java api in commit adcfddcdffffded it seems that it is very ineffective to feed nodes.i am feeding my model an input array of about floats and when i sample cpu usage using cpu profiler on android studio instrumented it seems that the feed method takes x time than running the inference if i leave everything the same but i use android libs compiled in tensorflow cpu time of feed method used to be fillnodefloat becomes negligible.it seems that putting a float array into the tensors floatbuffer is a very costly operation source code logs tf relevant inference code: tensorflow.feed(input_node_name input shape);tensorflow.run(output_names);tensorflow.fetch(output_names output); screenshot of cpu profilers call chart of cpu profilers top-down breakdown of the methods relevant inference code: tensorflow.fillnodefloat(input_node_name shape input);tensorflow.runinference(output_names);tensorflow.readnodefloat(output_names output); screenshot of cpu profilers call chart of cpu profilers bottom-up breakdown of the methods
284423050,15624,https://api.github.com/repos/tensorflow/tensorflow/issues/15624,llan-ml,2,0,0,0,0,0,"please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu lts xenial xerus tensorflow installed from source or binary binary tensorflow version use command below dev python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory n/a describe the problemthe tf-.-dev supports tf.sparsetensor when using tf.data.dataset.from_tensor_slices but it cannot infer the shape of new tensor after some operations such as tf.data.dataset.map .the shape of tensor becomes unknown which is troublesome for downstream operations for example we have to call set_shape if we want to feed the new tensor into a tf.layers.dense source code logsimport tensorflow as tfx tf.sparsetensor dense_shape= , )ds tf.data.dataset.from_tensor_slices(x)ds.output_shapes tensorshape( dimension() )ds ds.map(lambda x tf.sparse_tensor_to_dense(x))ds ds.batch()ds.output_shapes tensorshape( dimension(none dimension(none) )iterator ds.make_one_shot_iterator()next_elem iterator.get_next tensorshape( dimension(none dimension(none) )y tf.layers.dense(next_elem valueerror the last dimension of the inputs to dense should be defined found none .@mrry"
284402779,15621,https://api.github.com/repos/tensorflow/tensorflow/issues/15621,mapicccy,1,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem? none system information operating system centos linux kernel tensorflow installed from source tensorflow version python version python the output of bazel version : build target bazel-out/k-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu jan build timestamp thu jan build timestamp as int gcc/compiler version if compiling from source gcc gcc red hat cuda/cudnn version cuda cudnn gpu model and memory nvida tesla p describe the problemwhen exec fused_convd_bias_activation_op.py or import the package using import tensorflow.contrib.fused_conv.python.ops undefined symbol error occursthe source code file path is output of python fused_convd_bias_activation_op_test.py is as follows: tensorflow.python.framework.errors_impl.notfounderror usr/lib/python./site-packages/tensorflow/contrib/fused_conv/python/ops/_fused_convd_bias_activation_op.so undefined symbol zntensorflowfunctorpadinputineigengpudeviceeiilieeclerks_ns_tensormapins_tensorikilielieieeliens_makepointereeerkstarrayiilmeesg_ns_ins_iilielieieeliesb_eens_tensorformate it seems that something wrong when loading eigen library the other logsthe output of objdump t usr/lib/python./site-packages/tensorflow/contrib/fused_conv/python/ops/_fused_convd_bias_activation_op.so grep und grep eigen is as follows f und zntensorflowbrainpaddingeigenpaddingens_paddinge und zntensorflowfunctorpadinputineigengpudeviceeiilieeclerks_ns_tensormapins_tensorikilielieieeliens_makepointereeerkstarrayiilmeesg_ns_ins_iilielieieeliesb_eens_tensorformate und zntensorflowfunctorpadinputineigengpudeviceefilieeclerks_ns_tensormapins_tensorikflielieieeliens_makepointereeerkstarrayiilmeesg_ns_ins_iflielieieeliesb_eens_tensorformate f und znktensorflowopkernelcontexteigen_deviceineigengpudeviceeeerkt_v und zneigeninternaltensorexecutorikns_tensorassignopins_tensormapins_tensoriflielieieeliens_makepointereeekns_tensorreshapingopikns_dsizesiilieeekns_tensorshufflingopikns_iilieeekns_ise_kns_ins_ikflielieieelies_eeeeeeeeeens_gpudeviceelbeerunersq_rksr und zntensorflowfunctornhwctonchwineigengpudeviceeflieeclerks_ns_tensormapins_tensorikflielieleeliens_makepointereeens_ins_iflielieleeliesb_ee
284355424,15612,https://api.github.com/repos/tensorflow/tensorflow/issues/15612,codrut3,1,0,0,0,0,0,reuse the output buffer and allocate only one temporary tensor when data format is nhwc andgpu is used this is based on the observation that cudnn can perform the backward computationin place the same idea is used in pr this lowers gpu memory consumption and may improve performance because fewer distinct memory addresses are accessed it also permits a higher batch size.ive also added a new test for the gradient computation
284332662,15606,https://api.github.com/repos/tensorflow/tensorflow/issues/15606,fenrus75,2,0,0,0,0,0,"the allocator.h code tried to be clever and use byte alignment for sse/avx/etc use,and byte alignment for avx.unfortunately the ifdef in use from eigen is not useful the bazel build files donot propagate the tf_copts compiler flags when the allocator.cc/allocator.h files getcompiled to eigen does not see the actual avx using compiler flags...rather than changing compiler flag propagation throughout a whole bunch of code,theres an opportunity to just simplify the code and always use byte alignment.yes it wastes a bit of space but on the other hand now these allocations arecache line aligned which isnt a bad thing and an ifdef can be droppedsigned-off-by arjan van de ven arjan@linux.intel.com"
284307835,15604,https://api.github.com/repos/tensorflow/tensorflow/issues/15604,kirk86,20,0,0,0,0,0,i installed tf-nightly build and i get the following error on import of tensorflow. importerror libcublas.so cannot open shared object file no such file or directory .if i check for cuda i get the following: ldconfig v/usr/local/cuda-./targets/x_-linux/lib libnvgraph.so libnvgraph.so libnppicom.so libnppicom.so libnppial.so libnppial.so libcufftw.so libcufftw.so libcufft.so libcufft.so libnppif.so libnppif.so libcublas.so libcublas.so libnvblas.so libnvblas.so libnppi.so libnppi.so libcusolver.so libcusolver.so libnppidei.so libnppidei.so libnvrtc-builtins.so libnvrtc-builtins.so libnvrtc.so libnvrtc.so libnpps.so libnpps.so libcuinj.so libcuinj.so libnppig.so libnppig.so libopencl.so libopencl.so libnppicc.so libnppicc.so libnppist.so libnppist.so libnppisu.so libnppisu.so libnppim.so libnppim.so libcurand.so libcurand.so libcudart.so libcudart.so libnvtoolsext.so libnvtoolsext.so libnppitc.so libnppitc.so libnppc.so libnppc.so libcusparse.so libcusparse.so.../usr/local/cuda-./targets/x_-linux/lib libnppicc.so libnppicc.so libnppisu.so libnppisu.so libcufftw.so libcufftw.so libcufft.so libcufft.so libnppial.so libnppial.so libnppist.so libnppist.so libcublas.so libcublas.so libnvblas.so libnvblas.so libnppitc.so libnppitc.so libcusolver.so libcusolver.so libnvrtc.so libnvrtc.so libnvrtc-builtins.so libnvrtc-builtins.so libnppidei.so libnppidei.so libopencl.so libopencl.so libnppig.so libnppig.so libnppc.so libnppc.so libcudart.so libcudart.so libnvtoolsext.so libnvtoolsext.so libnvgraph.so libnvgraph.so libnppif.so libnppif.so libcusparse.so libcusparse.so libaccinj.so libaccinj.so libcuinj.so libcuinj.so libnppim.so libnppim.so libnppicom.so libnppicom.so libnpps.so libnpps.so libcurand.so libcurand.so... i that due to a name mismatch libcublas.so libcublas.so and if so how can we overcome this
284306364,15601,https://api.github.com/repos/tensorflow/tensorflow/issues/15601,codrut3,1,0,0,0,0,0,i discovered experimentally that cudnn computations can be performed in place therefore there is no need to allocate two temporary tensors in fusedbatchnorm for gpu and data format nhwc one is enough this lowers memory consumption and hence increases the maximum possible batch size.this might seem risky because nvidia doesnt mention the property but in fact the current implementation already uses it by doing forward_input_or_allocate_output in fusedbatchnormop .if data format is nchw and the input is forwarded then cudnn would be forcedto do the computation in place see line this is how i discovered that the whole approach works:i was trying to see if forwarding the input is a bug or not.i added several tests to ensure that the change is correct.while doing this i discovered that ops_testutil does not properly synchronize at the end.the reason seems to be the call context_->eigen_gpu_device().synchronize somehowit does nothing i think the problem is that eigen is not compiled with the flag eigen_cudacc .so i changed it to gpuutil::sync(device_.get
283683961,15530,https://api.github.com/repos/tensorflow/tensorflow/issues/15530,cbockman,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..-rc--ga python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version observed on cpu gpu gpu model and memory observed on cpu gpu exact command to reproduce see below describe the problemhow this arose:we are trying to set up a basic distributed tf version where we have separate pods on kubernetes doing validation and training a simple version with of each gcs is used as the backend to store model output checkpoints etc.).the validation pod periodically via experiments continuous_eval periodically polls for new checkpoints to evaluate if it doesnt find a new checkpoint it per the underlying code echoes out no new checkpoint ready for evaluation and continues to wait for a new one.in practice we found that even as the training pod produces new checkpoints the validation pod never picks up a new checkpoint beyond the first one i.e it collects an initial checkpoint does evaluation and then in all future cycles echoes out no new checkpoint ready for evaluation.in debugging we found that the checkpoint file the saver tries to load up is always found to be some earlier iteration of the file i.e it seems like the gcs file loader reads the file once and from then on out is continuous accessing a cached version of the data digging into the code further this appears to be an issue with how the file reader file_io.read_file_to_string and downstream methods loads gcs files we were able to replicate this behavior separately below. help appreciated is this intended behavior is there e.g some sort of gcs setting we have incorrectly set assuming were seeing something real is there any suggested remediation here with regards to our validation behavior our next step is going to be to try monkey-patching some of the tf functions to just pull the gcs file local to disk and read it from there...although this is of course not preferred.as a side note experiment does have a number of references to special handling around using gcs as the backend although i dont have enough context to say if this is relevant to what we are seeing or not source code logsbelow is a pair of scripts that will replicate this issue run the basic reader using the same loading interface from get_checkpoint_state and the basic writer simultaneously and the reader will initially catch whatever is in the file and then never update even as the writer continues to write.other things we tried changing the read/write path to local disk instead of gcs this unsurprisingly worked a version of a reader with a context manager below to try to reset the reader each loop and an explicit file.close not shown both had the same behavior i.e new reads didnt provide the updated file writing from the same file process that we read from probably unsurprisingly this does work i.e the writing activity either updates the local cache or otherwise convinces the reader to grab a fresh copy from gcs we didnt actually test which this might be basic reader: python#!/usr/bin/env pythonfrom tensorflow.python.lib.io import file_ioimport timeimport osfile=gs:// mypath os.environ google_application_credentials usr/src/app/gcloud/keys/google-auth.jsondef read counter while counter print(reading print(contents print(file_io.read_file_to_string(file print print(sleeping for a second time.sleep print print print counter read() basic writer python#!/usr/bin/env pythonfrom tensorflow.python.lib.io import file_ioimport timeimport osfile=gs:// mypath os.environ google_application_credentials usr/src/app/gcloud/keys/google-auth.jsondef write counter while counter with file_io.fileio(file mode=w as f f.write(str(counter print(wrote format(counter print print(sleeping for a second time.sleep print print print counter write() reader with context manager python#!/usr/bin/env pythonfrom tensorflow.python.lib.io import file_ioimport timeimport osfile=gs:// mypath os.environ google_application_credentials usr/src/app/gcloud/keys/google-auth.jsondef read counter while counter print(reading print(contents with file_io.fileio(file mode=r as f print(f.read print print(sleeping for a second time.sleep print print print counter read
282903724,15448,https://api.github.com/repos/tensorflow/tensorflow/issues/15448,maxfiedler,12,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow slightly altered stock example see below os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary from source python version bazel version if compiling from source gcc/compiler version if compiling from source gcc ubuntu ubuntu cuda/cudnn version gpu model and memory nvidia describe the problemcurrently the re-initializable iterator api using from_structure and iterator.make_initializer always resets the get_next op of the iterator to fetch the first element of its dataset instance again after running sess.run(training_init_op or sess.run(validation_init_op respectively is this really the intended behavior this means that if you want to switch between training and validation datasets within one epoch i.e in a shorter rhythm than the full dataset length you will always only iterate over the first batch_size number-of-training-steps-before-validation-step elements of the training set during training source code logsi guess the code example will make it clearer define training and validation datasets with the same structure.training_dataset tf.data.dataset.range()validation_dataset tf.data.dataset.range a reinitializable iterator is defined by its structure we could use the output_types and output_shapes properties of either training_dataset or validation_dataset here because they are compatible.iterator tf.data.iterator.from_structure(training_dataset.output_types training_dataset.output_shapes)next_element iterator.get_next()training_init_op iterator.make_initializer(training_dataset)validation_init_op iterator.make_initializer(validation_dataset)with tf.session as sess run epochs in which the training dataset is traversed followed by the validation dataset for i in range initialize an iterator over the training dataset print i sess.run(training_init_op for in range nel sess.run(next_element print(train type(nel nel initialize an iterator over the validation dataset sess.run(validation_init_op for in range nel sess.run(next_element print(valid type(nel nel) produces the output train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int apparently the latter elements of the training set and the latter elements of the validation set never get evaluated i dont really see the real-world use-case for this behavior i know that you can implement the other functionality via the feedable iterator scheme using one_shot_iterators but not using initializable iterators as highlighted by the code below pay attention to the different iterators used for training and validation here define training and validation datasets with the same structure.training_dataset tf.data.dataset.range().repeat()validation_dataset tf.data.dataset.range().repeat a feedable iterator is defined by a handle placeholder and its structure we could use the output_types and output_shapes properties of either training_dataset or validation_dataset here because they have identical structure.handle tf.placeholder(tf.string shape= )iterator tf.data.iterator.from_string_handle handle training_dataset.output_types training_dataset.output_shapes)next_element iterator.get_next you can use feedable iterators with a variety of different kinds of iteratortraining_iterator training_dataset.make_one_shot_iterator()validation_iterator validation_dataset.make_initializable_iterator()with tf.session as sess the iterator.string_handle method returns a tensor that can be evaluated and used to feed the handle placeholder training_handle sess.run(training_iterator.string_handle validation_handle sess.run(validation_iterator.string_handle loop forever alternating between training and validation for i in range print i i run steps using the training dataset note that the training dataset is the original set i.e we run epochs see repeat argument and we resume from where we left off in the previous while loop iteration for in range nel sess.run(next_element feed_dict={handle training_handle print(train type(nel nel run one pass over the validation dataset sess.run(validation_iterator.initializer for in range nel sess.run(next_element feed_dict={handle validation_handle print(valid type(nel nel) creates output train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int train class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int valid class numpy.int
282691137,15426,https://api.github.com/repos/tensorflow/tensorflow/issues/15426,nairvinitha,1,0,0,0,0,0,i am working on a tensorflow speech recognition challenge and following tutorial the model training is completed but im not able to freeze the model.this is what i get after typing the required command: c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\examples\speech_commands>python freeze.py i c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\platform\cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use avx avx i c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties:name geforce mx major minor memoryclockrate(ghz pcibusid totalmemory gib freememory gib i c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device device:gpu device name geforce mx pci bus id compute capability w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc not found unsuccessful tensorslicereader constructor failed to find any matching files for w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc not found unsuccessful tensorslicereader constructor failed to find any matching files for w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc not found unsuccessful tensorslicereader constructor failed to find any matching files for w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc not found unsuccessful tensorslicereader constructor failed to find any matching files for w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc not found unsuccessful tensorslicereader constructor failed to find any matching files for w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc not found unsuccessful tensorslicereader constructor failed to find any matching files fortraceback most recent call last file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\client\session.py line in do_call return fn(*args file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\client\session.py line in run_fn status run_metadata file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\framework\errors_impl.py line in exit c_api.tf_getcode(self.status.status))tensorflow.python.framework.errors_impl.notfounderror unsuccessful tensorslicereader constructor failed to find any matching files for node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev_/tensor_names save/restorev_/shape_and_slices node save/restorev recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:gpu send_device=/job:localhost/replica:/task:/device:cpu send_device_incarnation tensor_name=edge__save/restorev tensor_type=dt_float device=/job:localhost/replica:/task:/device:gpu: () during handling of the above exception another exception occurred:traceback most recent call last file freeze.py line in module tf.app.run(main=main argv= sys.argv unparsed file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\platform\app.py line in run sys.exit(main(_sys.argv flags_passthrough file freeze.py line in main models.load_variables_from_checkpoint(sess flags.start_checkpoint file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\examples\speech_commands\models.py line in load_variables_from_checkpoint saver.restore(sess start_checkpoint file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in restore self.saver_def.filename_tensor_name save_path file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\client\session.py line in run run_metadata_ptr file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\client\session.py line in run feed_dict_tensor options run_metadata file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\client\session.py line in do_run options run_metadata file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\client\session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.notfounderror unsuccessful tensorslicereader constructor failed to find any matching files for node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev_/tensor_names save/restorev_/shape_and_slices node save/restorev recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:gpu send_device=/job:localhost/replica:/task:/device:cpu send_device_incarnation tensor_name=edge__save/restorev tensor_type=dt_float device=/job:localhost/replica:/task:/device:gpu: () caused by op save/restorev defined at file freeze.py line in module tf.app.run(main=main argv= sys.argv unparsed file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\platform\app.py line in run sys.exit(main(_sys.argv flags_passthrough file freeze.py line in main models.load_variables_from_checkpoint(sess flags.start_checkpoint file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\examples\speech_commands\models.py line in load_variables_from_checkpoint saver tf.train.saver(tf.global_variables file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in init self.build file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in build self._build(self._filename build_save=true build_restore=true file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in build build_save=build_save build_restore=build_restore file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in build_internal restore_sequentially reshape file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in addrestoreops tensors self.restore_op(filename_tensor saveable preferred_shard file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\training\saver.py line in restore_op spec.tensor.dtype file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\ops\gen_io_ops.py line in restore_v shape_and_slices=shape_and_slices dtypes=dtypes name=name file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\framework\op_def_library.py line in apply_op_helper op_def=op_def file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\framework\ops.py line in create_op op_def=op_def file c:\users\vinithanair\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\framework\ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessnotfounderror see above for traceback unsuccessful tensorslicereader constructor failed to find any matching files for node save/restorev restorev dtypes= dt_float device=/job:localhost/replica:/task:/device:cpu: (_arg_save/const save/restorev_/tensor_names save/restorev_/shape_and_slices node save/restorev recv client_terminated=false recv_device=/job:localhost/replica:/task:/device:gpu send_device=/job:localhost/replica:/task:/device:cpu send_device_incarnation tensor_name=edge__save/restorev tensor_type=dt_float device=/job:localhost/replica:/task:/device:gpu: () os windows tensorflow version python version gpu cuda v and cudnn v i came across and where the suggested fix was to add to the model name but in this case im not able find the list of codes where im supposed to make the change how do i find the code that needs the fix or is there another issue that im unaware of?please help
282364582,15389,https://api.github.com/repos/tensorflow/tensorflow/issues/15389,icyblade,23,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu ubuntu lts gnu/linux generic x tensorflow installed from source or binary) :source tensorflow version use command below) :unknown source code is cloned from fadefdcbaabfeede python version bazel version if compiling from source gcc/compiler version if compiling from source) :gcc version cuda/cudnn version :cuda cudnn gpu model and memory tesla v-pcie-gb exact command to reproduce :see description below describe the problemwhile trying to compile the latest tensorflow(cloned from fadefdcbaabfeede such error will be raised: error home/ubuntu/tensorflow/tensorflow/contrib/seqseq/build error while parsing d file home/ubuntu/.cache/bazel/_bazel_ubuntu/adebbfbcefbee/execroot/org_tensorflow/bazel-out/k-py-opt/bin/tensorflow/contrib/seqseq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seqseq/kernels/beam_search_ops_gpu.cu.pic.d no such file or directory)in file included from external/eigen_archive/unsupported/eigen/cxx/tensor from third_party/eigen/unsupported/eigen/cxx/tensor from tensorflow/contrib/seqseq/kernels/beam_search_ops.h from tensorflow/contrib/seqseq/kernels/beam_search_ops_gpu.cu.cc::external/eigen_archive/unsupported/eigen/cxx/../../../eigen/core fatal error math_functions.hpp no such file or directory it turns out that in cuda math_functions.hpp is located at cuda/include/crt/math_functions.hpp rather than cuda/include/math_functions.hpp cuda does which leads to this error. ln s usr/local/cuda/include/crt/math_functions.hpp usr/local/cuda/include/math_functions.hpp will fix this problem and complete the compiling process reference source code logstraceback is available above
282217605,15370,https://api.github.com/repos/tensorflow/tensorflow/issues/15370,datbui,4,0,0,0,0,0,in many cases existed built-in losses in tensorflow do not satisfy needs we can add ssim or ssim as the loss function into tensorflow.there is existed solution provided on stackoverflow but it is better to have the built-in function with fully covered unit tests
282199682,15369,https://api.github.com/repos/tensorflow/tensorflow/issues/15369,headupinclouds,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu os x any tensorflow installed from source or binary source tensorflow version use command below master head python version n/a bazel version if compiling from source n/a cmake gcc/compiler version if compiling from source xcode any cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce cmake gxcode describe the problemcmakes object libraries dont play well with xcode generators in particular there is an incompatibility effectively a bug that prevents an object library from containing multiple files with the same base filename stem i.e tensorflow//core/platform/env_time.cc tensorflow//core/platform/posix/env_time.ccthis pattern occurs in quite a few places in the tensorflow source code and is otherwise perfectly reasonable for reference here is a minimal sample project that directly reproduces a test case originally shared in a post by matthew wheeler on the cmake mailing list like to help provide a fix for this but would like some input on the preferred approach prior to implementing anything i see a few options identify duplicates manually and add an alias for the offending files in the repository for i in failures do echo e include i i%.cc}_fix.cc done and then update cmake to include those files maybe fix could be replaced with a more unique directory name pro reasonably easy con the problem will likely occur again include source.cc violates some style guides iterate through each list of object files in cmake at generate time and identify duplicates automatically then map each of these files to an alias for the build using something like configure_file(${duplicate_file cmake_current_binary_dir}/${duplicat_file_w_suffix copyonly for xcode only pro automatic future proof con more complicated and users cant apply changes directly in their ide long term replace object libraries with standard libraries static or shared based on cmake_build_shared in addition to the xcode related bug above object libraries have a number of other limitations which make the cmake code more complicated or rather standard libraries have a number of benefits that could make the cmake code cleaner perhaps the most significant drawback is that object libraries cant be used with target_link_libraries so we lose the ability to pass along transitive dependency chains and scoped usage requirements from find_package future system dependencies using target_link_libraries this relates to proposal making the cmake build distribution friendly where common system dependencies would be included using find_package calls and linked directly to the tensorflow submodules target_link_libraries(tf_core_lib private tensorflow_external_packages zlib etc this would also allow most of the manual add_dependencies calls to be removed note ive already added cmake package config installation steps to most of the google repository dependencies in forks and will try to get this stuff accepted upstream.)the last one is broader in scope so im hoping there is an initial workaround based on some variation of or that would be accepted upstream in the near future for cmake xcode if there is interest in using standard libraries i can help work on putting an initial solution together in a branch for evaluation as a follow up effort.i understand cmake status is currently under discussion in any event id like to help get tensorflow building through cmake for easy integration with other cmake based projects including ios builds where xcode is required i also appreciate tensorflow is an incredibly complicated piece of sw and i appreciate the work that has gone in to supporting cmake builds to date thanks source code logsnumerous no such file or directory errors such as these: clang error no such file or directory users/developer/tensorflow/tensorflow/contrib/cmake/_builds/xcode-hid-sections/tensorflow.build/release/tf_core_lib.build/objects-normal/x_/env.oclang error no such file or directory users/developer/tensorflow/tensorflow/contrib/cmake/_builds/xcode-hid-sections/tensorflow.build/release/tf_core_lib.build/objects-normal/x_/env_time.oclang error no such file or directory users/developer/tensorflow/tensorflow/contrib/cmake/_builds/xcode-hid-sections/tensorflow.build/release/tf_core_lib.build/objects-normal/x_/tracing.o
282134289,15367,https://api.github.com/repos/tensorflow/tensorflow/issues/15367,CasiaFan,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary pip install tensorflow-gpu tensorflow version use command below python version exact command to reproduce describe the problemwhen i run following scripterror invalidargumenterror see above for traceback you must feed a value for placeholder tensor block_conv_bn/keras_learning_phase with dtype bool occured after searching in keras repository and stackoverflow i find it is caused by the design of learning phase parameter of bn layers which behave differently at training and testing time see here fchollet have added k.set_learning_phase for tensorflow to solve this problem so when i use keras instead of tf.keras no issue is reported i wonder if this part is still not integrated into tensorflow completely source code logs pythonimport tensorflow as tf import keras work normallykeras tf.kerasxception keras.applications.xceptionfrom keras import backend as kdata_path val.tfrecordif not isinstance(data_path tuple list data_path data_path feature image/encoded tf.fixedlenfeature tf.string image/class_id tf.fixedlenfeature tf.int create file queuefilename_queue tf.train.string_input_producer(data_path tfrecord file readerreader tf.tfrecordreader example_string reader.read(filename_queue decode recordfeatures tf.parse_single_example(example_string features=feature)image tf.decode_raw(features image/encoded out_type=tf.uint)image tf.cast(image dtype=tf.float restore shapeimage tf.reshape(image label tf.cast(features image/class_id dtype=tf.int)image_batch label_batch tf.train.shuffle_batch(tensors= image label batch_size capacity min_after_dequeue num_threads allow_smaller_final_batch=true convert label to one hot labelsess k.get_session declare learning phase for bn/dropoutk.set_learning_phase()label_batch tf.one_hot(label_batch dtype=tf.float)model_input keras.layers.input(tensor=image_batch)base_model xception(include_top=true weights=none no pre-trained weights used pooling=avg input_shape modify first layer classes=)model_output base_model(model_input)test_model keras.models.model(inputs=model_input outputs=model_output)test_model.load_weights(weights.h)optimizer tf.train.rmspropoptimizer(learning_rate=e decay=.)test_model.compile(optimizer=optimizer loss=categorical_crossentropy metrics= accuracy )acc_value keras.metrics.categorical_accuracy(label_batch model_output fit model using data from tf records queuecoord tf.train.coordinator()threads tf.train.start_queue_runners(sess=sess coord=coord)acc_value_batch sess.run( acc_value )print(acc_value_batch)coord.request_stop wait for threads to stopcoord.join(threads=threads)sess.close() logs: caused by op block_conv_bn/keras_learning_phase defined at file home/arkenstone/pycharmprojects/startdt/face_liveness_detect/model/patch_based_cnn/test_image.py line in module classes file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/applications/xception.py line in xception x batchnormalization(name=block_conv_bn)(x file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in call output super(layer self).__call__(inputs kwargs file usr/local/lib/python./dist-packages/tensorflow/python/layers/base.py line in call outputs self.call(inputs args kwargs file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/layers/normalization.py line in call training k.learning_phase file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/backend.py line in learning_phase phase array_ops.placeholder(dtype=bool name=keras_learning_phase file usr/local/lib/python./dist-packages/tensorflow/python/ops/array_ops.py line in placeholder return gen_array_ops._placeholder(dtype=dtype shape=shape name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_array_ops.py line in placeholder placeholder dtype=dtype shape=shape name=name file usr/local/lib/python./dist-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinvalidargumenterror see above for traceback you must feed a value for placeholder tensor block_conv_bn/keras_learning_phase with dtype bool node block_conv_bn/keras_learning_phase placeholder dtype=dt_bool shape=
281941131,15352,https://api.github.com/repos/tensorflow/tensorflow/issues/15352,coolchicha,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version :n/a gpu model and memory :n/a exact command to reproduce :here is the output from tf_env_collect.sh cat etc/issue linux ubuntu generic ubuntu smp fri nov utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux ubuntu generic ubuntu smp fri nov utc x x x gnu/linux check pips numpy protobuf post)tensorflow tensorflow-tensorboard check for virtualenv true tensorflow import tf.version tf.git_version bv..-rc--gcctf.compiler_version bv..-rc--gccsanity check array dtype=int env ld_library_path usr/lib/nx/x/xinerama:/usr/lib/nx/xdyld_library_path is unset nvidia-smi tensorflow-src/tensorflow/tools/tf_env_collect.sh line nvidia-smi command not found cuda libs you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.seeing a segmentation fault on matmul operation source code logshere is the source code:import tensorflow as tfa tf.random_normal b tf.random_normal res tf.matmul(a b)tf.session().run(res)here is the backtrace from gdb xfffb in eigen::internal::gemm_pack_lhs::operator()(float eigen::internal::tensorcontractionsubmapper::context::workerloop(int from tensorflow/python/../libtensorflow_framework.so xfffeabd in std::_function_handler::_m_invoke(std::_any_data const from tensorflow/python/../---type return to continue or q return to quit---libtensorflow_framework.so xfffeecc in from usr/lib/x_-linux-gnu/libstdc++.so xffffbcba in start_thread arg=xffeffd at pthread_create.c xffffedd in clone at sysdeps/unix/sysv/linux/x_/clone.s:include any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
281654334,15332,https://api.github.com/repos/tensorflow/tensorflow/issues/15332,Rikorose,4,0,0,0,0,0,related to tf.estimator.train writes a summary of all defined summaries scalar img in the given model_fn when calling monitoredtrainingsession here does not write summaries defined somewhere in the model_fn a workaround is defining summarysaverhook inside the model_fn which i think is not ideal since the estimator has all the relevant information for saving summaries steps save directory etc.).a possible solution would be adding a summarysaverhook during evaluation e.g here or create a new function monitoredevaluationsession in monitored_session.py that creates such a hook.cc ispirmustafa martinwicke
281537642,15322,https://api.github.com/repos/tensorflow/tensorflow/issues/15322,silentnuke,6,0,0,2,0,2,currently it doesnt seems like tensorflow support compiling with ndk r clang and c stl.is there any plans to update build flow to support that?also it would be nice to have pure cmake build for android
281513161,15320,https://api.github.com/repos/tensorflow/tensorflow/issues/15320,BogdanRuzh,1,0,0,0,0,0,system information system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu lts bit tensorflow installed from source or binary source tensorflow version use command below tensorflow r python version python version bazel version if compiling from source bazel release gcc/compiler version if compiling from source gcc ubuntu ubuntu cuda/cudnn version no cuda gpu model and memory no gpu but i-k with gb ddr exact command to reproduce run the script belowtested on two machines i-k with gb ddr two xeon x with gb ddr describe the problemwhen i build tensorflow with mkl it dropped cpu performance in a strange way performance of individual core is much higher but for multicore is much worse.its a big epic bottleneck for my project and i cant solve it by myself i will appreciate any help tf installation from sources with mkl support tensorflow r installed from source configured with jemalloc as malloc support and other configure settings ignored bazel build config=mkl c opt tensorflow/tools/pip_package:build_pip_package bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkg pip install tmp/tensorflow_pkg/tensorflow-..-cp-cpmu-linux_x_.whl run tests one core sall cores s tf installation with pip pip install tensorflow tensorflow-..-cp-cpmu-manylinux_x_.whl installed) run tests: one core sall cores s source code logs import time import numpy as np import tensorflow as tf from tensorflow.contrib import slim from tensorflow.contrib.slim.python.slim.nets.inception_v import inception_v inception_v_arg_scope input_shape features tf.placeholder(tf.float input_shape with slim.arg_scope(inception_v_arg_scope predictions end_points inception_v(features is_training=false remove to utilize all cores session_conf tf.configproto(intra_op_parallelism_threads inter_op_parallelism_threads with tf.session(config=session_conf as sess sess.run(tf.global_variables_initializer sess.run(tf.local_variables_initializer images np.random.random(input_shape consumption for i in range tick time.time sess.run(predictions feed_dict={features images consumption.append(time.time tick print np.mean(consumption
281359670,15308,https://api.github.com/repos/tensorflow/tensorflow/issues/15308,wbaek,1,0,0,0,0,0,fix possible compute high rank hessians
281217534,15285,https://api.github.com/repos/tensorflow/tensorflow/issues/15285,hanfeisun,2,0,0,0,0,0,currently tensorflow supports using accuracy as a metric for models performance.however for unbalanced datasets kappa coefficient is a commonly used metric would it be possible to add this one in the model.metrics
281211460,15284,https://api.github.com/repos/tensorflow/tensorflow/issues/15284,flx42,2,0,0,0,0,0,current image size for nightly-gpu : sh docker images tensorflow/tensorflow:nightly-gpurepository tag image id created sizetensorflow/tensorflow nightly-gpu fcfbb hours ago gb current dockerfile see two potential improvements we already switched the from to use the runtime base image of cuda we could go one step further and use nvidia/cuda:.-base this flavor of cuda is new with cuda it just installs the repos and libcudart you have to manually install the cuda libraries you want afterwards the gain wouldnt be that big with tensorflow since you use most of the libraries from the cuda toolkit but we will at least remove npp from the shipped image looks like many dependencies in the current dockerfile are required for jupyter can we provide a new tag without jupyter or stop shipping jupyter in the runtime image users will still be able to use the devel image.i have a quick dockerfile proof-of-concept with both improvements: from nvidia/cuda:.-base-ubuntu.label maintainer=gunhan gulsoy gunan@google.com>run echo usr/local/cuda-./extras/cupti/lib etc/ld.so.conf.d/cupti.conf apt-get update apt-get install y no-install-recommends libgomp libcudnn cuda-command-line-tools cuda-cudart cuda-cufft cuda-cublas cuda-cusparse cuda-curand cuda-cusolver python-pip rm rf var/lib/apt/lists/run pip no-cache-dir install upgrade pip setuptoolsrun pip no-cache-dir install is now gb down from gb and there is still room for improvement in cuda when cupti has its own package today we have to pull cuda-command-line-tools or if we disable cupti tracing not sure if possible
280830144,15254,https://api.github.com/repos/tensorflow/tensorflow/issues/15254,werner-rammer,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary) :from source tensorflow version use command below python version bazel version if compiling from source) :using cmake gcc/compiler version if compiling from source) :msvc c:\program files x)\microsoft visual studio vc\bin\amd\cl.exe cuda/cudnn version :cuda cudnn gpu model and memory :nvidia gtx gb exact command to reproduce describe the problemi build tensorflow from source following these instructions i use this cmake command: cmake g visual studio win t v,host=x dcmake_build_type=relwithdebinfo dswig_executable=e:/dev/swigwin-../swig.exe dtensorflow_enable_gpu=on dcudnn_home=c:\program files\nvidia gpu computing toolkit\cuda dtensorflow_build_shared_lib=on dtensorflow_win_cpu_simd_options=/arch:avx dtensorflow_enable_grpc_support=off and msbuild p:configuration=relwithdebinfo tensorflow.vcxproj (i also tried release as configuration same outcome).the result of the build process is tensorflow.dll i use a separate c program using qt to link against the built dll in general everything works fine i can load a saved tensorflow graph and run it inference).the problem is now that many gpu-ops are not linked into tensorflow.dll for example softmax in my example most ops run on gpu but softmax on cpu with a huge performance impact gpu use why i think this is the case tensorflow::logallregisteredkernels lists softmax but with cpu only looking at tensorflow.dll with dependecywalker has the same result softmax cpu only when i check tf_core_gpu_kernels.lib then gpu code is there e.g tf_core_gpu_kernels_generated_softmax_op_gpu.cu.cc.lib workaroundafter quite some time trying to figure this out i found a hackish workaround:looking at the output of msbuild increased verbosity level it looks as if the python script create_def_file.py is executed without using the tf_core_kernels.lib c:\python\python.exe e:/dev/tensorflow/tensorflow/contrib/cmake/tools/create_def_file.py input e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tensorflow_static.lib;e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tf_protos_cc.lib output e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tensorflow.def target tensorflow.dll what i did is the following create a def-file including the gpu kernels: c:\python\python.exe e:/dev/tensorflow/tensorflow/contrib/cmake/tools/create_def_file.py input e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tensorflow_static.lib;e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tf_protos_cc.lib;e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tf_core_gpu_kernels.lib output e:/dev/tensorflow/tensorflow/contrib/cmake/build/relwithdebinfo/tensorflow_wr.def target tensorflow.dll link tensorflow.dll with the created def file tensorflow_wr.def this did not work since i got a couple of missing-unresolved-externals errors as they were all related to lstm/rnn i ended up re-creating tf_core_gpu_kernels.lib issuing the lib.exe omitting tf_core_gpu_kernels_generated_gru_ops_gpu.cu.cc.obj and tf_core_gpu_kernels_generated_lstm_ops_gpu.cu.cc.obj finally linking tensorflow.dll worked after i manually dropped a couple of symbols from tensorflow_wr.def unresolved external symbols).with this workaround it works fine all ops including softmax run on gpu performance increased by a factor of i submit as an issue since i believe it should work out of the box!some more details on stackoverflow"
280783621,15243,https://api.github.com/repos/tensorflow/tensorflow/issues/15243,yongtang,2,0,0,1,0,0,this fix tries to adds support for explicit broadcasting in tensorflow as was suggested in this fix adds the op of tf.broadcast_to which is equivalent to the numpy.broadcast_to in numpy.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
280728601,15237,https://api.github.com/repos/tensorflow/tensorflow/issues/15237,qianyizhang,8,0,0,0,0,0,this is a duplicate request from pytorch issue which i even reuse their issue title the fix from the paper seems to be trivial but im not sure how tf should approach this perhaps just adding another parameter
280502581,15213,https://api.github.com/repos/tensorflow/tensorflow/issues/15213,rongjiecomputer,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows x tensorflow installed from source or binary source tensorflow version use command below master python version bazel version if compiling from source gcc/compiler version if compiling from source vs cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/asimilar to but specifically about running tfcompile on windows rather than linux to produce x_-windows-msvc binaries.xla/aot depends on llvm which has excellent windows support via cmake but bazel cannot interop with cmake llvm.build is auto-generated and the script to generate it is not open-sourced this make it difficult for external contributor to make improvement tensorflow/compiler might not need too much changes as already addressed some of them.one possible path is to let user to run cmake in host machine when invoking configure.py then feed cmake generated files into custom script to generate llvm.build .note: rumour has it that there is a google-internal tool called tfnative to generate h/.cpp files instead of lib binaries though i suspect that even if the tool is open-sourced it might not be immediately available for windows developers
280398494,15205,https://api.github.com/repos/tensorflow/tensorflow/issues/15205,gunan,1,0,0,0,0,0,our default dockerfiles now use cuda-cudnn.no need for this file anymore.cc flx
280226159,15188,https://api.github.com/repos/tensorflow/tensorflow/issues/15188,ghost,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below master branch commit ad python version n/a bazel version if compiling from source gcc/compiler version if compiling from source g cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce bazel build tensorflow:libtensorflow.so describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.im trying to compile using bazel build tensorflow:libtensorflow.so .im getting this error: error home/rdi/workspace/common/tensorflow/tensorflow/core/build c compilation of rule tensorflow/core:random_ops_op_lib failed exit in file included from tensorflow/core/framework/allocator.h from tensorflow/core/framework/tensor.h from tensorflow/core/framework/attr_value_util.h from tensorflow/core/framework/node_def_util.h from tensorflow/core/framework/shape_inference.h from tensorflow/core/framework/common_shape_fns.h from tensorflow/core/ops/random_ops.cc::./tensorflow/core/framework/numeric_types.h in constructor tensorflow::bfloat::bfloat(float):./tensorflow/core/framework/numeric_types.h error isnan was not declared in this scope if isnan(v
280058773,15180,https://api.github.com/repos/tensorflow/tensorflow/issues/15180,shahinkl,3,0,0,0,0,0,describe the problemgetting the below error when using stagingarea. valueerror fetch argument tf.operation group_deps type=noop cannot be interpreted as a tensor operation name group_depsop noop the error happens only after completing a few steps it would be a great help if someone can place simpler examples of proper usage of stagingarea source code logs compute_stage_put_op compute_stage.put(iterator.get_next if compute_stage_put_op.type stage compute_stage_ops.append(compute_stage_put_op
279753655,15155,https://api.github.com/repos/tensorflow/tensorflow/issues/15155,Petersingh01,1,0,0,0,0,0,"hi,i am new to tensorflow and i am trying to train model with my own data but i am getting below error e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbank e tensorflow/core/kernels/mfcc_mel_filterbank.cc input too short to compute filterbankand i am using below command to trainbazel run tensorflow/examples/speech_commands:train data_dir=sound wanted_words=yes,no data_url"
279703361,15150,https://api.github.com/repos/tensorflow/tensorflow/issues/15150,zotroneneis,1,0,0,0,0,0,i have done extensive testing of the variational_recurrent option in the tf.contrib dropout wrapper and neither me nor my colleagues can explain the extreme perplexity jumps that are caused by it.i am training an rnn language model on the penn treebank dataset the model code is very similar to the one provided in the tensorflow tutorial using the same learning parameters hidden sizes etc like the medium config i am using the newest tensorflow version consider the following models together with the dropout values used in the tf.contrib dropout wrapper if not mentioned no further regularization was used. model input dropout state dropout output dropout variational_recurrent=true perplexity test set perplexity validation set model input dropout state dropout output dropout variational_recurrent=true perplexity test set perplexity validation set model as a comparison input dropout output dropout variational_recurrent=false perplexity test set perplexity validation set i have tested various architectures with and without variational dropout i could not find an explanation for the fact that the perplexity sometimes jumps up to when using variational dropout also the effects vanish when tying the embedding and softmax weights.in general variational dropout does not improve but worsen the results which is different to the results reported in recent papers using variational dropout on the ptb dataset).to test this problem further i have adapted the official tensorflow tutorial to use variational dropout instead of standard dropout by removing lines and replacing lines with if is_training and config.keep_prob cell tf.contrib.rnn.dropoutwrapper(cell input_keep_prob=config.keep_prob output_keep_prob=config.keep_prob state_keep_prob=config.keep_prob variational_recurrent=true dtype=tf.float input_size=config.hidden_size)training the medium model with this configuration causes the same issues i.e perplexity system information have i written custom code as opposed to using a stock example script provided in tensorflow custom code os platform and distribution debian gnu/linux jessie tensorflow version v..-rc--ga python version python gcc/compiler version :tf.version tf.git_version v..-rc--gatf.compiler_version v..-rc--ga
279528940,15140,https://api.github.com/repos/tensorflow/tensorflow/issues/15140,tfboyd,9,0,0,0,0,0,update feb- the plan of record is to stick with cuda x until cuda that plan has an issue in that cuda and cause problems with xla that should be resolved with cuda the soft plan is we would move to when it comes out if it resolves the issues with xla update jan- cuda requires an upgrade to device driver cuda was moving device drivers is painful for production environments we are not going to move the default builds to cuda or we will stick with cuda likely until cuda we will move cudnn forward which will have a larger impact and not require device drive upgrades this space is developing as everyone involved evolves their processes and learns from the past while i cannot promise anything i do want to create a channel where we are building and testing the latest cuda x so we can track performance improvements and have some avenue for people to get those builds the testing infrastructure is large and maintaining this has a cost i hope to find a middle ground as i like perf testing the latest libraries original message the purpose of this thread is to keep cuda questions related to when it will be in tensorflow in a single area separate issues are fine i will try to keep this first comment updated with information as it comes out current status unknown waiting for rc and gathering information to formulate a plan p.s there is a tendency to treat tensorflow like a one way product i want to continue to change that with this type of dialog and transparency many people outside google will contribute to cuda support for tensorflow
279460310,15136,https://api.github.com/repos/tensorflow/tensorflow/issues/15136,ccordoba12,8,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce pip install tensorflow describe the problemwhen you run pip install tensorflowin a virtualenv with python or one of the dependencies pulled by pip is enum as the following console output shows pip install tensorflowcollecting tensorflow downloading tensorflow-..-cp-cpm-manylinux_x_.whl mb mb kb/s collecting numpy from tensorflow using cached numpy-..-cp-cpm-manylinux_x_.whlcollecting six from tensorflow using cached six-..-py.py-none-any.whlcollecting protobuf from tensorflow downloading protobuf-...post-cp-cpm-manylinux_x_.whl mb mb kb/s requirement already satisfied wheel in virtualenvs/tf/lib/python./site-packages from tensorflow)collecting enum from tensorflow downloading enum-..-py-none-any.whl this package is only necessary if python as described here its an error that tensorflow pullis it for python and besides this package breaks spyder code completion machinery in its editor as it has been verified by several users see for example:spyder-ide/spyder
279240779,15115,https://api.github.com/repos/tensorflow/tensorflow/issues/15115,secsilm,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary pip tensorflow version use command below python version python anaconda custom bit describe the problemi found the result tf.metrics.accuracy returns is incorrect when i trained my model to verify this i wrote a simple program. pythonimport tensorflow as tfsess tf.session()labels tf.placeholder(tf.int)predictions tf.placeholder(tf.int)acc tf.metrics.accuracy(labels predictions)my_acc tf.reduce_mean(tf.cast(tf.equal(labels predictions tf.float))feed_dict labels predictions sess.run(tf.global_variables_initializer())sess.run(tf.local_variables_initializer())sess.run(acc feed_dict sess.run(my_acc feed_dict you can see that acc and my_acc is different and acc is wrong i double checked the doc and still confused is there anything i missed thank you
278807621,15082,https://api.github.com/repos/tensorflow/tensorflow/issues/15082,deepanshuagarwal150,1,0,0,0,0,0,i executed the code and got this error file new.py line in main classifier tf.estimator.estimator(model_fn=model_fn)attributeerror module object has no attribute estimatorpls help
278737093,15067,https://api.github.com/repos/tensorflow/tensorflow/issues/15067,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where aws c sdk version was not high enough to support ecs.this fix updates aws c sdk to this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
278681274,15056,https://api.github.com/repos/tensorflow/tensorflow/issues/15056,ppwwyyxx,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..-rc--gabfcac dev python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :to truly get mean squared error one has to explictly use reduction=reduction.mean this indicates bad naming maybe squared_error is a better name also similar to other names in tf.losses
278607708,15049,https://api.github.com/repos/tensorflow/tensorflow/issues/15049,cuevas1208,1,0,0,0,0,0,"i have trained and saved a cnn model in python tensorflow i can successfully load and run the graph previously saved from my python model in tensorflow using cpu and c with no problem but when i tried to load the same graph using tensorflow using gpu c and i get the following error status state_=unique_ptr code=invalid_argument msg=no opkernel was registered to support op onehot with these attrs registered devices cpu,gpu registered kernels:\n no registered kernels>\n\n\t node one_hot onehot t=dt_float ti=dt_int tensorflow::status my system:windows cuda cudnn cmake cmake-..-win-xpython vs@cuevas"
278534895,15044,https://api.github.com/repos/tensorflow/tensorflow/issues/15044,devinsaini,4,0,0,0,0,0,this is a feature request_for datasets that represent a sequence or time series it can be useful to have a dataset op that creates a rolling window batch over the given dataset for example if i have a tf.data.dataset whose elements represent a time series line breaks separate elements): the rolling window batch would create a dataset with the following elements for window size and stride this operation will be extremely useful for extracting sub sequences from a time series for training rnns and reinforcement learning models
278491520,15041,https://api.github.com/repos/tensorflow/tensorflow/issues/15041,MeteorKepler,0,0,0,0,1,0,system information have i written custom code no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary tensorflow version use command below python version cuda/cudnn version gpu model and memory gtx titan x gb bazel version no use exact command to reproduce using python py describe the problemim using while_loop function to build a cnn because of the scale of input tensors and put the cnn feature into a lstm structure the problem is that if i only do the forward computatoin it is good but if i add the backward computation gradientdescentoptimizer().minimize(loss the memory is significantly insufficient i tried to split the two part of the model cnn and lstm part and both do well with the whole computation i think this is the fact single while_loop cnn net is treated as time distributed when computing gradient every temporary feature vector and gradients occupy the same memory location at each time step when coneected with a lstm the cnn net will be treated as many subnet in computing gradients at every time step in backward computation it will occupy new memory for its temporary feature vector and gradients the number of timesteps is very large so that the memory is significantly not enough.here is a simplified code of my project it will show my cnn+lstm structure source code import tensorflow as tfimport tensorflow.contrib as contribdef vgg_m(input_layer reuse=none input shape batch_size with tf.variable_scope(vgg_m reuse=reuse conv tf.layers.convd(input_layer padding=same activation=tf.nn.relu name=conv pool tf.layers.max_poolingd(conv padding=same name=pool norm tf.layers.batch_normalization(pool name=norm conv tf.layers.convd(norm padding=same activation=tf.nn.relu name=conv pool tf.layers.max_poolingd(conv padding=same name=pool norm tf.layers.batch_normalization(pool name=norm conv tf.layers.convd(norm padding=same activation=tf.nn.relu name=conv conv tf.layers.convd(conv padding=same activation=tf.nn.relu name=conv conv tf.layers.convd(conv padding=same activation=tf.nn.relu name=conv pool tf.layers.max_poolingd(conv padding=same name=pool flatten contrib.layers.flatten(pool fc tf.layers.dense(flatten name=fc return fc batch time_step image_h image_w image_channelinput_tensor tf.random_normal dtype=tf.float vgg_m is a cnn net whose input and output size is batch batch in order to create vgg_m variables for reuse later.vgg_m(input_tensor time_steps initial_t tf.constant dtype=tf.int)initial_outputs tf.tensorarray(dtype=tf.float size=time_steps)def should_continue(t args return t time_stepsdef iteration(t outputs compute cnn feature at time t single_output vgg_m(input_tensor t reuse=true outputs outputs_.write(t single_output return t outputs outputs tf.while_loop(_should_continue iteration initial_t initial_outputs transpose the batch dim and time dim to build a batch time_step feature and send to lstmoutputs tf.transpose(outputs.stack perm outputs tf.reshape(outputs lstm_cell tf.nn.rnn_cell.basiclstmcell()lstm_outputs lstm_state tf.nn.dynamic_rnn lstm_cell outputs sequence_length=tf.constant dtype=tf.int shape dtype=tf.float not really a loss just perform an loss exampleloss tf.reduce_max(tf.reduce_max(tf.reduce_max(lstm_outputs train_op tf.train.gradientdescentoptimizer(.).minimize(loss)sess tf.session()sess.run(tf.global_variables_initializer())sess.run(train_op log w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx titan xmajor minor memoryclockrate ghz pcibusid c:.total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id c w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate gib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate gib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib current allocation summary follows i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin....(many chunks i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin for mib was mib chunk state i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xac of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xac of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xac of size many chunks i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xccc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc summary of in-use chunks by size i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kib...(many chunks i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling gib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mib i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling gib i tensorflow/core/common_runtime/bfc_allocator.cc sum total of in-use chunks gib i tensorflow/core/common_runtime/bfc_allocator.cc stats:limit inuse maxinuse numallocs maxallocsize w tensorflow/core/common_runtime/bfc_allocator.cc x xxxxxxxxxxx w tensorflow/core/framework/op_kernel.cc resource exhausted oom when allocating tensor with shape w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib current allocation summary follows i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib client-requested for chunks b in use in bin b client-requested in use in bin....(similar chunks i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling gib i tensorflow/core/common_runtime/bfc_allocator.cc sum total of in-use chunks gib i tensorflow/core/common_runtime/bfc_allocator.cc stats:limit inuse maxinuse numallocs maxallocsize w tensorflow/core/common_runtime/bfc_allocator.cc x xxxxxxxxxxx w tensorflow/core/framework/op_kernel.cc resource exhausted oom when allocating tensor with shape w tensorflow/core/framework/op_kernel.cc resource exhausted oom when allocating tensor with shape node while/vgg_m/conv_/convolution convd t=dt_float data_format=nhwc padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/gpu: (while/vgg_m/conv_/relu while/vgg_m/conv_/convolution/enter) ....(same thing occuring again and again util it is closed is this an unknown bugi saw a different processing with loop_state in gradients function so is this an unknown bug with multiple loop operation(lstm contains loop as well
278437055,15037,https://api.github.com/repos/tensorflow/tensorflow/issues/15037,facaiy,1,0,0,0,0,0,because is closed by mistake hence the pr is reopened to resolve issue.because we dont see causal padding in other use cases expect of ntc we choose to modify code at convd instead of tf.nn.convolution.ref convd implementation in keras how to test x add test for layers.convd x add test for keras.layers.convd pass all tests
278118331,15002,https://api.github.com/repos/tensorflow/tensorflow/issues/15002,PatWie,4,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes i followed the official documentation for custom operations os platform and distribution e.g linux ubuntu linux pc generic ubuntu smp wed jun utc x x x gnu/linux tensorflow installed from source or binary tried both tensorflow version use command below version status comment v..-rc--gad not working from pip or from source v..-rc--gfd working from pypip python version irrelevant bazel version if compiling from source) : build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu oct build timestamp build timestamp as int gcc/compiler version if compiling from source irrelevant tried both g g cuda/cudnn version irrelevant* nvcc nvidia r cuda compiler drivercopyright c nvidia corporationbuilt on sun_sep___::_cdt_cuda compilation tools release v gpu model and memory irrelevant exact command to reproduce : bashcd tmpmkdir tf_issuecd tf_issuevirtualenv testsource test/bin/activatepip install tensorflow in some way either tensorflow-gpu or from wheel package created by bazelgit clone tf_custom_opcmake make describe the problemcompiling custom ops with cpp#include tensorflow/core/util/cuda_kernel_helper.h fails due to missing files /code/lib/python./site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h fatal error cuda/include/cuda.h no such file or directory this file cuda/include/cuda.h does not exists neither in the pip package nor in the git repository.removing include tensorflow/core/util/cuda_kernel_helper.h gives plenty of other issues /code/kernels/matrix_add_kernel.cu error namespace tensorflow has no member cudalaunchconfig/code/kernels/matrix_add_kernel.cu error namespace tensorflow has no member cudalaunchconfig/code/kernels/matrix_add_kernel.cu error expected a code/kernels/matrix_add_kernel.cu error identifier cfg is undefined/code/kernels/matrix_add_kernel.cu error namespace tensorflow has no member cudalaunchconfig/code/kernels/matrix_add_kernel.cu error expected a code/kernels/matrix_add_kernel.cu error identifier cfg is undefined as i already wrote in a related issue the commit ceeabbcebacadd is causing this issue by changing diff-#include third_party/gpus/cuda/include/cuda.h+#include cuda/include/cuda.h copying the old cuda.h gives ... /local/lib/python./site-packages/tensorflow/include/tensorflow/core/platform/default/mutex.h fatal error nsync_cv.h no such file or directory which does not exist too.this problem is not related to custom code it is related to ignore/omitting files in commit ceeabbcebacaddas mention in this affects many people in fact the entire way of writing customs ops with cuda seems to be broken copying own source-code to the tensorflow-repo was not necessary until tf interestingly even recent nips paper implementations state in their readme they only support tfv i dont think the proposed workaround of downgrading to tfv should be the way to go
278038965,14995,https://api.github.com/repos/tensorflow/tensorflow/issues/14995,boeddeker,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below v..-rc--ga python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problemthe function tf.estimator.estimator.export_savedmodel does not accept a pathlib.path object because tensorflow.python.util.compat.as_bytes used in tensorflow.python.estimator.export.get_timestamped_export_dir can not convert pathlib.path to bytes.here the code snippet from tensorflow.python.estimator.export.get_timestamped_export_dir : python export_dir os.path.join compat.as_bytes(export_dir_base compat.as_bytes(str(export_timestamp))) i would write a pr but i am not sure how to solve this problem in python the following works in python if i remember correctly it was py where os.path start to accept pathlib.path ): python export_dir compat.as_bytes(os.path.join export_dir_base str(export_timestamp))) since the name tensorflow.python.util.compat.as_bytes does not imply that the input is a path i am not sure if that would be a better place to solve the problem source code logshere some pseudo code i hope with this example the tensoflowers are able to reproduce this bug in py pythonfrom pathlib import pathtf.estimator.estimator(...).export_savedmodel path(path/to/save export_input_fn as_text=true
277671638,14967,https://api.github.com/repos/tensorflow/tensorflow/issues/14967,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where the bmp content length was not checked before reading the buffer as a result decode_bmp might trigger a crash if the content of bmp is incomplete.this fix fixes the issue by adding the needed check before reading the data.additional test cases have been added.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
277260911,14928,https://api.github.com/repos/tensorflow/tensorflow/issues/14928,MtDersvan,2,0,0,0,0,0,a fix that adresses the following issue candidate sampling ops accept a tensor instead of an int per batch as a num_true parameter interface change switching the attr to input i left out one check in computeaccidentalhits setshapefn and would be open to any suggestion on how to fix that
277168753,14924,https://api.github.com/repos/tensorflow/tensorflow/issues/14924,gyang274,1,0,0,0,0,0,"this is related to issue this is ok: import tensorflow as tfsess tf.interactivesession()xx tf.constant shape dtype=tf.float)yy tf.constant shape dtype=tf.float)zz xx yysess.run( zz ) however: x tf.constant shape= ,,,,, )y tf.constant shape= ,,,,, )z x ysess.run(z) gives an error: unimplementederror see above for traceback broadcast between and is not supported yet node mul mul t=dt_int device=/job:localhost/replica:/task:/device:cpu: (const const_) log: ---------------------------------------------------------------------------unimplementederror traceback most recent call last)"
276889868,14898,https://api.github.com/repos/tensorflow/tensorflow/issues/14898,joa23,2,0,0,0,0,0,bluf:build fails on osx../tensorflow/core/framework/variant.h error constexpr constructor calls non-constexpr function std::__::unique_ptr<_tp dp>::unique_ptr with tp=tensorflow::variant::valueinterface dp=std::__::default_delete::unique_ptr with tp=tensorflow::variant::valueinterface dp=std::__::default_delete
276889280,14897,https://api.github.com/repos/tensorflow/tensorflow/issues/14897,pangzhan27,4,0,0,0,0,0,system information tensorflow installed from source tensorflow version r python version bazel version gcc/compiler version cuda/cudnn version gpu model and memory geforce gtx describe the problemwhen applying the multirnncell as below an error occurs the code went well in tensorflow r source codeinput_list is a list of tensor with shape none n_hidden lstm tf.nn.rnn_cell.basiclstmcell(n_hidden)stacked_lstm tf.nn.rnn_cell.multirnncell( lstm *)outputs states tf.nn.static_rnn(stacked_lstm input_list dtype=tf.float errorvalueerror dimensions must be equal but are and for rnn/rnn/multi_rnn_cell/cell_/cell_/basic_lstm_cell/matmul op matmul with input shapes however when only applying one single lstm i works well.opinions:when calculating the basiclstmcell will be called where a class named linear will be initialized as an example in my case the variable self.weight in this class will be initialized as code from rnn_cell_impl.py *if self._linear is none self._linear linear( inputs h self._num_units true)but when multirnncell is the case for example a layers lstm in the second layer the weight should be h in last layer o in last layer disappointingly the weight will only be initialized once and stay with the shape due to the sentence if self._linear is none so that the reason why such error occurs.i try to comment out this sentence but since share variable mechanism is related it dosent work and induces other problem.valueerror trying to share variable rnn/multi_rnn_cell/cell_/basic_lstm_cell/kernel but specified shape and found shape any idea how to solve this problem efficiently
276885058,14896,https://api.github.com/repos/tensorflow/tensorflow/issues/14896,jackyko1991,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemwhen using cmake config to build tensorflow under windows after enabling gpu following error appears: cmake error at tf_core_framework.cmake list list sub-command remove_item requires two or more arguments.call stack most recent call first cmakelists.txt include) after checking tf_core_framwork.cmake it is requesting to remove tensorflow_source_dir}/tensorflow/core/platform/default/gpu_tracer.cc from core resources however this file is missing from the latest tensorflow commenting this line help to finish cmake config but i dont think this is a good practice.would the development team consider to update the cmake file source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
276791870,14884,https://api.github.com/repos/tensorflow/tensorflow/issues/14884,teaglin,1,0,0,0,0,0,system information os platform and distribution linux and mac os tensorflow installed from source or binary mac binary linux source tensorflow version use command below mac v..-rc--geee linux v..-rc--gfaa rc python version mac linux bazel version if compiling from source linux gcc/compiler version if compiling from source linux gcc cuda/cudnn version linux cuda cudnn gpu model and memory linux titanxp describe the problemi trained a model using the tensor flow object detection api with faster_rcnn_resnet i then exported the model using the provided export_inference_graph.py the model works on linux but does not work on mac both platforms are using tensor flow ive provided the crash log source code logs e tensorflow/core/common_runtime/executor.cc executor failed to create kernel invalid argument nodedef mentions attr t not in op
276532307,14857,https://api.github.com/repos/tensorflow/tensorflow/issues/14857,reyne-d,8,0,0,0,0,0,i know we can ues dataset.shuffle(buffer to shuffle dataset.but i have a large image dataset with images if i use the follwing code with dataset dataset.shuffle(buffer_size the cost of time to load images is too long for me.is there any way to shuffle the whole dataset in dataset api?? from tensorflow.contrib import datadef input_pipeline(filenames batch_size define a tf.contrib.data.dataset for iterating over one epoch of the data dataset data.textlinedataset(filenames dataset dataset.map(decode_func dataset dataset.shuffle(buffer_size equivalent to min_after_dequeue dataset dataset.batch(batch_size return an initializable iterator over the dataset which will allow us to re-initialize it at the beginning of each epoch return dataset.make_initializable_iterator
276522120,14855,https://api.github.com/repos/tensorflow/tensorflow/issues/14855,XiaXuehai,6,0,0,0,0,0,"i install the tensorflow on mac from source.but when i run the tensorboard,and got a error like this pro-:desktop xxh tensorboardtraceback most recent call last file users/xxh/anaconda/bin/tensorboard line in module from tensorboard.main import run_mainimporterror cannot import name run_main how to fix it"
276516232,14854,https://api.github.com/repos/tensorflow/tensorflow/issues/14854,JoshVarty,2,0,0,0,0,0,working on i used as a guide for my work here.i have added batch support for flip_left_right flip_up_down random_flip_left_right random_flip_up_down transpose_image rot i have corrected existing tests in image_ops_test.py and introduced a number of new tests based on existing tests for d inputs this is my first contribution to this repository and i have tried to follow the contributing guidelines however running pylint on image_ops_impl.py and image_ops_test.py revealed a number of pre-existing style violations ive tried to fix the ones relevant to my work but may have missed some
276504386,14850,https://api.github.com/repos/tensorflow/tensorflow/issues/14850,fesun,2,0,0,0,0,0,i built tensorflow v from source code sometimes it crashed inside kernel this happened on both linux and windows platform.linux call stack: c pywrap_tensorflow_internal.so+xeef cc_destroy_call_elem+xdfc pywrap_tensorflow_internal.so+xc grpc_call_stack_destroy+xc pywrap_tensorflow_internal.so+xbfc grpc_exec_ctx_flush+xcc pywrap_tensorflow_internal.so+xdad grpc_call_unref+xedc pywrap_tensorflow_internal.so+xfaae grpc::clientcontext::~clientcontext()+xec pywrap_tensorflow_internal.so+xe tensorflow::grpcremoteworker::rpcstate::~rpcstate()+xc pywrap_tensorflow_internal.so+xe tensorflow::grpcremoteworker::rpcstate::oncompleted(bool)+xe windows call stack efc ffa ec ucrtbase!abort+xe d:\rs\minkernel\crts\ucrt\src\appcrt\startup\abort.cpp eff ffa ad pywrap_tensorflow_internal!tensorflow::allocator::allocatedsize+xd ef ffa baf pywrap_tensorflow_internal!tensorflow::lookup::subtlemustcopyunlessstringorfloat<__int>+xee ef ffa ccac pywrap_tensorflow_internal!tensorflow::lookup::subtlemustcopyunlessstringorfloat<__int>+xef efa ffa aeb pywrap_tensorflow_internal!tensorflow::allocator::allocatedsize+xbc efd ffa e pywrap_tensorflow_internal!tensorflow::lookup::subtlemustcopyunlessstringorfloat<__int>+xcc ef ffa ca pywrap_tensorflow_internal!std::vector::~rpcstate+xfa efa ffa pywrap_tensorflow_internal!tensorflow::workerinterface::~workerinterface+xb efd ffa cf pywrap_tensorflow_internal!tensorflow::grpcremoteworker::rpcstate::oncompleted+xc efa ffa bae pywrap_tensorflow_internal!tensorflow::workercachepartial::~workercachepartial+xf both indicating that tensorflow rpc framework may have some bug
276409159,14840,https://api.github.com/repos/tensorflow/tensorflow/issues/14840,dongpilYu,1,0,0,0,0,0,change path
276292242,14826,https://api.github.com/repos/tensorflow/tensorflow/issues/14826,BKZero,2,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below python version but actually i am talking about c code bazel version if compiling from source all tried gcc/compiler version if compiling from source cuda/cudnn version n/a gpu model and memory cpu mode exact command to reproduce bazel run c opt tensorflow/cc/face:face describe the problemi add opencv as a third party lib to tensorflow and modify the workspace and build file to include it to the project it works well when i use tensorflow or version before it recently i update my tensorflow to the newest version it recommand i must update my bazel at least i use with jdk before).and when i update bazel and move my own code to the new project compiling seems ok but when i run the binary it seems not right i can not load a jpeg file when i use cv::imread it doesnt crash but return a cv::mat with size in the new project i can load a bmp file properly so i guess it is because the project does not link the libjpeg.but i never need to link the libjpeg manually because it is included in the opencv library so i guess there is a bug in the new version of tensorflow.i have tried the linkopt with ljpeg but it does not work source code logsworkspace file:new_local_repository name opencv path usr/local build_file opencv.build,)build file of opencv:cc_library name opencv srcs glob( lib/*.so hdrs glob( include/ /*.hpp includes include visibility visibility:public linkstatic build file of my codetf_cc_binary name face srcs face.cc includes deps tensorflow/cc:cc_ops tensorflow/cc:client_session tensorflow/core:tensorflow opencv//:opencv copts fopenmp linkopts lgomp ljpeg ,)my code cv::mat img cv::imread(pic.jpg std::cout<"
276277207,14823,https://api.github.com/repos/tensorflow/tensorflow/issues/14823,arixlin,1,0,0,0,0,0,add mobilenet frozen_graph.pb link
276263460,14819,https://api.github.com/repos/tensorflow/tensorflow/issues/14819,bri-jones,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary tensorflow version use command below v..-rc--ga python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see belowyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.the keras dropout layer constructor tensorflow/python/keras/_impl/keras/layers/core.py sets support_masking=true and then calls its super constructor which sets it back to false other layers defined in that module appear to set support_masking=true after the super constructor call source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem. from tensorflow.contrib.keras.api.keras.models import sequentialfrom tensorflow.contrib.keras.api.keras.layers import dropout inputlayer lstm masking if name main test true def model model sequential model.add(inputlayer model.add(masking model.add(dropout def model model sequential model.add(inputlayer model.add(masking model.add(lstm return_sequences=true model.add(dropout if test model else model() traceback most recent call last file expose_dropout_bug.py line in module model.add(dropout file venv/lib/python./site-packages/tensorflow/python/keras/_impl/keras/models.py line in add output_tensor layer(self.outputs file venv/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in call output super(layer self).__call__(inputs kwargs file venv/lib/python./site-packages/tensorflow/python/layers/base.py line in call output_mask self.compute_mask(inputs previous_mask file venv/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in compute_mask but was passed an input_mask str(mask))typeerror layer dropout does not support masking but was passed an input_mask tensor(masking/any shape dtype=bool
276218000,14812,https://api.github.com/repos/tensorflow/tensorflow/issues/14812,hantek,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu debian gnu/linux jessie tensorflow installed from source or binary from anaconda with command: pip install ignore-installed upgrade tensorflow version use command below v..-rc--ga python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version cuda gpu model and memory titan xp g exact command to reproduce :this fails: python import torch import tensorflow as tfsegmentation fault core dumped describe the problemi am using pytorch version for python with cuda support i installed it from the following command: conda install pytorch torchvision cuda c soumith when i use tensorflow alone it works fine i.e doing an import like python import tensorflow as tf has no problem also importing tensorflow before torch seems fine as well.however if i import pytorch before tensorflow it fails and reported a segmentation error as shown above
276194025,14809,https://api.github.com/repos/tensorflow/tensorflow/issues/14809,nikonikolov,2,0,0,0,0,0,despite the numerous submitted issues tf.layers.batch_normalization still feels completely unusable the major problems are it does not allow for input tensors with varying shapes it is complete nonsense to have a fixed batch size it should be allowed for the batch dimension to be vary one needs to manually update the running mean and variance this is very uncomfortable and a very common pitfall for many beginners while it would take just a couple of lines to do the update internally based on the value of the training parameter.i have recently seen too many custom implementations of a batch normalization layer because of the above problems and it will definitely be very useful if these problems are fixed asap.i am using tensorflow-gpu version
276171279,14803,https://api.github.com/repos/tensorflow/tensorflow/issues/14803,gunan,1,0,0,0,0,0,disable failing data_utils_test in cmake and bazel builds.-disable session_partial_run_test in bazel build it is already notrunning under cmake build.-increase cmake build log verbosity as we still canot see the rootcause of failures
276130886,14801,https://api.github.com/repos/tensorflow/tensorflow/issues/14801,sylvain-bougnoux,5,0,0,0,0,0,as many of us i am trying to get tf build successfully on windows using the latest version of everything as far as i can judge i could do it but with some hacks as it is too long for me to complete i would like to share what i did for help finalizing it is too early for a pr.i am using cmake though came out i have low cmake skill level.i am not trying the python bindings.vs is the community edition.without gpu it is easy the only issue is the heap overflow c or c the trick is to reduce parallel build by msbuild m p:cl_mpcount such that is approximately the number of core you really have at least it worked for me using zm did not work for me despite a lot of available memory g).with gpu it is more tricky the tf_core_gpu_kernels.vcxproj does not compile at all afaiu the cmake strategy changed from v to allow parallel computing cuda is now treated as another language without modifications nvcc simply returns with code error or nothing happen i am not sure here are my modifications from v.).from tensorflow/tensorflow/contrib/cmake adapt cmakelists.txt a little change cuda to cuda l add enable_language(cuda l the set(cuda_nvcc_flags directives do not work anymore see below add capabilities and in l as well l might not be needed it is only for performance change to and to l and similarly in l in tf_core_kernels.cmake add set_source_files_properties(${tf_core_gpu_kernels_srcs properties language cuda to recognize cu.cc extensions as cuda files in l rename cuda_add_library as add_library l edit this is the trick tf_core_gpu_kernels.vcxproj in the release section encompass cl.exe flags ie bigobj nologo ob with the xcompiler=/bigobj ob directive l these former flags are for the c compiler not for nvcc and result in the crash add just before expt-relaxed-constexpr still in the additionaloptions switch performdevicelink from false to true l..then everything compile msbuild on tf_tutorials_example_trainer.vcxproj and this tuto works the remaining point before pr is to avoid third step i.e give the right directives to nvcc by understanding how the cuda_nvcc_flags works and add the linking hope this solution will work without missing symbols otherwise it is a nightmare both cuda and cmake are not aware of vs cmake compilation is not incremental and takes about h could use precompiled headers especially in tf_core_kernels
276096018,14798,https://api.github.com/repos/tensorflow/tensorflow/issues/14798,carlthome,2,0,0,0,0,0,tensorflow lite provides a list of currently supported ops here and i wonder if xla could also have such a list its rough to develop and train a model with the full tensorflow python api only to get stuck during aot compilation because of missing ops kernels in the tfxla bridge
276033943,14792,https://api.github.com/repos/tensorflow/tensorflow/issues/14792,ZhengshengWei,1,0,0,0,0,0,fix document of maskedconv and maskedconvd could be revised too reasons are as follows
275652020,14749,https://api.github.com/repos/tensorflow/tensorflow/issues/14749,paduvi,0,0,0,0,0,4,i am trying to multi-level classify with hierarchical softmax by using tensorflow but i could find any existing hs implementation in tensorflow.is there any other way to implement hs using tensorflow?it would be helpful if hierarchical softmax support is given in tf
275539722,14736,https://api.github.com/repos/tensorflow/tensorflow/issues/14736,MyAusweis,6,0,0,0,0,1,dear alli have laptop lenovo g i with amd radeon r m-gb for graphic it is also gpu based on this link does tensorflow support this amd for computing the same like what tensorflow did with gpu from nvidia if it has to be configured from the source what is the setting of tensorflow that when i compile it it will run the gpu?thx
275424839,14731,https://api.github.com/repos/tensorflow/tensorflow/issues/14731,mpeniak,2,0,0,0,0,0,"hi guys,i have trained a custom ssd-mobilenet-v x input and currently running it via tensorflow android demo tensorflow mobile i would love to convert this model to the lite format and possibly quantize it and run it via tensorflow lite to see how much has the performance improved currently the inference takes around ms on google pixel version could you please let me know whats the best way to deploy my custom model for object detection?thank you very much in advance!martin peniak"
275415719,14729,https://api.github.com/repos/tensorflow/tensorflow/issues/14729,parsa-saadatpanah,7,0,0,0,0,0,this code: pythonimport tensorflow as tfx tf.constant dtype=tf.int changing this to tf.float solves the problemcell tf.nn.rnn_cell.lstmcell(num_units initial_state cell.zero_state(tf.shape(x dtype=tf.float)outputs state tf.nn.dynamic_rnn(cell x initial_state=initial_state dtype=tf.float)init_op tf.group(tf.global_variables_initializer tf.local_variables_initializer())with tf.session as sess sess.run(init_op print(sess.run( outputs state )) does not work because the inputs to the lstm are integers and they need to be float however in version i get this error: valueerror initializer for variable rnn/lstm_cell/kernel is from inside a control-flow construct such as a loop or conditional when creating a variable inside a loop or conditional use a lambda as the initializer. which has nothing to do with what is wrong with the code version however generates this error which correctly refers to the problem: typeerror tensors in list passed to values of concatv op have types int float that dont all match
275354586,14722,https://api.github.com/repos/tensorflow/tensorflow/issues/14722,theyonibomber,1,0,0,0,0,0,this is required when running some models as a step for initializing the graph etc
275298527,14713,https://api.github.com/repos/tensorflow/tensorflow/issues/14713,jmaye,10,0,0,0,0,0,ive been using the estimator api with the model_fn and input_fn as shown in the official examples for instance).this all looks great and wonderful however im now facing an issue for going further with it id like to use a model trained on a dataset and transfer it to another dataset in practice i would like to take the weights from the trained model up to the softmax layer and only initialize randomly this final layer then i can do fine-tuning on the new dataset which has different labels for instance.i havent found a way to do what i want is it something missing in the interface can we have something like a variable list to restore from a checkpoint and some other not ideally it would be also good to specify variables to be frozen does that all make sense
275282482,14712,https://api.github.com/repos/tensorflow/tensorflow/issues/14712,keven425,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary source python version python tensorflow version master bazel version homebrew build timestamp cuda/cudnn version n/a gpu model and memory n/a gcc/compiler version if compiling from source apple llvm version clang exact command to reproduce in tensorflow directorycd tensorflow/contrib/cmakemkdir buildcd buildcmake dcmake_build_type=release dpython_executable=/usr/local/bin/pythonmake tf_tutorials_example_trainer describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.im following the instructions here to build using cmake on mac however during make the following error is thrown source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem scanning dependencies of target tf_tutorials_example_trainer building cxx object cmakefiles/tf_tutorials_example_trainer.dir/users/kevenwang/virtualboxshared/another_tf/tensorflow/cc/tutorials/example_trainer.cc.o linking cxx executable tf_tutorials_example_trainerundefined symbols for architecture x ares_cancel referenced from on_readable_cb(grpc_exec_ctx void grpc_error in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o on_writable_cb(grpc_exec_ctx void grpc_error in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o ares_destroy referenced from grpc_ares_ev_driver_unref(grpc_ares_ev_driver in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o ares_free_data referenced from on_srv_query_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o on_txt_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_gethostbyname referenced from grpc_dns_lookup_ares_impl(grpc_exec_ctx char const char const char const grpc_pollset_set grpc_closure grpc_lb_addresses bool char in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o on_srv_query_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_getsock referenced from grpc_ares_notify_on_event_locked(grpc_exec_ctx grpc_ares_ev_driver in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o ares_inet_ntop referenced from on_hostbyname_done_cb(void int int hostent in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_init referenced from grpc_ares_ev_driver_create in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o maybe you meant grpc_ares_init grpc_resolver_dns_ares_init ares_library_cleanup referenced from grpc_ares_cleanup in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_library_init referenced from grpc_ares_init in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_parse_srv_reply referenced from on_srv_query_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_parse_txt_reply_ext referenced from on_txt_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_process_fd referenced from on_readable_cb(grpc_exec_ctx void grpc_error in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o on_writable_cb(grpc_exec_ctx void grpc_error in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o ares_query referenced from grpc_dns_lookup_ares_impl(grpc_exec_ctx char const char const char const grpc_pollset_set grpc_closure grpc_lb_addresses bool char in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_search referenced from grpc_dns_lookup_ares_impl(grpc_exec_ctx char const char const char const grpc_pollset_set grpc_closure grpc_lb_addresses bool char in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_set_servers_ports referenced from grpc_dns_lookup_ares_impl(grpc_exec_ctx char const char const char const grpc_pollset_set grpc_closure grpc_lb_addresses bool char in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o ares_strerror referenced from grpc_dns_lookup_ares_impl(grpc_exec_ctx char const char const char const grpc_pollset_set grpc_closure grpc_lb_addresses bool char in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o grpc_ares_init in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o on_hostbyname_done_cb(void int int hostent in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o on_srv_query_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o on_txt_done_cb(void int int unsigned char int in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o grpc_ares_ev_driver_create in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)ld symbol(s not found for architecture x_clang error linker command failed with exit code use v to see invocation)make tf_tutorials_example_trainer error make cmakefiles/tf_tutorials_example_trainer.dir/all error make cmakefiles/tf_tutorials_example_trainer.dir/rule error
275133489,14699,https://api.github.com/repos/tensorflow/tensorflow/issues/14699,x10000year,2,0,0,0,0,0,im using tf.estimator with custom model_fn when training the estimator usually outputs log like:info:tensorflow:loss step info:tensorflow:loss step sec)info:tensorflow:loss step sec)by default the printed steps should be however when i use the following function which is simplified to reproduce the bug in any place of the model: def my_op(inputs name=none with tf.variable_scope(name default_name=my_scope reuse=false count tf.get_variable(count shape initializer=tf.zeros_initializer trainable=false def myfunc return def myfunc tf.add_to_collection(tf.graphkeys.update_ops count.assign_add return tf.cond(tf.less myfunc myfunc return inputs the log becomes something like:info:tensorflow:loss step info:tensorflow:loss step sec)the global step is always after some tests i found that the global step is not updated if myfunc is not executed for example if i write tf.cond(tf.less(count myfunc myfunc) then the global step is always i suspect that this is caused by the tf.add_to_collection(tf.graphkeys.update_ops in myfunc in which my intend is to update a variable when some condition holds maybe op created inside tf.cond can not be used as a dependency outside but no error message is reported.if i cannot add ops to update_ops inside tf.cond does this imply that stateful operations like tf.layers.batch_normalization can not be used inside tf.cond so i cannot dynamically choose a network module from a set of network modules to execute if the network modules use any stateful operations like tf.layers.batch_normalization?my tensorflow version v..-rc--gafc rc
275094160,14693,https://api.github.com/repos/tensorflow/tensorflow/issues/14693,nbro,4,0,0,0,0,0,basiclstmcell is actually a layer as for a layer in mlps of lstm units each of these lstm units contains a cell each cell of an lstm unit contains a scalar value for the cec and a scalar representing the previous state.people are usually first introduced to mlps or feed-forward and fully connected neural networks before being introduced to rnns and in particular lstms why would you call basiclstmcell a cell if it can be thought more intuitively at least for me as a layer of lstm units as i describe them above containing just one scalar-based cell wouldnt it be less ambiguous to call a basiclstmcell basiclstmlayer ???moreover the first parameter to basiclstmcell s init method is num_units i.e the number of lstm units i.e the number of lstm cells and gates if we have gates for every lstm unit then the total number of gates in one layer of lstms is num_units it almost seems that you created tf to make it as confusing as possible to make it seem hard it also almost seems that the person who wrote the name of the class basiclstmcell is a different person of the person who wrote its init method whats going on a little bit of consistency for once no???a similar argument can be said for multirnncell which a lot more intuitively can be thought as a sequence of layers requestchange classes such as basiclstmcell and multirnncell to have more descriptive names of what they actually are in future versions of tf then change the corresponding documentation to be more compliant with these changes
275066270,14688,https://api.github.com/repos/tensorflow/tensorflow/issues/14688,huanyingjun,5,0,0,0,0,0,i want to write some c test binary using tensorflow lite.from the readme.md i can only see how to build the demo app.could you please tell me how to build tensorflow lite into a static library using android ndk
275009864,14672,https://api.github.com/repos/tensorflow/tensorflow/issues/14672,RainbowZephyr,2,0,0,0,0,0,can someone please explain the flags that can be enabled when compiling tensorflow from source i dont seem to understand the functionality of most of them and they are not documented
274998806,14670,https://api.github.com/repos/tensorflow/tensorflow/issues/14670,dhelleberg,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macosx tensorflow installed from source or binary source tensorflow version use command below master python version bazel version if compiling from source homebrew gcc/compiler version if compiling from source apple llvm version clang cuda/cudnn version gpu model and memory exact command to reproduce see below describe the problemim trying to convert a model from the object-detection framework to a tflite file heres the command i run bazel run config=opt tensorflow/contrib/lite/toco:toco input_file=(path)/models/research/object_detection/output_inference_graph/frozen_inference_graph.pb input_format=tensorflow_graphdef output_format=tflite output_file=(pwd)/mobilenet_v_._.lite inference_type=float input_type=float input_arrays=image_tensor output_arrays=detection_boxes input_shapes=,,,heres the output i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation tensorarraygatherv i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation stridedslice i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation stridedslice i tensorflow/contrib/lite/toco/import_tensorflow.cc converting unsupported operation logicaland f tensorflow/contrib/lite/toco/import_tensorflow.cc check failed getinputscount(node model->flags.drop_control_dependency vs i freezed the model using the python script from the object detection framework:python export_inference_graph.py input_type image_tensor pipeline_config_path samples/configs/ssd_mobilenet_v_coco.config trained_checkpoint_prefix ssd_mobilenet_v_coco___/model.ckpt output_directory output_inference_graph input_shape any idea why the conversion fails?thanks for any help"
274998035,14669,https://api.github.com/repos/tensorflow/tensorflow/issues/14669,jolespin,1,0,0,0,0,0,is it possible to set the random_seed in keras with tensorflow would you need to set it to a specific graph i have not found out how to set the random state in keras and if this doesnt exist could this be a feature in future versions
274527661,14622,https://api.github.com/repos/tensorflow/tensorflow/issues/14622,hernandezurbina,19,0,0,0,0,0,im trying to run tensorflow-gpu through virtualenv via pip in ubuntu i have installed cuda and cudnn v then tested both and they are working fine however when attempting to import tensorflow in python i get the following error traceback most recent call last file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow_internal fp pathname description file usr/lib/python./imp.py line in load_module return load_dynamic(name filename file file usr/lib/python./imp.py line in load_dynamic return load(spec)importerror libcublas.so cannot open shared object file no such file or directoryi located libcublas.so in usr/local/cuda/lib however i see that it references a new version of the library my question is whether something simple can be done like creating a symbolic link to the library with the name of libcublas.so or i have to wait for an update in tf that can run with cuda/cudnn cheers
274381281,14607,https://api.github.com/repos/tensorflow/tensorflow/issues/14607,apollo-time,3,0,0,0,0,0,are you planning to support tensorflow lite on windows specifically windows bit
274235595,14589,https://api.github.com/repos/tensorflow/tensorflow/issues/14589,LearnedVector,25,0,0,0,0,0,describe the problemare you planning to support tensorflow lite on raspberry pi specifically raspberry pi
274107645,14582,https://api.github.com/repos/tensorflow/tensorflow/issues/14582,chrisdonahue,0,0,0,1,0,0,the tf.contrib.ffmpeg.decode and tf.contrib.ffmpeg.encode functions are extremely verbose and make it nearly impossible to see other printed messages.corresponding methods for image such as tf.image.decode_png produce no output under normal conditions and it would be nice for audio/video methods to align with this ffmpeg on valid mp file with loglevel info and without hide_banner old semantics) sh ffmpeg loglevel info nostats i in.mp acodec pcm_sle ar out.wavffmpeg version n--gb copyright c the ffmpeg developers built with gcc gcc configuration enable-gpl enable-version disable-wthreads enable-avisynth enable-bzlib enable-fontconfig enable-freir enable-gnutls enable-iconv enable-libass enable-libbluray enable-libbsb enable-libcaca enable-libfreetype enable-libgme enable-libgsm enable-libilbc enable-libmodplug enable-libmfx enable-libmplame enable-libopencore-amrnb enable-libopencore-amrwb enable-libopenjpeg enable-libopus enable-librtmp enable-libschroedinger enable-libsnappy enable-libsoxr enable-libspeex enable-libtheora enable-libtwolame enable-libvidstab enable-libvo-amrwbenc enable-libvorbis enable-libvpx enable-libwavpack enable-libwebp enable-libx enable-libx enable-libxavs enable-libxvid enable-libzimg enable-lzma enable-decklink enable-zlib libavutil libavcodec libavformat libavdevice libavfilter libswscale libswresample libpostproc mp e estimating duration from bitrate this may be inaccurateinput mp from in.mp metadata title jet--low artist jon dattorro album tinnitus date comment hz q track genre noise duration start bitrate kb/s stream audio mp hz stereo sp kb/s wav edb using avstream.codec to pass codec parameters to muxers is deprecated use avstream.codecpar instead.output wav to out.wav metadata inam jet--low iart jon dattorro iprd tinnitus icrd icmt hz q iprt ignr noise isft lavf stream audio pcm_sle x hz stereo s kb/s metadata encoder lavc pcm_slestream mapping stream mp native pcm_sle native))press q to stop for helpsize kb time bitrate=.kbits/s speed x ffmpeg on valid mp file with loglevel error and with hide_banner new semantics) sh ffmpeg loglevel error nostats i in.mp acodec pcm_sle ar out.wav hide_banner ffmpeg on text file invalid with loglevel error and with hide_banner new semantics) sh ffmpeg loglevel error nostats i in.txt acodec pcm_sle ar out.wav hide_banneroutput file does not contain any stream
274098561,14580,https://api.github.com/repos/tensorflow/tensorflow/issues/14580,jasminezz,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :i do not modify any of the source code os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below python version bazel versionn cuda/cudnn versionn gpu model and memoryn describe the problemfirst i download mobilenet_v pretrained model from here the inference graph i use the commond:python export_inference_graph.py alsologtostderr model_name=mobilenet_v image_size output_file=/tmp/mobilenet_v__.pbnext freeze graph i use the command:python tensorflow/python/tools/freeze_graph.py input_graph tmp/mobilenet_v__.pb input_checkpoint=tmp/mobilenet_v_._.ckpt input_binary=true output_graph=tmp/frozen_mobilenet_v__.pb output_node_names=mobilenetv/predictions/reshape_finally i got the error i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use sse sse avx avx fmatraceback most recent call last file tensorflow/python/tools/freeze_graph.py line in module app.run(main=main argv= sys.argv unparsed file home/libs/anaconda/envs/python/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file tensorflow/python/tools/freeze_graph.py line in main flags.saved_model_tags file tensorflow/python/tools/freeze_graph.py line in freeze_graph input_meta_graph_def input_saved_model_dir saved_model_tags.split file tensorflow/python/tools/freeze_graph.py line in freeze_graph_with_def_protos saver saver_lib.saver(var_list=var_list file home/libs/anaconda/envs/python/lib/python./site-packages/tensorflow/python/training/saver.py line in init self.build file home/libs/anaconda/envs/python/lib/python./site-packages/tensorflow/python/training/saver.py line in build self._build(self._filename build_save=true build_restore=true file home/libs/anaconda/envs/python/lib/python./site-packages/tensorflow/python/training/saver.py line in build raise valueerror(no variables to save)valueerror no variables to saveanother machine:windows tensorflow python using the same operations i can get the frozen pb file successfully.all source code are not modified.i also changed the other models to test and get the same error need your help
273848401,14553,https://api.github.com/repos/tensorflow/tensorflow/issues/14553,qmick,1,0,0,0,0,0,this patch fixes
273736346,14542,https://api.github.com/repos/tensorflow/tensorflow/issues/14542,SnowWalkerJ,1,0,0,0,0,0,problem pythonmodel tf.keras.models.model()model.add(...)tf.keras.utils.plot_model(model to_file=model.png) output: traceback most recent call last file model.py line in module k.utils.plot_model(model to_file=model.png file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py line in plot_model dot model_to_dot(model show_shapes show_layer_names rankdir file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py line in model_to_dot if node_key in model.container_nodes:attributeerror model object has no attribute container_nodes environment-system ubuntu tensorflow-gpu bin v..-rc--ga
273441460,14518,https://api.github.com/repos/tensorflow/tensorflow/issues/14518,YoelShoshan,2,0,0,0,0,0,it would be very useful if a user will be able to provide the order of examples within a dataset with repetitions allowed as only indices are shuffled).this would allow having a more complicated logic which involves balancing data of different types).i assume it can be somehow supported by zip-ing together different datasets but it would be much easier and more flexible if we could just pass a list of indices probably light as well as it shouldnt be a big deal passing one list per epoch.please tell me if this feature already exists and if not please add it
273315585,14509,https://api.github.com/repos/tensorflow/tensorflow/issues/14509,shoyer,2,0,0,0,0,0,it would be convenient to have an explicit function for broadcasting in tensorflows python api like to numpy.broadcast_to or xlas broadcast and as requested on stackoverflow this would facilitate adding broadcasting-like behavior to the many tensorflow operations that dont support it out of the box.i understand that in general tensorflow does not implement numpys the strided n-dimensional array data model so unlike the case for numpy broadcasting e.g with eigen tensors can require a copy this is a good reason to not necessarily build such a version of broadcasting into ops however explicit broadcasting rather than using tile/expand_dims can still be very convenient
273261729,14504,https://api.github.com/repos/tensorflow/tensorflow/issues/14504,droidicus,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below tf.version tf.git_version v..-rc--ga python version bazel version if compiling from source n/a gcc/compiler version if compiling from source n/a cuda/cudnn version gpu model and memory nvidia tesla m gb exact command to reproduce see below describe the problemwhen trying to use an estimator that is derived from tf.keras.estimator.estimator_from_model and training with tf.estimator.train_and_evaluate it will work as expected if in a standalone non-distributed session however when in a distributed training cluster and the tf_config has the cluster information set there is a an explicit device assignment of an op to a device that is not valid in the current cluster spec.below is code to reproduce this issue when simulate_cluster is set to true an error is throws as shown in the log below when simulate_cluster is set to false the network is constructed and trained as intended it should be noted that the error occurs when calling tf.keras.estimator.model_to_estimator(keras_model=model and not when doing the training the cluster config is required for the distributed training to take place.the tf_config that is set below is derived from calling the code using the gcloud sdk as follows: gcloud ml-engine local train distributed parameter-server-count worker-count package-path=trainer module-name=trainer.task source code logsminimal example: pythonimport osimport numpy as npimport tensorflow as tftf.logging.set_verbosity(tf.logging.info)simulate_cluster trueif simulate_cluster os.environ tf_config environment cloud cluster worker localhost localhost ps localhost master localhost job args job_name trainer.task task index type master}}else os.environ tf_config inputs tf.keras.layers.input(shape=(,))outputs tf.keras.layers.dense()(inputs)model tf.keras.models.model(inputs outputs)model.compile(optimizer=adam loss=binary_crossentropy)est_keras tf.keras.estimator.model_to_estimator(keras_model=model invalidargumenterror thrown here if simulate_cluster is trueinput_name model.input_names data np.random.rand(,).astype(np.float)train_input_fn tf.estimator.inputs.numpy_input_fn({input_name:data data batch_size num_epochs=none shuffle=false)train_spec tf.estimator.trainspec(input_fn=train_input_fn max_steps=)eval_spec tf.estimator.evalspec(input_fn=train_input_fn steps=)tf.estimator.train_and_evaluate(est_keras train_spec eval_spec) invalidargumenterror emitted when simulate_cluster true : pythontraceback most recent call last file minimal.py line in module est_keras tf.keras.estimator.model_to_estimator(keras_model=model file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/estimator.py line in model_to_estimator save_first_checkpoint(keras_model est custom_objects keras_weights file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/estimator.py line in save_first_checkpoint model.set_weights(keras_weights file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in set_weights k.batch_set_value(tuples file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/backend.py line in batch_set_value get_session().run(assign_ops feed_dict=feed_dict file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/backend.py line in get_session initialize_variables(session file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/backend.py line in initialize_variables variables_module.is_variable_initialized(v for v in candidate_vars file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_tensor options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror cannot assign a device for operation loss/dense__loss/sub operation was explicitly assigned to job:master/task but available devices are job:localhost/replica:/task:/device:cpu job:localhost/replica:/task:/device:gpu job:localhost/replica:/task:/device:gpu job:localhost/replica:/task:/device:gpu job:localhost/replica:/task:/device:gpu make sure the device specification refers to a valid device node loss/dense__loss/sub sub t=dt_float device=/job:master/task: (loss/dense__loss/sub/x loss/dense__loss/const) caused by op uloss/dense__loss/sub defined at file minimal.py line in module est_keras tf.keras.estimator.model_to_estimator(keras_model=model file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/estimator.py line in model_to_estimator save_first_checkpoint(keras_model est custom_objects keras_weights file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/estimator.py line in save_first_checkpoint custom_objects file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/estimator.py line in clone_and_build_model target_tensors=target_tensors file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py line in compile output_loss weighted_loss(y_true y_pred sample_weight mask file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py line in weighted score_array fn(y_true y_pred file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/losses.py line in binary_crossentropy return k.mean(k.binary_crossentropy(y_true y_pred axis file usr/local/lib/python./dist-packages/tensorflow/python/keras/_impl/keras/backend.py line in binary_crossentropy output clip_ops.clip_by_value(output epsilon epsilon file usr/local/lib/python./dist-packages/tensorflow/python/ops/math_ops.py line in r_binary_op_wrapper return func(x y name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_math_ops.py line in sub sub x=x y=y name=name file usr/local/lib/python./dist-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinvalidargumenterror see above for traceback cannot assign a device for operation loss/dense__loss/sub operation was explicitly assigned to job:master/task but available devices are job:localhost/replica:/task:/device:cpu job:localhost/replica:/task:/device:gpu job:localhost/replica:/task:/device:gpu job:localhost/replica:/task:/device:gpu job:localhost/replica:/task:/device:gpu make sure the device specification refers to a valid device node loss/dense__loss/sub sub t=dt_float device=/job:master/task: (loss/dense__loss/sub/x loss/dense__loss/const) full logs tf_env and more are here"
273251609,14502,https://api.github.com/repos/tensorflow/tensorflow/issues/14502,yaroslavvb,2,0,0,0,0,0,you need sparse jobs if you want async training to proceed without having all workers available.this currently doesnt work because estimator uses json to dump values into tf_config env var and json does not allow numeric keys json.dumps automatically converts numeric keys into strings.however clusterspec for sparse job must have integers like local localhost and will crash if we have instead of suggestion modify sparse job support to allow strings as task indicessuggestion use pickle base encoding to transmit these valuesbase is shell-friendly it allows you to copy paste the value into export tf in terminal saving cluster_config cluster cluster_spec task task_spec pickle_string pickle.dumps(sparse_cluster_config pickle_string_encoded base.bencode(pickle_string pickle_string_encoded pickle_string_encoded.decode(ascii export_command export tf_pickle_base=%s%(pickle_string_encoded os.system(export_command loadingconfig_dict pickle.loads(base.bdecode(os.environ tf_pickle_base ))config.task_type config_dict task type config.task_id config_dict task index config.cluster_spec config_dict cluster return config cc ispirmustafa
273216613,14493,https://api.github.com/repos/tensorflow/tensorflow/issues/14493,theflofly,2,0,0,0,0,0,pull request opened
273137442,14482,https://api.github.com/repos/tensorflow/tensorflow/issues/14482,Sudhakar17,1,0,0,0,0,0,i would like to train a two cnn in single graph the architecture of model is capture one is not like end to end training in the warping of images(refer attachment i use scipy for mapping coordinates from the disparity map to an input image and i am unable to build a model(graph in tensorflow using scipy tensorflow supports only resize of images using bicubic/bilinear interpolation reference original paper is implemented using matconvnet in matlab
273136656,14480,https://api.github.com/repos/tensorflow/tensorflow/issues/14480,t-fi,5,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :source tensorflow version use command below) :.rc python version bazel version if compiling from source) :not sure think gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory :gtx exact command to reproduce :see attached describe the problemthe tf.data.dataset.batch functions seems to be slow when concatenating numpy arrays into batches.the attached code basically pushes dummy data num_items resembling the output of range through tensorflow by different approaches.there are basically two ways to go about this use the tf.data.dataset.range use custom generators and tf.data.dataset.from_generator() also the data can be batched to increase throughput as it is usually done in deep learning this can be done either in the generator itself or via tf.data.dataset.batch the latter appears to be x slower for numpy arrays somehow this is not the case for native tensorflow generation.i attached a benchmark to reproduce this please note that the behavior for larger amounts of data e.g images is similar resulting in only tens of images per second instead of hundreds comments to the source code: gen is a python generator resembling range() . gen_batch is also a python generator but yields batches of numbers instead of single numbers.these two generators are benchmarked first yielding millions of elements per second they should not be a bottleneck.then we define a few tf.data.dataset : ds_range_single is similar to gen using tf.data.dataset.range() ds_range_batch is similar to gen_batch using tf.data.dataset.range().batch() ds_npy_single uses gen with tf.data.dataset.from_generator() ds_npy_batch uses gen with tf.data.dataset.from_generator().batch() ds_npy_single_batch uses uses gen_batch and will perform much better than ds_npy_batch in fact it will be close to the native ds_range_batch source code logs pythonfrom time import timeimport numpy as npimport tensorflow as tfnum_items batch_size def gen for x in np.arange num_items dtype=np.int yield xdef gen_batch for x in np.arange num_items dtype=np.int).reshape(num_items//batch_size batch_size yield xstart time()for in gen passpy_single_generator_time time startstart time()for in gen_batch passpy_batch_generator_time time startds_range_single tf.data.dataset.range(num_items make_one_shot_iterator().get_next()ds_range_batch tf.data.dataset.range(num_items batch(batch_size).make_one_shot_iterator().get_next()ds_npy_single tf.data.dataset.from_generator gen output_types=tf.int output_shapes=tf.tensorshape make_one_shot_iterator().get_next()ds_npy_batch tf.data.dataset.from_generator gen output_types=tf.int output_shapes=tf.tensorshape batch(batch_size).make_one_shot_iterator().get_next()ds_npy_single_batch tf.data.dataset.from_generator gen_batch output_types=tf.int output_shapes=tf.tensorshape( batch_size make_one_shot_iterator().get_next()with tf.session as sess start time for in range(num_items sess.run(ds_npy_single npy_single_time time start start time for in range(num_items batch_size sess.run(ds_npy_batch npy_batch_time time start start time for in range(num_items batch_size sess.run(ds_npy_single_batch npy_single_batch_time time start start time for in range(num_items sess.run(ds_range_single range_single_time time start start time for in range(num_items batch_size sess.run(ds_range_batch range_batch_time time startprint(python single generator examples/s num_items/py_single_generator_time)print(python batch generator examples/s num_items/py_batch_generator_time)print(tf npy single examples/s num_items/npy_single_time)print(tf npy batch examples/s num_items/npy_batch_time)print(tf npy single batch examples/s num_items/npy_single_batch_time)print(tf range single examples/s num_items/range_single_time)print(tf range batch examples/s num_items/range_batch_time prints the following on my machine:python single generator examples/s python batch generator examples/s tf npy single examples/s tf npy batch examples/s tf npy single batch examples/s tf range single examples/s tf range batch examples/s
273114465,14475,https://api.github.com/repos/tensorflow/tensorflow/issues/14475,hookover,1,0,0,0,0,0,system infomationubuntu.cuda.cudnngtxtensorflow..python.memory g used gb nvidia-smi nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx off on n/a c p w w mib mib default processes gpu memory gpu pid type process name usage g usr/lib/xorg/xorg mib g compiz mib g token=cdaddbebabf mib run train script get this error: i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallycalled with args:namespace(cfg_file=./lstm/lstm.yml gpu_id max_iters network_name=lstm_train pre_train=none randomize=false restore set_cfgs=none)using config:{charset abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz exp_dir lstm_ctc font fonts/ubuntu-m.ttf gpu_id img_shape log_dir lstm_ctc max_char_len max_len min_len nchannels nclasses net_name lstm num_features pool_scale rng_seed root_dir srv/python/lstm_ctc_ocr_with_tf space_index space_token test time_step train batch_size display gamma learning_rate log_image_iters momentum num_epochs num_hid num_layers snapshot_infix snapshot_iters snapshot_prefix lstm solver rms stepsize weight_decay e val batch_size num_epochs print_num val_step output will be saved to srv/python/lstm_ctc_ocr_with_tf_../output/lstm_ctc logs will be saved to srv/python/lstm_ctc_ocr_with_tf_../logs/lstm_ctc/lstm_train/----- /gpu:tensor(data shape dtype=float)tensor(conv/biasadd shape dtype=float)tensor(time_step_len shape dtype=int)use network lstm_train in trainingw tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.e tensorflow/core/common_runtime/direct_session.cc internal failed initializing streamexecutor for cuda device ordinal internal failed call to cudeviceprimaryctxretain cuda_error_out_of_memory total memory reported traceback most recent call last file lstm/train_net.py line in module restore=bool(int(args.restore file lstm/../lib/lstm/train.py line in train_net with tf.session(config=config as sess file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in init super(session self).__init__(target graph config=config file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in init self._session tf_session.tf_newdeprecatedsession(opts status file usr/lib/python./contextlib.py line in exit next(self.gen file usr/local/lib/python./dist-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.internalerror failed to create session. i will be grateful to anyone for helping methanks everyone
272742903,14423,https://api.github.com/repos/tensorflow/tensorflow/issues/14423,momih,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu lts xenial xerus tensorflow installed from source or binary binary through pip install tensorflow-gpu tensorflow version use command below git version v..-rc--geee python version cuda/cudnn version cuda cudnn gpu model and memory geforce gtx m gb describe the problemi had changed my nvidia driver version to some time ago after that sometimes when id terminate tf code either with c in terminal or closing ipython tab in spyder it would successfully terminate with keyboardinterrupt however the other times when id terminate my entire computer would freeze and become unresponsive couldnt use ctrl alt f to login into a virtual console and kill the process as keyboard also became unresponsive however if i was playing music through spotify it would continue playing without any interruption this would happen with different files not one specific file but i noticed it would usually happen during the run of sess.run(tf.global_variables_initializer in any of the files it also has happened some other times like training completed in spyder console and python was idle and i closed the console tab in spyder training completed in spyder ipython tab and another file was run in terminal i closed the ipython tab which was idle in spyder when the new tf session in the terminal was initializing variables and then my computer froze completely i dont think the contents of the code mattered it would still freeze even if all my code did was define a variable and then initialize it.so i tried reverting back to nvidia to see if anything changed but it was still freezing now ive reverted back to nvidia and ive tried terminating tf when it is initializing variables and so far the freezing hasnt happened another thing id noticed after changing to nvidia is that tf.global_variables_initializer became very slow always taking seconds i found where i saw it could be because of cuda generating ptx so i tried calling the init a second time in the same session and it would run in milliseconds same for calling init on cpu i understand the init can be slow when run on gpu however i never noticed it running slow prior to when i changed to a newer nvidia driver even after the revert to it still runs slow source code logsid really like to know what i can log and how to do that im not sure if i can use gdb as my computer becomes unresponsive so i have no way of going into a terminal below is the sample code i would run and terminate during init to see if computer froze. import tensorflow as tfinitial tf.variable(tf.truncated_normal stddev seed=))sess tf.session()print starting initializationsess.run(tf.global_variables_initializer
272726639,14421,https://api.github.com/repos/tensorflow/tensorflow/issues/14421,powderluv,1,0,0,0,0,1,the current ios library is huge add the ability to selectivelyregister for the ops the tensorflow library will support thisgreatly reduces resultant binary based on the network.a full arm build was mb on my machine vs one selectivelyregistered for ssd mobilenet was only mb.also fixes a minor bug where the selected arch wasnt being passedto the compile_ios_protobuf.sh script.test:build_all_ios.sh a arm generates a fat binary for arm build_all_ios_sh a arm g downloads/op_inference_graph.pb generates a binary that is much smaller
272449977,14392,https://api.github.com/repos/tensorflow/tensorflow/issues/14392,hsm207,2,0,0,0,0,0,i am using tensorflow according to this blog post we can use the weights argument in the call to embedding to specify some matrix that represents a pre-trained word embeddings see the section titled preparing the embedding layer).however this code does not work: import tensorflow as tfimport numpy as npfrom tensorflow.python.estimator.model_fn import estimatorspecfrom tensorflow.contrib.keras.api.keras.layers import embedding densefrom tensorflow.contrib.keras.api.keras.initializers import constantdef model_fn(features labels mode x tf.constant labels tf.constant let m be our pre-trained word embeddings m np.array np.float with tf.name_scope(embedding_layer create an embedding layer and load m into it n embedding weights= m input_length name=embedding_matrix trainable=false lookup n(x lookup tf.print(lookup lookup preds dense()(lookup loss tf.reduce_mean(labels preds train_op tf.train.gradientdescentoptimizer(.).minimize(loss tf.train.get_global_step eval_metric_ops accuracy tf.metrics.accuracy(labels preds return estimatorspec(mode=mode loss=loss train_op=train_op eval_metric_ops=eval_metric_ops)model tf.estimator.estimator(model_fn)model.train(input_fn=lambda none steps=) lookup should print but instead random numbers are printed.the solution is to define n as follows: n embedding embeddings_initializer=constant(m input_length name=embedding_matrix trainable=false
272194604,14363,https://api.github.com/repos/tensorflow/tensorflow/issues/14363,ljanyst,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :source tensorflow version use command below) :bv..--gd python version bazel version if compiling from source gcc/compiler version if compiling from source) :gcc ubuntu ubuntu cuda/cudnn version gpu model and memory :tesla v-sxm-gb exact command to reproduce : git clone image-segmentation-fcn wget data_road.zip train.py data-dir data_road describe the problemit seems like i am hitting some sort of a cuda/cudnn synchronization/race issue please see the snippet in the next section for the exact error message the problem only happens with the kitti dataset the exact same tensorflow code works fine for the cityscapes dataset also the problem only happens on tesla v i tested the same exact software configuration on tesla k and geforce gtx ti as well and things work fine source code logs e tensorflow/stream_executor/cuda/cuda_driver.cc failed to synchronize the stop event cuda_error_illegal_address e tensorflow/stream_executor/cuda/cuda_timer.cc internal error destroying cuda event in context xff cuda_error_illegal_address e tensorflow/stream_executor/cuda/cuda_timer.cc internal error destroying cuda event in context xff cuda_error_illegal_address f tensorflow/stream_executor/cuda/cuda_dnn.cc failed to set stream for cudnn handle cudnn_status_mapping_errorzsh abort core dumped train.py data-dir data_road
271711338,14310,https://api.github.com/repos/tensorflow/tensorflow/issues/14310,tjingrant,4,0,0,0,0,0,as per of onnx-tf package arpith jacob tian jin gheorghe-teodor bercea from ibm research objectivewe are porting a subset of our package of onnx-tf from here specifically we want to enable users to do the following: import tensorflow as tfimport numpy as npfrom onnx import helperfrom onnx.onnx_pb import tensorprotox np.random.randn astype(np.float)y_ref np.clip(x np.inf)node_def helper.make_node relu x x )graph_def helper.make_graph node_def name=test inputs= helper.make_tensor_value_info(x tensorproto.float outputs= helper.make_tensor_value_info(x tensorproto.float input_dict output_dict tf.contrib.onnx.prepare(helper.make_model(graph_def))with tf.session as sess out sess.run(output_dict x feed_dict={input_dict x x})np.testing.assert_almost_equal(out y_ref current support for onnx:this implementation passes all the backend tests here except for rnn it supports all the models in the onnx model zoo what we will be doing:we are still working on fixing some of the tests as well as clearing as many todos as we can.the onnx rnn api changed very recently and we may do another pr for rnn support wed like your opinions:we have not imported the onnx package dependency as wed like to get tf teams opinion regarding whether/how we should import onnx package dependency the benefit is that we can check for the legality of onnx node/graph declaration also we need a bunch of proto definition like graphproto/tensorproto.@arpith-jacob doru
271556721,14295,https://api.github.com/repos/tensorflow/tensorflow/issues/14295,loretoparisi,1,0,0,0,0,0,there is only one example for the java api labelimage.java that is also outdated it would be great to add more examples for different tasks like text classification sentence matching seqseq etc.i have a small example that put the java api all together see tensorflow-java
271420015,14284,https://api.github.com/repos/tensorflow/tensorflow/issues/14284,yjmade,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos tensorflow installed from source or binary source tensorflow version use command below or master describe the problemwhen calling model_to_estimator the model_fn create by create_keras_model_fn didnt set export_outputs in the returned estimatorspec see this that make it unable to export to savedmodel the following error raised when call estimator.export_savedmodel() packages/tensorflow/python/estimator/export/export.py line in build_all_signature_defs raise valueerror(export_outputs must be a dict.)valueerror export_outputs must be a dict. i would like contrib a pr for fixing this if is ok
271414397,14283,https://api.github.com/repos/tensorflow/tensorflow/issues/14283,nisace,6,0,0,0,0,0,tensorflow moves tf dataset to core tf.data.dataset and doc/tutorial suggest to use tf.estimator to train models.however as recommended at the end of this page the dataset object and its iterator must be instantiated inside the input_fn function this means the iterations through the dataset will start over for each call to estimator.train(input_fn steps thus calling is with steps number of samples in epoch will lead to train the model on a subset of the dataset.thus my question is it possible to implement something like this with estimator dataset: for i in range(num_epochs train for some steps estimator.train(input_fn=train_input_fn steps=valid_freq validation_iterator evaluate on the validation set steps=none we evaluate on the full validation set estimator.evaluate(input_fn=valid_input_fn) without starting training samples iterations from scratch at each call to estimator.train(input_fn=train_input_fn steps=valid_freq) for example unlike here instantiate the dataset and its iterator outside input_fn i tried it but it does not work because then the input from the dataset iterator and the model from the estimator model_fn are not part of the same graph.thanks
271276742,14257,https://api.github.com/repos/tensorflow/tensorflow/issues/14257,sleighsoft,1,0,0,0,0,0,i would like to use the dataset api with the ganestimator / tfgan .i know that makeiterator cannot be cast to a tensor but i would like to pass it to generator_fn anyways is the conversion of generator_inputs to tensors really necessary?with the plain estimator api i also do not have this restriction.i am passing the following object to gan_model through generator_inputs : pythontextinput(initializer=
271246865,14254,https://api.github.com/repos/tensorflow/tensorflow/issues/14254,crizCraig,0,1,0,0,0,0,update restarting my machine fixed the issue.to reproduce run two processes with pythonimport tensorflow as tfimport timez tf.constant dtype=tf.float)z tf.cast(z tf.int)with tf.session as sess while true print(z is sess.run(z but should be time.sleep() second process output z is but should be z is but should be stopping the first process will fix the problem immediately without needing to restart the second process system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geee python version cuda/cudnn version cuda cudnn gpu model and memory geforce gtx gb
271115215,14230,https://api.github.com/repos/tensorflow/tensorflow/issues/14230,montanalow,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory nvidia exact command to reproduce bazel build config=opt config=cuda verbose_failures tensorflow/tools/pip_package:build_pip_package describe the problemdocumentation is incorrect source configurations lists bazel for tensorflow tensorflow rejects this version of bazel with an error message:current bazel version is expected at least source code logs bazel build config=opt config=cuda verbose_failures tensorflow/tools/pip_package:build_pip_package running in dbbbfwarning config values are not defined in any rc file opterror tmp/tensorflow/workspace traceback most recent call last):file tmp/tensorflow/workspace line tf_workspace file tmp/tensorflow/tensorflow/workspace.bzl line in tf_workspace check_version file tmp/tensorflow/tensorflow/workspace.bzl line in check_version fail(current bazel version is e...))current bazel version is expected at least
271112540,14229,https://api.github.com/repos/tensorflow/tensorflow/issues/14229,mpekalski,1,0,0,0,0,0,shouldnt train_monitors be renamed to train_hooks as far as i understand hooks replaced monitors.i look at tf v..-rc.tf.contrib.learn.experiment(estimator train_input_fn eval_input_fn eval_metrics=none train_steps=none eval_steps train_monitors =none eval_hooks =none local_eval_frequency=none eval_delay_secs continuous_eval_throttle_secs min_eval_frequency=none delay_workers_by_global_step=false export_strategies=none train_steps_per_iteration=none checkpoint_and_export=false
271073910,14226,https://api.github.com/repos/tensorflow/tensorflow/issues/14226,dcunhas,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geee python version bazel version if compiling from source gcc/compiler version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce performing resize_images with output of decode_image describe the problemthis problem was addressed in but was closed and are also relevant the basic problem is that the output of tf.image.decode_image cannot be passed to tf.image.resize_images it raises valueerror images contains no shape in the call to resize_images possible workarounds to this include using decode_jpeg decode_png or adding decoded_image.set_shape( none none none before calling tf.image.resize_images however as girving pointed out in nothing about the underlying op requires knowing a static shape source code logs decoded_image tf.image.decode_jpeg(tf.read_file(image_filename tf.image.resize_images(decoded_image
271034080,14219,https://api.github.com/repos/tensorflow/tensorflow/issues/14219,MadWombat,1,0,0,0,0,0,heres why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu os x tensorflow installed from source or binary source latest branch tensorflow version use command below v..-rc--gdfba python version bazel version if compiling from source gcc/compiler version if compiling from source apple llvm version clang cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce :see belowyou can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemthe version of the documentation mentions tf.layers.network in its examples of using the new function based layers here is a link to the specific doc so all you have to do is copy/paste the example code and run it source code logsthe most basic code to verify is import tensorflow as tfprint(tf.layers.network) on my system this results in attributeerror module tensorflow.python.layers.layers has no attribute network
271025373,14218,https://api.github.com/repos/tensorflow/tensorflow/issues/14218,dartdog,3,0,0,0,0,0,installed tf with pip install ignore-installed upgrade me this error on import:/home/tom/anaconda/envs/tf/lib/python./importlib/_bootstrap.py runtimewarning compiletime version of module tensorflow.python.framework.fast_tensor_util does not match runtime version return f(*args kwds)similar error reported form tensorboard:/home/tom/anaconda/envs/tf/lib/python./importlib/_bootstrap.py runtimewarning compiletime version of module tensorflow.python.framework.fast_tensor_util does not match runtime version return f(*args kwds)tensorboard rc at press ctrl+c to quit
270985658,14213,https://api.github.com/repos/tensorflow/tensorflow/issues/14213,Kayoku,3,0,0,0,0,0,hi im working on a fcn fully convolutional network and use tensorflow for this work i have a lot of images in input and they have very different dimensions like x or x so my input need to be dynamic its why i search a way to resize images for having the greater dimension to a specific value with respect to the aspect/ratio.unfortunatly tensorflow doesnt provide this feature or if im wrong please just ignore this issue.so i write my own code its probably not the most beautiful way to write this sorry if this code seems ugly python#!/usr/bin/env pythonimport tensorflow as tfmax_size image for example x)image_path your_image.jpg open imageimage_string tf.read_file(image_path)image tf.image.decode_jpeg(image_string channels take width/heightinitial_width tf.shape(image) initial_height tf.shape(image function for resizing def resize(x y take the greater value and use it for the ratio max tf.maximum(initial_width initial_height ratio tf.to_float(max tf.constant(max_size dtype=tf.float new_width tf.to_float(initial_width ratio new_height tf.to_float(initial_height ratio return tf.to_int(new_width tf.to_int(new_height useless function for the next conditiondef useless(x y return x ynew_w new_h tf.cond(tf.logical_or tf.greater(initial_width tf.constant(max_size tf.greater(initial_height tf.constant(max_size lambda resize(initial_width initial_height lambda useless(initial_width initial_height))resized_image tf.image.resize_images(image new_w new_h )image_int tf.cast(resized_image tf.uint)image_enc tf.image.encode_jpeg(image_int)fwrite tf.write_file(my_resized_image.jpeg image_enc)sess tf.session()sess.run( fwrite ) i think this feature could be very useful when working with image input to have a function here tf.image.resize_image_keep_aspect which allow us to do the operation more quickly and i hope more efficiently!) #!/usr/bin/env pythonimport tensorflow as tfmax_size image for example x)image_path your_image.jpg open imageimage_string tf.read_file(image_path)image tf.image.decode_jpeg(image_string channels=)resized_image tf.image.resize_image_keep_aspect(image max_size)image_enc tf.image.encode_jpeg(resized_image)fwrite tf.write_file(my_resized_image.jpeg image_enc)sess tf.session()sess.run( fwrite ) if the feature already exist sorry but i search on git/stackoverflow/google and didnt find any viable solution.if you have any question or if im not clear ask to me.have a nice day
270629559,14182,https://api.github.com/repos/tensorflow/tensorflow/issues/14182,KrishnaveniTK,70,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below python version python bazel version if compiling from source gcc/compiler version if compiling from source na cuda/cudnn version na gpu model and memory na exact command to reproduce import tensorflow as tf environment capture text can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version)this command also results in the same error./home/raju/anaconda/lib/python./importlib/_bootstrap.py runtimewarning compiletime version of module tensorflow.python.framework.fast_tensor_util does not match runtime version return f(*args kwds describe the problemwhen importing tensorflow i get this error i found some information on feature request nightly build for python yes we unfortunately copy the binary for ill look into creating a specific binary for linux source code logs$import tensorflow as tf result is home/raju/anaconda/lib/python./importlib/_bootstrap.py runtimewarning compiletime version of module tensorflow.python.framework.fast_tensor_util does not match runtime version return f(*args kwds
270615172,14181,https://api.github.com/repos/tensorflow/tensorflow/issues/14181,JulianStier,2,0,0,0,0,0,"system information have i written custom code yes os platform and distribution linux ubuntu tensorflow installed from source or binary binary tensorflow version v..-rc--geee python version python anaconda bit cuda/cudnn version none gpu model and memory none cat etc/issue linux bragi generic ubuntu smp fri oct utc x x x gnu/linuxversion zesty zapus)version_id=.version_codename=zesty are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux bragi generic ubuntu smp fri oct utc x x x gnu/linux check pips numpy numpydoc protobuf tensorflow tensorflow-tensorboard check for virtualenv false tensorflow import tf.version tf.git_version v..-rc--geeetf.compiler_version v..-rc--geeesanity check array dtype=int env ld_library_path is unsetdyld_library_path is unset nvidia-smi tf_env_collect.sh line nvidia-smi command not found describe the problemi am working on a tensorflow model which takes pretty much ram it is executed iteratively to process given tasks.however with increasing time the whole process starts consuming more and more ram although it should clean it up this sounds like as if id keep data of one graph over the iterations but i am almost sure that the graphs are cleanly separated.problem-------i reduced the code to the following import tensorflow as tf import numpy as np reps for i in range(reps with tf.graph().as_default as graph with tf.session(graph=graph as sess tf.constant(np.random.random((,,,)))i have gb ram available working on a ubuntu with cpu tensorflow this will give following error message after about the th or th iteration terminate called after throwing an instance of std::bad_alloc what std::bad_allocgiving the process some time after each iteration results in no improvement import tensorflow as tf import numpy as np import time reps for i in range(reps with tf.graph().as_default as graph with tf.session(graph=graph as sess tf.constant(np.random.random time.sleep()however it works if i force garbage collection invocation after each repetition import tensorflow as tf import numpy as np import gc reps for i in range(reps with tf.graph().as_default as graph with tf.session(graph=graph as sess tf.constant(np.random.random gc.collect()question--------now i wonder why i need to force garbage collection to run even though tensorflow should have closed the session and de-referenced the graph object.back to my original model i am not sure yet if the gc invocation actually helps the memory usage grows pretty intense especially when i am about to persist the model to disk.thanks for any insights"
270386267,14158,https://api.github.com/repos/tensorflow/tensorflow/issues/14158,eli99999,1,0,0,0,0,0,"ive exported a model to android that uses stfts tf.contrib.signal.stft(transwav frame_length frame_step fft_length=,window_fn=functools.partial(tf.contrib.signal.hann_window periodic=false pad_end=true)the model works properly in pyhtonbut when i load it in android using the downloaded compiled tensorflow compile org.tensorflow:tensorflow-android:+i get this error fatal exception main process org.tensorflow.demo pid java.lang.illegalargumentexception no opkernel was registered to support op cos with these attrs registered devices cpu registered kernels no registered kernels node stft/hann_window/cos cos t=dt_float device=/device:cpu: (stft/hann_window/truediv) which comes from the hann_windowany recommended work around"
270344680,14155,https://api.github.com/repos/tensorflow/tensorflow/issues/14155,abieler,1,0,0,0,0,0,system information manjaro linux tensorflow installed with pip tensorflow version v..-rc--geee python version cuda/cudnn version gpu model and memory geforce gtx gb describe the problemexecuting the code below in a loop slows down linearly withnumber of iteration as can be seen in the figures below.(the use case being a long running app where different modelsare loaded multiple times depending on user inputs pythongpu_options tf.gpuoptions(per_process_gpu_memory_fraction=.)config tf.configproto(gpu_options=gpu_options)with tf.session(config=config as sess saver tf.train.import_meta_graph(modelname saver.restore(sess tf.train.latest_checkpoint source code logsreproducible code and plots of the time increase can be found here reproduce do the following from withing the extracted zip archive: python train_model.pypython test_a.py ! import_metagraph
270157030,14143,https://api.github.com/repos/tensorflow/tensorflow/issues/14143,mrry,2,0,0,0,0,0,the savedmodelbuilder inadvertently strips all function definitions from the metagraphdef when clear_devices=true see this gist for a repro thanks eggie for finding it bug appears to stem from export_meta_graph which builds a graphdef by selective field copying when certain options such as clear_devices=true are passed passing clear_devices=false enables the code to succeed however it breaks compatibility between tf.data and potentially other code and savedmodel so we should find a sustainable fix.@sukritiramesh can you please take a look thanks
270037738,14132,https://api.github.com/repos/tensorflow/tensorflow/issues/14132,asimshankar,4,0,0,0,0,0,this applies only when eager execution has been enabled via tfe.enable_eager_execution() )currently the following does not work: pythonimport tensorflow as tfimport tensorflow.contrib.eager as tfetfe.enable_eager_execution()x tfe.variable(tf.ones name=x)x that last line assigning a value to an element of the tensor will fail with object does not support item assignment .while it would be possible to make item assignment for tensor and variable objects work when eager execution is enabled it is trickier to make the same line of code work with graph construction since the line x does not return a tf.operation object that can be provided to a session.run call).at this early stage of eager execution were taking the conservative approach and disallowing tensor assignment this is probably worth revisiting at a future date in the meantime the verbose form of item assignment using tf.scatter_update : pythontf.scatter_update(x
270037720,14131,https://api.github.com/repos/tensorflow/tensorflow/issues/14131,asimshankar,7,0,0,0,0,0,once eager execution is enabled via tfe.enable_eager_execution it cannot be disabled in the same process this means that eager and graph execution cannot be mixed in the same python session.this issue has been filed to track development of features to make the transition between eager and graph execution smoother allowing users to pick and choose portions of the computation that will be compiled into graphs for optimized execution and portions that will execute eagerly
270037697,14129,https://api.github.com/repos/tensorflow/tensorflow/issues/14129,asimshankar,2,0,0,0,0,0,this applies only when eager execution has been enabled via tfe.enable_eager_execution() )if the model does not involve dynamic control flow in python i.e changing the computation based on input then the same model code can be used to construct a tensorflow graph which can then be trained with distributed tensorflow example models like mnist resnet and the ptb rnn include unittests outlining how the same model code can be used to construct and train tensorflow graphs.a smoother path to distributed tensorflow when eager execution is enabled is being charted out
270012182,14127,https://api.github.com/repos/tensorflow/tensorflow/issues/14127,Carmezim,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu mac high sierra tensorflow installed from source or binary source tensorflow version use command below head at cfaedfeacdcecaef python version bazel version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce clone repo at head configure build with cpu support only and native arch optimizationbuilding tensorflow at tensorflow@cfaedfeacdcecaef head yields the errors below related to xla optimized for intel(r core(tm i-hq). tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function acos to type void register_libm_symbol(acos tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionacos(_a lcpp_x noexcept return acos((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double acos(long double lcpp_x noexcept return acosl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float acos(float lcpp_x noexcept return acosf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double acos(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function acosh to type void register_libm_symbol(acosh tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionacosh(_a lcpp_x noexcept return acosh((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double acosh(long double lcpp_x noexcept return acoshl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float acosh(float lcpp_x noexcept return acoshf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double acosh(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function asin to type void register_libm_symbol(asin tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionasin(_a lcpp_x noexcept return asin((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double asin(long double lcpp_x noexcept return asinl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float asin(float lcpp_x noexcept return asinf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double asin(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function asinh to type void register_libm_symbol(asinh tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionasinh(_a lcpp_x noexcept return asinh((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double asinh(long double lcpp_x noexcept return asinhl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float asinh(float lcpp_x noexcept return asinhf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double asinh(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function atan to type void register_libm_symbol(atan tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionatan(_a lcpp_x noexcept return atan((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double atan(long double lcpp_x noexcept return atanl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float atan(float lcpp_x noexcept return atanf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double atan(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function atan to type void register_libm_symbol(atan tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionatan(_a lcpp_y a lcpp_x noexcept^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double atan(long double lcpp_y long double lcpp_x noexcept return atanl(__lcpp_y lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float atan(float lcpp_y float lcpp_x noexcept return atanf(__lcpp_y lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double atan(double double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function atanh to type void register_libm_symbol(atanh tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionatanh(_a lcpp_x noexcept return atanh((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double atanh(long double lcpp_x noexcept return atanhl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float atanh(float lcpp_x noexcept return atanhf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double atanh(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function cbrt to type void register_libm_symbol(cbrt tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioncbrt(_a lcpp_x noexcept return cbrt((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double cbrt(long double lcpp_x noexcept return cbrtl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float cbrt(float lcpp_x noexcept return cbrtf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double cbrt(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function ceil to type void register_libm_symbol(ceil tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionceil(_a lcpp_x noexcept return ceil((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double ceil(long double lcpp_x noexcept return ceill(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float ceil(float lcpp_x noexcept return ceilf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double ceil(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function copysign to type void register_libm_symbol(copysign tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioncopysign(_a lcpp_x a lcpp_y noexcept^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioncopysign(long double lcpp_x long double lcpp_y noexcept applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float copysign(float lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double copysign(double double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function cos to type void register_libm_symbol(cos tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioncos(_a lcpp_x noexcept return cos((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double cos(long double lcpp_x noexcept return cosl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float cos(float lcpp_x noexcept return cosf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double cos(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function cosh to type void register_libm_symbol(cosh tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioncosh(_a lcpp_x noexcept return cosh((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double cosh(long double lcpp_x noexcept return coshl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float cosh(float lcpp_x noexcept return coshf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double cosh(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function erf to type void register_libm_symbol(erf tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionerf(_a lcpp_x noexcept return erf((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double erf(long double lcpp_x noexcept return erfl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float erf(float lcpp_x noexcept return erff(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double erf(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function erfc to type void register_libm_symbol(erfc tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionerfc(_a lcpp_x noexcept return erfc((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double erfc(long double lcpp_x noexcept return erfcl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float erfc(float lcpp_x noexcept return erfcf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double erfc(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function exp to type void register_libm_symbol(exp tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionexp(_a lcpp_x noexcept return exp((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double exp(long double lcpp_x noexcept return expl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float exp(float lcpp_x noexcept return expf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double exp(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function exp to type void register_libm_symbol(exp tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionexp(_a lcpp_x noexcept return exp((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double exp(long double lcpp_x noexcept return expl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float exp(float lcpp_x noexcept return expf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double exp(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function expm to type void register_libm_symbol(expm tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionexpm(_a lcpp_x noexcept return expm((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double expm(long double lcpp_x noexcept return expml(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float expm(float lcpp_x noexcept return expmf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double expm(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function fabs to type void register_libm_symbol(fabs tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionfabs(_a lcpp_x noexcept return fabs((double)__lcpp_x);}^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double fabs(long double lcpp_x noexcept return fabsl(__lcpp_x applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float fabs(float lcpp_x noexcept return fabsf(__lcpp_x applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double fabs(double tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc error reinterpret_cast cannot resolve overloaded function fdim to type void register_libm_symbol(fdim tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc note expanded from macro register_libm_symbol registry->register(#name reinterpret_cast(name applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functionfdim(_a lcpp_x a lcpp_y noexcept^/applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility long double fdim(long double lcpp_x long double lcpp_y noexcept return fdiml(__lcpp_x lcpp_y applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/include/c++/v/math.h note candidate functioninline libcpp_inline_visibility float fdim(float lcpp_x float lcpp_y noexcept return fdimf(__lcpp_x lcpp_y applications/xcode.app/contents/developer/platforms/macosx.platform/developer/sdks/macosx..sdk/usr/include/math.h note candidate functionextern double fdim(double double fatal error too many errors emitted stopping now ferror-limit warnings and errors generated.target tensorflow/tools/pip_package:build_pip_package failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path s
269757274,14109,https://api.github.com/repos/tensorflow/tensorflow/issues/14109,Jiawen1991,1,0,0,0,0,0,"hi,since two operations convdbackpropfilter and convdbackpropinput count most of the time for lots of applications(alexnet/vgg/gan/inception etc i am analyzing the complexity of these two operations back-propagation in tensorflow and i found out that there are three implementation versions custom fast and slot for convdbackpropfilter and convdbackpropinput while i profile all computations are passed to custom version instead of fast or slow which directly calls eigen function spatialconvolutionbackwardinput to do that the issue is:convdbackpropfilter uses eigen:tensormap.contract to do the tensor contraction and convdbackpropinput uses eigen:matrixmap.transpose to do the matrix transposition in the compute function beside these two functions i didnt see any convolutional operations which are needed for back-propagation theoretically beside convolutions what else would be run inside these two operations for back-propagation does anyone know how to analyze the computation complexity of back propagation operation in tensorflow?i am looking for any advise/suggestion thank you"
269737729,14107,https://api.github.com/repos/tensorflow/tensorflow/issues/14107,johnsrude,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu windows x tensorflow installed from source or binary tensorflow version use command below python version cuda/cudnn version cuda release v cudnn gpu model and memory nvidia p exact command to reproduce c:\python\python main.py log_dir=./logs image_dir={image dir val_dir validation dir batch_size training=true describe the problemunder i was able to use a batch size of put your max batch size here for training under i get resource exhausted errors for that batch size so use of gpu resources is going up not the right direction.for me here are the performance effects tensorflow gpu images/sec for batch size tensorflow gpu cant do batch size images/sec for batch size source code logs tf_bug.txt
269693092,14101,https://api.github.com/repos/tensorflow/tensorflow/issues/14101,pramodkaushik,1,0,0,0,0,0,"i am posting this here because a similar question on stackoverflow is still unanswered so i suspect it might be a bug adding gradient ops within a tf.while_loop for computing gradients of loop variables w.r.t external variables results in an error program reproducing the error: import numpy as npimport tensorflow as tftf.reset_default_graph()f lambda x tf.cumsum(x)g lambda x x - h lambda x xencoder_emb_inp tf.placeholder(dtype=tf.float shape= )encoder_outputs f(encoder_emb_inp)decoder_initial_state g(encoder_outputs)decoder_initial_output h(decoder_initial_state)def cond(time unused_state unused_output return tf.less(time def body(time state inputs step lambda s i tf.multiply(s,s tf.multiply(s,i next_state next_output step(state inputs next_grads tf.gradients(next_output decoder_initial_state tf.print(next_grads next_grads return time next_state next_output)initial_time tf.constant dtype=tf.int)final_time final_state final_outputs tf.while_loop(cond body loop_vars initial_time decoder_initial_state decoder_initial_output ) error message ipython-input--cc in body(time state inputs next_state next_output step(state inputs next_grads tf.gradients(next_output state next_grads tf.gradients(next_output decoder_initial_state tf.print(next_grads next_grads return time next_state next_output)/home/pramodkm/tensorflow/local/lib/python./site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys xs grad_ys name colocate_gradients_with_ops gate_gradients aggregation_method stop_gradients out_grads i loop_state.zeroslike(op i else out_grads i control_flow_ops.zeroslikeoutsideloop(op i with ops.name_scope(op.name grad pylint disable=protected-access/home/pramodkm/tensorflow/local/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.pyc in zeroslikeoutsideloop(op index if op_ctxt we are in a cond context use a switch to create zeros only when needed pred op_ctxt.pred branch op_ctxt.branch switch_val switch(op.inputs pred branch attributeerror whilecontext object has no attribute pred i am using tf-nightly-gpu devthanks"
269491721,14084,https://api.github.com/repos/tensorflow/tensorflow/issues/14084,wochinge,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu alpine linux running with docker tensorflow installed from source or binary binary conda-forge tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory using cpu exact command to reproduce classpath=$(${hadoop_home}/bin/hadoop classpath glob jupyter notebook describe the problemi am reading files from hdfs and also want to use hdfs as model_dir to store the tensorflow output reading and writing of the checkpoints works fine however the the events file just gets created but it does not get updated with the evaluation events when i kill the notebook then the output is written but not in between changing the model directory to a local one solves this also adding an extra summarysaverhook which points to a local directory did not help.i am running my code using the instructions here source code logsi normally use an estimator with a custom model function but i reduced it to the following: pythonimport tensorflow as tfimport numpyhdfs_namenode hdfs://xx.xx.xx.xx:files data/part--acdbb-f--da-cbacfc-c.csv.format(hdfs_namenode) output_directory tresults/.format(hdfs_namenode)feature_columns tf.feature_column.numeric_column(x shape= ) def input_function_training def input_function_evaluation def experiment_fn(run_config hparams estimator tf.estimator.dnnclassifier(hidden_units feature_columns=feature_columns model_dir=output_directory n_classes config=run_config return tf.contrib.learn.experiment(estimator=estimator train_input_fn=input_function_training eval_input_fn=input_function_evaluation)tf.contrib.learn.learn_runner.run(experiment_fn run_config=tf.contrib.learn.runconfig(model_dir=output_directory save_checkpoints_steps save_checkpoints_secs=none save_summary_steps=)) the logs do not show any problems: warning:tensorflow:runconfig.uid from tensorflow.contrib.learn.python.learn.estimators.run_config is experimental and may change or be removed at any time and without warning.info:tensorflow:using config task_type none task_id cluster_spec tensorflow.python.training.server_lib.clusterspec object at xfbe master num_ps_replicas num_worker_replicas environment local is_chief true evaluation_master tf_config gpu_options per_process_gpu_memory_fraction tf_random_seed none save_summary_steps save_checkpoints_secs none log_step_count_steps session_config none save_checkpoints_steps keep_checkpoint_max keep_checkpoint_every_n_hours model_dir hdfs://xx.xx.xx.xx:/tresults/}warning:tensorflow:runconfig.uid from tensorflow.contrib.learn.python.learn.estimators.run_config is experimental and may change or be removed at any time and without warning.warning:tensorflow:from opt/conda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/monitors.py basemonitor.__init from tensorflow.contrib.learn.python.learn.monitors is deprecated and will be removed after instructions for updating:monitors are deprecated please use tf.train.sessionrunhook.info:tensorflow:create checkpointsaverhook.info:tensorflow:saving checkpoints for into hdfs://xx.xx.xx.xx:/tresults/model.ckpt.warning:tensorflow:casting dtype float labels to bool.warning:tensorflow:casting dtype float labels to bool.info:tensorflow:starting evaluation at info:tensorflow:restoring parameters fromhdfs://xx.xx.xx.xx:/tresults/model.ckpt-info:tensorflow:evaluation info:tensorflow:evaluation info:tensorflow:finished evaluation at info:tensorflow:saving dict for global step accuracy accuracy_baseline auc auc_precision_recall average_loss global_step label/mean loss prediction/mean info:tensorflow:validation step accuracy accuracy_baseline auc auc_precision_recall average_loss label/mean loss prediction/mean global_step info:tensorflow:loss step info:tensorflow:saving checkpoints for into hdfs://xx.xx.xx.xx:/tresults/model.ckpt
269463917,14081,https://api.github.com/repos/tensorflow/tensorflow/issues/14081,DavidYKay,1,0,0,0,0,0,implement the gradient for cast in c so that it is available for tf_addgradients.this is the python code that i believe would need to be ported be asking bpiel for guidance if i get stuck
269395171,14069,https://api.github.com/repos/tensorflow/tensorflow/issues/14069,ragul28,2,0,0,0,0,0,system information i tried to train the model using tensorflow objdet api os platform and distribution linux ubuntu gcloud vm tensorflow installed from python pip tensorflow version python version bazel version if compiling from source cuda cudnn the typical installation steps followed describe the problemwhen i trying to train the mobilenet model the following error came up i already trained using the same steps in my local pc without any errors i couldnt find any solution related to this error source code logsragulh@ubuntugpu:~/project/models/research/object_detection python train.py logtostderr train_dir=training pipeline_config_path=ssd_mobilenet_v_lap.configtraceback most recent call last file train.py line in module from object_detection import trainer file home/ragulh/project/models/research/object_detection/trainer.py line in module from deployment import model_deploy file home/ragulh/project/models/research/slim/deployment/model_deploy.py line in module from tensorflow.python.eager import contextimporterror no module named eager
269268955,14050,https://api.github.com/repos/tensorflow/tensorflow/issues/14050,rasbt,1,0,0,0,0,1,as recently discussed on the tensorflow mailing list it would be nice if tensorboard would have an option to export a graph as graphviz dot file this would allow users and developers to create e.g python packages based on pygraphviz that can further modify the graph structure such as simplifying or summarizing the graph ops into a publication-ready figure etc.for example scikit-learn has such a function to export decision trees to dot files maybe the source code is useful as an example
269268508,14049,https://api.github.com/repos/tensorflow/tensorflow/issues/14049,rasbt,6,0,0,0,0,0,as recently discussed on the tensorflow mailing list it would be nice if tensorboard would include an export function that supports exporting the graph in a vector graphics format e.g svg or eps or both in addition to the current png export function.for instance i recently bumped into a case where i wanted to include the tensorboard graph as an example output of a tutorial section on tensorboard in my book and found that the png version is too low-res and not very helpful so that i had to manually redraw it also i like to include tensorboard graphs in reports some times after applying some stylistic changes and recently stumbled upon a browser utility called svg crowbar that can get the graph from tensorboard in svg format with some workarounds this indicates that it may already be in svg format and it would be nice to allow to export it to disk for styling and generating high res figures
269261873,14048,https://api.github.com/repos/tensorflow/tensorflow/issues/14048,meccaLeccaHi,0,0,0,0,1,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary via pip tensorflow version use command below v..-rc--geee python version bazel version if compiling from source n/a cuda/cudnn version cuda via pip cudnn gpu model and memory geforce gtx gbhost compiler version gcc exact steps to reproduce as per nvidia tensorflow demo git clone b update-models cd models/tutorials/image/imagenet python classify_image.py describe the problemtensorflow fails to run demo script despite having installed and re-installed as per the manual any help would be greatly appreciated source code logs w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx gbmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx gb pci bus id w tensorflow/core/framework/op_def_util.cc op batchnormwithglobalnormalization is deprecated it will cease to work in graphdef version use tf.nn.batch_normalization e tensorflow/stream_executor/cuda/cuda_dnn.cc could not create cudnn handle cudnn_status_internal_error e tensorflow/stream_executor/cuda/cuda_dnn.cc could not destroy cudnn handle cudnn_status_bad_param f tensorflow/core/kernels/conv_ops.cc check failed stream->parent()->getconvolvealgorithms conv_parameters.shouldincludewinogradnonfusedalgo
269224059,14042,https://api.github.com/repos/tensorflow/tensorflow/issues/14042,hanfeisun,3,0,0,0,0,0,i planned to visualize the evaluation result in tensorboard therefore i need to create the evaluation_hook using tf.train.summarysaverhook in model_fn and pass it into the estimatorspec however estimatorspec doesnt accept evaluation_hooks for now it only has training_hooks will evaluation_hooks be added in future versions
269217060,14041,https://api.github.com/repos/tensorflow/tensorflow/issues/14041,hanfeisun,1,0,0,0,0,0,in tensorflow estimator i want to use cross entropy as the evaluation metrics eval_metric_ops parameter of estimatorspec )however tf.metrics doesnt have this function also tensorflow estimator doesnt allow me to use tf.nn.sigmoid_cross_entropy_with_logits as the eval_metric_ops
269132764,14034,https://api.github.com/repos/tensorflow/tensorflow/issues/14034,freud14,1,0,0,0,0,0,"system informationi am running this on the graham supercomputer of compute canada i tested the bug on computation nodes but it also appears on login nodes without gpus have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux centos tensorflow installed from source or binary custom build with intel mkl i guess tensorflow version use command below bv..--gebf python version python default jun gcc on linux bazel version if compiling from source cuda/cudnn version gpu model and memory tesla p exact command to reproduce : import numpy as npimport tensorflow as tfa np.ones((,))u v np.linalg.svd(a full_matrices=false) without the import tensorflow as tf the bug doesnt appear.you can collect some of this information using our environment capture script describe the problemso basically when using intel mkl with the python code above you get a segmentation fault without the import tensorflow as tf the bug doesnt appear strangely when i change the size of the nd axis of matrix a to below it works at some point that i tested it was when setting the shape of the matrix a to something bigger like it just using all cpus without returning anything as if it was in a deadlock or something when setting mkl_num_threads to both bugs disappear this bug report seems related to all of these issues they are not identical to this problem but really similar so this bug report is just to let you know another symptom related to the same problem source code logs tf_env.txt"
268848507,14004,https://api.github.com/repos/tensorflow/tensorflow/issues/14004,tomzx,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no im running python tensorflow/examples/speech_commands/train.py os platform and distribution e.g linux ubuntu windows x tensorflow installed from source or binary pip install tf-nightly tf_nightly-...dev-cp-cpm-win_amd.whl tensorflow version use command below bunknown dev python version python bazel version if compiling from source n/a cuda/cudnn version cuda cudnn x gpu model and memory x nvidia gtx gb exact command to reproduce python tensorflow/examples/speech_commands/train.py describe the problemit appears that the currently nightlies or rc do not contain the gen_audio_ops module however the documentation for r or master or appears to indicate that the speech_commands demo should work.related source code logs traceback most recent call last file tensorflow/examples/speech_commands/train.py line in module import input_data file e:\tom\documents\git\tensorflow\tensorflow\examples\speech_commands\input_data.py line in module from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio file c:\users\tom\appdata\local\programs\python\python\lib\site-packages\tensorflow\contrib\framework\python\ops\audio_ops.py line in module from tensorflow.python.ops.gen_audio_ops import importerror no module named tensorflow.python.ops.gen_audio_ops
268556765,13983,https://api.github.com/repos/tensorflow/tensorflow/issues/13983,ecsark,1,0,0,0,0,0,problem descriptioni am trying to compute gradient of an aggregation on the currently available elements in tf.tensorarray in a tf.while_loop but got an invalidargumenterror tensorarray tensorarray__@while_/gradients could not write to tensorarray index because it has already been read minimum code to reproduce the error pythondef make_loop_test def cond(i return i def body(i var var_hist write current element var_hist var_hist.write(i var retrieve all current previous elements as well as the one appended just now and compute the sum util tf.reduce_sum(var_hist.gather(tf.range i take gradient where i think the problem comes from grad tf.gradients(util var return i var grad var_hist init_state e tf.tensorarray(dtype=tf.float size clear_after_read=false loop_i loop_var loop_var_hist tf.while_loop(_cond body init_state parallel_iterations return loop_i loop_var loop_var_hist.stack()with tf.session as sess print(sess.run(make_loop_test())) i used to add a bunch of tf.print statements and found the error was coming from gradient statement in the second iteration the error message looks weird to me since i am not writing to index at that time complete logs file ipython-input--dadcddf line in module print(sess.run(make_loop_test()))file ipython-input--dadcddf line in make_loop_test loop_i loop_var loop_var_hist tf.while_loop(_cond body init_state parallel_iterations=)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in while_loop result context.buildloop(cond body loop_vars shape_invariants)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop pred body original_loop_vars loop_vars shape_invariants)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop body_result body(*packed_vars_for_body)file ipython-input--dadcddf line in body grad tf.gradients(util var ) file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in gradients grad_scope op func_call lambda grad_fn(op out_grads))file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in maybecompile return grad_fn exit earlyfile home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in lambda grad_scope op func_call lambda grad_fn(op out_grads))file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/tensor_array_grad.py line in tensorarraygathergrad u_g g.scatter(indices grad)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/util/tf_should_use.py line in wrapped return add_should_use_warning(fn(*args kwargs))file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/tensor_array_ops.py line in scatter name=name)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/gen_data_flow_ops.py line in tensor_array_scatter_v name=name)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-access...which was originally created as op while_/tensorarraygatherv defined at:file home/ecsark/envs/conda/lib/python./runpy.py line in run_module_as_main main mod_spec) elided identical lines from previous traceback file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop body_result body(*packed_vars_for_body)file ipython-input--dadcddf line in body util tf.reduce_sum(var_hist.gather(tf.range i file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/util/tf_should_use.py line in fn return method(self args kwargs)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/tensor_array_ops.py line in gather element_shape=element_shape)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/ops/gen_data_flow_ops.py line in tensor_array_gather_v element_shape=element_shape name=name)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def)file home/ecsark/envs/conda/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback self._graph._extract_stack pylint disable=protected-accessinvalidargumenterror see above for traceback tensorarray tensorarray__@while_/gradients could not write to tensorarray index because it has already been read node while_/gradients/while_/tensorarraygatherv_grad/tensorarrayscatter/tensorarrayscatterv tensorarrayscatterv t=dt_float device=/job:localhost/replica:/task:/cpu: (while_/gradients/while_/tensorarraygatherv_grad/tensorarraygrad/tensorarraygradv while_/range while_/gradients/while_/sum_grad/tile while_/gradients/while_/tensorarraygatherv_grad/tensorarraygrad/gradient_flow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu debian gnu/linux wheezy tensorflow installed from source or binary binary tensorflow version use command below v..--gcee python version bazel version if compiling from source cuda/cudnn version na gpu model and memory na
268536507,13982,https://api.github.com/repos/tensorflow/tensorflow/issues/13982,gdeer81,3,0,0,0,0,0,implement the gradient for floor in c so that it is available for tf_addgradients@suharshs i will implement this one with some guidance from bpiel i believe this is the python code ill be porting over
268425733,13971,https://api.github.com/repos/tensorflow/tensorflow/issues/13971,satendrapratap,1,0,0,0,0,0,we are trying text_classification.py example of tensorflow and separated the training and prediction parts of the code and trained the model using model_dir and tried to predict using the same model_dir.model is being saved but when we comment the training code and try to predict something we get the following error: invalidargumenterror see above for traceback assign requires shapes of both tensors to match lhs shape rhs shape node save/assign assign t=dt_float class= loc:@embedsequence/embeddings use_locking=true validate_shape=true device=/job:localhost/replica:/task:/cpu: (embedsequence/embeddings save/restorev) where and are words count in training and prediction data.but when we uncomment the preparation of train_input_fn without using it anywhere in the code without training the model but using the same model_dir then prediction works fine.train_input_fn tf.estimator.inputs.numpy_input_fn x={words_feature x_train y=y_train batch_size=len(x_train num_epochs=none shuffle=true)we are wondering what exactly is being done by this function which actually overcome the error without training the model using the function at all we have gone through many posts but following post looks more relevant where someone said that prediction cant be done without doing at least some training though the model was already trained and restored using model_dir system information tried both python python tf version python c import tensorflow as tf print(tf.git_version tf.version v..-rc--geced
267338326,13875,https://api.github.com/repos/tensorflow/tensorflow/issues/13875,jthestness,1,0,0,0,0,0,running into a build issue when trying to use mpi collectives we are able to build a recent version of tf using nvidias changes for cuda cudnn however it appears that the build strips the mpi collectives library ops python c import tensorflow.contrib.mpi_collectives as mpitraceback most recent call last file string line in module file mnt/home/lib/python./site-packages/tensorflow/contrib/mpi_collectives/__init__.py line in module from tensorflow.contrib.mpi_collectives.mpi_ops import size file mnt/home/lib/python./site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py line in module mpiallreduce file mnt/home/lib/python./site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py line in load_library expected_op name))nameerror could not find operator mpisize in dynamic library mpi_collectives.so it seems like the issue might be caused by commit cfe which changes linking behavior but were unable to bisect due to commit order dependencies.anyone have an idea how to fix the issue were willing to update the mpi collectives code and submit a pr fix system information have i written custom code as opposed to using a stock example script provided in tensorflow weve applied nvidias cuda cudnn patches for mixed-precision per this page os platform and distribution e.g linux ubuntu tensorflow version use command below eabbefafbdfbcefdbfc python version bazel version cuda/cudnn version cuda rc/cudnn rc exact command to reproduce python c import tensorflow.contrib.mpi_collectives as mpi
267306472,13868,https://api.github.com/repos/tensorflow/tensorflow/issues/13868,icetortoise,2,0,0,0,0,0,cat etc/issue mac os x are we in docker no compiler apple llvm version clang-..)target x_-apple-darwin..thread model posixinstalleddir applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin uname a check pips numpy protobuf tensorflow tensorflow-tensorboard rc check for virtualenv true tensorflow import tf.version tf.git_version v..-rc--gebtf.compiler_version v..-rc--gebsanity check array dtype=int env ld_library_path is unsetdyld_library_path is unset nvidia-smi var/tmp/collect.sh line nvidia-smi command not found cuda libs describe the problemunder tensorflow/examples/speech_commands there are three models for speech commands the default one(conv and low-latency-conv works when i install them to android demo app(tensorflow/examples/android low-latency-svdf does not work in the android app it works on pc though.if i copy the svdf model into the app app crashes when startup reporting model file error like this: not a valid tensorflow graph serialization value for attr t of int is not in the list of allowed values float int qint quint qint nodedef count_nonzero/sum sum t=dt_int tidx=dt_int keep_dims=false (count_nonzero/toint count_nonzero/const op(tensorflowinferenceinterface.java at org.tensorflow.demo.speechactivity.oncreate(speechactivity.java at android.app.activity.performcreate(activity.java at android.app.instrumentation.callactivityoncreate(instrumentation.java at android.app.activitythread.performlaunchactivity(activitythread.java at android.app.activitythread.handlelaunchactivity(activitythread.java at android.app.activitythread.-wrap(unknown source at android.app.activitythread$h.handlemessage(activitythread.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at com.android.internal.os.zygote$methodandargscaller.run(zygote.java at com.android.internal.os.zygoteinit.main(zygoteinit.java caused by java.io.ioexception not a valid tensorflow graph serialization value for attr t of int is not in the list of allowed values float int qint quint qint nodedef count_nonzero/sum sum t=dt_int tidx=dt_int keep_dims=false (count_nonzero/toint count_nonzero/const op(tensorflowinferenceinterface.java more
266670078,13823,https://api.github.com/repos/tensorflow/tensorflow/issues/13823,yaroslavvb,1,0,0,0,0,0,tf.eye(n creates a constant of size n*a better implementation would use fill instead of constantthis is similar to issue where zeros is getting refactored to use fill instead of constant
266663409,13822,https://api.github.com/repos/tensorflow/tensorflow/issues/13822,vishvananda,7,0,0,0,3,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source branch tensorflow version use command below dev python version bazel version if compiling from source cuda/cudnn version gpu model and memory nvidia ti g exact command to reproduce run the script below describe the problemkeras convolutional layers cannot be used multiple times without creating a name conflict this is especially bad when trying to copy layers from one model to another see the second example below this was working a few weeks ago here is a simple test case that used to work: python#!/usr/bin/env pythonimport tensorflow as tffrom tensorflow.contrib.keras.api.keras.layers import convdfrom tensorflow.contrib.keras.api.keras.layers import inputa input(shape=(none c convd c(a)c(a) i added some print statement to different versions of the code it looks like the convolutional ops are created with the following names in the old code convd/convolution convd/convolution_/in the new code they are convd/convolution convd/convolution/it looks like the name scope code was changed recently i specifically notice that some of the scope handling was moved to init where it used to happen when the function was called this is a slightly more complex version of the code that shows that copying convolutional layers doesnt work: python#!/usr/bin/env pythonimport tensorflow as tffrom tensorflow.contrib.keras.api.keras.layers import convdfrom tensorflow.contrib.keras.api.keras.layers import inputfrom tensorflow.contrib.keras.api.keras.models import modela input(shape=(none b ab convd b)mod model(inputs=a outputs=b)a input(shape=(none b afor layer in mod.layers b layer(b)mod model(inputs=a outputs=b) this is an example of the traceback: traceback most recent call last file root/vish/test.py line in module b layer(b file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py line in call output super(layer self).__call__(inputs kwargs file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/layers/base.py line in call outputs self.call(inputs args kwargs file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/layers/convolutional.py line in call outputs self._convolution_op(inputs self.kernel file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in call return self.conv_op(inp filter file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in call return self.call(inp filter file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in call name=self.name file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in convd data_format=data_format name=name file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op_helper op_def=op_def file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op self._add_op(ret file opt/rh/rh-python/root/usr/lib/python./site-packages/tensorflow/python/framework/ops.py line in add_op is already used op.name)valueerror cannot add op with name convd/convolution as that name is already used
266489869,13805,https://api.github.com/repos/tensorflow/tensorflow/issues/13805,jieliuu,1,0,0,0,0,0,tensorflow serving api only supports python it is not work for python
266401667,13799,https://api.github.com/repos/tensorflow/tensorflow/issues/13799,ownedu,1,0,0,0,0,0,in tensorflow/tensorflow/contrib/verbs/rdma.cc when calling ibv_query_gid the gid_index field is hard-coded as which could not work well in real world.to fix this it is better to add a user-specified option
266243042,13789,https://api.github.com/repos/tensorflow/tensorflow/issues/13789,gdeer81,2,0,0,0,0,0,implement the gradient for lrn in c so that it is available for tf_addgradientsthis is the python code that i think would need to be ported using this issue to call dibs on this gradient port per bpiels suggestioni was also advised to mention suharshs
265962057,13766,https://api.github.com/repos/tensorflow/tensorflow/issues/13766,mrry,10,0,0,0,0,0,the tf.data.dataset class does not currently recognize a tf.sparsetensor object as a component of a dataset element this makes it difficult to use the full capabilities of sparsetensor -producing ops such as tf.parse_single_example in dataset.map transformations and then manipulating those elements using operations like dataset.batch the existing tf.train.batch and related functions for queue-based pipelines support tf.sparsetensor and we should add support to dataset for parity.this stack overflow answer suggests some possible workarounds in the meantime
265945773,13763,https://api.github.com/repos/tensorflow/tensorflow/issues/13763,Jiawen1991,1,0,0,0,0,0,"hi,i am trying to modify the tensorflow scheduler and runtime to change the operation priorities as my understanding tensorflow has inter-operation and intra-operation thread pool with a scheduler scheduling operations for different threads and there is also a fifo queue of operations for operations waiting the workflow between them is that operations are sent to the inter-op thread pool from the executor and then that work is running through xla compiler to eventually be executed on the intra-op thread pool schedule is in inside a specific threadpool and is to schedule function for the threads in the pool the thread scheduler selects a subset of threads to run at any given moment when the tasks are passed to the threadpool they are added to one of the scheduler threads fifo queues and then the scheduler will pick up the tasks distributed to the available worker threads i have all the control and data dependencies got from the graph in tensorflow by using tensorboard now i am not sure whether my understanding for the tensorflow scheduler and runtime is correct or not.the problem is that i still have no clue how and where to modify the threadpool/scheduler/ready_queue or others to change the operations priority/sequence for different threads at the tensorflow runtime by modifying the source code does anyone have any ideas"
265945706,13762,https://api.github.com/repos/tensorflow/tensorflow/issues/13762,Jiawen1991,1,0,0,0,0,0,"hi all,if one cpu has cores and each core has two threads does tf runtime use the os threads scheduler to match the logical cores and threads to the physical cores and threads or does it has its own policy to match the threads and cores does tf only recognize cores and but not the whole device(cpu like the system does the os only knows the number/id of cores and sockets and doesnt know how many cpu devices does the device in tf correspond to the physical core e.g tf recognizes devices in the above cpu?i found out that there are inter/intra global and local threadpools in tensorflow/core/common_runtime/local_device.cc and tensorflow/core/common_runtime/dirrect_session.cc does that mean each device or core have its own local inter/intra threadpool and there is only one inter/intra global threadpool in the whole tf runtime?thank you for your time"
265496614,13715,https://api.github.com/repos/tensorflow/tensorflow/issues/13715,zhoujinhai,1,0,0,0,0,0,my python version is cuda version is and cudnn version is but when i import tensorflow i have get the follow error message how i can solve it! c:\users\sxy>pythonpython v..:defaa jun msc v bit amd on wintype help copyright credits or license for more information import tensorflowtraceback most recent call last file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow_internal fp pathname description file c:\users\sxy\appdata\local\programs\python\python\lib\imp.py line in load_module return load_dynamic(name filename file file c:\users\sxy\appdata\local\programs\python\python\lib\imp.py line in load_dynamic return load(spec)importerror dll load failed during handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\sxy\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow_internal fp pathname description file c:\users\sxy\appdata\local\programs\python\python\lib\imp.py line in load_module return load_dynamic(name filename file file c:\users\sxy\appdata\local\programs\python\python\lib\imp.py line in load_dynamic return load(spec)importerror dll load failed failed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help
265457285,13705,https://api.github.com/repos/tensorflow/tensorflow/issues/13705,szalpal,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow its a c code from this tutorial os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary source tensorflow version use command below release python version bazel version if compiling from source cuda/cudnn version no cuda gpu model and memory no gpu exact command to reproduce g i opt/tensorflow i opt/tensorflow/bazel-genfiles loader.cpp describe the problemi have a problem while trying to use tensorflow in external app i took the code from the tutorial above built tensorflow with following command bazel build tensorflow:libtensorflow_cc.so now i want to build my external application with tensorflow while compiling with given command i receive an error i also tried compiling with cmake and proper include_directories directive but to no avail source code logsthe problematic line of code is: #include tensorflow/core/public/session.h compiling with command g i opt/tensorflow i opt/tensorflow/bazel-genfiles loader.cpp generates error: in file included from opt/tensorflow/tensorflow/core/framework/tensor.h from opt/tensorflow/tensorflow/core/public/session.h from loader.cpp::/opt/tensorflow/third_party/eigen/unsupported/eigen/cxx/tensor fatal error unsupported/eigen/cxx/tensor no such file or directorycompilation terminated. whole code snippet can be seen in the tutorial link above remarksa similar issue is but it is closed without specific information if its resolved or not there is a comment which states that if similar issue happens in future it should be opened as new issue the use case there was not precisely using external app on ubuntu but on raspberrypi instead.thus im submitting new issue for this case
265344062,13694,https://api.github.com/repos/tensorflow/tensorflow/issues/13694,lewleib,1,0,0,0,0,0,after installing docker i attempted to run tensorflow in jupyter my run command was lewiss-macbook-pro:mytensorflow lewleib docker run it p p v users/lewleib/mytensorflow:/notebooks tensorflow/tensorflow in the jupyter notebook i ran import tensorflow as tf hello tf.constant(hello tensorflow sess tf.session print(sess.run(hello this resulted in modulenotfounderror no module named tensorflow i have tried starting in different directories and added prefix gcr.io with the same results thanks
264786812,13649,https://api.github.com/repos/tensorflow/tensorflow/issues/13649,hanyh,7,0,0,0,0,0,"i am trying to use dropoutwrapper with lstmblockfusedcell as follows: cell tf.contrib.rnn.lstmblockfusedcell(num_units,forget_bias cell tf.contrib.rnn.dropoutwrapper(cell,dropout i get an exception that the lstmblockfusedcell is not an rnncellmessage the parameter cell is not a rnncell which is raised form like_rnncell during dropoutwrapper initialization it is checking for those proprieties on the cell:checks that a given object is an rnncell by using duck typing conditions hasattr(cell output_size hasattr(cell state_size hasattr(cell zero_state callable(cell lstmblockfusedcell does not have output_size state_size or zero_state properties should lstmblockfusedcell act like rnncell to allow using various wrappers"
264730145,13644,https://api.github.com/repos/tensorflow/tensorflow/issues/13644,rryan,9,0,0,0,0,0,requested via discuss if you would like to see this feature in tf.contrib.signal
264668334,13641,https://api.github.com/repos/tensorflow/tensorflow/issues/13641,rmlarsen,0,0,0,0,0,3,this initial version of svd gradients has the following restrictions only supports statically known inner matrix dimensions m and n.backpropagating through u and v i.e backpropagating through svd nodes with compute_uv=true has further restrictions a only supports real tensors b only supports square and almost square matrices where the number of rows and columns differ by at most c full_matrices must be true also this does not currently have severe implications given the restriction in b).support for dynamic shapes and a i think is straightforward to fix but b is probably a deeper issue having to do with the lack of uniqueness of the decomposition and will require some analysis i think that if we understand b we can get around the restriction in c as well.im marking this as contributions welcome in the hope that somebody with better math skills than myself will help out
264322422,13610,https://api.github.com/repos/tensorflow/tensorflow/issues/13610,tongda,4,0,0,0,0,0,i am running binary tf on ubuntu python version i have nvidia titan xp installed.i use dataset api to build input pipeline what i want is to extract image features using cnn layers in the input pipeline the code looks like this def extract_feats(image with tf.device(/gpu end_points vgg.vgg_(tf.expand_dims(image is_training=(mode modekeys.train spatial_squeeze=false final_conv_layer end_points vgg_/conv/conv feats spatial_pyramid_pooling(final_conv_layer bin_size mode=avg return tf.reshape(feats shape=(bin_size bin_size tf.shape(final_conv_layer features features.map(extract_feats) when running the code my cpu usage is more than i have an cores threads cpu while the gpu usage is i suspect that the input pipeline built from dataset api are forced to run on cpu i tried to set log_device_placement=true and i can see that the operation is placed on gpu.since i want to extract vectors with same length from variable-sized images using spp pooling i have to process these images one by one using dataset.map before calling dataset.batch so i hope the operations inside dataset could be run on gpu
264258928,13607,https://api.github.com/repos/tensorflow/tensorflow/issues/13607,yaroslavvb,7,0,0,0,0,0,following instructions here try to rebuild this op i ran into issue with nsync headers fixed by following while trying to load the so file i run into tensorflow.python.framework.errors_impl.notfounderror max_align_bytes_op.so undefined symbol ztintensorflowopkerneleso the definition for tensorflow::opkernel is missingtf commit allenlavoie
263985914,13594,https://api.github.com/repos/tensorflow/tensorflow/issues/13594,Threynaud,1,0,0,0,0,0,"hello,i am running in the the same issue than described in this stack overflow question i know that github is used for features requests and bugs but this question didnt get an answer and i am not the only one running in the problem this is how the doc illustrates how to add and save runtime statistics: run_options tf.runoptions(trace_level=tf.runoptions.full_trace)run_metadata tf.runmetadata()summary sess.run( merged train_step feed_dict=feed_dict(true options=run_options run_metadata=run_metadata)train_writer.add_run_metadata(run_metadata step%d i train_writer.add_summary(summary i) given that there is no evident way to call sess.run in the training phase with the estimator api i am genuinely wondering how to write this kind of summary is there a workaround?i was thinking about using a sessionrunhook to create something to pass to the estimatorspec but i am really not familiar with that"
263671195,13557,https://api.github.com/repos/tensorflow/tensorflow/issues/13557,codrut3,1,0,0,0,0,0,"currently the gradient of tf.dynamic_stitch is not correct for duplicated indices this isissue drasmuss submitted a pull request for this issue but it was ultimately not mergeddue to a performance drop.while working on this i realised that the problem is more complicated than what followsfrom the discussion in in fact the solution proposed in is not correct becauseone can have duplicated indices in the same input tensor the following example shows this: import tensorflow as tfx tf.zeros((,))y tf.dynamic_stitch x )with tf.session as sess print(y print(sess.run(y analytic numeric tf.test.compute_gradient(x y print(analytic print(analytic print(numeric print(numeric) the output even with is y analytic numeric my fix is to add a new kernel op gatherdisjoint that can handle all the inputs together.this replaces the n calls to gather in data_flow_grad._dynamicstitchgrads with asingle gather_disjoint .the implementation is very similar to gather i just zero out duplicated slices at the end.ive put this op in core if you would like i can move it to contrib just let me know where.ive also added tests for the op and new tests for the gradient of dynamic stitch herei inspired myself from drasmuss pull request.ive run the script by drasmuss again: import timeimport numpy as npimport tensorflow as tfstitch_size n_inputs input_shape reps with tf.device(cpu idxs tf.constant(np.random.randint(stitch_size size=input_shape dtype=tf.int for in range(n_inputs vals tf.constant(np.random.uniform size=input_shape dtype=tf.float for in range(n_inputs y tf.dynamic_stitch(idxs vals grad tf.gradients(y vals)total_time for in range(reps with tf.session as sess start time.time sess.run(grad total_time time.time startprint(total_time reps) the results are as follows:with my fix:cpu gpu without my fix:cpu gpu so with the fix it runs faster on gpu the slowdown is due to the memory copies from host to deviceand back for input and output ive checked with nvprof and not because of my fix.ive run this script using local builds of the same master branch with and without my patch.let me know if the op should be moved somewhere else or it should be made hidden"
263568878,13536,https://api.github.com/repos/tensorflow/tensorflow/issues/13536,bdaskalov,1,0,0,0,0,1,system information irrelevant for this bug have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu linux ubuntu any tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geee python version python continuum analytics inc bazel version if compiling from source n/a cuda/cudnn version irrelevant gpu model and memory irrelevant exact command to reproduce irrelevant describe the problemtf.contrib.seqseq.beamsearchdecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step.the root of the problem is that the while_loop body in dynamic_decode assumes that sequences are independent and will finish only once in the same time beamsearchdecoder creates a tree-like structure where a beam index can be reused in a later step for a state that originates from a different parent index this causes the decoding loop to sometimes record the wrong sequence length for a beam member then this wrong sequence length is passed to beamsearchdecoder.finalize which returns a truncated sequence source code logsi use the following code to workaround the problem this causes the right sequence to be returned but still the length returned by dynamic_decode is wrong. pythonclass fixedbeamsearchdecoder(seqseq.beamsearchdecoder def finalize(self outputs final_state sequence_lengths beamsearchdecoder does not follow the correct semantics of the the finished flag which results in taking wrong length here and getting wrong decoded string we substitute the sequence length recorded by dynamic_decoder which is wrong because of the wrong finished flag returned by beamsearchdecoder.step with the length recorded in beamsearchstate which is correct return super().finalize(outputs final_state final_state.lengths
263509105,13530,https://api.github.com/repos/tensorflow/tensorflow/issues/13530,mellvinbaker,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow it is a customized version of the deep wide example code fairly close to original code os platform and distribution windows server r tensorflow installed from nightly build whl through pip this was tried after numerous other versions including install through pip tensorflow version use command below bunknown dev python version and bazel version if compiling from source not compiling cuda/cudnn version cuda cudnn gpu model and memory tesla m gpu gb exact command to reproduce see attached script.-for the record the server vm has xeon physical cores and gb ram allocated the cpu only machine is a new skylake i with gb ram describe the problemto start i submitted to stack overflow and have not been able to garner assistance after multiple edits to make sure it was framed correctly i truly believe this is a bug since i am sticking so close to the example code but if i have made a mistake i am deeply sorry to all of you.i am working on a wide and deep model following the framework in the tensorflow wide and deep tutorial model works fine when built the old way load entire dataset from pandas convert to tensors feed in input_fn which is ok for running on a cpu however to make it work on the gpu the dataset is too large to fit into gpu memory so batching is necessary i tried using the pandas_input_fn to batch data to the video card and noticed i get spikes of activity followed by long lulls while the next batch is prepared the odd thing is this happens even if i run it on a machine with cpu only the lulls are almost the exact same length so it isnt simply the video card crushing through a simple model faster than the proc can deliver it it seems like it is always waiting to begin loading the next batch until the last one is done training if this function simply cannot be used in this way can we get an example of deep and wide using the dataset api or a manual build of deep and wide using layers and queues at the moment the example code for the dataset api using make_one_shot_iterator for canned estimators doesnt run.)i increased the complexity of the model to make sure it wasnt too easy to compute and still have the same issue i have tried increasing the number of threads allocated to pandas_input_fn i have tried increasing the queue size to far larger than seems reasonable x dataset size which helps a bit but not much i am not sure if the slowdown is when it is queueing or de-queueing but i have been unable to solve the issue after two weeks of troubleshooting the data i am working with is columns k rows.i have created a generic script that generates fake values to simulate the problem however there are far fewer fake columns than real ones so the gap between steps is not nearly as long but still noticeable code attached source code logsattached pandas_input_example.txt
263476001,13526,https://api.github.com/repos/tensorflow/tensorflow/issues/13526,tsoernes,2,0,0,0,0,0,system informationfedora x fc.x_)tensorflow installed from source: tf.version tf.git_version bv..-rc--gdtf.compiler_version bv..-rc--gd python version anaconda)bazel installed from their fedora/copr repositories version non-git)no cuda or compatible gpu)intel mkl c gcc red hat bazel build c opt config=mkl tensorflow/tools/pip_package:build_pip_package notice the mkl flag in the bazel build describe the problemconfiguration and bazel build finished without error when attempting to import tensorflow in python i get this import tensorflow as tftraceback most recent call last file stdin line in module file home/torstein/progs/tensorflow/tensorflow/__init__.py line in module from tensorflow.python import file home/torstein/progs/tensorflow/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file home/torstein/progs/tensorflow/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.platform import self_check file home/torstein/progs/tensorflow/tensorflow/python/platform/self_check.py line in module from tensorflow.python.platform import build_infoimporterror cannot import name build_info
263391607,13519,https://api.github.com/repos/tensorflow/tensorflow/issues/13519,smsorin,1,0,0,0,0,0,the latest documentation on tensorflow.org expains that for embedding_lookup functions ids are obtained typically from featurevaluetold the last term is not found anywhere else on the site probably some remnant from ancient functions.it should probably be replaced with index_table_from or something similar
263273894,13511,https://api.github.com/repos/tensorflow/tensorflow/issues/13511,vervacity,4,0,0,0,0,0,looking at the code for tensorforest it appears that it only allows multiclass including binary classification or regression scalar and vector however there does not seem to be an obvious way to do a multi output classification setup in which there are multiple binary tasks that should all be predicted by a single model could this feature be implemented or would you be able to guide in the easiest hack to convert to a multi output setup apologies if this should actually go to stack overflow please let me know thank you
262974094,13496,https://api.github.com/repos/tensorflow/tensorflow/issues/13496,alsrgv,0,0,0,1,0,0,the goal is to make custom operators compilation a breeze. in import tensorflow as tfcouldnt import dot_parser loading of dot files will not be possible.in tf.sysconfig.get_compile_flags()out : -i/home/devadmin/env/lib/python./site-packages/tensorflow/include i/home/devadmin/env/lib/python./site-packages/tensorflow/include/external/nsync/public d_glibcxx_use_cxx_abi= in tf.sysconfig.get_link_flags()out : -l/home/devadmin/env/lib/python./site-packages/tensorflow ltensorflow_framework
262868243,13492,https://api.github.com/repos/tensorflow/tensorflow/issues/13492,yaroslavvb,1,0,0,0,0,0,scenario runtime error during session.run create new session new session fails to evaluate any tensor giving same runtime error as in step the following example fails to evaluate tf.constant with error tensorflow.python.framework.errors_impl.invalidargumenterror cannot assign a device for operation diag using day old version from this commit tensorflow as tfdef main sess tf.session with tf.device(/gpu bad_op tf.diag print(about to run tensor bad_op try sess.run(bad_op except print(first run failed trying something else sess tf.session good_op tf.constant sess.run(good_op)main() i thought its something to do with tf_extendgraph not being called but manually adding extendgraph before second call results in segmentation fault import tensorflow as tfdef main sess tf.session with tf.device(/gpu bad_op tf.diag print(about to run tensor bad_op try sess.run(bad_op except print(first run failed trying something else sess tf.session good_op tf.constant from tensorflow.python import pywrap_tensorflow as tf_session from tensorflow.python.framework import errors with errors.raise_exception_on_not_ok_status as status tf_session.tf_extendgraph(sess._session sess.graph.as_graph_def().serializetostring none sess.run(good_op)main
262416795,13464,https://api.github.com/repos/tensorflow/tensorflow/issues/13464,mohammadul,1,0,0,0,0,0,"dear all,i happened to have seemingly successfully built tensorflow cpu only for python bit with visual studio on windows.apart from numerous minor manual tweaks the main things that i had to do are the following modify tensorflow-..\tensorflow\contrib\cmake\tools\create_def_file.py modify tensorflow-..\tensorflow\tools\git\gen_git_source.py modify some of the automatically generated py files copy some folders like tensorflow\core\profiler to tensorflow\python\profiler for the final distribution package add blank init__.py files in some folders for the final distribution package translate some cmake build routines to matlab as it was easier for me to debug p i have tested some example codes and found to be running as expected i however believe to make thorough tests to claim whether my build is okay or not.i have found in different forums that many users had struggled to build for python bit in windows and even if built ended up with import errors missing files etc i dont know whether they had finally succeeded or not.so for those who are really desperate to have one i have shared the wheel package here beware that there is a high possibility of abi mismatch if used. acknowledgement i would like to acknowledge wingware for providing me a professional license for wing ide which i have used and in general use extensively for debugging purposes in python._disclaimer the above shared wheel package is for illustrative purposes only which i hope will provide some useful information regarding the build process it is supplied as is without any warranties and support i assume no responsibility or liability for its use of any kind"
262046757,13442,https://api.github.com/repos/tensorflow/tensorflow/issues/13442,AyubQuadri,1,0,0,0,0,0,gpu tios windows cuda tool kit version cudnn version tensorflow-gpu verision after installing cuda tool kit v checked bandwith testc:\program files\nvidia corporation\nvsmi\nvidia-smi.exe path%path variables c:\program files\nvidia gpu computing toolkit\cuda\v.\binc:\program files\nvidia gpu computing toolkit\cuda\v.\libnvvpc:\program files\cuda\bin--------------------------created a virtual environment tensorflow-gpu conda install tensorflow-gpu to check the tensorflow installationimport tensorflow as tfto check the devices available from tensorflow.python.client import device_lib device_lib.list_local_devices()it show only cpu not the gpu stack please help me out where i am getting wrong.in the virtual environment tensorflow-gpu some times when i load import tensorflow as tfit thoughs dll missing errorplease help me out almost spent days in it c:\windows\system>pythonpython anaconda custom bit default may msc v bit amd on wintype help copyright credits or license for more information import tensorflow as tftraceback most recent call last file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\program files\anaconda lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\program files\anaconda lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)modulenotfounderror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\program files\anaconda lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\program files\anaconda lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\program files\anaconda lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\program files\anaconda lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\program files\anaconda lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)modulenotfounderror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help
261933813,13433,https://api.github.com/repos/tensorflow/tensorflow/issues/13433,georgh,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary tested on both tensorflow version use command below for pip cfbebdecacafcc for self-compiled python version python gcc on linux bazel version if compiling from source cuda/cudnn version not used gpu model and memory not used describe the problemevery tf.variable occupies always twice the necessary memory once for the tf.constant vector that is created for the initializer and once as the persistend storage.code to reproduce: import osos.environ tf_cpp_min_vlog_level print allimport tensorflow as tfruns n int m int print(testing allocation of f mb.format(n*m does not matter which version you use:v tf.variable(tf.ones( m n tf.float name=var)#v tf.get_variable(shape=(m,n initializer=tf.ones_initializer dtype=tf.float name=var trainable=false)init tf.global_variables_initializer()for i in range(runs print(start session with tf.session as sess print(start init sess.run(init) in my self-compiled version the output contains the following lines tensorflow/core/common_runtime/bfc_allocator.cc extending allocation by gib bytes tensorflow/core/common_runtime/bfc_allocator.cc total allocated bytes gib for the pip version the current allocation is not displayed but observing htop during the execution or using mprof reveals the same.the issue seems to occur because assign does not reuse the memory of tf.ones but instead allocates additional memory related lines assign_op.h returns null in this case because the memory of tf.ones has a ref count of i dont know why see op_kernel.cc is therefore false.i tried to comment out the input->refcountisone check but then it still doesnt work because of the output_attr.isequalorlessrestrictivethan check.if you remove this check too the memory usage finaly drops to the expected value the memory of tf.ones is reused but this is not a real solution because i dont know how this would effect other operations and it seems to break the memory freeing.i think this bug is quiet serious because it affects nearly all computations is this an already known bug"
261587651,13377,https://api.github.com/repos/tensorflow/tensorflow/issues/13377,samwhitlock,9,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below v python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce bazel build tensorflow/tools/pip_package:build_pip_package describe the problemwith bazel version building on the latest stable tagged release v is not possible i get the following error: error home/ubuntu/.cache/bazel/_bazel_ubuntu/eacfffecce/external/io_bazel_rules_closure/closure/private/defs.bzl the set constructor for depsets is deprecated and will be removed please use the depset constructor instead you can temporarily enable the deprecated set constructor by passing the flag incompatible_disallow_set_constructor=falseerror error loading package extension file closure/private/defs.bzl has errorserror error loading package extension file closure/private/defs.bzl has errorsinfo elapsed time sfailed build did not complete successfully packages loaded) when i pass the flag mentioned in the error message incompatible_disallow_set_constructor=false to the pip package build command i get a similar message: error home/ubuntu/.cache/bazel/_bazel_ubuntu/eacfffecce/external/org_python_pypi_backports_weakref/build.bazel no such package org_python_license the set constructor for depsets is deprecated and will be removed please use the depset constructor instead you can temporarily enable the deprecated set constructor by passing the flag incompatible_disallow_set_constructor=false and referenced by org_python_pypi_backports_weakref//:licenseerror analysis of target tensorflow/tools/pip_package:build_pip_package failed build abortedinfo elapsed time sfailed build did not complete successfully packages loaded currently loading tensorflow/core this problem does not exist on master it would be very useful to have even a tiny stable release v or something to include this fix for those of us that like to build against a stable release
261534553,13375,https://api.github.com/repos/tensorflow/tensorflow/issues/13375,tongda,1,0,0,0,0,0,i am working on tensorflow and python i found that i have to write many unnecessary code to cast python iterator such as return value of map filter zip and dict_keys to list so that tf.convert_to_tensor could work supporting convert python iterator to tensor could not only help python user write concise code without many list(xxx but also memory saving
261369239,13363,https://api.github.com/repos/tensorflow/tensorflow/issues/13363,edumotya,2,0,0,0,0,0,so far tf.contrib.signal.stft does not allow a larger fft_length than frame_length raise valueerror(frame_length d may not be larger than fft_length d frame_length_static fft_length_static i think it is worth to allow this option by zero-padding the input frames matching fft_length since it is the usual proceeding that provides a smooth time-freq representation
261272588,13360,https://api.github.com/repos/tensorflow/tensorflow/issues/13360,thjashin,6,0,0,0,0,0,hi is there any plan to support a tf.reduce_median operator now one have to resort to tf.nn.top_k for implementing this
261075000,13348,https://api.github.com/repos/tensorflow/tensorflow/issues/13348,Noahyt,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu mac osx tensorflow installed from source or binary tensorflow version use command below python version describe the problemwhen i try to combine the dataset api with the resnet architecture provided at tensorflow/models/official/resnet the graph cannot be constructed because the dimension of the input data is not passed to the model constructing function source code logsskeleton code: with tf.session as sess print(initialized features_placeholder tf.placeholder(prepared_x.dtype prepared_x.shape labels_placeholder tf.placeholder(dtype=tf.float shape=prepared_t.shape dataset tf.contrib.data.dataset.from_tensor_slices((features_placeholder labels_placeholder dataset dataset.shuffle(buffer_size dataset dataset.batch(batch_size dataset dataset.repeat(num_epoch iterator dataset.make_initializable_iterator next_x_test next_t_test iterator.get_next next_x_test tf.to_float(next_x_test name=tofloat sess.run(iterator.initializer feed_dict={features_placeholder prepared_x labels_placeholder prepared_t print(next_x_test print(next_t_test model resnet_v(resnet_size num_classes=num_bins output model(next_x_test,is_training=true) the last line throws an error on compiling valueerror the last dimension of the inputs to dense should be defined found none.this makes reference back to the resent_v definition where the final layer is a dense layer.it appears that the dataset api is not passing dimension information hence the final dense layer does not know how to construct itself"
260873929,13336,https://api.github.com/repos/tensorflow/tensorflow/issues/13336,dieka13,0,0,0,0,0,1,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu linux ubuntu generic ubuntu smp wed jun utc x x x gnu/linux tensorflow installed from source or binary) :pip tensorflow version use command below) :tf.version python version bazel version if compiling from source cuda/cudnn version :release v gpu model and memory :gtx titan black gb gtx gb exact command to reproduce :run estimator_cnn.py describe the problemcontinued from this discussion i want to export a custom estimator multiple cnn layer ctc loss in multi gpu setting derived from cifar multi gpu example using export_savemodel but i encountered this error: invalidargumenterror see above for traceback cannot assign a device for operation save/shardedfilename could not satisfy explicit device specification device:gpu because no supported kernel for gpu devices is available. this should not occured in please see discussion source code logsenvironment tf_env.txt error trace error.txt code tf_bug.zip
260616654,13312,https://api.github.com/repos/tensorflow/tensorflow/issues/13312,guillaumekln,1,0,0,3,0,1,attempted to fix by representing the alignment_history field with a tensor instead of a tensorarray however ebrevdo pointed out that this approach led to a quadratic time and space overhead.this pr fixes the issue by directly adding the support for tensorarray in the beamsearchdecoder state as proposed by ebrevdo.@ebrevdo let me know what you think of this implementation thanks
260571953,13308,https://api.github.com/repos/tensorflow/tensorflow/issues/13308,sjperkins,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow ive adapted the zeroout operator from the adding a new op example os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary gpu tensorflow version use command below python c import tensorflow as tf print(tf.git_version tf.version)(v..-rc--geee python version bazel version if compiling from source) :n/a cuda/cudnn version :n/a gpu model and memory :n/a exact command to reproduce :test operator shard_fails.zip make python test_op.py describe the problemwhen the above c operator runs itll print the number of threads in the pool and then segfault on the shard call source code logsc operator code: cpp#define eigen_use_threads#include tensorflow/core/lib/core/threadpool.h#include tensorflow/core/framework/op.h#include tensorflow/core/framework/op_kernel.h#include tensorflow/core/framework/shape_inference.h#include tensorflow/core/util/work_sharder.husing namespace tensorflow;register_op(zeroout input(to_zero int output(zeroed int setshapefn( (::tensorflow::shape_inference::inferencecontext c c->set_output c->input return status::ok class zerooutop public opkernel public explicit zerooutop(opkernelconstruction context opkernel(context void compute(opkernelcontext context override grab the input tensor const tensor input_tensor context->input auto input input_tensor.flatallocate_output input_tensor.shape output_tensor auto output_flat output_tensor->flatdevice()->tensorflow_cpu_worker_threads()->workers printf(pool threads d\n pool->numthreads shard(pool->numthreads pool n int start int end for(int i=start i::_m_invoke(std::_any_data const long long std::_any_data const functor args#=::_m_invoke(std::_any_data const long long std::_any_data const functor args#=::_m_invoke(std::_any_data const from home/sperkins/venv/mb/local/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow_internal.so xfac in std::_function_handler
260364180,13296,https://api.github.com/repos/tensorflow/tensorflow/issues/13296,av8ramit,1,0,0,0,0,0,fix broken links add links check to sanity fix broken links add links check to sanity fix broken link in export.md
260222585,13290,https://api.github.com/repos/tensorflow/tensorflow/issues/13290,swaritras,0,1,0,0,0,0,sws@sws-aspire-t source activate tensorflow(tensorflow sws@sws-aspire-t export tf_binary_url sws@sws-aspire-t tensorflow pip install ignore-installed upgrade tf_binary_urlbash syntax error near unexpected token tensorflow sws@sws-aspire-t tensorflow pip install ignore-installed upgrade tf_binary_urlbash syntax error near unexpected token tensorflow sws@sws-aspire-t tensorflow pip install ignore-installed upgrade tf_binary_urlbash syntax error near unexpected token tensorflow sws@sws-aspire-t pip install ignore-installed upgrade tf_binary_urlcollecting tensorflow from downloading mb mb kb/s collecting protobuf from tensorflow downloading protobuf-..-cp-cpm-manylinux_x_.whl mb mb kb/s collecting wheel from tensorflow downloading wheel-..-py.py-none-any.whl kb kb mb/s collecting numpy from tensorflow downloading numpy-..-cp-cpm-manylinux_x_.whl mb mb kb/s collecting six from tensorflow using cached six-..-py.py-none-any.whlcollecting setuptools from protobuf>=..->tensorflow downloading setuptools-..-py.py-none-any.whl kb kb kb/s installing collected packages six setuptools protobuf wheel numpy tensorflowexception:traceback most recent call last file usr/local/lib/python./site-packages/pip/basecommand.py line in main status self.run(options args file usr/local/lib/python./site-packages/pip/commands/install.py line in run prefix=options.prefix_path file usr/local/lib/python./site-packages/pip/req/req_set.py line in install kwargs file usr/local/lib/python./site-packages/pip/req/req_install.py line in install self.move_wheel_files(self.source_dir root=root prefix=prefix file usr/local/lib/python./site-packages/pip/req/req_install.py line in move_wheel_files isolated=self.isolated file usr/local/lib/python./site-packages/pip/wheel.py line in move_wheel_files clobber(source lib_dir true file usr/local/lib/python./site-packages/pip/wheel.py line in clobber shutil.copyfile(srcfile destfile file usr/local/lib/python./shutil.py line in copyfile with open(dst wb as fdst:permissionerror errno permission denied usr/local/lib/python./site-packages/six.py(tensorflow sws@sws-aspire-t pip install ignore-installed upgrade tf_binary_urlcollecting tensorflow from using cached six from tensorflow using cached six-..-py.py-none-any.whlcollecting wheel from tensorflow using cached wheel-..-py.py-none-any.whlcollecting numpy from tensorflow using cached numpy-..-cp-cpm-manylinux_x_.whlcollecting protobuf from tensorflow using cached protobuf-..-cp-cpm-manylinux_x_.whlcollecting setuptools from protobuf>=..->tensorflow using cached setuptools-..-py.py-none-any.whlinstalling collected packages six wheel numpy setuptools protobuf tensorflowexception:traceback most recent call last file usr/local/lib/python./site-packages/pip/basecommand.py line in main status self.run(options args file usr/local/lib/python./site-packages/pip/commands/install.py line in run prefix=options.prefix_path file usr/local/lib/python./site-packages/pip/req/req_set.py line in install kwargs file usr/local/lib/python./site-packages/pip/req/req_install.py line in install self.move_wheel_files(self.source_dir root=root prefix=prefix file usr/local/lib/python./site-packages/pip/req/req_install.py line in move_wheel_files isolated=self.isolated file usr/local/lib/python./site-packages/pip/wheel.py line in move_wheel_files clobber(source lib_dir true file usr/local/lib/python./site-packages/pip/wheel.py line in clobber os.utime(destfile st.st_atime st.st_mtime))permissionerror errno operation not permitted
260046771,13265,https://api.github.com/repos/tensorflow/tensorflow/issues/13265,ngc92,1,0,0,0,0,0,the following code will run just fine but will not save any variables to checkpoints: pythonimport tensorflow as tfgraph tf.graph()with graph.as_default v tf.get_variable(test shape dtype=tf.float)save tf.train.checkpointsaverhook(test_dir with graph.as_default tf.train.create_global_step a tf.constant with tf.train.monitoredsession(hooks= save as sess sess.run(a) this is because the checkpointsaverhook does not find an existing saver so it creates a new one in its constructor which is executed with a different default graph
260035238,13263,https://api.github.com/repos/tensorflow/tensorflow/issues/13263,terrytangyuan,1,0,0,0,0,0,the canned estimators implementations do not seem to pass the params to their parent class estimator for example dnnclassifier puts all the parameters e.g hidden_units optimizer etc inside the model_fn and then pass the model_fn to the parent class here this makes it difficult for reporting or parameter tuning purposes i saw params is an exposed property for estimator however its mostly empty for canned estimators since its not used is this on purpose?thanks
259969947,13256,https://api.github.com/repos/tensorflow/tensorflow/issues/13256,ppwwyyxx,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu centos tensorflow installed from source or binary binary tensorflow version use command below v..-rc--gabd dev python version pythonimport tensorflow as tfimport osa tf.get_variable(w shape dtype=tf.float)with tf.session as sess sess.run(tf.global_variables_initializer saver tf.train.saver saver.save(sess model)os.remove(model.data--of-)with tf.session as sess saver tf.train.saver saver.restore(sess model) the above code segfaults in todays nightly build i expect an exception
259941552,13249,https://api.github.com/repos/tensorflow/tensorflow/issues/13249,abhishek-neurala,3,0,0,0,0,0,system informationmemory gbprocessor intel core i-hq cpu ghz gpu geforce gtx pcie/sseos type bit have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version cuda cudnn gpu model and memory gtx gb exact command to reproduce bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package describe the problemsince today morning ive been getting this error in compiling tf with cudnn where it seems to be looking for a v file even though ive configured it to use i do not see the same error when building with cudnn i compiled it last night with the same exact configuration and it was compiling fine ive even wiped my system clean and reinstalled the whole environment to make sure there werent any corrupted libraries but the error still persists source code logserror tensorflow/tensorflow/stream_executor/build c compilation of rule tensorflow/stream_executor:cuda_platform failed exit tensorflow/stream_executor/cuda/cuda_dnn.cc in member function cudnnstatus_t perftools::gputools::cuda::wrap::wrappershim__cudnnsetrnndescriptor_v::operator()(perftools::gputools::cuda::cudaexecutor args tensorflow/stream_executor/cuda/cuda_dnn.cc error cudnnsetrnndescriptor_v has not been declared cudnnstatus_t retval name(args tensorflow/stream_executor/cuda/cuda_dnn.cc note in expansion of macro perftools_gputools_cudnn_wrap macro(cudnnsetrnndescriptor_v tensorflow/stream_executor/cuda/cuda_dnn.cc note in expansion of macro cudnn_dnn_routine_each_r cudnn_dnn_routine_each_r(perftools_gputools_cudnn_wrap tensorflow/stream_executor/cuda/cuda_dnn.cc in function int perftools::gputools::cuda::{anonymous}::cudnndatatypetobytesize(cudnndatatype_t):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in member function int perftools::gputools::cuda::cudnnrnnparamsdescriptor::getregioncountperlayer const:tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnnrnninputmode_t perftools::gputools::cuda::{anonymous}::tocudnnrnninputmode(perftools::gputools::dnn::rnninputmode):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnndirectionmode_t perftools::gputools::cuda::{anonymous}::tocudnnrnndirectionmode(perftools::gputools::dnn::rnndirectionmode):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnnrnnmode_t perftools::gputools::cuda::{anonymous}::tocudnnrnnmode(perftools::gputools::dnn::rnnmode):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnndatatype_t perftools::gputools::cuda::{anonymous}::tocudnndatatype(perftools::gputools::dnn::datatype perftools::gputools::dnn::datalayout):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnnconvolutionfwdalgo_t perftools::gputools::cuda::{anonymous}::toconvforwardalgo(perftools::gputools::dnn::algorithmdesc):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnnconvolutionbwddataalgo_t perftools::gputools::cuda::{anonymous}::toconvbackwarddataalgo(perftools::gputools::dnn::algorithmdesc):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc in function cudnnconvolutionbwdfilteralgo_t perftools::gputools::cuda::{anonymous}::toconvbackwardfilteralgo(perftools::gputools::dnn::algorithmdesc):tensorflow/stream_executor/cuda/cuda_dnn.cc warning control reaches end of non-void function wreturn-type tensorflow/stream_executor/cuda/cuda_dnn.cc at global scope:tensorflow/stream_executor/cuda/cuda_dnn.cc warning bool perftools::gputools::cuda::tensoropmathenabled defined but not used wunused-function static bool tensoropmathenabled tensorflow/stream_executor/cuda/cuda_dnn.cc warning tensorflow::thread::threadpool perftools::gputools::cuda::wrap::getcudathreadpool defined but not used wunused-function static port::threadpool getcudathreadpool tensorflow/stream_executor/cuda/cuda_dnn.cc warning perftools::gputools::dnn::algorithmdesc perftools::gputools::cuda::{anonymous}::getcudnnconvolutionforwardalgorithm(perftools::gputools::stream perftools::gputools::cuda::cudaexecutor void int const perftools::gputools::dnn::algorithmconfig bool const perftools::gputools::cuda::scopedtensordescriptor const perftools::gputools::cuda::scopedfilterdescriptor const perftools::gputools::cuda::scopedconvolutiondescriptor const perftools::gputools::cuda::scopedtensordescriptor perftools::gputools::scratchallocator perftools::gputools::devicememory
259908240,13242,https://api.github.com/repos/tensorflow/tensorflow/issues/13242,yongtang,0,0,0,3,0,0,this fix tries to address the request raised in where it was not possible to decode video like the existing op of decode_audio .this fix adds the support of tf.contrib.ffmpeg.decode_video by invoking ffmpeg the same fashion as tf.contrib.ffmpeg.decode_audo so that video could be stored in the tensor frames height width channel at the moment the output format is rgb .this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
259784466,13232,https://api.github.com/repos/tensorflow/tensorflow/issues/13232,habibutsu,1,0,0,0,0,0,getting rid from unnecessary call inspect.stack in making of decorator in tf.contrib.framework.add_arg_scopep.s.on my laptop this call gives big overhead for initialization of tensorflow
259732595,13228,https://api.github.com/repos/tensorflow/tensorflow/issues/13228,bodokaiser,1,0,0,0,0,0,a paper i am reimplementing recently uses image gradient loss numpy offers np.gradient to achieve this task i.e np.gradient(image axis np.gradient(image axis however tensorflow lacks this feature or at least documentation about how to use tf.gradients to get this done.therefore i propose to either send a pr where i add a gradient image op which uses fixed d convolution i.e.: pythonimport numpy as npimport scipy as spimport tensorflow as tffrom skimage import iofrom skimage import colorfrom matplotlib import pyplot as pltimage color.rgbgray(io.imread(image.jpg))image_rs image.reshape list(image.shape xgrad np.gradient(image axis=)ygrad np.gradient(image axis=)image_ph tf.placeholder(tf.float image_rs.shape)x_weight tf.reshape(tf.constant tf.float y_weight tf.reshape(x_weight xgrad_ts tf.nn.convd(image_ph x_weight same)ygrad_ts tf.nn.convd(image_ph y_weight same)with tf.session as sess xgrad ygrad sess.run( xgrad_ts ygrad_ts feed_dict={image_ph image_rs print(xgrad.shape fig axes plt.subplots axes imshow(image axes imshow(xgrad axes imshow(ygrad axes imshow(image axes imshow(xgrad axes imshow(ygrad plt.show() or have someone update documentation of tf.gradients .for the first i could provide a pr if this is considered interesting for tensorflow and not too implementation specific
259649989,13221,https://api.github.com/repos/tensorflow/tensorflow/issues/13221,yaroslavvb,1,0,0,0,0,0,so im trying to figure out why my resnets are running out of memory and it seems that theres a memory leak in tile and zeros_like operations.those ops have memory allocated during each session run but theres no log_memory deallocation messages corresponding to them the sum of missing deallocations matches the amount of memory leaked as reported by allocator as max_bytes_in_use accessed through tf.contrib.memory_stats.maxbytesinuse op)heres a simplified repro at each sess.run the memory grows by gb until it crashes with oom i run it i see run gbs in use run gbs in use run gbs in use run gbs in use w tensorflow/core/common_runtime/bfc_allocator.cc allocator gpu__bfc ran out of memory trying to allocate mib current allocation summary follows.... offending ops: gradients/leaky_relu_grad/zeros_like mbgradients/sum_grad/tile mb version:ubuntu official tensorflow linux gpu python nightly wheel from today dev__git_version v..-rc--gedeecommit
259612538,13220,https://api.github.com/repos/tensorflow/tensorflow/issues/13220,kevinbache,2,0,0,0,0,0,im getting a cpu-only bazel build failure on osx tensorflow has already been configured with default options.message below bazel build config=opt tensorflow/tools/pip_package:build_pip_package verbose_failureswarning users/kevin/projects/tensorflow/tensorflow/core/build in includes attribute of cc_library rule tensorflow/core:framework_headers_lib external/nsync/public resolves to external/nsync/public not below the relative path of its package tensorflow/core this will be an error in the future since this rule was created by the macro cc_header_only_library the error might have been caused by the macro implementation in users/kevin/projects/tensorflow/tensorflow/tensorflow.bzl::.warning users/kevin/projects/tensorflow/tensorflow/contrib/learn/build in py_library rule tensorflow/contrib/learn:learn target tensorflow/contrib/learn:learn depends on deprecated target tensorflow/contrib/session_bundle:exporter no longer supported switch to savedmodel immediately.warning users/kevin/projects/tensorflow/tensorflow/contrib/learn/build in py_library rule tensorflow/contrib/learn:learn target tensorflow/contrib/learn:learn depends on deprecated target tensorflow/contrib/session_bundle:gc no longer supported switch to savedmodel immediately.info found target...error private/var/tmp/_bazel_kevin/ceabafeed/external/nsync/build c compilation of rule nsync//:nsync_cpp failed exit cc_wrapper.sh failed error executing command cd private/var/tmp/_bazel_kevin/ceabafeed/execroot/org_tensorflow exec env path=/anaconda/bin:/users/kevin/bin:/usr/local/cellar/coreutils/./libexec/gnubin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin pwd=/proc/self/cwd python_bin_path=/anaconda/bin/python python_lib_path=/anaconda/lib/python./site-packages tf_need_cuda tf_need_opencl tmpdir=/var/folders/g/fdvqnqsywtljqlwgn/t external/local_config_cc/cc_wrapper.sh u_fortify_source fstack-protector wall wthread-safety wself-assign fcolor-diagnostics fno-omit-frame-pointer g o d_fortify_source dndebug ffunction-sections fdata-sections march=native md mf bazel-out/local-py-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.d fpic iquote external/nsync iquote bazel-out/local-py-opt/genfiles/external/nsync iquote external/bazel_tools iquote bazel-out/local-py-opt/genfiles/external/bazel_tools isystem external/nsync/public isystem bazel-out/local-py-opt/genfiles/external/nsync/public isystem external/bazel_tools/tools/cpp/gcc x c std=c dnsync_atomic_cpp dnsync_use_cpp_timepoint i./external/nsync//platform/c i./external/nsync//platform/gcc i./external/nsync//platform/x i./external/nsync//public i./external/nsync//internal i./external/nsync//platform/posix d_posix_c_source=l pthread wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted c external/nsync/internal/time_internal.c o bazel-out/local-py-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.o).in file included from external/nsync/internal/time_internal.c::in file included from external/nsync//platform/c++/platform.h::in file included from library/developer/commandlinetools/usr/include/c++/v/mutex::in file included from library/developer/commandlinetools/usr/include/c++/v/__mutex_base::/library/developer/commandlinetools/usr/include/c++/v/__threading_support error unknown type name mach_port_tmach_port_t libcpp_thread_get_port();^/library/developer/commandlinetools/usr/include/c++/v/__threading_support error unknown type name mach_port_tmach_port_t libcpp_thread_get_port library/developer/commandlinetools/usr/include/c++/v/__threading_support error use of undeclared identifier pthread_mach_thread_np return pthread_mach_thread_np(pthread_self errors generated.target tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path s tensorflow commit edeeebffbacefffbecabceac mac osx sierra g) command line tools pkgutil pkg-info=com.apple.pkg.cltools_executablespackage-id com.apple.pkg.cltools_executablesversion volume location install-time groups com.apple.findsystemfiles.pkg-group environment capture script cat tf_env.txt cat etc/issue darwin macbook-pro.localdomain darwin kernel version thu jun pdt root:xnu-..~/release_x x_mac os x are we in docker no compiler apple llvm version clang-..)target x_-apple-darwin..thread model posixinstalleddir library/developer/commandlinetools/usr/bin uname a darwin macbook-pro.localdomain darwin kernel version thu jun pdt root:xnu-..~/release_x x check pips numpy numpydoc protobuf tensorflow tensorflow-tensorboard check for virtualenv false tensorflow import tf.version tf.git_version v..-rc--geeetf.compiler_version v..-rc--geeesanity check array dtype=int)traceback most recent call last file users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import importerror no module named tensorflow.python.pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file string line in module file users/kevin/projects/tensorflow/tensorflow/__init__.py line in module from tensorflow.python import file users/kevin/projects/tensorflow/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import importerror no module named tensorflow.python.pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help env ld_library_path is unsetdyld_library_path is unset nvidia-smi tools/tf_env_collect.sh line nvidia-smi command not found cuda libs bazel information bazel infobazel-bin private/var/tmp/_bazel_kevin/ceabafeed/execroot/org_tensorflow/bazel-out/local-py-opt/binbazel-genfiles private/var/tmp/_bazel_kevin/ceabafeed/execroot/org_tensorflow/bazel-out/local-py-opt/genfilesbazel-testlogs private/var/tmp/_bazel_kevin/ceabafeed/execroot/org_tensorflow/bazel-out/local-py-opt/testlogscharacter-encoding file.encoding iso defaultcharset iso--command_log private/var/tmp/_bazel_kevin/ceabafeed/command.logcommitted-heap-size mbexecution_root private/var/tmp/_bazel_kevin/ceabafeed/execroot/org_tensorflowgc-count gc-time msinstall_base var/tmp/_bazel_kevin/install/edcbbdeaffjava-home library/java/javavirtualmachines/jdk.._.jdk/contents/home/jrejava-runtime java(tm se runtime environment build b by oracle corporationjava-vm java hotspot(tm bit server vm build b mixed mode by oracle corporationmax-heap-size mbmessage_log private/var/tmp/_bazel_kevin/ceabafeed/message.logoutput_base private/var/tmp/_bazel_kevin/ceabafeedoutput_path private/var/tmp/_bazel_kevin/ceabafeed/execroot/tensorflow/bazel-outpackage_path workspace%release release homebrewserver_pid used-heap-size mbworkspace users/kevin/projects/tensorflow
258688638,13143,https://api.github.com/repos/tensorflow/tensorflow/issues/13143,zoeysgithub,0,1,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
258472889,13125,https://api.github.com/repos/tensorflow/tensorflow/issues/13125,Reavern,2,0,0,0,0,0,hi there im new to python tensorflow im using raspbian stretch python when i run retrain.py from the examples provided using these code: python retrain.py bottleneck_dir=tf_files/bottlenecks how_many_training_steps model_dir=tf_files/inception output_graph=retrained_graph.pb output_labels=retrained_labels.txt image_dir=img input_binary=true there are no outputs for output_labels retrained_labels.txt output_graph retrained_graph.pb).the codes below are the console outputs for the few last lines console output from the command above: info:tensorflow step train accuracy info:tensorflow step cross entropy info:tensorflow step validation accuracy n=)info:tensorflow:final test accuracy n=)info:tensorflow:froze variables.converted variables to const ops. any solutions thanks
258283277,13094,https://api.github.com/repos/tensorflow/tensorflow/issues/13094,desire2020,8,0,0,0,0,0,arxiv path is a recently proposed parallelization-friendly rnn cell the author released his own pytorch version of the sru we are looking forward to an offical tensorflow implementation with cudnn optimizations
257989725,13061,https://api.github.com/repos/tensorflow/tensorflow/issues/13061,dmacvicar,11,0,0,0,0,0,the following is a proposal and i dont have it fully working yet so a pull request is too early and i want to gather some feedback if an issue is not appropriate let me know and i will look for a different medium system informationalmost any linux distribution describe the problemmost linux distributions have similar policies/limitations not accepting bundling of system libraries specially those security-sensitive requiring building the whole from source and allowing patching the sources requiring building offline without internet access not having enough manpower to package hundred of packages in a dependency chain just to get packages like maven built in these source-chains environments.because of cmake makes things easier it is a build tool with a few dependencies that does not need complex bootstrapping building bazel itself will bring you into the java maven dependency chain which we already proved in opensuse to expand into hundred of packages.both the bazel and cmake build files do not play well with but cmake already improves the cmake build files also pull the sources of different projects that is because the top cmakelists.txt includes: cmakeset(cmake_module_path project_source_dir}/external) into the load path.that external directory is full of snippets like tensorflow/contrib/cmake/external/jpeg.cmake tensorflow/contrib/cmake/external/gif.cmake etcthose snippets use the external project cmake api to build directly from git or tarballs upstream this clashes with it is a bit sad that things like curl used to be looked up in the system and developers deliberately bundled them without making the cmake snippet first look for it and if not configure the bundled one proposalthe proposal is to make the cmake build support both the google/mac user type of build with bundled sources and the classical linux distribution build.this would be a gradual refactoring steps could be add to cmakelists.txt an option: cmakeoption(tensorflow_system_libraries use system libraries on replicate the external directory as platform and conditionally make it use one or the other: cmakeif(tensorflow_system_libraries set(cmake_module_path project_source_dir}/platform)else set(cmake_module_path project_source_dir}/external)endif() so that then the part that does: cmake external dependenciesinclude(zlib)include(gif)include(png)include(jpeg) would just work replace incrementaly and one by one the dep.cmake snippets in platform/ .example with gif.cmake:using the standard usr/share/cmake/modules/findgif.cmake included in cmake which may use pkg-config for some modules the module itself documents it would then set gif_libraries gif_include_dir etc.unfortunately the variable that the original set is called gif_static_libraries because it assumes it would be static i think we could fix that later so that naming ends making sense but lets not focus on that for now.the gif.cmake i cited above would become a simple: cmakefind_package(gif required dummy targets other targets depend on.add_custom_target(gif these can be removed if we fix cmakelistsadd_custom_target(gif_copy_headers_to_destination)set(gif_static_libraries gif_libraries})set(gif_include_dir gif_include_dir}) and that is more or less enough to move forward with this dependency repeat.it may need some hacks also in the top level cmake files however...i think this approach is doable and could be turned into making the cmake build fully linux distro enabled upstream slowly cleaning the naming etc removing these copy_headers targets so that the platform version of the cmake files do not need to create dummy targets etc similarly the sqlite one becomes: cmakefind_package(sqlite)set(sqlite_static_libraries sqlite_libraries})set(sqlite_headers sqlite_include_dir})add_custom_target(sqlite)add_custom_target(sqlite_copy_headers_to_destination) with the only difference is that the find module was not included in cmake so i just copied it intoplatform from somewhere with this method i have been able to move forward and forward with the build current blockers cmake build does not work out of the box pr issue with missing tensorflow/core/debug/debug_service.grpc.pb.h //cc meaksh dincamihai ncounter
257979659,13060,https://api.github.com/repos/tensorflow/tensorflow/issues/13060,piyushshri,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu pyinstaller tensorflow installed from source or binary binary tensorflow version use command below python version cuda/cudnn version cuda cudnn gpu model and memory x nvidia geforce describe the problemas tensorflows load_op_library finds paths according to the os it is being run on i believe this is a problem with tensorflow in pyinstaller environment i am trying to make an executable of my python code which uses tensorflow the executable gets generated correctly but when i try to run it i get the following error: traceback most recent call last file detection_init.py line in module import lib.tensorboxdetector as tensorboxdetector file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file lib/tensorboxdetector.py line in module from lib.train import build_forward file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file lib/train.py line in module import tensorflow.contrib.slim as slim file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/__init__.py line in module file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/bayesflow/__init__.py line in module file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/bayesflow/python/ops/csiszar_divergence.py line in module file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py line in module file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/framework/__init__.py line in module file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/framework/python/ops/__init__.py line in module file usr/local/lib/python./dist-packages/pyinstaller/loader/pyimod_importers.py line in load_module exec(bytecode module.__dict file tensorflow/contrib/framework/python/ops/checkpoint_ops.py line in module file tensorflow/contrib/util/loader.py line in load_op_library file tensorflow/python/framework/load_library.py line in load_op_librarytensorflow.python.framework.errors_impl.notfounderror tensorflow/contrib/util/tensorflow/contrib/framework/python/ops/_checkpoint_ops.so cannot open shared object file no such file or directory failed to execute script detection_init if we look carefully pyinstaller is expecting the file checkpoint_ops.so in directory tensorflow/contrib/util/tensorflow/contrib/framework/python/ops but theres no directory like this checkpoint_ops.so is located at tensorflow/contrib/framework/python/ops how can this be fixed
257784000,13044,https://api.github.com/repos/tensorflow/tensorflow/issues/13044,Coderx7,4,0,0,0,0,0,i tried to use tensorflow object detection api with my own dataset everything was working just fine until all of a sudden it crashed with the following error messages info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step info:tensorflow:error reported to coordinator class tensorflow.python.framework.errors_impl.invalidargumenterror assertion failed unable to decode bytes as jpeg png or gif node case/if_/decode_image/cond_jpeg/cond_png/assert/assert assert t= dt_string summarize device=/job:localhost/replica:/task:/cpu: (case/if_/decode_image/cond_jpeg/cond_png/is_gif case/if_/decode_image/cond_jpeg/cond_png/assert/assert/data info:tensorflow:global step loss sec/step info:tensorflow:finished training saving model to disk traceback most recent call last file train.py line in module tf.app.run file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\platform\app.py line in run sys.exit(main(_sys.argv flags_passthrough file train.py line in main worker_job_name is_chief flags.train_dir file c:\users\master\anaconda\envs\anaconda\lib\site-packages\object_detection-.-py..egg\object_detection\trainer.py line in train saver=saver file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py line in train sv.saver.save(sess sv.save_path global_step=sv.global_step file c:\users\master\anaconda\envs\anaconda\lib\contextlib.py line in exit next(self.gen file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\supervisor.py line in managed_session self.stop(close_summary_writer=close_summary_writer file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\supervisor.py line in stop stop_grace_period_secs=self._stop_grace_secs file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\coordinator.py line in join six.reraise(*self._exc_info_to_raise file c:\users\master\anaconda\envs\anaconda\lib\site-packages\six.py line in reraise raise value file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\queue_runner_impl.py line in run enqueue_callable file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\client\session.py line in single_operation_run target_list_as_strings status none file c:\users\master\anaconda\envs\anaconda\lib\contextlib.py line in exit next(self.gen file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\framework\errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status tensorflow.python.framework.errors_impl.invalidargumenterror assertion failed unable to decode bytes as jpeg png or gif node case/if_/decode_image/cond_jpeg/cond_png/assert/assert assert t= dt_string summarize device=/job:localhost/replica:/task:/cpu: (case/if_/decode_image/cond_jpeg/cond_png/is_gif case/if_/decode_image/cond_jpeg/cond_png/assert/assert/data g:\tensorflow_section\models-master\object_detection>when i upgraded to the the error has changed and this is what i get now info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step info:tensorflow:global step loss sec/step w c:\tf_jenkins\home\workspace\rel-win\m\windows-gpu\py\\tensorflow\core\framework\op_kernel.cc invalid argument shape mismatch in tuple component expected got info:tensorflow:error reported to coordinator class tensorflow.python.framework.errors_impl.invalidargumenterror shape mismatch in tuple component expected got node batch/padding_fifo_queue_enqueue queueenqueuev tcomponents= dt_string dt_int dt_float dt_int dt_float dt_int dt_int dt_int dt_int dt_int dt_int dt_int dt_bool dt_int dt_bool dt_int dt_float dt_int dt_string dt_int dt_string dt_int timeout_ms device=/job:localhost/replica:/task:/cpu: (batch/padding_fifo_queue reshape shape sparsetodense shape merge shape merge shape sparsetodense shape sparsetodense shape cast shape cast shape expanddims shape reshape shape reshape shape info:tensorflow:global step loss sec/step info:tensorflow:finished training saving model to disk traceback most recent call last file train.py line in module tf.app.run file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\platform\app.py line in run sys.exit(main(_sys.argv flags_passthrough file train.py line in main worker_job_name is_chief flags.train_dir file c:\users\master\anaconda\envs\anaconda\lib\site-packages\object_detection-.-py..egg\object_detection\trainer.py line in train saver=saver file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py line in train sv.stop(threads close_summary_writer=true file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\supervisor.py line in stop stop_grace_period_secs=self._stop_grace_secs file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\coordinator.py line in join six.reraise(*self._exc_info_to_raise file c:\users\master\anaconda\envs\anaconda\lib\site-packages\six.py line in reraise raise value file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\training\queue_runner_impl.py line in run enqueue_callable file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\client\session.py line in single_operation_run target_list_as_strings status none file c:\users\master\anaconda\envs\anaconda\lib\contextlib.py line in exit next(self.gen file c:\users\master\anaconda\envs\anaconda\lib\site-packages\tensorflow\python\framework\errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status tensorflow.python.framework.errors_impl.invalidargumenterror shape mismatch in tuple component expected got node batch/padding_fifo_queue_enqueue queueenqueuev tcomponents= dt_string dt_int dt_float dt_int dt_float dt_int dt_int dt_int dt_int dt_int dt_int dt_int dt_bool dt_int dt_bool dt_int dt_float dt_int dt_string dt_int dt_string dt_int timeout_ms device=/job:localhost/replica:/task:/cpu: (batch/padding_fifo_queue reshape shape sparsetodense shape merge shape merge shape sparsetodense shape sparsetodense shape cast shape cast shape expanddims shape reshape shape reshape shape g:\tensorflow_section\models-master\object_detection>i have no idea what is causing the issue could this be that some images have wrong extensions for example an image which was actually a png with channels has been saved as a jpg if this is the case how to spot the faulty image file or even better why does not tf use the correct type/number of channels by itself right now the error is not descriptive enough it doesnt give any hint which image file is corrupted or is making the problem if the cause of these errors is what i pointed out earlier then they should be caught when the tf record is being created or if tf records are not the only means of inputs then the same mechanism for knowing the culprit needs to be implemented as wellanyway if all my thoughts are wrong then i would appreciate any help regarding this issue system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu windows x build tensorflow installed from source or binary) :binary used pip install tensorflow version use command below and python version anaconda bazel version if compiling from source cuda/cudnn version cuda cudnn v and v after upgrading to cudnnv is in the path gpu model and memory gtx g
257404847,13018,https://api.github.com/repos/tensorflow/tensorflow/issues/13018,rasmusbergpalm,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geee python version python continuum analytics inc bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce import numpy as npfrom tensorflow.contrib.data import dataseta= np.arange np.arange() dataset.from_tensor_slices(np.asarray(a describe the problem dataset has a nice padded_batch feature which allows padding batches to ensure all tensors have the same size which is very nice for the common use-case of having sequences of different lengths.however it seems impossible to create such a dataset from a list of numpy array with different lengths see command above.it would be very nice to have a dataset.from_variable_length_tensor_slices or such that could take such inputs.i know im supposed to use a textlinedataset or something similar and then use a series of map functions but i think tensorflow should support doing all of the pre-processing outside of tensorflow instead of forcing it into a delayed execution graph paradigm which is harder to reason about and debug also my data is stored in json files and requires complex pre-processing so the textlinedataset/map approach doesnt cut it
257069654,12997,https://api.github.com/repos/tensorflow/tensorflow/issues/12997,edgimar,3,0,0,0,0,0,though the tf.decode_image function can handle a few different image encodings all of these encodings can only support a maximum of channels in an image rgba one format which can support a larger number of channels is tiff which is currently not supported in tf perhaps due to its complex specification it would be convenient if tf could support decoding tiff or some other format that is able to represent an arbitrary number of channels n which could be decoded into an h w n shaped tensor
256865118,12984,https://api.github.com/repos/tensorflow/tensorflow/issues/12984,kdatta,6,0,0,0,0,0,no custom code.system snapshot:gcc centos bazel building from commit id debeaadeafafacfb from master. user@master tensorflow bazel build config=mkl copt=-march=knl copt=-o s c opt tensorflow/tools/pip_package:build_pip_packagewarning ignoring http_proxy in environment.error home/user/tensorflow/tensorflow/tools/pip_package/build no such package boringssl java.io.ioexception error downloading to home/user/.cache/bazel/_bazel_user/bdcecfaafcfce/external/boringssl/eacdbdbccdbccdabde.tar.gz checksum was aabfbdefededccdbdcccecfe but wanted ffcfdccdcfabffdaebfcfb and referenced by tensorflow/tools/pip_package:licenses.error analysis of target tensorflow/tools/pip_package:build_pip_package failed build aborted.info elapsed time s user@master tensorflow
256865095,12983,https://api.github.com/repos/tensorflow/tensorflow/issues/12983,amygdala,2,0,0,1,0,0,it would be great to be able to pass in an alternate source url to the mnist.read_data_sets function to make it easier to use fashion mnist etc.does this seem like a reasonable way to do it
256837158,12979,https://api.github.com/repos/tensorflow/tensorflow/issues/12979,d4l3k,22,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu arch linux tensorflow installed from source or binary source tensorflow version use command below master python version python bazel version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce bazel build verbose_failures tensorflow/contrib/android:libtensorflow_inference.so crosstool_top=//external:android/crosstool host_crosstool_top=@bazel_tools//tools/cpp:toolchain cpu=armeabi-va describe the problemgithub tarball checksums have changed making it impossible to build tensorflow since the checksums dont match any more source code logs error home/travis/tensorflow/tensorflow/contrib/android/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf java.io.ioexception error downloading to home/travis/.cache/bazel/_bazel_travis/cbafcbfffbbdea/external/protobuf/badafaaddebeaedcaecdfea.tar.gz checksum was efdeeebcfcdadffbaaabebbdafbabbdbd but wanted dbdceedcebcbaaacedadfbfad and referenced by tensorflow/contrib/android:libtensorflow_inference.so tmp/foo curl l shasum total received xferd average speed time time time current dload upload total spent left speed k k k kefdeeebcfcdadffbaaabebbdafbabbdbd tmp/foo curl shasum total received xferd average speed time time time current dload upload total spent left speed k k k kdbdceedcebcbaaacedadfbfad
256786094,12974,https://api.github.com/repos/tensorflow/tensorflow/issues/12974,dartdog,4,0,0,0,0,0,this is opened here since it relates to estimator and canned estimator models not providing a method to get output for tb it shouldnt be a tensorboard issue and i attempted to get an answer at stackoverflow here i have been using the estimator interface in tf including the creation of the data input function: training_input_fn tf.estimator.inputs.pandas_input_fn(x=training_data y=training_label batch_size shuffle=true num_epochs=none) and building the nn: dnnclassifier tf.estimator.dnnclassifier feature_columns=dnn_features hidden_units n_classes model_dir=./tmp/ccsprop optimizer=tf.train.proximaladagradoptimizer learning_rate l_regularization_strength and executing it dnnclassifier.train(input_fn=training_input_fn steps=) after much searching i see no easy way to add tensorboard output without resorting to recreating the model from scratch and indicated here some of the help on so tbcall=tf.train.summarysaverhook(save_steps output_dir=./tb summary_op=tf.summary.merge_all()) dnnclassifier.train(input_fn=training_input_fn steps hooks=tbcall) that gives this error exactly one of scaffold or summary_op must be providedthe answer on so has now been edited to show code from creating a model from scratch it seems so is there no way to get basic info from tf.estimator.dnnclassifier to tensorboard and more generically from other tf.estimators one would expect the canned estimators to provide this for model introspection and tuning
256772242,12971,https://api.github.com/repos/tensorflow/tensorflow/issues/12971,yongtang,1,0,0,0,0,0,this fix is an effort to try to address the request raised in where it was not possible to split utf strings with tf.string_split .this fix adds an additional attr of encoding so that utf could be specified for string_split .this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
256633972,12960,https://api.github.com/repos/tensorflow/tensorflow/issues/12960,chaos-dd,1,0,0,0,0,0,i found that currently dataset api does not support pre read data as old reading data api if we have so much train data the feature the speed of reading data is critical
256578178,12954,https://api.github.com/repos/tensorflow/tensorflow/issues/12954,Yevgnen,1,0,0,0,0,0,since i think this issue has nothing to do with the system information i would temporarily ignore them tensorflow version use command below) :v..--gacba describe the problemthe dense layers defined in tensorflow.python.layers.core build kernel and bias in build function while the build function is called in call function the may cause that sometime one define a layer but not call it immediately which causes an unexpected variable naming issue source code logsfor example when i try to implement a toy seqseq model the following code with tf.variable_scope(output self._output_layer core_layers.dense self._vocab_size name=output_layer if targets is not none training embedding_targets tf.nn.embedding_lookup self._embedding targets outputs final_state tf.nn.dynamic_rnn cell=self._cell inputs=embedding_targets sequence_length=lengths dtype=_float time_major=true logits self._output_layer(outputs else infer or sampling batch_size tf.unstack(tf.shape(encoder_outputs eos_ids tf.ones( batch_size dtype=tf.int name=eos eos_step_embedded tf.nn.embedding_lookup self._embedding eos_ids def loop_fn_transition(time cell_output cell_state loop_state def get_input output_logits self._output_layer(cell_output kernel/bias of dense defined here predictions tf.argmax(output_logits axis next_input tf.nn.embedding_lookup self._embedding predictions return next_input elements_finished time lengths emit_output cell_output cell_state cell_state loop_state none return elements_finished get_input cell_state emit_output loop_state def loop_fn(time cell_output cell_state loop_state if cell_state is none elements_finished lengths next_input eos_step_embedded cell_state encoder_final_state emit_output none loop_state none return elements_finished next_input cell_state emit_output loop_state else return loop_fn_transition(time cell_output cell_state loop_state cell_outputs final_state tf.nn.raw_rnn self._cell loop_fn) i got by inspecting the checkpoint file) basic_seqseq/decoder/output_layer/bias dt_float basic_seqseq/decoder/output_layer/kernel dt_float in training mode while got basic_seqseq/decoder/rnn/output_layer/bias dt_float basic_seqseq/decoder/rnn/output_layer/kernel dt_float in inference mode so i cant restore a training checkpoint when inference due to notfounderror see above for traceback key basic_seqseq/decoder/rnn/output_layer/kernel not found in checkpoint
256560760,12952,https://api.github.com/repos/tensorflow/tensorflow/issues/12952,yongtang,1,0,0,0,0,0,this fix tries to address the request from where axis was not supported for tf.unique this fix adds support of axis for tf.unique in this fix the additional input axis has been specified as an d vector so that it is possible to optionally provide an axis or not in case axis is provided no axis is used in case axis= x is used axis for tf.unique is x ref fix fixes signed-off-by yong tang yong.tang.github@outlook.com
256532521,12948,https://api.github.com/repos/tensorflow/tensorflow/issues/12948,ahundt,19,0,0,0,0,0,densenet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis unfortunately this has the side effect of quadratic memory growth in tensorflow as completely new blocks of memory are allocated after each concat operation resulting in poor performance during all phases of execution.this is a feature request for a new allocation=shared option for operations such as tf.concat(allocation=shared which works seamlessly with later operations that might modify the data such as batchnorm which might also need to share memory this would make it is possible to utilize the memory-efficient implementation of densenets a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations this image from the paper pytorch implementation illustrates the shared memory approach:! densenet shared memory pytorch efficient densenet implementation keras densenet implementation with naive allocations works with tensorflow backend.this functionality would also be useful for any other application or future network design that employs recursive concatenations just in case i didnt find it in my search perhaps a mechanism already exists that can meet these goals
256497857,12943,https://api.github.com/repos/tensorflow/tensorflow/issues/12943,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where float for convd and convd is incomplete.this fix adds float support for convd float support for convd float support for convd updated docs for float support of convd already supported).this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
256068792,12888,https://api.github.com/repos/tensorflow/tensorflow/issues/12888,cancan101,47,0,0,7,0,7,support exporting and loading models in onnx format.see
255965114,12882,https://api.github.com/repos/tensorflow/tensorflow/issues/12882,a-doumoulakis,1,0,0,0,0,0,add the ability to use trisycl as the sycl implementation in tensorflow when opencl sycl support is enabled relates to note that trisycl is still work in progress and many features are missing currently only the host device can be used trisycl/trisycl#).computecpp is a much more reliable way of using sycl with tensorflow
255610970,12851,https://api.github.com/repos/tensorflow/tensorflow/issues/12851,eaplatanios,6,0,0,0,0,0,mrry this is a comment i originally posted to the redesigning input pipelines issue but i think it went unnoticed given the amount of comments on that thread given that issue is now closed for good reason i decided to post it as a separate issue.i currently cannot see a way currently to unzip a dataset lets say we have a trainable model that has both a train/fit method and a infer/predict method lets call the type of the potentially nested structure of inputs to our model i and the type of training inputs which are only needed when training e.g supervision labels ti in this case we want the train method to accept datasets with elements of type i ti i.e a tuple of i and ti and the predict method to accept datasets with elements of type i or i ti in which case it would ignore the labels we also want the model to only have one underlying graph supporting all these types of input the way i could see doing that was for the underlying model to construct two iterators one with elements type i and one with type ti and initialize them according to the provided datasets however if somebody provides a dataset with elements of type i ti to the train method there is no way to unzip this dataset and initialize both iterators one has to use dataset.map twice which is not efficient i think but please correct me if im wrong and which may also not pull matching elements from the datasets if each pull advances the current index in the original first dataset im not sure if that happens
255434122,12832,https://api.github.com/repos/tensorflow/tensorflow/issues/12832,opensourcemattress,1,0,0,0,0,0,registrations of convd operations with fp fp for batch_norms in tf.layers and tf.contrib.layers related issue dont understand how fp operations must be implemented in low-level with cuda and i didnt make any additional optimizations.there is no implementation for fused batch_norm yet.i copied part of code for dtypes from convd test to convd test and seems like all works with fp i get inf and large absolute errors with large numbers i suppose that its fine at least cpu and gpu implementations return almost similar values i skip such cases in test.example of such case use_gpu false dtype dtype float data_format ndhwc expected actual use_gpu false dtype dtype float data_format ndhwc expected actual inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf fp using may result in inf values and large absolute errors when used with large numbers skipping use_gpu true dtype dtype float data_format ndhwc expected actual use_gpu true dtype dtype float data_format ndhwc expected actual inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf fp using may result in inf values and large absolute errors when used with large numbers skippingfp is not fully covered by tests because i not sure how to do it
255161002,12806,https://api.github.com/repos/tensorflow/tensorflow/issues/12806,resec,1,0,0,0,0,0,adds support for various android arch in makefile build.this resolves
254900430,12786,https://api.github.com/repos/tensorflow/tensorflow/issues/12786,kobejean,3,0,0,0,0,0,closes added a tf.roll op that works similarly to numpys np.roll this was a feature requested in and was marked as contributions welcome usage:rolls the elements of a tensor by the offsets of shift along the dimensionsof axis elements that roll passed the last position will wrap aroundto the first.for example t is roll(t shift axis shifting along multiple dimensions t is roll(t shift axis shifting along the same axis multiple times t is roll(t shift axis shift shift i specifies the number of places by which elements are shifted along the dimension specified by axis i negative shifts will roll the elements in the opposite direction.axis axis i specifies the dimension that the shift shift i should occur if the same axis is referenced more than once the total shift for that axis will be the sum of all the shifts that belong to that axis.output has the same shape and size as the input the elements are shifted by the offsets of shift along the dimensions of axis unit tests: running main from test_main.cc running tests from test case global test environment set-up tests from rolloptest run rolloptest.scalarindices i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use sse avx avx fma ok rolloptest.scalarindices ms run rolloptest.scalarindices_complex ok rolloptest.scalarindices_complex ms run rolloptest.simple_twod ok rolloptest.simple_twod ms run rolloptest.simple_threed ok rolloptest.simple_threed ms run rolloptest.simple_twod ok rolloptest.simple_twod ms run rolloptest.simple_threed ok rolloptest.simple_threed ms run rolloptest.zeroshift_threed ok rolloptest.zeroshift_threed ms run rolloptest.zerosize_threed ok rolloptest.zerosize_threed ms run rolloptest.duplicateshifts_twod ok rolloptest.duplicateshifts_twod ms run rolloptest.error_inputmustbevectororhigher ok rolloptest.error_inputmustbevectororhigher ms run rolloptest.error_axismustbescalarorvector ok rolloptest.error_axismustbescalarorvector ms run rolloptest.error_shiftmustbescalarorvector ok rolloptest.error_shiftmustbescalarorvector ms run rolloptest.error_shiftandaxismustbesamesize ok rolloptest.error_shiftandaxismustbesamesize ms run rolloptest.error_axisoutofrange ok rolloptest.error_axisoutofrange ms tests from rolloptest ms total global test environment tear-down tests from test case ran ms total passed tests benchmarks: running main from test_main.ccbenchmark time(ns iterations i tensorflow/core/platform/cpu_feature_guard.cc your cpu supports instructions that this tensorflow binary was not compiled to use sse avx avx fmabm_cpu_roll_outer mb/s m items/sbm_cpu_roll_outer mb/s m items/sbm_cpu_roll_outer mb/s m items/sbm_cpu_roll_outer mb/s m items/sbm_cpu_roll_all mb/s m items/sbm_cpu_roll_all mb/s m items/sbm_cpu_roll_all mb/s m items/sbm_cpu_roll_all mb/s m items/s i also tried to implement this op for the gpu but gave up on it after struggling with it for a week this is the error i received i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name tesla k major minor memoryclockrate(ghz pcibusid e.totalmemory gib freememory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device device:gpu device name tesla k pci bus id e compute capability e tensorflow/stream_executor/cuda/cuda_driver.cc could not synchronize on cuda context cuda_error_illegal_address no stack trace available f tensorflow/core/common_runtime/kernel_benchmark_testlib.cc non-ok-status device_->sync status internal gpu sync failedaborted core dumped) if anyone has a clue of what might be the problem id appreciate the help the code for the gpu implementation is on this branch on my fork thanks
254893052,12781,https://api.github.com/repos/tensorflow/tensorflow/issues/12781,armafire,1,0,0,0,1,0,"hello,compiling tensorflow source on skylake-x intel i with config=opt gives the following error in the snappy external module error home/armafire/.cache/bazel/_bazel_armafire/efbefcbeabbed/external/snappy/build c compilation of rule snappy//:snappy failed exit crosstool_wrapper_driver_is_not_gcc failed error executing command cd home/armafire/.cache/bazel/_bazel_armafire/efbefcbeabbed/execroot/org_tensorflow exec env cuda_toolkit_path=/usr/local/cuda cudnn_install_path=/usr/local/cuda gcc_host_compiler_path=/usr/bin/gcc pwd=/proc/self/cwd python_bin_path=/usr/bin/python python_lib_path=/usr/local/lib/python./dist-packages tf_cuda_clang tf_cuda_compute_capabilities tf_cuda_version tf_cudnn_version tf_need_cuda tf_need_opencl external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc u_fortify_source d_fortify_source fstack-protector fpie wall wunused-but-set-parameter wno-free-nonheap-object fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections march=native std=c march=native md mf bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.d frandom-seed=bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.o fpic iquote external/snappy iquote bazel-out/local_linux-opt/genfiles/external/snappy iquote external/bazel_tools iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools isystem external/bazel_tools/tools/cpp/gcc wno-shift-negative-value wno-implicit-function-declaration no-canonical-prefixes wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted fno-canonical-system-headers c external/snappy/snappy.cc o bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.o ccplus warning command line option wno-implicit-function-declaration is valid for c/objc but not for c external/snappy/snappy.cc in member function void snappy::snappysinkallocator::flush(size_t external/snappy/snappy.cc warning comparison between signed and unsigned integer expressions wsign-compare for int i i blocks_.size i in file included from external/snappy/snappy-internal.h from external/snappy/snappy.cc external/snappy/snappy.cc in instantiation of bool snappy::snappyscatteredwriter::appendfromself(size_t size_t with allocator snappy::snappysinkallocator size_t long unsigned int external/snappy/snappy.cc required from void snappy::snappydecompressor::decompressalltags(writer with writer snappy::snappyscatteredwriter"
254778133,12761,https://api.github.com/repos/tensorflow/tensorflow/issues/12761,Mistobaan,2,0,0,0,0,0,it would be nice to be able to just add a org_tensorflow in bazel and be able to build it as a dependency at the best of my knowledge the syntaxnet dockerfile is the best example on having tensorflow as a submodule still building it is not straightforward and probably does not work if you activate cuda/xla/mlk because those depends on other bazel subprojects.are there any plans to support this use case or i am missing something in the best practices to build tensorflow as a subpackage
254597089,12750,https://api.github.com/repos/tensorflow/tensorflow/issues/12750,manishvatsa,6,0,0,0,0,0,loading the model in android give below error:fatal exception main process tensorflow.lgsi.com.posapplication pid java.lang.runtimeexception unable to start activity componentinfo{tensorflow.lgsi.com.posapplication/tensorflow.lgsi.com.posapplication.mainactivity java.lang.u nsupportedoperationexception loading a savedmodel is not supported in android file a bug at if this feature is important to you at android.app.activitythread.performlaunchactivity(activitythread.java at android.app.activitythread.handlelaunchactivity(activitythread.java at android.app.activitythread.-wrap(activitythread.java at android.app.activitythread$h.handlemessage(activitythread.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at com.android.internal.os.zygoteinit$methodandargscaller.run(zygoteinit.java at com.android.internal.os.zygoteinit.main(zygoteinit.java caused by java.lang.unsupportedoperationexception loading a savedmodel is not supported in android file a bug at if this feature is important to you at org.tensorflow.savedmodelbundle.load(native method at org.tensorflow.savedmodelbundle.load(savedmodelbundle.java at tensorflow.com.posapplication.tagger.postagger.(postagger.java at tensorflow.lgsi.com.posapplication.tagger.postagger.getinspostagger(postagger.java at tensorflow.com.posapplication.mainactivity.oncreate(mainactivity.java at android.app.activity.performcreate(activity.java at android.app.instrumentation.callactivityoncreate(instrumentation.java at android.app.activitythread.performlaunchactivity(activitythread.java at android.app.activitythread.handlelaunchactivity(activitythread.java at android.app.activitythread.-wrap(activitythread.java at android.app.activitythread$h.handlemessage(activitythread.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at com.android.internal.os.zygoteinit$methodandargscaller.run(zygoteinit.java at com.android.internal.os.zygoteinit.main(zygoteinit.java i/art starting model is saved in python by using below api: builder tf.saved_model.builder.savedmodelbuilder(r./tmp/model)builder.add_meta_graph_and_variables(session tf.saved_model.tag_constants.serving )builder.save(true model is loading in andoid using below api: inferenceinterface new tensorflowinferenceinterface(context.getassets model_file
254562139,12746,https://api.github.com/repos/tensorflow/tensorflow/issues/12746,AakashKumarNain,1,0,0,0,0,0,lets say i define a layer using the tf.layers api as shown below: conv tf.layers.convd(input_img filters kernel_size padding=same name=conv) now i can build a whole network defining such layers can you please introduce another functionality for the tf.layers api so that for each layer we can set the weights in a single line like this: conv.set_weights(weights or something like this conv.set_params(param_values) this would be very very useful
254545880,12744,https://api.github.com/repos/tensorflow/tensorflow/issues/12744,MMMzq,0,3,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
253832976,12693,https://api.github.com/repos/tensorflow/tensorflow/issues/12693,DedongZhang,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows pro tensorflow installed from source or binary install binary with gpu version by pip based on python tensorflow version use command below python version bazel version if compiling from source) :none cuda/cudnn version :cuda v./cudnn v gpu model and memory :geforce gtx ti gb describe the problemi am trying to open tensorboard after running mnist_with_summaries.py source code from the tensorboard tutorials however after i run tensorboard logdir=/tmp/tensorflow/mnist in cmd it stuck and did not showing anything else i try to use tensorboard logdir=/tmp/tensorflow/mnist debug and have the same result capture
253489854,12665,https://api.github.com/repos/tensorflow/tensorflow/issues/12665,bpiel,2,0,0,0,0,0,gradients for the nn ops convd and maxpool ported from python are simple enough that i grouped them into a single pr i can split into two if necessary.cc suharshs dguerra
253482206,12663,https://api.github.com/repos/tensorflow/tensorflow/issues/12663,Mistobaan,8,0,0,0,0,0,is not clear from the docs what is the difference between the two i assumed that one has thread locking gfile and the other does not but both say the same thing in the docs
253449257,12658,https://api.github.com/repos/tensorflow/tensorflow/issues/12658,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where it was not possible to have copyfile with streaming the original implementation copies the whole content of the file to a stringbuffer and write to the file this could be an issue if the file size is too large than the memory of the host).this fix streams the copyfile operation.*also sendfile is used if the file system is posix*this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
253447422,12657,https://api.github.com/repos/tensorflow/tensorflow/issues/12657,ydzhang12345,0,0,0,0,1,0,i am using tensorflow built from source.i run the newly released speech_commands example its in tensorflow/examples/speech_commands following all the stuffs and all good however after i used the freeze.py script and generated the pb graph i am unable to load it back the code i used to load pb graph is: def load_graph(frozen_graph_filename with tf.gfile.gfile(frozen_graph_filename rb as f graph_def tf.graphdef graph_def.parsefromstring(f.read with tf.graph().as_default as graph tf.import_graph_def graph_def input_map=none return_elements=none name=imported op_dict=none producer_op_list=none return graphgraph load_graph(./test_loading_graph_back.pb) error:file usr/local/lib/python./dist-packages/tensorflow/python/framework/importer.py line in import_graph_def raise valueerror(no op named s in defined operations node.op)valueerror no op named decodewav in defined operations
253331831,12649,https://api.github.com/repos/tensorflow/tensorflow/issues/12649,kwotsin,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu android lollipop tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce running app on android studio describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.i have met with a very peculiar problem that seem to show that a certain model inception v which i got from the tf-slim library seem to consume more memory than usual and cause a bufferoverflow problem like in here com.example.android.androidevaluateimagenet d/androidruntime shutting down vm com.example.android.androidevaluateimagenet e/androidruntime fatal exception main process com.example.android.androidevaluateimagenet pid java.nio.bufferoverflowexception at java.nio.floatbuffer.put(floatbuffer.java at org.tensorflow.tensor.writeto(tensor.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.fetch(tensorflowinferenceinterface.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.fetch(tensorflowinferenceinterface.java at com.example.android.androidevaluateimagenet.tensorflowimageclassifier.recognizeimage(tensorflowimageclassifier.java at com.example.android.androidevaluateimagenet.mainactivity.getinferencetime(mainactivity.java at com.example.android.androidevaluateimagenet.mainactivity$.onclick(mainactivity.java at android.view.view.performclick(view.java at android.view.view$performclick.run(view.java at android.os.handler.handlecallback(handler.java at android.os.handler.dispatchmessage(handler.java at android.os.looper.loop(looper.java at android.app.activitythread.main(activitythread.java at java.lang.reflect.method.invoke(native method at java.lang.reflect.method.invoke(method.java at com.android.internal.os.zygoteinit$methodandargscaller.run(zygoteinit.java at com.android.internal.os.zygoteinit.main(zygoteinit.java:) specifically if you try an image resolution higher than x in the model it crashes with the error the interesting thing is that this doesnt happen for a significantly larger model the inception resnet v although it supposedly consumes a lot more memory with that larger inception resnet v model i can use a resolution of over with no issues i have rebooted my device switched it and run it on another identical device and another brand of device but still theres this problem i cant exactly locate the issue but specifically here is what i have traced in the error:specific error stack trace: tensorflowimageclassifier.java inferenceinterface.fetch(outputname outputs); tensorflowinferenceinterace.java public void fetch(string var float var this.fetch(var floatbuffer.wrap(var tensor.java public void writeto(floatbuffer var if(this.dtype datatype.float throw incompatiblebuffer(var this.dtype else bytebuffer var this.buffer var.put(var.asfloatbuffer floatbuffer.java public floatbuffer put(floatbuffer src if src this throw new illegalargumentexception int n src.remaining if n remaining throw new bufferoverflowexception for int i i n i put(src.get return this i am not sure why this happens as everything else works in fact if i try a resolution below the inception v model works perfectly fine note that i got the checkpoint model from tf-slim and froze it i believe the method i used to freeze it works well since there is no problem for all other models except inception v so i can only conclude the problem lies within the layers but i am not exactly sure how i can find out which layer is causing the problem or even if it is because of the layers im not sure how to fix it i have included the layers of inception v and inception resnet v in order here:inception v layers resnet v layers it is really the problem within the model then i thought it could be because of a faulty implementation of a certain operation that is causing overly huge memory consumed.as an alternative fix is there a way to check and raise the limit of the buffer size for the app to run successfully
252224461,12522,https://api.github.com/repos/tensorflow/tensorflow/issues/12522,kjanjua26,3,0,0,0,0,0,"i am trying to stack cnn before lstm however i am experience a little problem my lstm ctc works fine however i want to pass extracted feature from cnn to lstm instead of whole image the code is here error i am facing is file trainer.py line in module train file trainer.py line in train logits inputs targets seq_len,w b model.get_train_model file home/kamranjanjua/tf_cnnlstm/tlstmaug/model.py line in get_train_model outputs tf.nn.bidirectional_dynamic_rnn(forwardh,backwardh,x,seq_len,dtype=tf.float file home/kamranjanjua/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/rnn.py line in bidirectional_dynamic_rnn time_major=time_major scope=fw_scope file home/kamranjanjua/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/rnn.py line in dynamic_rnn dtype=dtype file home/kamranjanjua/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/rnn.py line in dynamic_rnn_loop input size depth of inputs must be accessible via shape inference,valueerror input size depth of inputs must be accessible via shape inference but saw value none. any help in this matter would be appreciated i am kind of stuck here system information have i written custom code as opposed to using a stock example script provided in tensorflow custom code os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below using this on purpose since my older code works in and i didnt update it for the new version python version gpu model and memory titanx gb"
252130279,12508,https://api.github.com/repos/tensorflow/tensorflow/issues/12508,mvsusp,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information tensorflow/tensorflow:latest container ubuntu linux installed from pip tensorflow version v..--gcdfc python bazel version if compiling from source exact command to reproduce pythonclassifier dnnclassifier(feature_columns=feature_columns hidden_units n_classes model_dir=model_path)classifier.export_savedmodel(model_path script.serving_input_receiver_fn describe the problemtrying to export the model dnnclassifier throws the exception: bashexception during training a default input_alternative must be provided traceback most recent call last file algo.py line in train nn.export_savedmodel(model_path script.serving_input_receiver_fn default_output_alternative_key=none file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in export_savedmodel actual_default_output_alternative_key file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py line in build_all_signature_defs raise valueerror(a default input_alternative must be provided.) the problem happens because dnnclassifier constructor creates a head with name none
251996295,12496,https://api.github.com/repos/tensorflow/tensorflow/issues/12496,adamcrume,2,0,0,0,0,0,the ssl certificate for not expired on june
251893974,12484,https://api.github.com/repos/tensorflow/tensorflow/issues/12484,DEKHTIARJonathan,1,0,0,0,0,0,"hello everyone,as you may find on pypi tensorflow is not reported as a package which is compatible with python which doesnt prevent to be able to install it with pip properly it just mess up any api call to the pypi api which is not good for all our friends using ci tools).the reason is actually quite simple some classifiers were missing*. python actual classifiers as declared on pypi for tf classifiers development status beta intended audience developers intended audience education intended audience science/research license osi approved apache software license programming language python only python is flagged topic scientific/engineering mathematics topic software development libraries topic software development libraries python modules the proposed changes to uniformised the two setup.py and allow py compatibility classifiers development status beta intended audience developers intended audience education intended audience science/research license osi approved apache software license programming language python programming language python programming language python programming language python topic scientific/engineering topic scientific/engineering mathematics topic scientific/engineering artificial intelligence topic software development topic software development libraries topic software development libraries python modules so the main objective of this pr is basically to fix the aforesaid issues in the two setup.py i was able to find.i also took the opportunity to uniformise the two setup.py files they now declare the same audiences and topics i hope it will be fine else i can still revert the changes and just keep the changes related to python compatibility.please have a great day,best regards,jonathan dekhtiar"
251834024,12474,https://api.github.com/repos/tensorflow/tensorflow/issues/12474,tfboyd,14,0,0,4,0,0,things have moved forward i strongly suggest building from head with cuda and cudnn all of the necessary codes should be in the tf tag but given we are still working on new features for fp i would build from head if that is of interest i do not like to share anything i have not personally tested as i know how frustrating trying to get things to compile can be. everything below this line is out dated as of oct this is an unofficial and very not supported patch to make it possible to compile tensorflow with cudarc and cudnn or cuda cudnn during testing on v volta and resnet fp using cuda rc cudnn was significantly faster than cuda cudnn which was not a surprise i am about to test on ps i am sharing this patch informally so those that are interested can play with cudnn as well as cuda rc before we have the official release as we have more interesting code e.g fp models i will share it in this issue i expect nvidia will start to submit official cudnn patches very soon note this patch may work on more recent versions of tensorflow but it will likely bit rot so keep that in mind apply the cudnn patch and then fast-forwarding the branch might be the best approach my git skills are not strong so do what you think is best download the patches cuda-.-and-cudnn-.-support.patch eigen.fafb.cuda.diff clone the tensorflow repo checkout the revision that the tensorflow patch can apply to: git checkout dbbbfcbabbbcbd apply the tensorflow patch: git apply downloads/-cuda-.-and-cudnn-.-support.patch run configure when it asks for the cuda version put or when it asks for the cudnn version put entering worked fine for me make sure you put the paths to the right cuda and cudnn versions and have your ldconfig or ld_libary_path set to point to the cuda folder. ./configure attempt to build tensorflow so that eigen is downloaded this build will fail if building for cudarc but will succeed for cuda bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package apply the eigen patch: bash cd p bazel-out/../../../external/eigen_archive patch p downloads/eigen.fafb.cuda.diff build tensorflow successfully bash cd bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package i have run this process myself on ubuntu with python thank for nvidia for the early patch and reedwm who created most of these instructions if you are using python and gcc here is a whl where i followed the instructions above and one with cuda and cudnn which i have yet to test stress again this was created by me and not the tensorflow build team my/our goal is to engage with anyone that wants to try this out and try to have a little fun
251827441,12473,https://api.github.com/repos/tensorflow/tensorflow/issues/12473,wsdm2018,1,0,0,0,0,0,"the gradient of scatter_nd_add always return none if the gradient is not implemented it should raise an exception pythonimport tensorflow as tfimport numpy as np import matplotlib.pyplot as pltrng np.randomx tf.variable(np.random.random astype(float))indice tf.variable(np.random.randint size dtype=tf.int)x_val np.random.random indice_val np.random.randint size val_val np.random.random tf graph inputx tf.placeholder(float)y tf.placeholder(float set model weightsw tf.variable(np.random.random astype(float name=weight)b tf.variable(np.random.random astype(float name=bias construct a linear modelpred tf.add(tf.multiply(x w b)y tf.scatter_nd_add(x indice pred)grad tf.gradients(y w,b"
251724227,12454,https://api.github.com/repos/tensorflow/tensorflow/issues/12454,ppwwyyxx,1,0,0,0,0,0,os platform and distribution e.g linux ubuntu archlinux tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geee python version pythonimport tensorflow as tfx tf.get_variable(asdfds shape dtype=tf.int)x*. prints: traceback most recent call last file a.py line in module x typeerror unsupported operand type(s for variable and float variable with matching dtype can multiply with float the problem here is int cannot multiply with float.since usually the dtype of some tensor is not directly written in code this misleading message can cause confusions
251636784,12448,https://api.github.com/repos/tensorflow/tensorflow/issues/12448,bpiel,1,0,0,0,0,0,registers the biasaddgrad op as the gradient for biasadd by implementing biasaddgradhelper.unlike the other tested ops in nn_grad_tests.cc biasadd takes two arguments the test i added does test both arguments but not simultaneously not sure if thats acceptable guidance appreciated.this is only my second pr to tensorflow when giving feedback please assume that i barely know what im doing
251431578,12416,https://api.github.com/repos/tensorflow/tensorflow/issues/12416,sumitbinnani,32,0,0,0,0,0,problem descriptionthe binary version for gpu provided here on installation page of ubuntu expects cudnn v however the documentation mentions the version of cudnn required as kindly rectify the error in documentation system information linux ubuntu tensorflow installed from binary gpu version tensorflow version v..-rc--geee
251425534,12414,https://api.github.com/repos/tensorflow/tensorflow/issues/12414,georgesterpu,5,0,0,0,0,0,system information have i written custom code yes os platform and distribution manjaro linux arch linux repo tensorflow installed from source or binary binary tensorflow version python version bazel version cuda/cudnn version cuda cudnn cudnn gpu model and memory nvidia gtx gb exact command to reproduce the problemwhen using a tensorflow wheel built with cuda support my app prints the following warning message at the end of a training epoch w tensorflow/core/framework/op_kernel.cc out of range end of sequence node iteratorgetnext iteratorgetnext output_shapes output_types= dt_float dt_int dt_int dt_int device=/job:localhost/replica:/task:/cpu: (iterator) the code trains a seqseq model and i assume the message gets printed somewhere downstream of seqseq.dynamic_decode the message still gets printed even when the nn cells are not wrapped with a tf.contrib.rnn.devicewrapper with device field indicating a gpu only works fine on non-cuda builds.all of this happens while the code is protected with the try/except statements for epoch in range(num_epochs session.run(iterator.initializer while true try session.run( operation except tf.errors.outofrangeerror break now the only cheeky thing is that i am using the binaries from arch linux repositories but these are far from being dodgy. python-tensorflow build script problem was in tensorflow and persists in tensorflow also tested on a laptop without dedicated gpu but same os and packages works fine
251287023,12397,https://api.github.com/repos/tensorflow/tensorflow/issues/12397,theflofly,1,0,0,0,0,0,the gradient computation in the c api works as follow.lets say that we have the following graph tanh assign matmul const var const here our output is tanh and our input is var the gradient method does a bfs from var to tanh to count for each node how many backprop we should expect if a node has two outgoing edges it will be ready only when both would have been backpropagated and the gradient summed in our case var has assign matmul these values are saved into a pending array.then the gradient method does a bfs from tanh to var this is the actual backpropagation the error is backpropagated until we reach var when a node is reached pending is decreased by one and if pending the node is ready and is added to the queue of nodes to be processed if it is not ready it will be reached again in the future and will be ready at some point.in our case doing a bfs from var to tanh give us expected gradients one from assign and one from matmul whereas doing a bfs from tanh to var we will reach var only once because we cant reach assign from tanh in that case the pending count will never reach zero var will never be ready and the bfs will end at the end a check is done and if pending nodes are still there an error is raised.this pr updates the gradient method to ignore nodes that have outgoing edges and are not in the list of output tanh is the only one in the list of outputs in our case).the unit tests have been updated to use variable with assign nodes and not const because differentiating w.r.t const make less sense and the error would have been catched before
251228337,12391,https://api.github.com/repos/tensorflow/tensorflow/issues/12391,bpiel,1,0,0,0,0,0,this branch implements llossgrad the gradient for lloss.it is a port of the python implementation is my first pr to tensorflow when giving feedback please assume that i barely know what im doing.side question i plan to do a few more gradients is it ok if i batch a few simple ones like this one into a single pr
251073013,12372,https://api.github.com/repos/tensorflow/tensorflow/issues/12372,hexahedria,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geee python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce describe the problem tf.contrib.data.dataset objects do not correctly deal with nested dictionary structures when using a dataset with a nested dictionary the inner dictionaries are replaced with the first tensor in that inner dictionary and following tensors are restored for incorrect keys.this is not an issue with tf.contrib.framework.nest only with datasets which appear to instead use tensorflow.contrib.data.python.util.nest the particular difference causing the bug appears to be vs source code logs pythonimport tensorflow as tffrom tensorflow.contrib.data.python.util import nest as data_nesttest_value a aa tf.constant ab tf.constant b tf.constant print tf.contrib.framework.nest.map_structure(lambda t t.shape test_value a aa tensorshape ab tensorshape( dimension b tensorshape( dimension these are the correct shapesd tf.contrib.data.dataset.from_tensors(test_value)print d.output_shapes a tensorshape b tensorshape( dimension incorrectprint data_nest.map_structure(lambda t t.shape test_value a tensorshape b tensorshape( dimension incorrect
251033518,12366,https://api.github.com/repos/tensorflow/tensorflow/issues/12366,areel,1,0,0,1,0,0,statesrequirements to run tensorflow with gpu supportcudnn v.this is now incorrect.tensorflow or later requires cudnn cudnn_.dll)i also suggest you add a comment and link to this gist just saved me hours.regardsaidan
250813166,12345,https://api.github.com/repos/tensorflow/tensorflow/issues/12345,ed-alertedh,27,0,0,0,0,0,system informationn/a describe the problem backgroundpep added support for type hints in python these are purely annotations and are not enforced by the interpreter however there are tools such as mypy which can be run to check for consistency in the annotations the typeshed initiative has started to build external collections of type annotations for commonly used libraries.when adding type annotations to a codebase it is best if you can achieve near coverage otherwise uncertainty propagates out from everywhere the untyped code is called a codebase using tf would likely struggle to gain much benefit from type-checking in any of the core code built on top of tf benefits of adding type annotations the expected inputs and outputs of functions become much clearer code completion is able to provide more useful suggestions boosting productivity by reducing amount of time spent referring to docs static analysis can uncover latent bugs case study here difficulties/drawbacks people may be encouraged to overly constrain types removing some of the flexibility of a dynamic language but given that googles python style-guide discourages power features i would argue that striving towards code that is explicit is a similar philosophy the protobuf compiler would need to be augmented to generate type annotations the tensorflow python codebase is huge so at this point adding the annotations would be a huge undertaking tensorflow still supports python and which do not have the type annotation syntax so if this were implemented it would probably have to be in external pyi files which is harder to maintain compared to inline type annotations in the source code final thoughtsi realise that this would be a major undertaking and wouldnt be likely to ship any time soon but im curious to gauge googles thoughts on this new feature in python im about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations i probably still will give it a shot but i suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited
250792885,12340,https://api.github.com/repos/tensorflow/tensorflow/issues/12340,3rd3,1,0,0,0,0,0,it would be nice if once could use tf.multinomial with arbitrary tensors instead of just rank ones currently this is only possible by pretending the extends along the other dimensions make for more examples in the batch i.e tf.reshape(tf.multinomial(tf.reshape(x none num_classes num_samples x.get_shape().as_list
250638165,12330,https://api.github.com/repos/tensorflow/tensorflow/issues/12330,iramusa,2,0,0,0,0,0,there currently exists tf.hessians function which returns nd order y derivatives w.r.t to a specified tensor x currently x has to be one dimensional.in my application i need to differentiate with respect to a tensor that has dimensions batch_size x layer_width i am interested in second derivative of output from neural net with respect to a particular layer i can bring dimensionality down to x layer_width by setting batch_size to unfortunately this still does not let me use tf.hessian.would it be possible to add functionality where hessian w.r.t effectively one-dimensional tensors can be computed so that for example tensors of shape x n or x x n can be used it feels like it shouldnt be a hard problem but im completely lost when i look at tf.gradients code.thanks
250561550,12321,https://api.github.com/repos/tensorflow/tensorflow/issues/12321,yangjunpro,2,0,0,0,0,0,we know that tf follows the define-and-run paradigm while some other dl frameworks support define-by-run paradigm such as pytorch and chainer define-and-run actually reduce the mind-set overhead for dl modeling guys due to that it can help reduce the try-and-feedback loop.mxnet(another originally define-and-run dl framework already adds support for dynamic graph through gluon far as i know there are tensorflow fold project which provides dynamic batching support but it doesnt provides the define-by-run support.is there any plan for tfer to add the dynamic graph support in the near future?if not my team is going to work on this stuff before jumping in i just want to make sure our energies are invested on unique features rather than duplicated ones thanks
250526181,12316,https://api.github.com/repos/tensorflow/tensorflow/issues/12316,resec,3,0,0,0,0,0,this pr resolves issue added a training argument to tf.nn.rnn_cell.dropoutwrapper identical to tf.layers.dropout .also added to testcase to tf.contrib.rnn.python.kernel_tests/core_rnn_cell_test.py for this new argument
250521351,12314,https://api.github.com/repos/tensorflow/tensorflow/issues/12314,carlthome,7,0,0,0,0,0,as there are now cpu fft implementations it seems like a small extra step to add the fft ops to the xla bridge surely tf.spectral is an immensely important suite of functions for a huge number of use cases
249932437,12257,https://api.github.com/repos/tensorflow/tensorflow/issues/12257,Kongsea,0,0,0,6,0,0,add a new loss function focal_loss which was described in this paper
249895784,12252,https://api.github.com/repos/tensorflow/tensorflow/issues/12252,jmlipman,1,0,0,0,0,0,i made a very simple neural network on tf and i wanted to use mean absolute error loss function however i got this error right after i created the optimizer:no gradients provided for any variable check your graph for ops that do not support gradients between variables and loss tensor(...)this is what i did: cost tf.metrics.mean_absolute_error(pred y) optimizer tf.train.adadeltaoptimizer(learning_rate=learning_rate).minimize(cost) i tried another loss function and it worked in fact i read that its difficult to provide a gradient when the absolute value is involved but i did exactly the same in keras and it works in fact i also did the following which is basically the mean absolute error and it works as well! cost tf.reduce_mean(tf.abs(tf.subtract(pred y)))optimizer tf.train.adadeltaoptimizer(learning_rate=learning_rate).minimize(cost) why the function doesnt work?jm
249871785,12249,https://api.github.com/repos/tensorflow/tensorflow/issues/12249,ngc92,4,0,0,0,0,0,minimal example: import tensorflow as tfdef model_fn(features dict labels tf.tensor mode str passestimator tf.estimator.estimator(model_fn) results in file usr/local/lib/python./dist-packages/tensorflow/python/estimator/estimator.py line in init verify_model_fn_args(model_fn params file usr/local/lib/python./dist-packages/tensorflow/python/estimator/estimator.py line in verify_model_fn_args args set(_model_fn_args(model_fn file usr/local/lib/python./dist-packages/tensorflow/python/estimator/estimator.py line in model_fn_args return tuple(tf_inspect.getargspec(fn).args file usr/local/lib/python./dist-packages/tensorflow/python/util/tf_inspect.py line in getargspec if d.decorator_argspec is not none inspect.getargspec(target file usr/lib/python./inspect.py line in getargspec raise valueerror(function has keyword-only arguments or annotationsvalueerror function has keyword-only arguments or annotations use getfullargspec api which can support them system information os platform and distribution linux mint tensorflow version v..--gfd python version
249854777,12246,https://api.github.com/repos/tensorflow/tensorflow/issues/12246,nolanliou,3,0,0,0,0,0,related to a single machine profiling the embedding_lookup_sparse with tfprof the result shows that gather op takes a lot of time i checked the code and found that the cpu version gather op is single-thread then i modify the gather op to multi-threads the result shows about x speedup profiling result node name requested bytes total execution time accelerator execution t ime cpu execution time old gather gather mb ms us ms multi-threads gather gather mb ms us ms
249542439,12202,https://api.github.com/repos/tensorflow/tensorflow/issues/12202,elbaro,1,0,0,0,0,0,versionv..-rc--gfd problemthe keras built-in models in tf.contrib.keras.applications cannot be used as a subgraph in tf example causecalling keras.applications.inceptionv(weights=imagenet)(input_tensor is supposed to load pre-trained weights only for related variables but it initializes the entire tf graph
249505359,12193,https://api.github.com/repos/tensorflow/tensorflow/issues/12193,mayuresh51,1,0,0,0,0,0,build started project tf_core_framework configuration debug x generating force_rebuild generating e:/aimldl/tensorflow/tensorflow/tensorflow/core/util/version_info.cc the system cannot find the path specified.>c:\program files x)\msbuild\microsoft.cpp\v.\v\microsoft.cppcommon.targets error msb cmd.exe exited with code when i try to build the tf_label_image_example project tf_core_framework errors out with code please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
248958171,12132,https://api.github.com/repos/tensorflow/tensorflow/issues/12132,BKZero,4,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow custom yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary from pip tensorflow version use command below v..--gcdfc python version python bazel version if compiling from source cuda/cudnn version cpu version no cuda gpu model and memory cpu version no cuda exact command to reproduce describe the problemthe depthwise+pointwise structure is faster than the traditional convolution layer theoretically but the implemetation of tensorflow make it slower it doesnt make sense.here is part of my network defination:#net slim.convd(net scope=conv end_points conv net net slim.separable_convd(net,none, , ,depth_multiplier=,stride=,rate=,normalizer_fn=slim.batch_norm,scope=conv--depthwise end_points conv--depthwise net net slim.convd(net depth stride normalizer_fn=slim.batch_norm scope=conv--pointwise end_points conv--pointwise net net slim.max_poold(net scope=pool end_points pool net net slim.convd(net padding=valid scope=conv end_points conv net net slim.separable_convd(net,none, , ,depth_multiplier=,stride=,rate=,normalizer_fn=slim.batch_norm,scope=conv-depthwise end_points conv-depthwise net net slim.convd(net depth stride normalizer_fn=slim.batch_norm scope=conv-pointwise end_points conv-pointwise net net slim.max_poold(net scope=pool end_points pool net i just change the network defination from net slim.convd(net scope=conv-)end_points conv netto:net slim.separable_convd(net,none, , ,depth_multiplier=,stride rate=,normalizer_fn=slim.batch_norm,scope=conv--depthwise)end_points conv--depthwise netnet slim.convd(net depth stride normalizer_fn=slim.batch_norm scope=conv--pointwise)end_points conv--pointwise neti do not think i am doing something wrong so where the problem is"
248804144,12115,https://api.github.com/repos/tensorflow/tensorflow/issues/12115,izaid,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos sierra tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version na gpu model and memory na exact command to reproduce tfcompile currently the xla compiler seems to not support complex dtypes i ran into this when trying to use the spectral ops ffd in an aot compilation is it possible to fix this alternatively is there a workaround that succeeds now i thought i could maybe bitcast a float to a complex but it seems bitcast is also not implemented for xla
248329188,12071,https://api.github.com/repos/tensorflow/tensorflow/issues/12071,oduerr,5,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes see below os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary tensorflow version use command below v..--gcdfc python version bazel version if compiling from source cuda/cudnn version on cpu gpu model and memory exact command to reproduce tf.norm at see below for code import numpy as npimport tensorflow as tfprint(tf.git_version tf.version v..--gcdfc x tf.placeholder(tf.float shape=(,none))z tf.norm(x ord=euclidean axis name=logit)var_grad tf.gradients(z x )with tf.session as sess x np.array grad ok grad nan e grad ok e grad inf dtype=np.float sess.run(tf.global_variables_initializer print(sess.run((z var_grad feed_dict={x x result array( .e e e e dtype=float array nan inf dtype=float describe the problem nan is calculated for the gradient of tf.norm at zero values for extremely small values inf is calculated note that the exact result should be in all cases above above is a minimal example to reproduce it the problem occurred in a real world scenario when implementing a custom loss function the entropy in and two embeddings where too close to each other distance practically source code logssee above output of logfile cat etc/issue darwin olivers-mbp-.fritz.box darwin kernel version tue apr pdt root:xnu-....~/release_x x_mac os x are we in docker echo are we in docker num echo are we in docker ec echo are we in docker c version uname a darwin olivers-mbp-.fritz.box darwin kernel version tue apr pdt root:xnu-....~/release_x x check pips numpy protobuf tensorflow check for virtualenv echo check for virtualenv on_b echo check fo sys echo check for virtualenv echo check for virtualenv cat etc/issue darwin olivers-mbp-.fritz.box darwin kernel version tue apr pdt root:xnu-....~/release_x x_mac os x are we in docker no compiler apple llvm version clang-..)target x_-apple-darwin..thread model posixinstalleddir applications/xcode.app/contents/developer/toolchains/xcodedefault.xctoolchain/usr/bin uname a darwin olivers-mbp-.fritz.box darwin kernel version tue apr pdt root:xnu-....~/release_x x check pips numpy protobuf tensorflow check for virtualenv true tensorflow import tf.version tf.git_version v..--gcdfctf.compiler_version v..--gcdfcsanity check array dtype=int env ld_library_path is unsetdyld_library_path is unset nvidia-smi tf_env_collect.sh.txt line nvidia-smi command not found cuda libs"
248316912,12069,https://api.github.com/repos/tensorflow/tensorflow/issues/12069,ppwwyyxx,0,0,0,1,0,0,values is not defined
248249456,12060,https://api.github.com/repos/tensorflow/tensorflow/issues/12060,ruchirdhar,1,0,0,0,0,0,hi i installed tensorflow for cpu and while trying to run command to check the version of tensor i am prompted error message please help me to get this resolvedplease see below the log:microsoft windows version c microsoft corporation all rights reserved.c:\windows\system>pythonpython v..:defaa jun msc v bit amd on wintype help copyright credits or license for more information import tensorflow as tftraceback most recent call last file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(mname file c:\program files\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\program files\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow file c:\program files\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflowduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\program files\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\program files\python\lib\site-packages\tensorflow\python\__init__.py line in module raise importerror(msg)importerror traceback most recent call last file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(mname file c:\program files\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\program files\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file c:\program files\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow file c:\program files\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflowfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help
248247030,12059,https://api.github.com/repos/tensorflow/tensorflow/issues/12059,snu-ceyda,2,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below v..--gcdfc python version python bazel version if compiling from source cuda/cudnn version gpu model and memory nvidia titan x pascal g exact command to reproduce python sparse_debug.py debug describe the problemthere seems to be a bug using tensorflow debugger with sparse tensors below is just a simple example it fails when run with or without the debug option it works when localclidebugwrappersession line is removed this prevents the use of the debugger while using sparse_placeholders unless im missing something. this issue also reports the same error but isnt related to tfdbg source code logssparse_debug.py import tensorflow as tffrom tensorflow.python import debug as tf_debuga=tf.sparse_placeholder(tf.float,shape=(none,,),name=tensor)b=tf.sparse_placeholder(tf.float,shape=(none,,),name=tensor)add=tf.sparse_add(a,b)sess tf.session()sess tf_debug.localclidebugwrappersession(sess)a_val=( ,, , ,, , , ,(,,))b_val=( ,, , ,, , , ,(,,))res=sess.run(add,feed_dict={a:a_val,b:b_val}) traceback file sparse_debug.py line in module res=sess.run(add,feed_dict={a:a_val,b:b_val file usr/local/lib/python./dist-packages/tensorflow/python/debug/wrappers/framework.py line in run self._run_call_count file usr/local/lib/python./dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py line in on_run_start request.feed_dict file usr/local/lib/python./dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py line in update_run_calls_state self._tensor_filters file usr/local/lib/python./dist-packages/tensorflow/python/debug/cli/cli_shared.py line in get_run_start_intro feed_dict_lines.append(feed_key.name)attributeerror sparsetensor object has no attribute name"
248202967,12056,https://api.github.com/repos/tensorflow/tensorflow/issues/12056,betterenvi,6,0,1,2,0,0,currently tensorflow doesnt support crf decoding viterbi decoding for tensor although tf.contrib.crf.viterbi_decode function can do crf decoding it can only be used at test time since it accepts numpy arrays as inputs.implementing crf decoding for tensor benefits us a lot as this makes our model more portable e.g we can freeze model save it to a pb file and then load it in golang at test time).this pr implements crf decoding for tensor and adds test code for it this prs change is compatible with current api
248135560,12052,https://api.github.com/repos/tensorflow/tensorflow/issues/12052,tpankaj,73,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows server tensorflow installed from source or binary binary tensorflow version use command below rc python version bazel version if compiling from source n/a cuda/cudnn version cuda v cudnn gpu model and memory nvidia geforce gtx ti gb exact command to reproduce n/a describe the problemplease upgrade tensorflow to support cuda and cudnn nvidia claims this will provide a x performance boost on pascal gpus
248134981,12051,https://api.github.com/repos/tensorflow/tensorflow/issues/12051,ppwwyyxx,1,0,0,0,0,0,this is a feature request.tf pip packages might be built with different c abi the released binaries are built with old abi if a user manually compile it with gcc the default is to use new cxx abi unless explicitly changed).as someone who wrote custom ops this could cause trouble the op has to be compiled with the same abi otherwise there will be issues like therefore the user of my ops would need to be aware of what abi hes using and change the flags manually.i hope there is an api simply tells what abi should be used when compiling user ops similar to tf.sysconfig.get_include which tells what path to include
248041077,12040,https://api.github.com/repos/tensorflow/tensorflow/issues/12040,sdschulze,1,0,0,0,0,0,consider the following silly autoencoder-style network which performs a strided convolution followed by a transposed convolution: import numpy as npimport tensorflow as tfn m k w tf.variable(tf.truncated_normal k stddev=e dtype=tf.float))b tf.variable(tf.truncated_normal stddev=e dtype=tf.float))w tf.variable(tf.truncated_normal k stddev=e dtype=tf.float))b tf.variable(tf.truncated_normal stddev=e dtype=tf.float))x tf.placeholder(tf.float n m x tf.nn.tanh(tf.nn.convd(x w same bx tf.nn.convd_transpose(x w n m same bloss tf.nn.l_loss(x x_)train_step tf.train.adamoptimizer(e-).minimize(loss)check_step tf.add_check_numerics_ops()sess tf.interactivesession()sess.run(tf.global_variables_initializer())for i in range loss_val sess.run( train_step check_step loss feed_dict={x np.random.randn(n m print(iteration loss format(i loss_val)) on the cpu this runs fine using cuda with k though i reproducibly get the following error: name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id e tensorflow/core/kernels/check_numerics_op.cc abnormal_detected_host xdb gradients/convd_grad/convdbackpropfilter:traceback most recent call last file mintrans.py line in module feed_dict={x np.random.randn(n m file home/sschulze/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/sschulze/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file home/sschulze/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file home/sschulze/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror gradients/convd_grad/convdbackpropfilter tensor had nan values node checknumerics checknumerics t=dt_float message=gradients/convd_grad/convdbackpropfilter device=/job:localhost/replica:/task:/gpu: (gradients/convd_grad/convdbackpropfilter checknumerics caused by op uchecknumerics defined at file mintrans.py line in module check_step tf.add_check_numerics_ops file home/sschulze/.local/lib/python./site-packages/tensorflow/python/ops/numerics.py line in add_check_numerics_ops check_op array_ops.check_numerics(output message=message file home/sschulze/.local/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in check_numerics message=message name=name file home/sschulze/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file home/sschulze/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file home/sschulze/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack invalidargumenterror see above for traceback gradients/convd_grad/convdbackpropfilter tensor had nan values node checknumerics checknumerics t=dt_float message=gradients/convd_grad/convdbackpropfilter device=/job:localhost/replica:/task:/gpu: (gradients/convd_grad/convdbackpropfilter checknumerics_) this code is the minimal example that i could reproduce the error with i am running tensorflow-gpu installed via pip on ubuntu with python version cuda is from the nvidia repo and cudnn is my gpu is an nvidia gtx ti with mib of memory.to reproduce the error run the above code with cuda enabled.my suspicion is that when the kernel is too large with respect to the input or the output of the strided convolution a bug is triggered i am not entirely sure how the size of the kernel must relate to the other convolution parameters but i could produce errors both in the convd and the convd_transpose op
247899951,12027,https://api.github.com/repos/tensorflow/tensorflow/issues/12027,yongtang,1,0,0,1,0,0,this fix fixes the issue raised in where condition_variable_any has to be used in c++.this fix defines conditionvariableformutex as std::condition_variable_any in c and std::condition_variable otherwise.the fix is verified with bazel build s config=opt cxxopt=-std=c this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
247849528,12022,https://api.github.com/repos/tensorflow/tensorflow/issues/12022,justinshapiro,1,0,0,0,0,0,problemi am trying to host a tensorforestestimator model on google clouds ml engine everything works right but at the very end the model fails to export with stack trace: traceback most recent call last): ... file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/experiment.py line in train_and_evaluate export_results self._maybe_export(eval_result)file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/experiment.py line in maybe_export eval_result=eval_result))file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py line in export return self.export_fn(estimator export_path kwargs)file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py line in export_fn checkpoint_path=checkpoint_path)file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in export_savedmodel actual_default_output_alternative_key)file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py line in build_all_signature_defs for input_key inputs in input_alternatives.items()file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py line in dictcomp in output_alternatives.items()}file root/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py line in build_standardized_signature_def input_tensors output_tensors)file root/.local/lib/python./site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py line in predict_signature_def signature_constants.predict_method_name)file root/.local/lib/python./site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py line in build_signature_def signature_def.outputs item .copyfrom(outputs item )typeerror none has type nonetype but expected one of bytes unicode based on that trace im thinking the error is in the make_export_strategy function with default_output_alternative_key=none so what i did is set default_output_alternative_key=default but then got the error: valueerror requested default_output_alternative default but available output_alternatives are none so this shows that there are no output alternatives and my model is single-headed here is the code: def serving_input_fn feature_placeholders column name tf.placeholder(dtype=column dtype shape= none for column in columns_list if column derived n and column column_role label features key tf.expand_dims(tensor for key tensor in feature_placeholders.items return inputfnops features=features labels=none default_inputs=feature_placeholders def get_experiment_fn(args def experiment(run_config hparams return experiment estimator=tensorforestestimator params=foresthparams num_trees=args.num_trees max_nodes min_split_samples num_features num_classes=args.num_projections regression=true model_dir=args.job_dir graph_builder_class=randomforestgraphs config=run_config report_feature_importances=true train_input_fn=get_input_fn project_name=args.project data_location=args.train_data dataset_size=args.train_size batch_size=args.train_batch_size train_steps=args.train_steps eval_input_fn=get_input_fn project_name=args.project data_location=args.eval_data dataset_size=args.eval_size batch_size=args.eval_batch_size eval_steps=args.eval_steps eval_metrics=get_eval_metrics export_strategies make_export_strategy serving_input_fn default_output_alternative_key=none exports_to_keep return experimentdef main args get_arg_parser().parse_args learn_runner.run experiment_fn=get_experiment_fn(args run_config=runconfig(model_dir=args.job_dir hparams=hparams( args.__dict if name main main() this seems like a bug but i could be wrong system information cat etc/issue darwin mbmagenic darwin kernel version fri mar pst root:xnu-..~/release_x x_mac os x are we in docker no compiler apple llvm version clang-..)target x_-apple-darwin..thread model posixinstalleddir library/developer/commandlinetools/usr/bin uname a darwin mbmagenic darwin kernel version fri mar pst root:xnu-..~/release_x x check pips numpy protobuf post)tensorflow check for virtualenv false tensorflow import tf.version tf.git_version v..--gcdfctf.compiler_version v..--gcdfcsanity check array dtype=int env ld_library_path is unsetdyld_library_path is unset nvidia-smi tf.sh line nvidia-smi command not found cuda libs
247814686,12017,https://api.github.com/repos/tensorflow/tensorflow/issues/12017,sayrer,1,0,0,1,0,0,please go to stack overflow for help and support system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu mac os x with clang/llvm tensorflow installed from source or binary) :source tensorflow version use command below python version bazel version if compiling from source describe the problemtheres an ifdef in mutex.h that uses shared_timed_mutex when compiled as c and up the file doesnt compile because c requires using condition_variable_any rather than condition_variable with that kind of mutex source code logs in file included from external/org_tensorflow/tensorflow/stream_executor/platform/mutex.h::external/org_tensorflow/tensorflow/stream_executor/platform/default/mutex.h error no matching member function for call to wait_for std::cv_status s cv->wait_for(*mu std::chrono::milliseconds(ms external/org_chromium_clang_mac/include/c++/v/__mutex_base note candidate function not viable no known conversion from perftools::gputools::mutex_lock to unique_lock
247701839,12001,https://api.github.com/repos/tensorflow/tensorflow/issues/12001,andreas-eberle,4,0,0,0,0,0,some recent papers e.g have shown that transposed separable convolutions can be a great choice for decoders in encoder decoder architectures.can you add a seperable_convd_transpose operation comparable to the convd_transpose operation
247671040,12000,https://api.github.com/repos/tensorflow/tensorflow/issues/12000,rasitsimsek,4,0,0,0,0,0,if you build a debug version of the current tensorflow version on windows with cmake following error is occurred:(clcompile target c:\program files x)\microsoft visual studio\\professional\vc\tools\msvc\..\include\xutility error c binary no operator found whichtakes a left-hand operand of type tensorflow::boosted_trees::utils:: anonymous-namespace::indicesrowiterator or there is no acceptable conversion compiling source file c:\development\dev\test_projects\deeplearning\tensorflow\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc c:\development\dev\test_projects\deeplearning\tensorflow_build\tf_core_kernels.vcxproj the reason is indicesrowiterator::operator is missing.adding these lines bool operator const indicesrowiterator other const qcheck_lt iter other.iter return row_idx other.row_idx solves the problem.###system informationtensorflow windows visualstudio cmake
247587203,11988,https://api.github.com/repos/tensorflow/tensorflow/issues/11988,jingxil,3,0,0,0,0,0,"system information tensorflow version v..-rc--gfd python describe the problemthe return of zero_state function of tensorflow.contrib.seqseq.attentionwrapper is not compatible with tensorflow.nn.dynamic_rnn function it seems that if output.shape.ndims in copy_one_through function does not work once those scalar elements in return tuple are modified to have the shape like batch_size the error disappears code to reproduce the bug import tensorflow as tfbatch_size=hidden_units attention_size max_len inputs tf.constant(.,shape=(batch_size,max_len,hidden_units))context tf.constant(.,shape=(batch_size,max_len,hidden_units))input_lens cell tf.contrib.rnn.lstmcell(hidden_units)am tf.contrib.seqseq.bahdanauattention(attention_size context)wrapper tf.contrib.seqseq.attentionwrapper(cell,am)tf.nn.dynamic_rnn(wrapper,inputs,input_lens,dtype=tf.float log traceback most recent call last file test.py line in module tf.nn.dynamic_rnn(wrapper,inputs,input_lens,dtype=tf.float file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in dynamic_rnn dtype=dtype file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in dynamic_rnn_loop swap_memory=swap_memory file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in while_loop result context.buildloop(cond body loop_vars shape_invariants file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop pred body original_loop_vars loop_vars shape_invariants file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop body_result body(*packed_vars_for_body file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in time_step skip_conditionals=true file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in rnn_step final_output_and_state copy_some_through(new_output new_state file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in copy_some_through for state new_state in zip(flat_state flat_new_state file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in listcomp for state new_state in zip(flat_state flat_new_state file usr/local/lib/python./dist-packages/tensorflow/python/ops/rnn.py line in copy_one_through return array_ops.where(copy_cond output new_output file usr/local/lib/python./dist-packages/tensorflow/python/ops/array_ops.py line in where return gen_math_ops._select(condition=condition t=x e=y name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_math_ops.py line in select name=name file usr/local/lib/python./dist-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op set_shapes_for_outputs(ret file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in set_shapes_for_outputs shapes shape_func(op file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in call_with_requiring return call_cpp_shape_fn(op require_shape_fn=true file usr/local/lib/python./dist-packages/tensorflow/python/framework/common_shapes.py line in call_cpp_shape_fn debug_python_shape_fn require_shape_fn file usr/local/lib/python./dist-packages/tensorflow/python/framework/common_shapes.py line in call_cpp_shape_fn_impl raise valueerror(err.message)valueerror shapes must be equal rank but are and for rnn/while/select op select with input shapes"
247244637,11956,https://api.github.com/repos/tensorflow/tensorflow/issues/11956,atilaorh,3,0,0,0,0,0,there is a surging interest in geometric computer vision and a large number of recent papers leveraging an operation(with small variations dubbed correlation layer there is a cuda kernel for this operation in the flownet papers authors fork of caffe is there plan on including it in tensorflow i couldnt locate a relevant issue anywhere and this is why i am raising this issue
247241361,11954,https://api.github.com/repos/tensorflow/tensorflow/issues/11954,haraldurt,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes os platform and distribution e.g linux ubuntu ubuntu lts tensorflow installed from source or binary) :source tensorflow version use command below) :(unknown rc python version exact command to reproduce : input_tensor tf.placeholder(tf.float none dilated tf.nn.convolution(input_tensor tf.zeros dilation_rate padding=same)print(dilated.get_shape displays expected describe the problemthe documentation for tf.nn.convolution has the spatial dimensions of the output given as: if padding same output_spatial_shape i ceil(input_spatial_shape i strides i ) which suggests that input_spatial_shape should not affect output_spatial_shape as is the case in the code block above.this problem arises when using dilated convolutions as part of a larger model containing recurrent layers in which one spatial dimension is left undefined to allow for unrolling the recurrent layers out during training along the undefined dimension.this might be related to a previously fixed problem with undefined batch sizes
246870750,11919,https://api.github.com/repos/tensorflow/tensorflow/issues/11919,jart,1,0,0,2,0,0,avramit i recommend this cherry pick of cl which makes import tensorflow go much faster x faster on my machine i would also request that it be mentioned in the release notes
246637161,11898,https://api.github.com/repos/tensorflow/tensorflow/issues/11898,korrawat,1,0,0,0,0,0,another fix to quantization tutorial after currently following the steps in the quantization tutorial will lead to weird results as pointed out in this is because the graph file and the evaluation script are unmatched.from this image recognition tutorial there are two pretrained graphs with two evaluation scripts classify_image_graph_def.pb in inception---.tgz is to be used with the older classify_image.py inception_v____frozen.pb in inception_v____frozen.pb.tar.gz is to be used with label_image weird results came from using classify_image_graph_def.pb with label_image the graphs are different see also models especially in the last layer the two scripts extract information from the last layer in different ways so it works with its corresponding graph but not the other further changes in the image recognition tutorial source the usage of python api points to option above but the c api points to option this might cause confusion as the two graphs/scripts are not equivalent now that theres a python implementation of the newer label_image by freedomtan should we update the tutorial to use this python implementation instead should this graph transform readme be updated as well currently it uses the pre-trained graph hence inputs=mul outputs=softmax if it is updated to the graph these should instead say inputs=input outputs=inceptionv/predictions/reshape
246517411,11868,https://api.github.com/repos/tensorflow/tensorflow/issues/11868,jjallaire,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no using example from keras documentation here os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary cpu version tensorflow version use command below v..--gcdfc python version os x system version bazel version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce : pythonfrom tensorflow.contrib.keras.python.keras.applications.resnet import resnetfrom tensorflow.contrib.keras.python.keras.preprocessing import imagefrom tensorflow.contrib.keras.python.keras.applications.resnet import preprocess_input decode_predictionsimport numpy as npmodel resnet(weights=imagenet)img_path elephant.jpgimg image.load_img(img_path target_size x image.img_to_array(img)x np.expand_dims(x axis=)x preprocess_input(x)preds model.predict(x)print(predicted decode_predictions(preds top describe the problemthe above code yields the following output: python(predicted un uwest_highland_white_terrier un utoilet_tissue un usea_urchin however the same code run using stand-alone keras yields this: python(predicted un uindian_elephant un utusker un uafrican_elephant note to reproduce under stand-alone keras substitute this code for the imports at the top: pythonfrom keras.applications.resnet import resnetfrom keras.preprocessing import imagefrom keras.applications.resnet import preprocess_input decode_predictions also note that you need the elephant.jpg file in the working directory to reproduce you can find that file here
246472575,11859,https://api.github.com/repos/tensorflow/tensorflow/issues/11859,alex-taffe,10,0,0,0,0,0,please go to stack overflow for help and support system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu linux ubuntu server ltslinux poweredge-r generic ubuntu smp tue jul utc x x x gnu/linux tensorflow installed from source or binary) :source tensorflow version use command below) :master r r python version bazel version if compiling from source) :build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri jul build timestamp build timestamp as int cuda/cudnn version :cuda cudnn gpu model and memory :gtx gb exact command to reproduce : ./configure;bazel build c opt config=cuda tensorflow/tools/pip_package:build_pip_package - premade script : alex@poweredge-r sh tf_env_collect.sh collecting system information...tf_env_collect.sh linux unexpected operatortf_env_collect.sh linux unexpected operatortraceback most recent call last file tmp/check_tf.py line in module import tensorflow as tf;importerror no module named tensorflowwrote environment to tf_env.txt you can review the contents of that file.and use it to populate the fields in the github issue template.cat tf_env.txtalex@poweredge-r cat tf_env.txt cat etc/issue linux poweredge-r generic ubuntu smp tue jul utc x x x gnu/linux are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux poweredge-r generic ubuntu smp tue jul utc x x x gnu/linux check pips numpy check for virtualenv false tensorflow import traceback most recent call last file string line in module>importerror no module named tensorflow env ld_library_path usr/local/cuda-./libdyld_library_path is unset nvidia-smi fri jul nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx off e off n/a c p w w mib mib default processes gpu memory gpu pid type process name usage no running processes found cuda libs usr/local/cuda-./lib/libcudart_static.a/usr/local/cuda-./lib/libcudart.so.../usr/local/cuda-./doc/man/man/libcudart./usr/local/cuda-./doc/man/man/libcudart.so describe the problemi am trying to compile tensorflow from source when i run the above command the bazel build fails this is the configuration i used as well as the error it seems like some other people had this issue but the threads were for much older versions and none of the solutions fixed it please specify the location of python default is usr/bin/python found possible python library paths:/usr/local/lib/python./dist-packages/usr/lib/python./dist-packagesplease input the desired python library path to use default is usr/local/lib/python./dist-packagesdo you wish to build tensorflow with jemalloc as malloc support y/n jemalloc as malloc support will be enabled for tensorflow.do you wish to build tensorflow with google cloud platform support y/n ygoogle cloud platform support will be enabled for tensorflow.do you wish to build tensorflow with hadoop file system support y/n no hadoop file system support will be enabled for tensorflow.do you wish to build tensorflow with xla jit support y/n no xla jit support will be enabled for tensorflow.do you wish to build tensorflow with verbs support y/n no verbs support will be enabled for tensorflow.do you wish to build tensorflow with opencl support y/n yopencl support will be enabled for tensorflow.please specify which c compiler should be used as the host c compiler default is usr/bin/g please specify which c compiler should be used as the host c compiler default is usr/bin/gcc please specify the location where computecpp for sycl is installed default is usr/local/computecpp do you wish to build tensorflow with cuda support y/n ycuda support will be enabled for tensorflow.please specify the cuda sdk version you want to use e.g leave empty to default to cuda please specify the location where cuda toolkit is installed refer to readme.md for more details default is usr/local/cuda please specify the cudnn version you want to use leave empty to default to cudnn please specify the location where cudnn library is installed refer to readme.md for more details default is usr/local/cuda :please specify a list of comma-separated cuda compute capabilities you want to build with.you can find the compute capability of your device at note that each additional compute capability significantly increases your build time and binary size default is do you want to use clang as cuda compiler y/n nvcc will be used as cuda compiler.please specify which gcc should be used by nvcc as the host compiler default is usr/bin/gcc do you wish to build tensorflow with mpi support y/n no mpi support will be enabled for tensorflow.please specify optimization flags to use during compilation when bazel option config=opt is specified default is march=native add config=mkl to your bazel command to build with mkl support.please note that mkl on macos or windows is still not supported.if you would like to use a local mkl instead of downloading please set the environment variable tf_mkl_root every time before build.configuration finished...............error skipping tensorflow/tools/pip_package:build_pip_package error loading package tensorflow/tools/pip_package encountered error while reading extension file cuda/build_defs.bzl no such package local_config_cuda//cuda traceback most recent call last file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line create_local_cuda_repository(repository_ctx file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line in create_local_cuda_repository host_compiler_includes(repository_ctx cc file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line in host_compiler_includes get_cxx_inc_directories(repository_ctx cc file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line in get_cxx_inc_directories set(includes_cpp)depsets cannot contain mutable itemswarning target pattern parsing failed.error error loading package tensorflow/tools/pip_package encountered error while reading extension file cuda/build_defs.bzl no such package local_config_cuda//cuda traceback most recent call last file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line create_local_cuda_repository(repository_ctx file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line in create_local_cuda_repository host_compiler_includes(repository_ctx cc file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line in host_compiler_includes get_cxx_inc_directories(repository_ctx cc file home/alex/tensorflow/third_party/gpus/cuda_configure.bzl line in get_cxx_inc_directories set(includes_cpp)depsets cannot contain mutable itemsinfo elapsed time sfailed build did not complete successfully packages loaded currently loading tensorflow/tools/pip_package
246189821,11831,https://api.github.com/repos/tensorflow/tensorflow/issues/11831,yhu9,2,0,0,0,0,0,on the tutorial of creating estimators using tf.contrib.learn there doesnt seem to be any mention of how to create your own runconfig object in order to specify the configurations for an estimator run the configuration in particular i wanted to find was on how to write summaries after custom sized steps i eventually found it in the runconfig description but i think it would be worthwhile to mention it in the tutorial.link to the tutorial to the runconfig description was wondering if the tutorial could be updated to show how to create your own runconfig object and use it with the estimator
246189479,11830,https://api.github.com/repos/tensorflow/tensorflow/issues/11830,patrikerdes,7,0,0,0,0,0,as described in a lot of time is spent doing inspect.stack when importing import tensorflow.contrib with python there does not seem to be any issue with python by replacing inspect.stack with only getting the current and previous frame and then getting the frame info for just the previous frame this unnecessary overhead is removed.the time to import tensorflow.contrib as reported by time python c import tensorflow.contrib before and after my commit:when|python python before|.|.|after|.|.|there are unit tests that would catch if the decorator_name would be incorrect after my change specifically these tests testsetsdecoratornametofunctionthatcallsmakedecoratorifabsent testunwrapreturnsdecoratorlistfromoutermosttoinnermost testunwrapboundmethodsall python unit tests passed in docker i didnt find any issues when linting the changed file
246186523,11829,https://api.github.com/repos/tensorflow/tensorflow/issues/11829,patrikerdes,7,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos sierra tensorflow installed from source or binary source tensorflow version use command below v..--gbafa rc python version bazel version if compiling from source homebrew cuda/cudnn version cpu only build gpu model and memory cpu only build exact command to reproduce time python c import tensorflow.contrib the problemdoing import tensorflow.contrib take seconds on my machine when doing it with python with python it takes seconds.investigating this revealed that a lot of time is spent in inspect.stack in the function make_decorator in python/util/tf_decorator.py the stack is inspected to find the name of the caller of the function with python inspect.stack is fast but with python each call to inspect.stack take approximately seconds and there are calls made which account for the difference in time between python and referenceskeras by default imports tensorflow.contrib when the tensorflow backend is used therefore keras is slow to import when using python is a stackoverflow question referencing this issue
246129853,11824,https://api.github.com/repos/tensorflow/tensorflow/issues/11824,tiagofrepereira2012,0,0,0,0,0,2,"hey,i just wanted to activate this pr ported maxout layer from improved documentation created test unitscould someone please review this one?thanks"
246082557,11821,https://api.github.com/repos/tensorflow/tensorflow/issues/11821,st--,3,0,0,0,0,0,"as also noted in already but that ticket had been closed there is a bug in the implementation of self_adjoint_eigvals whereas self_adjoint_eig works fine but it is rather inefficient to compute eigenvectors and their gradients if i only require the eigenvalues system information have i written custom code as opposed to using a stock example script provided in tensorflow no stock usage of tf.self_adjoint_eigvals minimal failing example below os platform and distribution linux ubuntu tensorflow installed from source tensorflow version v..--gccb python version bazel version if compiling from source binary install cuda/cudnn version gpu model and memory geforce gtx with gb memory exact command to reproduce see below minimal failing example: pythona tf.random_normal((,))covar tf.matmul(tf.transpose(a a)eigvals eigvect_holder tf.self_adjoint_eig(covar)eigvects tf.transpose(eigvect_holder)pure_eigvals tf.self_adjoint_eigvals(covar)tf.session().run(tf.gradients(eigvals a works finetf.session().run(tf.gradients(pure_eigvals a throws the error below error message: python---------------------------------------------------------------------------valueerror traceback most recent call last)/home/stj/pio/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py in maybecompile(scope op func grad_fn try xla_compile op.get_attr(_xlacompile xla_separate_compiled_gradients op.get_attr(/home/stj/pio/lib/python./site-packages/tensorflow/python/framework/ops.py in get_attr(self name raise valueerror(no attr named name in str(self._node_def x self._node_def.attr name valueerror no attr named xlacompile in name selfadjointeigv_op selfadjointeigvinput matmul_attr key t value type dt_float attr key compute_v value b false during handling of the above exception another exception occurred:invalidargumenterror traceback most recent call last)/home/stj/pio/lib/python./site-packages/tensorflow/python/framework/common_shapes.py in call_cpp_shape_fn_impl(op input_tensors_needed input_tensors_as_shapes_needed debug_python_shape_fn require_shape_fn graph_def_version node_def_str input_shapes input_tensors input_tensors_as_shapes status except errors.invalidargumenterror as err:/usr/lib/python./contextlib.py in exit__(self type value traceback try next(self.gen except stopiteration:/home/stj/pio/lib/python./site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status compat.as_text(pywrap_tensorflow.tf_message(status pywrap_tensorflow.tf_getcode(status finally:invalidargumenterror shape must be rank but is rank for gradients_/selfadjointeigv__grad/matmul op matmul with input shapes during handling of the above exception another exception occurred:valueerror traceback most recent call last)"
245990435,11812,https://api.github.com/repos/tensorflow/tensorflow/issues/11812,paolof89,13,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu distributor id ubuntudescription ubuntu ltsrelease tensorflow installed from source or binary) :pip install tensorflow-gpu tensorflow version use command below) :v..--gcdfc python version cuda/cudnn version :nvcc nvidia r cuda compiler drivercopyright c nvidia corporationbuilt on tue_jan__::_cst_cuda compilation tools release v gpu model and memory description d controller product gkgl tesla k vendor nvidia corporation physical id bus info pci@ba version a width bits clock mhz capabilities bus_master cap_list configuration driver=nvidia latency resources iomemory:-ff iomemory:-f irq memory:-ffffff memory:-ffffffff memory:-ffffff code example :estimator kerasregressor(build_fn=self.create_model_function input_dim=self.input_dim output_dim=self.output_dim self.model_parameters)param_grid epochs batch_size neurons dropout grid gridsearchcv(estimator=estimator param_grid=param_grid n_jobs describe the problemthe internalerror occurred when i fit a sklearn.gridsearchcv object.the error occurred only if i use gpu and i i use gridsearch object it works fine on cpu and on single model fitting using keras wrapper error logfile home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/model_selection/_search.py line in fit return self._fit(x y groups parametergrid(self.param_grid file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/model_selection/_search.py line in fit for parameters in parameter_iterable file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/parallel.py line in call n_jobs self._initialize_backend file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/parallel.py line in initialize_backend self._backend_args file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/_parallel_backends.py line in configure self._pool memmapingpool(n_jobs backend_args file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/pool.py line in init super(memmapingpool self).__init__( poolargs file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/pool.py line in init super(picklingpool self).__init__( poolargs file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/pool.py line in init self._repopulate_pool file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/pool.py line in repopulate_pool w.start file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/process.py line in start self._popen self._popen(self file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/context.py line in popen return popen(process_obj file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/popen_fork.py line in init self._launch(process_obj file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/popen_fork.py line in launch code process_obj._bootstrap file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/process.py line in bootstrap self.run file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/process.py line in run self._target(*self._args self._kwargs file home/aateam/.conda/envs/amplifon-dev/lib/python./multiprocessing/pool.py line in worker result true func(*args kwds file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/_parallel_backends.py line in call return self.func(*args kwargs file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/parallel.py line in call return func(*args kwargs for func args kwargs in self.items file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/externals/joblib/parallel.py line in listcomp return func(*args kwargs for func args kwargs in self.items file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/sklearn/model_selection/_validation.py line in fit_and_score estimator.fit(x_train y_train fit_params file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/keras/wrappers/scikit_learn.py line in fit self.model self.build_fn( self.filter_sk_params(self.build_fn file home/aateam/amplifon/amplifon-adv-planning/src/libs/amplifon_objects.py line in create_test_model model.add(dense(neurons input_dim=input_dim activation=last_activation file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/keras/models.py line in add layer(x file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/keras/engine/topology.py line in call output self.call(inputs kwargs file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/keras/layers/core.py line in call output k.dot(inputs self.kernel file home/aateam/.conda/envs/amplifon-dev/lib/python./site-packages/keras/backend/tensorflow_backend.py line in dot out tf.matmul(x y file home/aateam/.local/lib/python./site-packages/tensorflow/python/ops/math_ops.py line in matmul a b transpose_a=transpose_a transpose_b=transpose_b name=name file home/aateam/.local/lib/python./site-packages/tensorflow/python/ops/gen_math_ops.py line in mat_mul transpose_b=transpose_b name=name file home/aateam/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file home/aateam/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file home/aateam/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()internalerror see above for traceback blas gemm launch failed a.shape b.shape m n k node dense_/matmul matmul t=dt_float transpose_a=false transpose_b=false device=/job:localhost/replica:/task:/gpu: (_arg_dense__input dense_/kernel/read node mul recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__mul tensor_type=dt_float device=/job:localhost/replica:/task:/cpu
245736657,11786,https://api.github.com/repos/tensorflow/tensorflow/issues/11786,cheind,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary rc python version cuda/cudnn version gpu model and memory gtx exact command to reproduce :the following sample is taken from here and works in tf pythonimport tensorflow as tfimport numpy as npdef read_py_function(filename label return np.zeros labeldef resize_function(image_decoded label image_decoded.set_shape( none none none image_resized tf.image.resize_images(image_decoded return image_resized labelfilenames np.array( /var/data/image.jpg var/data/image.jpg )labels np.array dataset tf.contrib.data.dataset.from_tensor_slices((filenames labels))dataset dataset.map lambda filename label tf.py_func read_py_function filename label tf.uint label.dtype ))dataset dataset.map(_resize_function) in rc the following error is produced cannot convert a list containing a tensor of dtype dtype int to dtype uint tensor is tf.tensor pyfunc shape=) this is due to the breaking change mentioned in release notes to fix one now has to introduce an explicit tuple like so pythondataset dataset.map lambda filename label tuple(tf.py_func read_py_function filename label tf.uint label.dtype ))) this should at least be mentioned in the api docs programmer guide
245728388,11785,https://api.github.com/repos/tensorflow/tensorflow/issues/11785,reedkotler,1,0,0,0,0,0,i would like to propose that we make a separate repository in github for jupyter notebooks.i think it should be separate from the main tensorflow github so that people can check it out without having to check out all of tensorflow.ive created such a github and will work on it by myself for now but i think eventually there should be something more official or at least maybe what i have can be considered official enough to have pointers to it from the main tensorflow github
245669964,11777,https://api.github.com/repos/tensorflow/tensorflow/issues/11777,liuzqt,2,0,0,0,0,0,im running tensorflow on android and it reports a exception of one of my op ceil the exception info is as below: caused by java.lang.illegalargumentexception no opkernel was registered to support op ceil with these attrs registered devices cpu registered kernels no registered kernels> node model/frame/ceil ceil t=dt_double (model/frame/truediv at org.tensorflow.session.run(native method) i believe ceil is a basic op and it should has cpu implement so maybe i miss sth i clone tensorflow master branch and build the lib and java interface on that
245580907,11770,https://api.github.com/repos/tensorflow/tensorflow/issues/11770,JohnScolaro,2,0,0,0,0,0,with the old input pipeline functions in tf.train.batch you could specify the allow_smaller_final_batch parameter which would allow or disallow a smaller final batch with the new input pipeline functions in tf.contrib.data the batch function allows a smaller final batch by default and to the best of my knowledge there is no way to skip this last half-batch to ensure all batch sizes are equal.is there a possibility that a allow_smaller_final_batch flag could be added to the new batch function
245574566,11768,https://api.github.com/repos/tensorflow/tensorflow/issues/11768,jhseu,1,0,0,1,0,0,just jenkins testing for now
245288401,11736,https://api.github.com/repos/tensorflow/tensorflow/issues/11736,YusukeSuzuki,3,0,0,0,0,0,i cant build tf from source on ubuntu and pyenv.tf tf are ok to be build normally system information ubuntu tf from git checkout r python bazel from deb cuda cudnn gtx ti x python environment pyenv install pyenv virtualenv tf.pyenv local tf. required packages on are all installed my config cat cat tf_configure.bazelrc build action_env python_bin_path=/home/yusuke/.pyenv/shims/python build action_env python_lib_path=/home/yusuke/.pyenv/versions/tf./lib/python./site-packagesbuild define python_bin_path=/home/yusuke/.pyenv/shims/pythonbuild define python_lib_path=/home/yusuke/.pyenv/versions/tf./lib/python./site-packagesbuild force_python=pybuild host_force_python=pybuild python_path=/home/yusuke/.pyenv/shims/pythontest force_python=pytest host_force_python=pytest define python_bin_path=/home/yusuke/.pyenv/shims/pythontest define python_lib_path=/home/yusuke/.pyenv/versions/tf./lib/python./site-packagesrun define python_bin_path=/home/yusuke/.pyenv/shims/pythonrun define python_lib_path=/home/yusuke/.pyenv/versions/tf./lib/python./site-packagesbuild define with_jemalloc=truebuild define with_xla_support=truebuild:opt cxxopt=-march=native copt=-march=nativebuild action_env tf_need_cuda=build action_env tf_need_opencl=build action_env tf_cuda_clang=build action_env cuda_toolkit_path=/usr/local/cudabuild action_env tf_cuda_version=.build action_env gcc_host_compiler_path=/usr/bin/gccbuild action_env tf_cudnn_version=build action_env cudnn_install_path=/usr/local/cuda-.build action_env tf_cuda_compute_capabilities fail logscommand bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package verbose_failures action_env=pyenv_version=tf action_env=pyenv_root=$home/.pyenv action_env=path=$pyenv_root/bin:$path action_env=pyenv_virtual_env=/home/yusuke/.pyenv/versions/../envs/tf action_env=virtual_env=/home/yusuke/.pyenv/versions/../envs/tf action_env=pyenv_root=/home/yusuke/.pyenv action_env=pyenv_shell=bash action_env=dyld_library_path=/usr/local/cuda/lib:/usr/local/cuda/lib action_env=ld_library_path=/usr/local/cuda/lib:/usr/local/cuda/lib action_env=pyenv_virtualenv_init s error log error home/yusuke/.cache/bazel/_bazel_yusuke/deacfefce/external/farmhash_archive/build.bazel c compilation of rule farmhash_archive//:farmhash failed crosstool_wrapper_driver_is_not_gcc failed error executing command cd home/yusuke/.cache/bazel/_bazel_yusuke/deacfefce/execroot/org_tensorflow exec env ld_library_path=/usr/local/cuda/lib:/usr/local/cuda/lib path=/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/snap/bin:/usr/local/cuda/bin pwd=/proc/self/cwd external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc u_fortify_source d_fortify_source fstack-protector fpie wall wunused-but-set-parameter wno-free-nonheap-object fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections g std=c g md mf bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.d frandom-seed=bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o iquote external/farmhash_archive iquote bazel-out/host/genfiles/external/farmhash_archive iquote external/bazel_tools iquote bazel-out/host/genfiles/external/bazel_tools isystem external/farmhash_archive/src isystem bazel-out/host/genfiles/external/farmhash_archive/src isystem external/bazel_tools/tools/cpp/gcc no-canonical-prefixes wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted fno-canonical-system-headers c external/farmhash_archive/src/farmhash.cc o bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status python cant open file external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc errno no such file or directorytarget tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path s other command bazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package verbose_failures s error log error home/yusuke/.cache/bazel/_bazel_yusuke/deacfefce/external/protobuf/build c compilation of rule protobuf//:js_embed failed crosstool_wrapper_driver_is_not_gcc failed error executing command cd home/yusuke/.cache/bazel/_bazel_yusuke/deacfefce/execroot/org_tensorflow exec env ld_library_path=/usr/local/cuda/lib:/usr/local/cuda/lib path=/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/home/yusuke/bin:/home/yusuke/.local/bin:/home/yusuke/.local/bin:/home/yusuke/.pyenv/plugins/pyenv-virtualenv/shims:/home/yusuke/.pyenv/shims:/home/yusuke/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/snap/bin:/usr/local/cuda/bin pwd=/proc/self/cwd external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc u_fortify_source d_fortify_source fstack-protector fpie wall wunused-but-set-parameter wno-free-nonheap-object fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections g std=c g md mf bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d frandom-seed=bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o iquote external/protobuf iquote bazel-out/host/genfiles/external/protobuf iquote external/bazel_tools iquote bazel-out/host/genfiles/external/bazel_tools isystem external/bazel_tools/tools/cpp/gcc no-canonical-prefixes wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted fno-canonical-system-headers c external/protobuf/src/google/protobuf/compiler/js/embed.cc o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status python cant open file external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc errno no such file or directorytarget tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path s
244953919,11700,https://api.github.com/repos/tensorflow/tensorflow/issues/11700,guoying1030,1,0,0,0,0,0,"root@a-r-i export_dir python predict.py traceback most recent call last file predict.py line in module load_graph(quantized_graph.pb file predict.py line in load_graph tf.import_graph_def(graph_def file usr/lib/python./site-packages/tensorflow/python/framework/importer.py line in import_graph_def input_name,)))valueerror graph_def is invalid at node uwhile/add_/y more inputs specified while/switch than the op expects"
244916748,11692,https://api.github.com/repos/tensorflow/tensorflow/issues/11692,meijun,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu linux centos tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory tesla km mib exact command to reproduce python c import tensorflow as tf print tf.session().run(tf.reduce_logsumexp(float(-inf describe the problemthe doc of tf.reduce_logsumexp says it computes log(sum(exp(elements across dimensions of a tensor))).however it does not when the tensor is inf source code logs python c import tensorflow as tf print tf.session().run(tf.reduce_logsumexp(float(-inf))) prints nan -------------------- python c import tensorflow as tf print tf.session().run(tf.log(tf.reduce_sum(tf.exp(float(-inf))))) prints -inf
244804748,11678,https://api.github.com/repos/tensorflow/tensorflow/issues/11678,athulase,2,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu centos tensorflow installed from source or binary source tensorflow version use command below latest python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce bazel build config=mkl c opt tensorflow/tools/pip_package:build_pip_packagei was trying to build tensorflow and hitting this error missing input file mkl//:license.i did it a week back no problem faced everything worked fine with same command line
244794999,11675,https://api.github.com/repos/tensorflow/tensorflow/issues/11675,ebrevdo,0,0,0,2,0,0,does what it says on the box follow instructions in third_party/toolchains/cpus/arm/build_raspberry_pi.sh
244643883,11665,https://api.github.com/repos/tensorflow/tensorflow/issues/11665,lsorber,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see below describe the problemfeature request add a local_init_feed_dict to tf.train.scaffold it would be useful to be able to create local variables which are not saved or restored and have them initialized by a tf.train.monitoredtrainingsession with a feed_dict in the example below the variable x_var is forced to be part of the global_variables collection in order to be able to initialize the variable with a feed_dict this has the undesirable consequence that the variable will be saved to disk source code logs pythonimport tensorflow as tfimport numpy as np data that we wish to sample but not save to disk.x np.eye dtype=np.float create a graph that samples rows from x randomly.graph tf.graph()with graph.as_default x_init tf.placeholder(tf.float shape=x.shape here we want to use tf.graphkeys.local_variables but cant because there is no feed_dict for that collection in tf.train.scaffold x_var tf.variable(x_init trainable=false collections= tf.graphkeys.global_variables queue tf.randomshufflequeue capacity=x.shape min_after_dequeue dtypes= tf.float shapes= x.shape enqueue_op queue.enqueue_many( x_var row queue.dequeue sample a few rows from x.with graph.as_default sess_params scaffold tf.train.scaffold init_feed_dict={x_init x init_fn=lambda scaffold sess sess.run(enqueue_op with tf.train.monitoredtrainingsession( sess_params as sess print(sess.run(row print(sess.run(row
244598635,11663,https://api.github.com/repos/tensorflow/tensorflow/issues/11663,frreiss,0,0,0,0,0,1,while tracing back through the python logging apis to find their implementation i ran into a dead end at the generated file gen_logging_ops.py this generated file contains no information about where it came from it took a fair amount of time poking through bazel build files to track down the rule that generates this file from the output of a program that is linked against an object file that is produced by compiling logging_ops.cc .other developers dont seem to have gotten that far for example the reporter who gave up without finding logging_ops.cc .this pr adds the name of the original c source file to the generated python files for built-in operators ive modified python_op_gen_main.cc to check whether it is running from one of the special executables that the bazel build creates for these internal operators if the program is running from inside such a rule it generates a header comment with the name of the original c source file: python wrappers around tensorflow ops.this file is machine generated do not edit.original c source file logging_ops.cc note the last line original c source file this should be enough information for someone new to the project to track down the implementation of a built-in operator.i would have preferred to modify the register_op macro to incorporate information about the original source file in the opdef itself but doing so would have required modifying op_def.proto hence this approach of checking the executable name in python_op_gen_main.cc
244459010,11651,https://api.github.com/repos/tensorflow/tensorflow/issues/11651,PaddyT,5,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu scientific linux tensorflow installed from source or binary binary tensorflow version use command below v..-rc--geced gpu python version bazel version if compiling from source n/a cuda/cudnn version gpu model and memory n/a describe the problemim using the tf.extract_image_patches operation to extract a number of overlapping frames from an image with variable size the current gradient operation fails unless the input image dimensions are fixed.specifically in my case my code fails here rows_in rows_out and cols_out are all none .for more background my application is in speech processing and my image is a spectrogram-like array of features which has a fixed height but can vary in length with the duration of the input audio im trying to split it into overlapping windows dynamically to pass to a classifier downstream.in my case the input has shape x none x x where the unknown dimension is typically my image patches are x and the result has shape x none x none x i feel it should be possible to modify the gradient op to accommodate dynamic input dimensions would this be a reasonable feature addition?the current gradient implementation was added here this feature request would tie in with source code/logsi think the information provided explains my problem well enough let me know if i should provide a more detailed problem description.an expanded code snippet can also be found here tf.extract_image_patches(images=input_image ksizes strides rates padding=valid file diss/waveform-asr/tests/test_graphs/test_wavenet.py line in module alpha file diss/waveform-asr/waveasr/models/wavenet.py line in init self.train_op self.optimizer.minimize(loss=self.objective global_step=self.global_step file miniconda/envs/diss/lib/python./site-packages/tensorflow/python/training/optimizer.py line in minimize grad_loss=grad_loss file miniconda/envs/diss/lib/python./site-packages/tensorflow/python/training/optimizer.py line in compute_gradients colocate_gradients_with_ops=colocate_gradients_with_ops file miniconda/envs/diss/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in gradients grad_scope op func_call lambda grad_fn(op out_grads file miniconda/envs/diss/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in maybecompile return grad_fn exit early file miniconda/envs/diss/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in lambda grad_scope op func_call lambda grad_fn(op out_grads file miniconda/envs/diss/lib/python./site-packages/tensorflow/python/ops/array_grad.py line in extractimagepatchesgrad rows_out int(ceil((rows_in ksize_r_eff stride_r))typeerror unsupported operand type(s for nonetype and int
244452691,11650,https://api.github.com/repos/tensorflow/tensorflow/issues/11650,santon834,2,0,0,0,0,0,describe the problemgru units wikipedia):! image documentation:state_keep_prob unit tensor or float between and output keep probability if it is constant and no output dropout will be added state dropout is performed on the output states of the cell.in gru units the output state is the memory state when variational_recurrent=true the same temporal dropout mask is applied to the output state in each time step with the remaining outputs divided by the dropout probability this leads to exponential growth of the memory state and exploding outputs given long enough time series):! image correct way is probably to divide u_z u_r and u_h and not the output state source code logs def length(sequence used tf.sign(tf.reduce_max(tf.abs(sequence axis length tf.reduce_sum(used axis return length def gru(x units act in_dp out_dp tmp_dp gru_cell tf.contrib.rnn.grucell(units activation=act kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=true wrapped_gru_cell tf.contrib.rnn.dropoutwrapper(gru_cell input_keep_prob=in_dp output_keep_prob=out_dp state_keep_prob=tmp_dp variational_recurrent=true dtype=x.dtype input_size=x.get_shape outputs state tf.nn.dynamic_rnn(wrapped_gru_cell x dtype=x.dtype sequence_length=length(x return outputs state running with parameters all exploded):act=tf.nn.tanh in_dp out_dp tmp_dp n_hidden system information cat etc/issue linux mvdslab generic ubuntu smp mon jun utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux mvdslab generic ubuntu smp mon jun utc x x x gnu/linux check pips numpy numpydoc protobuf tensorflow tensorflow-gpu check for virtualenv false tensorflow import tf.version tf.git_version v..--gbcddtf.compiler_version v..--gbcddsanity check array dtype=int env ld_library_path home/anton/torch/install/lib:/usr/local/cuda-./libdyld_library_path home/anton/torch/install/lib:/home/anton/torch/install/lib nvidia-smi thu jul nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m quadro k off on n/a c p w w mib mib default processes gpu memory gpu pid type process name usage g usr/lib/xorg/xorg mib g el-token=deafecdcdd mib cuda libs usr/local/cuda-./lib/libcudart.so.../usr/local/cuda-./lib/libcudart_static.a/usr/local/cuda-./doc/man/man/libcudart./usr/local/cuda-./doc/man/man/libcudart.so
244425196,11646,https://api.github.com/repos/tensorflow/tensorflow/issues/11646,germanmarky,1,0,0,0,0,0,government/military/veteran affiliation government catch-all protected characteristic added to opening paragraph
244300577,11638,https://api.github.com/repos/tensorflow/tensorflow/issues/11638,ZekunZh,3,0,0,0,0,0,"in function call of class poolingd when the input data_format=channels_first it should transform input tensor from n,c,h to n,c,h,w batch_size channels height width meaning that we should expand dimension on the last dimension however in the code we use inputs array_ops.expand_dims(inputs expanding on the second dimension and transforming from n,c,h to n,,c,h then the pool_shape and strides are looking at the third dimension which is not consistant with our expand_dims(inputs used before.i think the code should be changed to inputs array_ops.expand_dims(inputs and return array_ops.squeeze(outputs using will expand and squeeze on the last dimension transforming from n,c,h to n,c,h and then doing pool_shape and strides on the third dimension source code def call(self inputs there is no tf op for d pooling hence we make the inputs d if self.data_format channels_last inputs array_ops.expand_dims(inputs pool_shape self.pool_size strides self.strides data_format nhwc else inputs array_ops.expand_dims(inputs pool_shape self.pool_size strides self.strides data_format nchw"
244137774,11621,https://api.github.com/repos/tensorflow/tensorflow/issues/11621,mari-linhares,3,0,0,0,0,0,i have built a custom estimator and i created an input function using the datasets api when i call estimator.predict(input_fn i see the following warning:warning:tensorflow:input graph does not contain a queuerunner that means predict yields forever this is probably a mistake.the input function looks like this: def get_input_fn(review word_to_id create a input function for classify_sentiment_analysis.py args review str review sentence word_to_id list list with all words represented in the embedding the index is the word index in the embedding return sequence of indexes that map that words to the embedding def word_to_index(sequence convert a sequence of words into a sequence of integers id_sequence unk vector for unkown words for word in sequence if word in word_to_id id_sequence.append(word_to_id.index(word else id_sequence.append(unk return np.array(id_sequence def input_fn input function make review a sequence of words review_split review.split converting words to indexes review_id word_to_index(review_split calculates the length of the sequence x_len len(review_split creates the dataset from in memory data ds tf.contrib.data.dataset.from_tensors(review_id the model expects a batch ds ds.batch creates iterator x ds.make_one_shot_iterator().get_next dict_x x x rnn_common.rnnkeys.sequence_length_key x_len no label needed since were only using this input function for prediction if training make sure to return a label return dict_x none return input_fn thank you
244065185,11610,https://api.github.com/repos/tensorflow/tensorflow/issues/11610,iamshang1,1,0,0,0,0,0,"while developing a hierarchical attention network we have discovered that changing the batch size of the input effects the output of dynamic rnns while keeping everything else constant in other words feeding in and individually with batch size will give a different result than feeding in together with batch size we are currently running bidirectional dynamic rnns with grus on the cpu-version of tensorflow while the change in output is small when a network has many layers of rnns the differences become amplified in our case changing the batch size from to changes the network accuracy on our test set from to system information and shortened sample code below system information cat etc/issue linux pc.ornl.gov el.x smp tue jun edt x x x gnu/linuxversion maipo)version_id=.redhat_bugzilla_product_version=.redhat_support_product_version are we in docker no compiler c gcc red hat copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux pc.ornl.gov el.x smp tue jun edt x x x gnu/linux check pips numpy numpydoc protobuf tensorflow check for virtualenv false tensorflow import tf.version tf.git_version v..--gcdfctf.compiler_version v..--gcdfcsanity check array dtype=int env ld_library_path is unsetdyld_library_path is unset nvidia-smi tf_env_collect.sh line nvidia-smi command not found cuda libs source code logs import numpy as npimport tensorflow as tffrom tensorflow.contrib.rnn import lstmcell grucellembeddings np.random.rand(,).astype(np.float)embeddings embeddings.mean()embeddings embeddings.std()*.)#doc input and line countline tf.placeholder(tf.int shape= none, )num_words tf.reduce_sum(tf.cast(tf.greater(line,),tf.int),)word_embeds tf.nn.embedding_lookup(tf.get_variable(embeddings initializer=embeddings,dtype=tf.float),line) word_outputs_fw,word_outputs_bw tf.nn.bidirectional_dynamic_rnn(grucell(),grucell word_embeds,sequence_length=num_words dtype=tf.float)word_outputs tf.concat((word_outputs_fw word_outputs_bw),)init_op tf.global_variables_initializer()sess tf.session()sess.run(init_op)a np.array( ,,,,,,,,, )b np.array( ,,,,,,,,, )ab np.array( ,,,,,,,,, , ,,,,,,,,, )feed_dict line:a}print sess.run(word_outputs,feed_dict=feed_dict)feed_dict line:b}print sess.run(word_outputs,feed_dict=feed_dict)feed_dict line:ab}print sess.run(word_outputs,feed_dict=feed_dict sample outputbelow the first two matrices are the results of feeding in two inputs one at a time with batch size while the second two matrices are the results of feeding in two inputs together with batch size you can see that the outputs are not exactly the same while the differences between the two are small this becomes a major issue when there are multiple layers of rnns as the differences become more pronounced after each layer"
244017930,11604,https://api.github.com/repos/tensorflow/tensorflow/issues/11604,kwotsin,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below master python version bazel version if compiling from source cuda/cudnn version gpu model and memory gtx m exact command to reproduce bazel build c opt cxxopt=-std=c linkopt=-lm cpu=armeabi-va host_crosstool_top=@bazel_tools//tools/cpp:toolchain crosstool_top=//external:android/crosstool tensorflow/compiler/aot:inception_v verbose_failures describe the problemi am currently trying to use tfcompile to compile a quantized inception_v model for android following the instructions given in the documentation here but i have gotten this error below: info found target...error home/kwotsin/android/tensorflow/tensorflow/compiler/aot/build executing genrule tensorflow/compiler/aot:gen_inception_v failed bash failed error executing command cd home/kwotsin/.cache/bazel/_bazel_kwotsin/cffaadebeecebd/execroot/tensorflow exec env ld_library_path=:/usr/local/cuda/lib:/usr/local/cuda/lib path=/usr/local/cuda/bin:/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/kwotsin/android/sdk/tools:/home/kwotsin/android/sdk/platform-tools python_bin_path=/usr/bin/python python_lib_path=/usr/local/lib/python./dist-packages tf_need_cuda tf_need_opencl bin/bash c source external/bazel_tools/tools/genrule/genrule-setup.sh bazel-out/host/bin/tensorflow/compiler/aot/tfcompile graph=tensorflow/compiler/aot/inception_v.pb config=tensorflow/compiler/aot/inception_v.config.pbtxt entry_point=__tensorflow_compiler_aot__inception_v cpp_class=inception_v_cpp target_triple=armv-none-android out_header=bazel-out/arm-linux-androideabi-.-va-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v.h out_object=bazel-out/arm-linux-androideabi-.-va-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v.o com.google.devtools.build.lib.shell.abnormalterminationexception process terminated by signal w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices e tensorflow/core/common_runtime/executor.cc executor failed to create kernel not found no registered const opkernel for xla_cpu_jit devices compatible with node inceptionv/convd_a_x/weights/read/___cf___quantized_const const dtype=dt_quint value=tensor
243795717,11585,https://api.github.com/repos/tensorflow/tensorflow/issues/11585,snassimr,1,0,0,0,0,0,if any new option was added to ensure a reproducible results by setting some seed parameter?in keras the issue was discussed here the issue the instability in results is due to weights random initialization
243704101,11575,https://api.github.com/repos/tensorflow/tensorflow/issues/11575,meijieru,4,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version :cudnn v gpu model and memory :pascal titan x exact command to reproduce :none feature request for getting unique row we can use numpys unique by specifying the axis however in tensorflow unique only support d tensor which is not convenient
243684111,11573,https://api.github.com/repos/tensorflow/tensorflow/issues/11573,discoveredcheck,0,0,0,1,0,0,the current rnn implementation executes a user definedfunction the call method of subclasses of rnncells inside atf.while loop weight normalisation requires a one-time normalization of the transition matrices prior toentering the while loop the following edits have been made intensorflow/python/ops to enable this functionality rnncell now has a prepare method it does nothing as implementedin the base class a call to cell.prepare has been added just before entering dynamic_rnn_loop()subclasses of rnncell may implement normalization in the cellsprepare method one implementation with basiclstmcelland associated tests have been added to contrib note that anywrappers to be used with a weight-normalized cell need to beappropriately subclassed as illustrated with theprepareablemultirnncell example in contrib
243195224,11521,https://api.github.com/repos/tensorflow/tensorflow/issues/11521,dicaormu,1,0,0,0,0,0,i was looking for a function that allows me like in python to print the graph in a readable way if you open a github issue here is our policy: heres why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu macos x tensorflow installed from source or binary binary tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :in python i can do: pythonimport tensorflow as tfsess tf.interactivesession()with tf.gfile.fastgfile(graphname.pb rb as f graph_def tf.graphdef graph_def.parsefromstring(f.read tf.import_graph_def(graph_def name all operationssess.graph.get_operations() that prints the operations of my graphin go i just have the option: graph tf.newgraph()if err graph.import(model err nil log.fatal(err f err os.create(logwritter.txt if err nil log.fatal(err graph.writeto(f) it prints the binary graph describe the problemwhen using a model previously trained by somebody else it is very useful to know the nodes to reference them it should be nice to have the same option with go
242961138,11499,https://api.github.com/repos/tensorflow/tensorflow/issues/11499,iurilarosa,1,0,0,0,0,0,"system information os platform and distribution e.g linux ubuntu linux virgo-wn.roma.infn.it el.x smp thu nov cdt x x x gnu/linuxversion nitrogen)version_id=.redhat_bugzilla_product_version=.redhat_support_product_version tensorflow installed from source or binary from pip tensorflow version use command below v..--gcdfc python version python cuda/cudnn version cuda cudnn gpu model and memory tesla km memory mib describe the problemeverytime i run a code with simple operations and heavy computation functions the simple ones like add or reshape run on the gpu while the heavy ones like matmul bincount tensordot run always on cpu this is very strange because tensorflow sees the gpu but uses it only for simple functions while i expect the inverse behavior.i noticed it when i used timeline to profile my codes source code logsa simple example of the code i use: import numpyimport tensorflow as tffrom tensorflow.python.client import timelinematrix tf.zeros dtype tf.int)matrix tf.ones dtype tf.int)matrix tf.add(matrix,)product tf.matmul(matrix,matrix session tf.session(config=tf.configproto(log_device_placement=true))run_options tf.runoptions(trace_level=tf.runoptions.full_trace)run_metadata tf.runmetadata()image session.run(product options=run_options run_metadata=run_metadata create the timeline object and write it to a jsontl timeline.timeline(run_metadata.step_stats)ctf tl.generate_chrome_trace_format()with open(timelinedb.json w as f f.write(ctf) and here the logs w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name tesla kmmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla km pci bus id device mapping:/job:localhost/replica:/task:/gpu device name tesla km pci bus id i tensorflow/core/common_runtime/direct_session.cc device mapping:/job:localhost/replica:/task:/gpu device name tesla km pci bus id add add job:localhost/replica:/task:/gpu i tensorflow/core/common_runtime/simple_placer.cc add add)/job:localhost/replica:/task:/gpu:matmul matmul job:localhost/replica:/task:/cpu i tensorflow/core/common_runtime/simple_placer.cc matmul matmul)/job:localhost/replica:/task:/cpu:add/y const job:localhost/replica:/task:/gpu i tensorflow/core/common_runtime/simple_placer.cc add/y const)/job:localhost/replica:/task:/gpu:ones const job:localhost/replica:/task:/cpu i tensorflow/core/common_runtime/simple_placer.cc ones const)/job:localhost/replica:/task:/cpu:zeros const job:localhost/replica:/task:/gpu i tensorflow/core/common_runtime/simple_placer.cc zeros const)/job:localhost/replica:/task:/gpu i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcupti.so locally i attach the json generated change txt to json and open it with chrome://tracing/) timeline.txt"
242941166,11497,https://api.github.com/repos/tensorflow/tensorflow/issues/11497,glaporte,3,0,1,0,0,0,"hello,first of all i would like to thank the developer of tensorflow for this great library.can tensorflow-serving(cpu works on xbox or ps i mean to compile it)thank you"
242832525,11489,https://api.github.com/repos/tensorflow/tensorflow/issues/11489,StanislawAntol,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow reproducible script below os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary binary/pip tensorflow version use command below python version bazel version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce : pyimport tensorflow as tffrom tensorflow.python.framework import graph_utilfrom tensorflow.core.framework import graph_pborig_model input_map_orig.pbmapped_model input_map_modified.pb create a graph with uint placeholder that gets cast to float before going through image whiteningtf.reset_default_graph()img tf.placeholder(tf.uint shape none none name=input_tensor)float_img tf.cast(img tf.float tofloat)whiten_fn lambda img tf.image.per_image_standardization(img)whitened tf.map_fn(whiten_fn float_img name=image_whitening)identity tf.identity(whitened name=id write graphoutput_names id with tf.session as sess input_graph_def sess.graph_def output_graph_def graph_util.convert_variables_to_constants(sess input_graph_def output_names tf.train.write_graph(output_graph_def orig_model false load the graph againtf.reset_default_graph()graph_def graph_pb.graphdef()with open(orig_model rb as f graph_def.parsefromstring(f.read rewire the graph with a float placeholder directlyimg tf.placeholder(tf.float shape none none name=input)input_map tofloat img tf.import_graph_def(graph_def input_map=input_map name write the modified graphgraph tf.get_default_graph()output_names id with tf.session(graph=graph as sess graph tf.get_default_graph input_graph_def graph.as_graph_def output_graph_def graph_util.convert_variables_to_constants(sess input_graph_def output_names tf.train.write_graph(output_graph_def mapped_model false try to load the modified graphtf.reset_default_graph()graph_def graph_pb.graphdef()with open(mapped_model rb as f graph_def.parsefromstring(f.read())g tf.import_graph_def(graph_def describe the problemi noticed this issue trying to modify via input_map an object detection model from tf/models/object_detection to directly accept float instead of uint i have managed to reproduce the using the above minimal example it seems like one of the attributes of one of the operations doesnt get fully updated possibly related to source code logstraceback: traceback most recent call last file input_map_bug.py line in module g tf.import_graph_def(graph_def file home/s.antol/cv/pyvirtualenvs/tf./lib/python./site-packages/tensorflow/python/framework/importer.py line in import_graph_def op_to_bind_to node.name))valueerror specified colocation to an op that does not exist during import tofloat in image_whitening/tensorarrayunstack/tensorarrayscatter/tensorarrayscatterv looking through the protobuf text version the node below has s loc:@tofloat instead of what i think should be s:loc:@input : node name image_whitening/tensorarrayunstack/tensorarrayscatter/tensorarrayscatterv op tensorarrayscatterv input image_whitening/tensorarray input image_whitening/tensorarrayunstack/range input input input image_whitening/tensorarray attr key t value type dt_float attr key class value list s loc:@tofloat
242229458,11440,https://api.github.com/repos/tensorflow/tensorflow/issues/11440,strman,7,0,0,0,0,2,similar to this issue for the tensorflow linear model tutorial the project implies that it will end with a program that based on input data outputs a or given census data about a person such as age gender education and occupation the features we will try to predict whether or not the person earns more than dollars a year the target label we will train a logistic regression model and given an individuals information our model will output a number between and which can be interpreted as the probability that the individual has an annual income of over dollars. however it seems that the tutorial is incomplete the last steps have you calculate the accuracy of the trained model the first line of the output should be something like accuracy which means the accuracy is feel free to try more features and transformations and see if you can do even better!and then point you in the direction of the full example code if youd like to see a working end-to-end example you can download our example code and set the model_type flag to wide when i run the final program my output looks like this accuracy accuracy/baseline_label_mean accuracy/threshold_._mean auc auc_precision_recall global_step labels/actual_label_mean labels/prediction_mean loss precision/positive_threshold_._mean recall/positive_threshold_._mean only accuracy is explained in the instructions and it doesnt seem that there are final steps to complete the tutorial to take a set of given values and predict income_bracket can someone provide a code example or point to documentation on how to extract final predictions after training the model?thanks
242011601,11429,https://api.github.com/repos/tensorflow/tensorflow/issues/11429,horance-liu,0,0,0,1,0,0,saverdef v has been deprecated so modify default version with v
241787630,11414,https://api.github.com/repos/tensorflow/tensorflow/issues/11414,petewarden,0,0,5,1,0,0,every open source project deserves a mascot heres teensy the tensorflow pony and hes ready to serve:! img
241589715,11403,https://api.github.com/repos/tensorflow/tensorflow/issues/11403,netheril96,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below python version description:the current api is immensely stateful each function and object creation affects and is affected by countless mutable global states running a declaration of variable or summary mutates the graph and collections and may return different results based on the current namescope or variablescope or control_dependencies a function running twice can easily return wildly different result even though it looks pure but it isnt because almost no tensorflow function is pure declaring something and then deleting it wont restore the global state as it may or may not be added somewhere in the graph or the collections or some other hidden objects reasoning about what changes what is incredibly difficult.what i would like to see is a api where all state mutations are explicit when i declare a variable it is just a variable and not added to anything unless i call add the current entangled interfaces can be built on top of that preserving convenience functions such as global_variable_initializers for those who prefer
241551063,11399,https://api.github.com/repos/tensorflow/tensorflow/issues/11399,sheerun,11,0,0,0,0,0,describe the problemtype of issue feature requestthe string_split function has mostly good behavior when splitting utf strings by single-character delimiter but fails to do it properly on null-width delimiter because of its documented behavior if delimiter is an empty string each element of the source is split into individual strings each containing one byte this includes splitting multibyte sequences of utf-.)for models like seqseq one needs a split function that can split utf strings into individual characters that can be processed by model as units also embeddings having properly of being easily joined as utf strings.could tensorflow provide alternative implementation of string_split that is utf aware
241544432,11397,https://api.github.com/repos/tensorflow/tensorflow/issues/11397,eaplatanios,1,0,0,0,0,0,i added a couple links to my scala api repository in the documentation pages im not sure if its in the expected format but feel free to rephrase if needed.it would be great if we could also add a link in the following page i could not find it in the documentation sources asimshankar will take care of adding a link to so were good with that one thanks
241518747,11392,https://api.github.com/repos/tensorflow/tensorflow/issues/11392,byronyi,6,0,0,0,0,0,introduction===this pr implements gdr out-of-band transport for tensorflow distributed runtime complementary to current grpc transport it uses grpc as control plane to setup rendezvous for each tensor transmission and utilizes gpu direct rdma whenever possible to transmit tensors in remote gpu memory through network interface card nic bypassing host memory and cpu entirely it gracefully falls back to ordinary rdma or even grpc when gdr is not available.design===the gdr out-of-band transport is designed to avoid any unnecessary memory copies especially for large tensors mb that typically requires registration of tensor buffers to nic on the fly which is rather slow as described in the design trade-off of the verbs runtime the verbs runtime thus chooses to manage its own nic-registered buffers and copy the tensors from/to those buffers for every single tensor transfer.we show that however such design trade-off is not always relevant in this patch we manage both computation and communication buffers in a unified manner by pre-registration of large buffers to nic and allocating small tensors from the buffer pool using a bfc allocator it is possible to avoid both buffer registration on the fly and memory copies all together.for the actual tensor transport we rely on grpc to transmit the remote buffer information this greatly simplifies our design and there are only types of rdma messages a single read to retrieve the tensor data bypassing remote cpu and another invalidate using write with imm to release the tensor buffer on the remote side the remote side will only be polling the invalidate message and unref the tensor buffers that read by its peer.environment===to fully utilize gdr the target environment has to meet conditions there is an rdma capable device with corresponding ofed package installed detailed information is available from your infiniband/roce vendor which could be verified through ibv_devinfo e.g ibv_devinfohca_id mlx transport infiniband fw_ver node_guid a::f sys_image_guid a::f vendor_id xc vendor_part_id hw_ver x board_id mt phys_port_cnt device ports port state port_active max_mtu active_mtu sm_lid port_lid port_lmc x link_layer ethernet port state port_active max_mtu active_mtu sm_lid port_lid port_lmc x link_layer ethernet there is a gdr capable gpu i.e of fermi kepler or later architecture with corresponding driver installed the pci-e topology could be confirmed by nvidia-smi topo m for example in the following topology gpu and gpu are adjacent to mlx and tensors on these devices could benefit from gdr in current implementation nvidia-smi topo m gpu gpu gpu gpu mlx cpu affinitygpu x phb soc soc soc gpu phb x soc soc soc gpu soc soc x phb phb gpu soc soc phb x phb mlx soc soc phb phb xlegend x self soc connection traversing pcie as well as the smp link between cpu sockets(e.g qpi phb connection traversing pcie as well as a pcie host bridge typically the cpu pxb connection traversing multiple pcie switches without traversing the pcie host bridge pix connection traversing a single pcie switch nv connection traversing a bonded set of nvlinks the nv_peer_mem kernel module is installed.how to build and run in gdr mode===to test it out on a gdr capable environment choose to enable gdr in your configure script. do you wish to build tensorflow with gdr support y/n ygdr support will be enabled for tensorflow. change your protocol to grpc+gdr to enable gdr in your deployment. server tf.train.server(cluster job_name=local task_index protocol=grpc+gdr default protocol is grpc currently the out-of-band transport service listens to the same ip and port address as specified in grpc.a successful initialization looks like this i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla km pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla km pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla km pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla km pci bus id i tensorflow/contrib/gdr/gdr_memory_manager.cc rdma server is listening on i tensorflow/contrib/gdr/gdr_memory_manager.cc instrumenting cpu allocator cuda_host_bfc i tensorflow/contrib/gdr/gdr_memory_manager.cc instrumenting cpu allocator cpu_pool i tensorflow/contrib/gdr/gdr_memory_manager.cc instrumenting cpu allocator cpu_rdma_bfc i tensorflow/contrib/gdr/gdr_memory_manager.cc numa node for device mlx is i tensorflow/contrib/gdr/gdr_memory_manager.cc instrumenting gpu allocator with bus_id the last line suggests that the gpus with bus id mapped to pci bus id prefixed will benefit from gdr and host memory bypass which is gpu and gpu in this case.caveats===in current implementation only tensors that reside in host memory or in gpu memory such that the gpu is adjacent to an rdma capable nic will use direct rdma as its transport when rdma is available but not gdr a temporary tensor copy on host memory will be used as rdma source/destination and copied from/to the target device when there is no rdma device present it can even fallback to the original grpc runtime while it is theoretically possible to mix gdr enabled tf with non-gdr deployments in the same job make sure the environment is properly setup so the gdr mode is enabled whenever possible i.e do not fall back to grpc when it is not absolutely necessary
241455995,11377,https://api.github.com/repos/tensorflow/tensorflow/issues/11377,theflofly,9,0,0,0,0,0,this pull request adds the optimizer base class and the gradientdescentoptimizer class to the c api.more details here
241446647,11373,https://api.github.com/repos/tensorflow/tensorflow/issues/11373,redserpent7,1,0,0,0,0,0,"hi,i feel something odd is happening with my inception_resnet_v training i am what im seeing is correct but it seems the training will take forever at the current rate i am training slim against the full imagenet dataset approx m images the training is running on gpus and the batch size is the default if my calculations are correct this means that each epoch should consist of approximately steps the training has been running for steps so far approx epochs yet the accuracy just reached i have been evaluating the model after each steps and at the start the accuracy used to increase by after each step up until steps yet now it seems to be rapidly slowing down as now i am getting in approximate increase every steps after reaching the steps mark.i am not sure if this is expected as i do not have any data to compare it with but at this rate it can take several months maybe even years to reach a viable accuracy rate i am trying to reach somewhere above at least.now is this expected are there any ways to speed up training beside adding gpus and if the latter is my only option is it possible to continue training on the same checkpoints once the number of gpus has increased"
241435099,11370,https://api.github.com/repos/tensorflow/tensorflow/issues/11370,christopherhelf,2,0,0,0,0,0,hi!currently the graph_transforms tool includes only convd and matmul ops when folding batch normalization scaling/multiplication into its weights as in fold_batch_norms.cc googles mobilenet example uses depthwise convolution extensively so it would be nice to include this feature for the depthwiseconvdnative operation the problem here is that weights are ordered differently for this operation and contain the channel_muliplier which would need to be checked when trying to bake subsequent multiplications
241254648,11350,https://api.github.com/repos/tensorflow/tensorflow/issues/11350,coopie,5,0,0,0,0,0,tensorflow version i am using monitoredtrainingsession extensively to train models and for the most part does exactly what i want it to do however i would like to be able to extract or pass in a filewriter object so i can report other summaries from inside my evaluation routines this is because only one filewriter can write to the events log:at the moment this is the best workaround i can do: pythonsess tf.train.monitoredtrainingsession hack this is so we use the same summary writer obj for both summaries only way to do it.summary_writer sess._hooks ._summary_writer then i can use the summary_writer to manually report summaries in the evaluation part of my code: pythonmean_test_loss summary tf.summary(value tf.summary.value(tag=mean_test_loss simple_value=mean_test_loss) )summary_writer.add_summary(summary current_step) it would be nice to have a non-hacky way to get the summary_writer from the session so only one filewriter writes to the events file.one thought i had is to have a filewriter object as part of the scaffold that is used to build the session that way anyone can get hold of the scaffold and use the one summary_writer designated to write to the events log.i would be happy to help implementing this if other people are interested
241149812,11341,https://api.github.com/repos/tensorflow/tensorflow/issues/11341,Warvito,1,0,0,0,0,0,support for tf.float dtype was added to a bunch of ops can we add it for convd too please?convd is important to development of videos and medical images systems since both consumes a lot of memory it would be good to have fp support to allow deeper models
240810633,11309,https://api.github.com/repos/tensorflow/tensorflow/issues/11309,dustinvtran,1,0,0,0,0,0,consider a mixture of dimensional gaussians in tensorflow: pythonds tf.contrib.distributionscat ds.categorical(probs comps ds.multivariatenormaldiag(loc scale_diag=tf.ones ds.multivariatenormaldiag(loc scale_diag=tf.ones ds.multivariatenormaldiag(loc scale_diag=tf.ones()) mix ds.mixture(cat=cat components=comps) this works because each multivariatenormaldiag distribution has a batch shape of it is compatible with the categorical s batch shape.now consider a mixture of dimensional bernoullis or gammas or laplace or studentts we need an equivalent multivariatebernoulli multivariategamma etc distribution which allows us to fix the batch shape and increase the event shape.are there plans to make such distributions available what about edge cases such as a matrixvariate k-variate bernoulli where additional parameter dimensions determine the batch shape and the event shape is fixed at k)?@ebrevdo jvdillon issue motivated by
240703061,11301,https://api.github.com/repos/tensorflow/tensorflow/issues/11301,pradyumnanpk,2,0,0,0,0,0,have i written custom code as opposed to using a stock example script provided in tensorflow i am using resnet code from tensorflow models with some modifications os platform and distribution e.g linux ubuntu fedora tensorflow installed from source or binary binary tensorflow version use command below v..-rc--gfd python version cuda/cudnn version cuda_..-cudnnv gpu model and memory nvidia titan x gbi am training a resnet model on my own data from scratch after successfully running for steps it crashed with the following error e tensorflow/stream_executor/cuda/cuda_blas.cc failed to run cublas routine cublassgemm_v cublas_status_execution_failedtraceback most recent call last file resnet_main.py line in module if z file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file resnet_main.py line in main tf.logging.info(loading checkpoint s ckpt_state.model_checkpoint_path file resnet_main.py line in train mon_sess.run(model.train_op file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run return self._sess.run(*args kwargs file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run run_metadata=run_metadata file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in run return self._sess.run(*args kwargs file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.internalerror blas sgemm launch failed m n k node unit__/sub/conv/convd convd t=dt_float data_format=nhwc padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/gpu: (unit__/sub/leaky_relu unit__/sub/conv/dw/read node train_step/update recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__train_step/update tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () caused by op uunit__/sub/conv/convd defined at file resnet_main.py line in module if z file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file resnet_main.py line in main tf.logging.info(loading checkpoint s ckpt_state.model_checkpoint_path file resnet_main.py line in train model.build_graph file s/chopin/l/grad/prady/pycharmprojects/gesturerecognition/frame/resnet_model.py line in build_graph self._build_model file s/chopin/l/grad/prady/pycharmprojects/gesturerecognition/frame/resnet_model.py line in build_model x res_func(x filters filters self._stride_arr false file s/chopin/l/grad/prady/pycharmprojects/gesturerecognition/frame/resnet_model.py line in bottleneck_residual x self._conv(conv x out_filter out_filter file s/chopin/l/grad/prady/pycharmprojects/gesturerecognition/frame/resnet_model.py line in conv return tf.nn.convd(x kernel strides padding=same file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in convd data_format=data_format name=name file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file s/chopin/l/grad/prady/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()internalerror see above for traceback blas sgemm launch failed m n k node unit__/sub/conv/convd convd t=dt_float data_format=nhwc padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/gpu: (unit__/sub/leaky_relu unit__/sub/conv/dw/read node train_step/update recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__train_step/update tensor_type=dt_float device=/job:localhost/replica:/task:/cpu e tensorflow/stream_executor/event.cc error destroying cuda event in context x cuda_error_misaligned_addresson another machine with exact configuration it is still running successfully steps as of now i dont understand why it crashed on one machine after running for steps
240693771,11300,https://api.github.com/repos/tensorflow/tensorflow/issues/11300,PKUEcho,4,0,0,0,0,0,question has been asked in this thread but got no answer just curious whats the plans of tf team to support android gpus or no plan at all for commercial devices such as nexus i understand opencl is not included in android now but we still have renderscript and some work has already been put to this end.it would be great to know it from tf developers
240645926,11295,https://api.github.com/repos/tensorflow/tensorflow/issues/11295,JackieLeeTHU11,1,0,0,0,0,0,i user inception_v as a base network for classification during training the batchsize during testing if the batchsize everything is ok however if the batchsize is smaller than the results are different and the precision declines as the batchsize drops if the batchsize the network will failed i also used inception_v and inception_v the same problems appered however if the base network is replaced with alex network tensorflow everything goes well i also replace the inception_v with vgg slim and everything goes well the bug is associated with inception_v~v i think i have not used inception_v properly did anyone encounter similar problems
240495451,11280,https://api.github.com/repos/tensorflow/tensorflow/issues/11280,slandersson,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary pip install tensorflow version use command below python version exact command to reproduce : tensorboard logdir=gs://mybucket describe the problemwhen trying to run tensorboard from a google cloud storage bucket the following error occurs: tensorflow.python.framework.errors_impl.unimplementederror file system scheme gs not implemented even after running gs authentication gcloud auth application-default login source code logsi was following this guide on training a pet object detector
240457950,11276,https://api.github.com/repos/tensorflow/tensorflow/issues/11276,drasmuss,1,0,0,0,0,0,uses weakref so that per_graph_layer_name_uids doesnt prevent graphs from being garbage collected.fixes
240435691,11273,https://api.github.com/repos/tensorflow/tensorflow/issues/11273,drasmuss,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below python version exact command to reproduce pythonimport osimport tensorflow as tfimport matplotlib.pyplot as pltimport numpy as npimport psutildef memory pid os.getpid py psutil.process(pid memory_use py.memory_info return memory_usememory_usage for i in range memory_usage.append(memory print(iter i memory_usage with tf.graph().as_default x tf.constant(np.ones dtype=np.float memory leak x tf.layers.dense(x units no memory leak with tf.variable_scope(layer reuse=false x tf.matmul(x tf.get_variable w shape dtype=tf.float initializer=tf.ones_initializer()))plt.figure()plt.plot(memory_usage)plt.xlabel(iterations)plt.ylabel(memory usage)plt.show describe the problemthere is some kind of memory leak when repeatedly building graphs containing tf.layers elements the example above shows the memory usage comparing what i think should be roughly equivalent implementations one using tf.layers.dense and the other using manually created kernels/matmul ops when using tf.layers.dense the memory usage continually increases whereas the manual approach shows memory being periodically cleaned up by garbage collection so my guess would be that there is some internal reference to the tf.layers elements that is preventing them from being garbage collected.not using tf.layers.dense :! non_layer tf.layers.dense :! with_layer
240083105,11235,https://api.github.com/repos/tensorflow/tensorflow/issues/11235,wm901115nwpu,3,0,0,0,0,0,"i try to quantify the mobilenet(in the ssd_mobilenet_v_coco the tensorflow i use is v.,bazel build tensorflow/tools/quantization:quantize_graph bazel-bin/tensorflow/tools/quantization/quantize_graph input=bazel build tensorflow/tools/quantization:quantize_graph bazel-bin/tensorflow/tools/quantization/quantize_graph input=ssd_mobilenet_v_coco___/frozen_inference_graph.pb--output_node_names print_nodes output=/tmp/quantized_graph.pb mode=eightbit logtostderrbut i dont decide the out_node_names"
240065795,11232,https://api.github.com/repos/tensorflow/tensorflow/issues/11232,derekhh,3,0,0,0,0,0,descriptionim trying to use tensorflow for java in a dataflow pipeline currently everything appears to be working but since dataflow only supports cpu instances inference time seems to be quite slow in my previous experiments ive seen that building tensorflow from source with mkl support usually provides a very significant speed gain.since im currently using tensorflow for java directly from maven repository i wont be able to get mkl support would it be possible to enable mkl support for tensorflow in java
239846069,11186,https://api.github.com/repos/tensorflow/tensorflow/issues/11186,danijar,6,0,0,0,0,0,tf.identity(tensor does or does not create a copy of the tensor based on whether its on the same device this can lead to bugs that are hard to find the current way of ensuring a copy is to perform an arithmetic/logic operation that doesnt change the value such as tensor tensor or tf.equal(tensor true needless to say this makes code hard to read moreover different treatment is needed for different tensor types can we have a tf.copy(tensor that does this for us
239824282,11182,https://api.github.com/repos/tensorflow/tensorflow/issues/11182,kwotsin,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary source tensorflow version use command below python version bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce : /home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph in_graph=./frozen_inception_resnet_v_for_mobile.pb out_graph=./quantized_inception_resnet_v_for_mobile_new.pb inputs=placeholder_only outputs=inceptionresnetv/logits/predictions transforms add_default_attributes strip_unused_nodes(type=float shape remove_nodes(op=identity op=checknumerics fold_constants(ignore_errors=true fold_batch_norms fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order describe the problemusing the above quantization method the quantization tool works so well that there is hardly a noticeable difference in accuracy drop less than for a model like inception v with a size reduction and slightly faster speed however when using the exact same files to be run on mobile the performance gets so poor that theres more than accuracy decrease im unsure whether the issue lies with the quantization not getting optimized on mobile architectures arm instead of the usual desktop amd architecture or whether there is a problem in the operations for the tensorflow mobile library.note that quantize_nodes is totally unusable when used to quantize the model the model size increases a little and then causes the app to crash instantly the error log produced when using quantize_nodes is this com.mindorks.tensorflowexample e/art no implementation found for long org.tensorflow.contrib.android.runstats.allocate tried java_org_tensorflow_contrib_android_runstats_allocate and java_org_tensorflow_contrib_android_runstats_allocate com.mindorks.tensorflowexample a/libc fatal signal sigsegv code fault addr x in tid pool--thread-) also for almost every model i ran the following error appeared: no implementation found for long org.tensorflow.contrib.android.runstats.allocate() what does this mean and how could i resolve it?fyi not sure if it makes a difference but when i built my lib_tensorflow_inference.so file and the jar file for using the tf library on mobile the tensorflow version was cloned from the master branch and not git checked out would this make a difference?further weird phenomenon:although i built my tf from source and bazel built the graph transform tool the following warnings still appear w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. thank you
239751694,11171,https://api.github.com/repos/tensorflow/tensorflow/issues/11171,MarvinTeichmann,2,0,0,0,0,0,in the paper training deep nets with sublinear memory cost chen et al introduced a very good idea to greatly reduce gpu memory requirements the idea bowls down to discarding the output of some nodes during the forward pass and recompute those values when they are needed again in the backward pass only the output of some key ops is kept in memory during back-prop all forward computation from those key nodes are redone they also describe how this can be implemented in a graph based computation model by mirroring non-key ops this is depicted in figure see below).performing this kind of graph manipulation in mxnet is quite easy and i have played around with this myself i am able to reduce the memory cost of a sota segmentation model from mb to mb for the cost of about increase in computational time given that we have plenty ti and view p gpus i am very happy to pay this cost).for me as deep learning researcher this is a totally awesome killer feature in most of my experiments i am limited by the amount of available gpu memory doing node mirroring allows me to try a whole bunch of new stuff i was always wanting to do is anything like this planned to be implemented in tensorflow any time soon in the current api is there already a way to build and or manipulate the computational graph to perform node mirroring like in figure question two i dont mind if it gets messy copying some nodes inside the graph is possible in tensorflow gradient flow can also be stopped for the first copy what is missing is to utilize the second node for gradient computation i dont know how i can archive this using the python api in tensorflow any ideas with this
239655317,11157,https://api.github.com/repos/tensorflow/tensorflow/issues/11157,mattfeel,12,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no using stock examples os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary pip tensorflow version use command below python version anaconda bit bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce im running the seqseq example in models/tutorials/rnn/translate verbatim.you can collect some of this information using our environment capture script system information w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.wrote environment to tf_env.txt you can review the contents of that file.and use it to populate the fields in the github issue template.cat tf_env.txt cat etc/issue linux gcrgdl generic ubuntu smp mon jun utc x x x gnu/linuxversion lts xenial xerus)version_id=.version_codename=xenial are we in docker no compiler c ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose uname a linux gcrgdl generic ubuntu smp mon jun utc x x x gnu/linux check pips numpy numpydoc protobuf tensorflow check for virtualenv false tensorflow import tf.version tf.git_version v..--gcdfctf.compiler_version v..--gcdfcsanity check array dtype=int env ld_library_path usr/local/cuda:/usr/local/cuda/lib:dyld_library_path is unset nvidia-smi thu jun nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m tesla km off off n/a c p w w mib mib default processes gpu memory gpu pid type process name usage no running processes found cuda libs usr/local/cuda-./lib/libcudart.so.../usr/local/cuda-./lib/libcudart_static.a/usr/local/cuda-./doc/man/man/libcudart./usr/local/cuda-./doc/man/man/libcudart.so.you can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.i get exception typeerror cant pickle thread.lock objects it happens on different machines with the same python version just running your example code verbatim source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem.traceback most recent call last file translate.py line in module tf.app.run file home/t-mabruc/anaconda/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file translate.py line in main train file translate.py line in train model create_model(sess false file translate.py line in create_model dtype=dtype file home/t-mabruc/models/tutorials/rnn/translate/seqseq_model.py line in init softmax_loss_function=softmax_loss_function file home/t-mabruc/anaconda/lib/python./site-packages/tensorflow/contrib/legacy_seqseq/python/ops/seqseq.py line in model_with_buckets decoder_inputs :bucket file home/t-mabruc/models/tutorials/rnn/translate/seqseq_model.py line in lambda lambda x y seqseq_f(x y false file home/t-mabruc/models/tutorials/rnn/translate/seqseq_model.py line in seqseq_f dtype=dtype file home/t-mabruc/anaconda/lib/python./site-packages/tensorflow/contrib/legacy_seqseq/python/ops/seqseq.py line in embedding_attention_seqseq encoder_cell copy.deepcopy(cell file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y copier(memo file home/t-mabruc/anaconda/lib/python./site-packages/tensorflow/python/layers/base.py line in deepcopy setattr(result k copy.deepcopy(v memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y copier(x memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy_list append(deepcopy(a memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y reconstruct(x memo rv file home/t-mabruc/anaconda/lib/python./copy.py line in reconstruct state deepcopy(state memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y copier(x memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy_dict y deepcopy(key memo deepcopy(value memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y reconstruct(x memo rv file home/t-mabruc/anaconda/lib/python./copy.py line in reconstruct state deepcopy(state memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y copier(x memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy_dict y deepcopy(key memo deepcopy(value memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y reconstruct(x memo rv file home/t-mabruc/anaconda/lib/python./copy.py line in reconstruct state deepcopy(state memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y copier(x memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy_dict y deepcopy(key memo deepcopy(value memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y reconstruct(x memo rv file home/t-mabruc/anaconda/lib/python./copy.py line in reconstruct state deepcopy(state memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy y copier(x memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy_dict y deepcopy(key memo deepcopy(value memo file home/t-mabruc/anaconda/lib/python./copy.py line in deepcopy rv reductor()typeerror cant pickle thread.lock objects
239511194,11141,https://api.github.com/repos/tensorflow/tensorflow/issues/11141,cryptox31,0,0,0,0,1,0,bazel-bin/inception/imagenet_train num_gpus batch_size train_dir=/tmp data_dir=/root/kits/datasettraceback most recent call last file root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py line in module tf.app.run file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py line in main inception_train.train(dataset file root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py line in train num_preprocess_threads=num_preprocess_threads file root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py line in distorted_inputs num_readers=flags.num_readers file root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py line in batch_inputs example_serialized file root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py line in parse_example_proto bbox tf.concat ymin xmin ymax xmax file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/ops/array_ops.py line in concat dtype=dtypes.int).get_shape file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor as_ref=false file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/framework/ops.py line in internal_convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto assertcompatible(values dtype file root/anaconda/envs/tensor/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in assertcompatible dtype.name repr(mismatch type(mismatch).__name__))typeerror expected int got list containing tensors of type message instead.using tensorflow inside anacondapython version machine power os ubuntu can someone tell me why am i getting this error
239426477,11137,https://api.github.com/repos/tensorflow/tensorflow/issues/11137,sddi,1,0,0,0,0,0,cd tensorflow/tensorflow/examples/learn/ python wide_n_deep_tutorial.py it shows that traceback most recent call last file wide_n_deep_tutorial.py line in module gender tf.feature_column.categorical_column_with_vocabulary_list(attributeerror module object has no attribute feature_columnmy tensorflow is rc and do i need to upgrade?thank you
238965953,11089,https://api.github.com/repos/tensorflow/tensorflow/issues/11089,yongtang,5,0,0,0,0,0,this is for s filesystem support in tensorflow it utilizes tensorflows filesystem c interface similar to currently available hdfs or gcs support the code depends on awss c sdk apache license code is placed under the directory of tensorflow/contrib/stodo list x add the implementation of s file system x work out the build configuration currently build a so not sure the best way in bazel x add tests similar to gcs_file_system_test.cc this fix is related to
238907490,11085,https://api.github.com/repos/tensorflow/tensorflow/issues/11085,Dakkerad,19,0,0,0,0,0,hello!i have seen and read some requests for opencl support and gpu support on mac os this seems to have been abandoned am i correct?but it also seems like apple is really trying to make metal big is this something you have thought of implementing i understand its not just made by thinking of it but i would just like to know if any progress is made with gpu support for mac os
238777088,11080,https://api.github.com/repos/tensorflow/tensorflow/issues/11080,huyong1109,1,0,0,0,0,0,a delete method in contrib.lookup.mutablehashtable would be useful when there are too many key value pairs and we can only handle the recent ones is there any possibility that the delete method will be implemented thanks vrv ebrevdo
238767818,11078,https://api.github.com/repos/tensorflow/tensorflow/issues/11078,ckalas,1,0,0,0,0,0,i cannot run any of the examples from the new object detection api i followed the documented prepare inputs and run locally but when i run the below command i get the following error. python train.py pipeline_config_path=/home/chris/tensorflow/models/object_detection/samples/configs/ssd_inception_v_pets.config train_dir=. traceback most recent call last file train.py line in module tf.app.run file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file train.py line in main worker_job_name is_chief flags.train_dir file home/chris/tensorflow/models/object_detection/trainer.py line in train data_augmentation_options file home/chris/tensorflow/models/object_detection/trainer.py line in create_input_queue prefetch_queue_capacity=prefetch_queue_capacity file home/chris/tensorflow/models/object_detection/core/batcher.py line in init num_threads=num_batch_queue_threads file usr/local/lib/python./dist-packages/tensorflow/python/training/input.py line in batch name=name file usr/local/lib/python./dist-packages/tensorflow/python/training/input.py line in batch tensor_list as_tensor_list(tensors file usr/local/lib/python./dist-packages/tensorflow/python/training/input.py line in as_tensor_list return tensors k for k in sorted(tensors) typeerror unorderable types tuple str() i get the feeling this error is because it isnt processing the record file correctly or something along those lines is wrong however as far as i can tell i followed everything correctly i edited the paths in the config file and triple checked that they point to real files
238730773,11073,https://api.github.com/repos/tensorflow/tensorflow/issues/11073,TethysSun,1,0,0,0,0,0,im having a problem when building tensorflow from source on mac os: tensorflow bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkgmon jun mdt using tmpdir var/folders//qlvhncymyvgvflwgn/t/tmp.xxxxxxxxxx.xadnti~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles tensorflow~/tensorflow/var/folders//qlvhncymyvgvflwgn/t/tmp.xxxxxxxxxx.xadnti tensorflowmon jun mdt building wheelusage setup.py global_opts cmd cmd_opts cmd cmd_opts or setup.py help cmd cmd or setup.py help-commands or setup.py cmd helperror invalid command bdist_wheel my python version: >python versionpython python versionpython rc>pip listbackports.weakref rc)bleach htmllib markdown numpy pip protobuf setuptools six tensorflow virtualenv werkzeug wheel i tried pip install wheel pip install wheel pyenv global which didnt work. >pip install wheelrequirement already satisfied wheel in library/frameworks/python.framework/versions/./lib/python./site-packages>pyenv virtualenv requirement already satisfied setuptools in users/jason/.pyenv/versions/../envs/../lib/python./site-packagesrequirement already satisfied pip in users/jason/.pyenv/versions/../envs/../lib/python./site-packages>pyenv virtualenv tensorflowpyenv-virtualenv users/jason/.pyenv/versions/tensorflow already exists. does anyone encounter this problem or know how to fix it?thank you
238721760,11071,https://api.github.com/repos/tensorflow/tensorflow/issues/11071,hellolovetiger,1,0,0,0,0,0,seems distributed tensorflow cannot train graph with two optimizers in sync mode one for local update the other for ps update)there are three parts in my graph each worker has its own copy of vars as the ps server but are defined with local variable collections= tf.graphkeys.local_variables each worker has its own forward-backward loop based on the local vars and its own optimizer local variable ps variable as the gradient and apply to ps variable with syncreplicasoptimizer.apply_gradients broadcast the ps variable to the local variablethe three parts are run in this way run subgraph several times then run subgraph in distributed sync mode to update ps params and then run subgraph source code logs if args.job_name ps server.join elif args.job_name worker is_chief args.task_index num_gpus len(worker_spec ps_device job:ps/cpu worker_device job:worker/task:%d/gpu args.task_index with tf.device tf.train.replica_device_setter(cluster=cluster ps_device=ps_device worker_device=worker_device global_step tf.variable(args.start_step name=global_step trainable=false print building ps params ps_tparams init_tparams print building local params with tf.device(worker_device worker_tparams init_tparams(is_local=true define variable in collection tf.graphkeys.local_variables print building graph print local update x x_mask y y_mask cost build_graph(worker_tparams config opt tf.train.momentumoptimizer(config.lr config.mr updates worker_tparams grads tf.gradients(cost updates colocate_gradients_with_ops=true clipped_grads tf.clip_by_global_norm(grads config.clip_grads train_op opt.apply_gradients(zip(clipped_grads updates print reduce average ps_updates ps_tparams avg_grads tf.sub(var ps_var for var ps_var in zip(updates ps_updates bopt tf.train.momentumoptimizer(config.blr config.bmr use_nesterov=true bopt tf.train.syncreplicasoptimizerv(bopt replicas_to_aggregate=num_gpus total_num_replicas=num_gpus update_op bopt.apply_gradients(zip(avg_grads ps_updates global_step=global_step print broadcast broadcast_ops for kk pp in ps_tparams.items broadcast_ops.append(worker_tparams kk .assign(pp).op others related to sync mode chief_queue_runner bopt.get_chief_queue_runner sync_init_op bopt.get_init_tokens_op sv tf.train.supervisor is_chief=is_chief logdir=config.ckp init_op=tf.global_variables_initializer local_init_op=tf.local_variables_initializer global_step=global_step if is_chief print(worker d initializing session args.task_index else print(worker d waiting for session to be initialized args.task_index sess_config tf.configproto(allow_soft_placement=true log_device_placement=false device_filters= /job:ps job:worker/task:%d args.task_index with sv.managed_session(server.target config=sess_config as sess print(worker d session initialization completed args.task_index if is_chief chief worker will start the chief queue runner and call the init op sess.run(sync_init_op sv.start_queue_runners(sess chief_queue_runner ) the non-chief works stuck at sv.managed_session and showing below message again and again: i tensorflow/core/distributed_runtime/master_session.cc start master session adececad with config: the code can run successfully when there is no local optimizer system information linux ubuntu cuda tf rc gpu geforce gtx
238616472,11065,https://api.github.com/repos/tensorflow/tensorflow/issues/11065,deep-r,1,0,0,0,0,0,i am trying to install tensorflow on windows for python bit it says successfully installed but when i test it using import tensorflow as tf command in the command prompt it shows a long error saying dll load failed and failed to load native tensorflow runtime.i had tried installing it earlier but it didnt work when i uninstalled from cmd pip uninstall tensorflow it said successfully uninstalled but when i try reinstalling it says requirement already satisfied and gives the same long error dll load failed etc when i type import tensorflow as tf this doesnt make sense please help.please see images.! image
238389793,11043,https://api.github.com/repos/tensorflow/tensorflow/issues/11043,eugeny-stoyka,1,0,0,0,0,0,i configured project for building and execute next command in project directory: bazel build c opt config=cuda tensorflow/cc:tutorials_example_trainer after that i get this error error home/eugeny/git/tensorflow/tensorflow/core/kernels/build c compilation of rule tensorflow/core/kernels:multinomial_op_gpu failed clang failed error executing command usr/bin/clang md mf bazel-out/local_linux-py-opt/bin/tensorflow/core/kernels/_objs/multinomial_op_gpu/tensorflow/core/kernels/multinomial_op_gpu.cu.d remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status clang error unknown argument nvcc_options=relaxed-constexpr clang error unknown argument nvcc_options=ftz=true clang error cannot find libdevice for sm provide path to different cuda installation via cuda-path or pass nocudalib to build without linking with libdevice clang error cannot find cuda installation provide its path via cuda-path or pass nocudainc to build without cuda includes clang error cannot find libdevice for sm provide path to different cuda installation via cuda-path or pass nocudalib to build without linking with libdevice clang error cannot find cuda installation provide its path via cuda-path or pass nocudainc to build without cuda includes clang error cannot find cuda installation provide its path via cuda-path or pass nocudainc to build without cuda includes target tensorflow/cc:tutorials_example_trainer failed to build im trying to build it with gpu support by using next libs:bazel gcc cuda version of cuda/bin/gcc is cudnn protobuf what shall i change for normal building
238359692,11039,https://api.github.com/repos/tensorflow/tensorflow/issues/11039,bmabey,2,0,0,0,0,0,when tensor dicts have heterogeneous key types python blows up due to the way sorted is being used.in python you can sort dicts with heterogeneous types: python in d z a b in sorted(d out z a b) in python you get an error: python in d z a b in sorted(d typeerror traceback most recent call last ipython-input--d in module sorted(d) btw i ran into this issue when using the train script in the new object detection model/api
238203839,11017,https://api.github.com/repos/tensorflow/tensorflow/issues/11017,rubenvereecken,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux mint tensorflow installed from source or binary binary pip tensorflow version use command below v..-rc--gfd bazel version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce n/a describe the problemthe tensorflow debugger does not seem to be working with queues data never seems to be fetched by the queuerunner threads be it from a file using tf.tfrecordreader and tf.parse_single_example or preloaded using tf.train.slice_input_producer instead the coordinator.should_stop is true right away this is only the case after wrapping the session in a tf.python.debug.localclidebugwrappersession the example should make things clearer.moreover another error occurs at coordinator.join(threads) .i am aware of the faq entry on threads but that does not explain why the data fetching threads would not be working source code logsto make it easiest to replicate i simply took the example on working with preloaded data and wrapped the session in there with the debugger i uploaded the gist with two lines added to reproduce run the file once you drop in the debugger run once it then exits the full output is below: extracting tmp/data/train-images-idx-ubyte.gzextracting tmp/data/train-labels-idx-ubyte.gzextracting tmp/data/tk-images-idx-ubyte.gzextracting tmp/data/tk-labels-idx-ubyte.gztraceback most recent call last file ex.py line in module tf.app.run(main=main argv= sys.argv unparsed file home/ruben/anaconda/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file ex.py line in main run_training file ex.py line in run_training coord.join(threads file home/ruben/anaconda/lib/python./site-packages/tensorflow/python/training/coordinator.py line in join six.reraise(*self._exc_info_to_raise file home/ruben/anaconda/lib/python./site-packages/six.py line in reraise raise value file home/ruben/anaconda/lib/python./site-packages/tensorflow/python/training/queue_runner_impl.py line in run enqueue_callable sess.make_callable(enqueue_op)attributeerror localclidebugwrappersession object has no attribute make_callable the stacktrace is about coord.join(threads but this is only possible because coord.should_stop never seems to be false which would indicate there is data to load without the added debugger lines the example simply works
237840277,10984,https://api.github.com/repos/tensorflow/tensorflow/issues/10984,serchsm,2,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows intel core i-u tensorflow installed from source or binary source tensorflow version use command below) :..-rc bazel version if compiling from source) :no cuda/cudnn version :no gpu model and memory :no exact command to reproduce set up toolchain for for bit vcvarsall amd invoked cmake c:\projects\tensorflow\tensorflow\contrib\cmake\build>cmake a x dcmake_build_type=release dswig_executable=c:/tools/swigwin-..\swigwin-../swig.exe dpython_executable=c:\users\sergio.murillo\appdata\local\programs\python\python/python.exe dpython_libraries=c:\users\sergio.murillo\appdata\local\programs\python\python\libs\python.lib dtensorflow_win_cpu_simd_options=/arch:avx to build the pip package msbuild p:configuration=release filelogger tf_python_build_pip_package.vcxproj you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemim opening a new issue as suggested in issue to track avx support on windows.i followed the instructions to built tensorflow on windows using cmake and wanted to enable avx but when it was time to build with msbuild it returned errors all similar to this: c:\projects\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj default target clcompile target c:\projects\tensorflow\third_party\eigen\unsupported\eigen\cxx\src\fixedpoint\packetmathavx.h error c mm_extract_epi identifier not found compiling source file c:\projects\tensorflow\tensorflow\core\framework\allocator_registry.cc c:\projects\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj c:\projects\tensorflow\third_party\eigen\unsupported\eigen\cxx\src\fixedpoint\packetmathavx.h error c mm_extract_epi identifier not found compiling source file c:\projects\tensorflow\tensorflow\core\framework\allocator_registry.cc c:\projects\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj all errors with the same code c identifier not found regarding mm_extract_epi and mm_extract_epi i do have immintrin.h in c:\program files x)\microsoft visual studio vc\include but mm_extract_epi and mm_extract_epi are not defined in that file source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
237813785,10979,https://api.github.com/repos/tensorflow/tensorflow/issues/10979,Santanu3dutta,1,0,0,0,0,0,os windows python version:.. installed the cpu-only version of tensorflow and via native pip...not anacondasuccesfully installed tensorflow:but while importing tensorflow below error is coming import tensorflow as tftraceback most recent call last file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\santanu\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\santanu\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file pyshell line in module import tensorflow as tf file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\santanu\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\santanu\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\santanu\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help.how to resolve this
237800172,10977,https://api.github.com/repos/tensorflow/tensorflow/issues/10977,shriyanka,6,0,0,0,0,0,"hello,recently i trained a model using tf.nn.max_pool_with_argmax on gpu and its working fine on gpu i wanted to use the model on cpu but it seems that its not supported on cpu how can i use this on cpu?will there be any support for it in near future or any suggestions on how to use this on cpu would be great.thanks"
237773120,10972,https://api.github.com/repos/tensorflow/tensorflow/issues/10972,hholst80,3,0,0,0,0,0,the following works and creates a tf.text.summary which i can find via tensorboard: import tensorflow as tfsess tf.interactivesession()summary_op tf.summary.text(config/config tf.convert_to_tensor(hello world))summary_writer tf.summary.filewriter(/tmp/tensorboard sess.graph)text sess.run(summary_op)summary_writer.add_summary(text summary_writer.add_summary(text summary_writer.add_summary(text summary_writer.flush()summary_writer.close() ! image we change the order of the filewriter and the summary_op above it does not log anything: import tensorflow as tfsess tf.interactivesession()summary_writer tf.summary.filewriter(/tmp/tensorboard sess.graph)summary_op tf.summary.text(config/config tf.convert_to_tensor(hello world))text sess.run(summary_op)summary_writer.add_summary(text summary_writer.add_summary(text summary_writer.add_summary(text summary_writer.flush()summary_writer.close() ! image
237136362,10843,https://api.github.com/repos/tensorflow/tensorflow/issues/10843,svenstaro,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu arch linux tensorflow installed from source or binary trying to compile from source tensorflow version use command below release bazel version if compiling from source cuda/cudnn version gpu model and memory ti gb exact command to reproduce the pkgbuild you are not familiar with this format basically just execute prepare and then build describe the problemim the packager for the official tensorflow package in arch linux and i was trying to update to and it fails to build full build log attached we always build in clean chroots.llvm gcc source code logs log.txt
237132164,10842,https://api.github.com/repos/tensorflow/tensorflow/issues/10842,danielwatson6,1,0,0,0,0,0,currently the best results of the luong attention paper minh-thang luong hieu pham christopher d manning effective approaches to attention-based neural machine translation emnlp cannot be reproduced with the implementation of tf.contrib.seqseq.luongattention features that are missing local attention attend to a window of time steps rather than to all of the time steps of the encoder output global attention the window size should be a hyperparameter that the user can tune different scoring functions currently the scoring function is limited to a dot products between each encoder output and the decoder output the paper shows better results with general scoring all of the encoder outputs are multiplied by one learnable matrix and also explores the option of using bahdanau-like scoring concatenate and multiply by a learnable matrix then apply tanh and take a dot product with a learnable vector predictive alignments while the probability function can be replaced it would be nice to add predictive alignment as a function and make the implementation of both monotonic and predictive alignments behave well with local attention limited to a time window changes shape of learnables input-feeding approach please correct me in this one if i am mistaken the current implementation is missing the final step that computes a prediction by concatenating the context vector with the decoder output weights them and applies tanh let this be s_t=tanh(w c_t h_t passing s_t through a softmax layer gives the prediction distribution but passing it as is to the next input improved performance in the paper.on a sidenote it also seems to be that there is no difference in the key and query vectors between the bahdanau and luong attention mechanisms when there should be bahdanau attention has a computation pathway starting from the previous decoder output h_{t a_t c_t h_t while luong attention starts from the current output h_t a_t c_t s_t
237054570,10838,https://api.github.com/repos/tensorflow/tensorflow/issues/10838,Iolaum,0,0,3,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu kernel tensorflow installed from source or binary compiling tf from source tensorflow version use command below bazel version if compiling from source cuda/cudnn version cuda cudnn gpu model and memory ti gb notebook version intel i-hq cpu exact command to reproduce compiling from source following documentation describe the problemi compiled tensorflow from source the process finished successfully and the binary managed to install and run successfully what seems strange is that during the process i got k lines of warnings i am linking to them at the end of the issue i am wondering if thats expected behavior or indication of a small or not_so_small problem.one thing that may affect this is bazel installation i followed bazel installation instructions and used the recommended apt method this led to me running into this issue installing openjdk--jdk on top of the ibm-java-jdk as suggested in a comment solves the problem although i am not sure how much technical debt this solution caries which may have manifested in some of the warnings produced during compilation source code logs configuration script options: {shell configureplease specify the location of python default is usr/bin/python usr/bin/pythonfound possible python library paths usr/local/lib/python./dist-packages usr/lib/python/dist-packagesplease input the desired python library path to use default is usr/local/lib/python./dist-packages /usr/local/lib/python./dist-packagesdo you wish to build tensorflow with mkl support y/n ymkl support will be enabled for tensorflowdo you wish to download mkl lib from the web y/n yplease specify optimization flags to use during compilation when bazel option config=opt is specified default is march=native do you wish to use jemalloc as the malloc implementation y/n yjemalloc enableddo you wish to build tensorflow with google cloud platform support y/n nno google cloud platform support will be enabled for tensorflowdo you wish to build tensorflow with hadoop file system support y/n nno hadoop file system support will be enabled for tensorflowdo you wish to build tensorflow with the xla just-in-time compiler experimental y/n nno xla jit support will be enabled for tensorflowdo you wish to build tensorflow with verbs support y/n nno verbs support will be enabled for tensorflowdo you wish to build tensorflow with opencl support y/n nno opencl support will be enabled for tensorflowdo you wish to build tensorflow with cuda support y/n ycuda support will be enabled for tensorflowdo you want to use clang as cuda compiler y/n nnvcc will be used as cuda compilerplease specify the cuda sdk version you want to use e.g leave empty to use system default please specify the location where cuda toolkit is installed refer to readme.md for more details default is usr/local/cuda please specify which gcc should be used by nvcc as the host compiler default is usr/bin/gcc please specify the cudnn version you want to use leave empty to use system default please specify the location where cudnn library is installed refer to readme.md for more details default is usr/local/cuda please specify a list of comma-separated cuda compute capabilities you want to build with.you can find the compute capability of your device at note that each additional compute capability significantly increases your build time and binary size. default is info starting clean this may take a while consider using async if the clean takes more than several minutes.configuration finished console output of compilation command the output is too big to be placed within the issue i ve put it in its own repository
237004841,10834,https://api.github.com/repos/tensorflow/tensorflow/issues/10834,SivaMohanRanga,0,1,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out it shouldnt be a tensorboard issue those go here why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
237002106,10833,https://api.github.com/repos/tensorflow/tensorflow/issues/10833,decentralion,1,0,0,0,0,0,tensorboard can now be found in its own repo
236630559,10788,https://api.github.com/repos/tensorflow/tensorflow/issues/10788,czarina,1,0,0,0,0,0,"hi,im trying to follow your ios guide in the readme which tells me to use tensorflow/contrib/ios_examples but this folder is completely missing.can anyone advise"
236584546,10779,https://api.github.com/repos/tensorflow/tensorflow/issues/10779,paulcwatts,8,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu mac os x tensorflow installed from source or binary binary tensorflow version use command below v..-rc--gfd bazel version if compiling from source n/a cuda/cudnn version none gpu model and memory n/a exact command to reproduce python versionpython python m virtualenv venvusing base prefix library/frameworks/python.framework/versions/.new python executable in venv/bin/pythonalso creating executable in venv/bin/pythoninstalling setuptools pip wheel...done source venv/bin/activate pip install tensorflowcollecting tensorflow using cached tensorflow-..-cp-cpm-macosx___x_.whl pythonpython v..:cdb mar gcc apple inc build dot on darwintype help copyright credits or license for more information from tensorflow.examples.tutorials.mnist import input_data mnist input_data.read_data_sets(/tmp/data/)traceback most recent call last file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in do_open encode_chunked=req.has_header(transfer-encoding file library/frameworks/python.framework/versions/./lib/python./http/client.py line in request self._send_request(method url body headers encode_chunked file library/frameworks/python.framework/versions/./lib/python./http/client.py line in send_request self.endheaders(body encode_chunked=encode_chunked file library/frameworks/python.framework/versions/./lib/python./http/client.py line in endheaders self._send_output(message_body encode_chunked=encode_chunked file library/frameworks/python.framework/versions/./lib/python./http/client.py line in send_output self.send(msg file library/frameworks/python.framework/versions/./lib/python./http/client.py line in send self.connect file library/frameworks/python.framework/versions/./lib/python./http/client.py line in connect server_hostname=server_hostname file library/frameworks/python.framework/versions/./lib/python./ssl.py line in wrap_socket context=self session=session file library/frameworks/python.framework/versions/./lib/python./ssl.py line in init self.do_handshake file library/frameworks/python.framework/versions/./lib/python./ssl.py line in do_handshake self._sslobj.do_handshake file library/frameworks/python.framework/versions/./lib/python./ssl.py line in do_handshake self._sslobj.do_handshake()ssl.sslerror ssl certificate_verify_failed certificate verify failed ssl.c:)during handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file venv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py line in read_data_sets source_url train_images file venv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py line in maybe_download temp_file_name urlretrieve_with_retry(source_url file venv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py line in wrapped_fn return fn(*args kwargs file venv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py line in urlretrieve_with_retry return urllib.request.urlretrieve(url filename file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in urlretrieve with contextlib.closing(urlopen(url data as fp file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in urlopen return opener.open(url data timeout file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in open response self._open(req data file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in open open req file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in call_chain result func(*args file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in https_open context=self._context check_hostname=self._check_hostname file library/frameworks/python.framework/versions/./lib/python./urllib/request.py line in do_open raise urlerror(err)urllib.error.urlerror urlopen error ssl certificate_verify_failed certificate verify failed ssl.c:)> this doesnt reproduce with tensorflow
236454067,10761,https://api.github.com/repos/tensorflow/tensorflow/issues/10761,nicaogr,9,0,0,0,0,1,could you add an equivalent to numpy roll on tensor in tensorflow in order to allow the user to roll a tensor along one of the axis of the tensor
236351269,10749,https://api.github.com/repos/tensorflow/tensorflow/issues/10749,Sushobhan04,1,0,0,0,0,0,as mentioned in the issue the tf.fftd gives different result compared to np.fft.fft is there a reason for this note numpy gives proper fourier transform after np.fft.fftshift and i have taken care of that in my code.the differences are not visible here but the mean squared error is significant.! image image image image is the fft using tensorflow and third one is using numpy you can see the difference in the corners the fourth image is the difference between the two images times tf has some features while numpy does not)i am working on an application which uses fft in backpropagation and thus it is of absolute importance that the fft in numpy are same as fft by tf.my question is why is there a difference and how can i get the same fft as numpy
236348366,10748,https://api.github.com/repos/tensorflow/tensorflow/issues/10748,sampepose,1,0,0,0,0,0,progress on this issue
236314785,10744,https://api.github.com/repos/tensorflow/tensorflow/issues/10744,chenliu0831,1,0,0,0,0,0,this markdown version lock could cause versioning conflict for downstream e.g if requirements are compiled and resolved by pip-compile wondering if theres a reason to lock this version and its already not sync with the ci install script those upgrade with locked version also doesnt seem right to me
235894943,10703,https://api.github.com/repos/tensorflow/tensorflow/issues/10703,adityaatluri,0,0,0,0,0,3,"hi,i am trying to add new backend to tensorflow as a first step i started changing bazel files around commit here when i enable xla rocm during configure and run bazel build s config=opt config=rocm tensorflow/tools/pip_package:build_pip_package i am getting the following error: error no such package local_config_rocm error loading package external the repository named local_config_rocm could not be resolved.info elapsed time s it would be great if someone can parse the commit mentioned and suggest changes thank you"
235614018,10684,https://api.github.com/repos/tensorflow/tensorflow/issues/10684,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in where import numpy as npfrom tensorflow.python.ops import math_opsimport tensorflow as tflabel tf.constant(np.array dtype=tf.bool)math_ops.equal(label will raise an exception of type mismatch.this fix updated the assertcompatible so that is compatible with tf.bool types.additional test cases have been added.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
235537334,10678,https://api.github.com/repos/tensorflow/tensorflow/issues/10678,Moriadry,0,0,1,0,0,0,here are native methods to support creating string tensors by string bytes the implement of tensor class and test will be soon available
235522480,10676,https://api.github.com/repos/tensorflow/tensorflow/issues/10676,Tanno0518,0,0,1,0,0,0,"hello,i am evaluating the dsp process time by the bellows source code the result was short process time.as one factor i think that the reason is quantization to input data of dsp.could you please tell me a change procedure for quantization and could i please have the pb file(not quantizate before the bellows quantization pb file tensorflow_inception_v_stripped_optimized_quantized.pb i want to compare between dsp process time(quantizate input data and cpu process time(not quantizate input data"
235288382,10655,https://api.github.com/repos/tensorflow/tensorflow/issues/10655,mebersole,4,0,0,0,0,0,would like to add a command-line argument that allows tensorboard to run at a different url location than the root domain so for example command-line base_url runhere url location for request there are locations where only minimal ports are open and it would be great to use nginx or similar to route tensorboard through port
235270726,10652,https://api.github.com/repos/tensorflow/tensorflow/issues/10652,olyaromanyuk,1,0,0,0,0,0,system information custom code a minimal reproducible example provided below linux ubuntu tensorflow installed from binary using pip tensorflow version and cuda cudnn geforce gtx problemvariable scopes and sharing works different in versions and i try to enter a non-reusing variable scope after executing tf.get_variable_scope().reuse_variables this results in different values of tf.get_variable_scope().reuse inside this scope when using version it is false while when using it is true the sharing variable guide states that setting reuse false inside a reusing scope is not the desired behavior but the guide is completely the same for both versions moreover no changes in handling variable scopes are mentioned in release notes for version source code logs import tensorflow as tfprint tf.__version__with tf.variable_scope(foo assert tf.get_variable_scope().reuse false tf.get_variable_scope().reuse tf.get_variable_scope().reuse_variables assert tf.get_variable_scope().reuse true tf.get_variable_scope().reuse with tf.variable_scope(tf.get_variable_scope reuse=false print tf.get_variable_scope().reuse output version false version true
235239308,10650,https://api.github.com/repos/tensorflow/tensorflow/issues/10650,artcg,2,0,0,0,0,0,minimal code to reproduce import tensorflow as tft tf.constant d tf.dimension()t tf.reshape(t d ) gives stack trace: typeerror expected binary or unicode string got the reason this would be useful to allow is because when you access tensor shapes with e.g t.shape you get it in tf.dimension so if you want to assign relative to the current shape then you dont need to convert to intif there is some reason why tensorflow doesnt allow this behaviour than i think at least the stack trace should be more verbose e.g in the example above it shouldnt be complaining about it should be complaining about the value in tf.dimension class
235093281,10638,https://api.github.com/repos/tensorflow/tensorflow/issues/10638,liu115,1,0,0,0,0,0,hi i am using the embedding_attention_seqseq with output_projection the document from says outputs a list of the same length as decoder_inputs of d tensors with shape batch_size x num_decoder_symbols containing the generated outputs.but the output seems to be the output before projection when i used it so i go through the source code from embedding_attention_seqseq is base on the embedding_attention_decoder and attention_decoder it has to give output_size to attention_decoder but output_size is set to none when output_projection is not none. pythondef embedding_attention_seqseq(encoder_inputs decoder_inputs cell num_encoder_symbols num_decoder_symbols embedding_size num_heads output_projection=none feed_previous=false dtype=none scope=none initial_state_attention=false skip output_size none if output_projection is none cell core_rnn_cell.outputprojectionwrapper(cell num_decoder_symbols output_size num_decoder_symbols if isinstance(feed_previous bool return embedding_attention_decoder decoder_inputs encoder_state attention_states cell num_decoder_symbols embedding_size num_heads=num_heads output_size=output_size output_projection=output_projection feed_previous=feed_previous initial_state_attention=initial_state_attention) when output_size is none the output_size is simply the cells output_size and so the shape of output for embedding_attention_seqseq will be batch_size x cells output_size rather than batch_size x num_decoder_symbols pythondef attention_decoder(decoder_inputs initial_state attention_states cell output_size=none num_heads loop_function=none dtype=none scope=none initial_state_attention=false skip if output_size is none output_size cell.output_size skip with variable_scope.variable_scope(attnoutputprojection output linear( cell_output attns output_size true if loop_function is not none prev output outputs.append(output return outputs state thanks
235078819,10635,https://api.github.com/repos/tensorflow/tensorflow/issues/10635,Moriadry,0,0,1,0,0,0,some spelling mistakes
235063324,10631,https://api.github.com/repos/tensorflow/tensorflow/issues/10631,mmdlearn,1,0,0,0,0,0,ps c:\windows\system cd tensorflowps c:\windows\system\tensorflow bazel build c opt copt=-mavx copt=-mavx copt=-mfma copt=-mfpmath=both copt=-msse copt=-msse copt=-msse copt=-msse copt=-msse tensorflow/tools/pip_package:build_pip_package>>.....................error c:/windows/system/tensorflow/tensorflow/tools/pip_package/build error loading package tensorflow/core:encountered error while reading extension file protobuf.bzl no such package protobuf traceback most recent call last file c:/windows/system/tensorflow/tensorflow/workspace.bzl line apply_patch(repo_ctx repo_ctx.attr.patch_file file c:/windows/system/tensorflow/tensorflow/workspace.bzl line in apply_patch execute_and_check_ret_code(repo_ctx cmd file c:/windows/system/tensorflow/tensorflow/workspace.bzl line in execute_and_check_ret_code fail(non-zero return code when more arguments>))non-zero return code when executing c:\tools\msys\usr\bin\bash.exe c patch p d c:/users/godw/appdata/local/temp/_bazel_godw/nseddbsr/external/protobuf i c:/windows/system/tensorflow/third_party/protobuf/add_noinlines.patch:stdout:stderr usr/bin/bash patch command not found and referenced by tensorflow/tools/pip_package:included_headers_gather
235036539,10618,https://api.github.com/repos/tensorflow/tensorflow/issues/10618,waleedka,1,0,0,0,0,0,i use crop_and_resize in a standard way as such: pool tf.image.crop_and_resize(features boxes box_indicies pool_shape method=bilinear) and it works great then i tried to train on multiple gpus and i got this error: outofrangeerror box_index has values outside batch_size node tower_/mask_rcnn/roi_align/cropandresize cropandresize t=dt_float extrapolation_value method=bilinear device=/job:localhost/replica:/task:/gpu: (tower_/mask_rcnn/activation_/relu tower_/mask_rcnn/roi_align/stopgradient tower_/mask_rcnn/roi_align/stopgradient tower_/mask_rcnn/roi_align/cropandresize/crop_size node tower_/mask_rcnn/mrcnn_mask/reshape recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__tower_/mask_rcnn/mrcnn_mask/reshape tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () i verified that box_index is within the correct range all zeros in my case because i use one image per batch and verified that all the other inputs to crop_and_resize look good i ended up spending almost two days checking and rechecking every line of my code to make sure i didnt make a mistake somewhere it drove me nuts finally i tried downgrading tf to and suddenly everything worked and to make sure this is the problem i upgraded to rc again and got the error again a few points that might help on it works whether i use gpu or gpus on rc it works on gpu but fails on gpus the way i do multi-gpu training is by running copies of my model one on each gpu shared weights my input batch size is and i split the inputs by the batch dimension and feed one sample to each gpu and then i concatenate the outputs to get a batch size of again and then apply the loss function if youre wondering why im using rc its because has an issue that causes batch normalization to run on cpu and it was fixed in rc system information ubuntu python tensorflow rc installed using sudo pip install upgrade on ec p.xlarge gpus
235008134,10612,https://api.github.com/repos/tensorflow/tensorflow/issues/10612,lakshayg,8,0,0,0,0,0,hochreiters group has recently come up with a new dropout technique and activation function in a recent paper presented experiments demonstrate that these lead to better learning in standard feed forward networks i would like to implement these components in tensorflow creating this issue to gauge the communitys interest level in such components feedback will be extremely helpful
234991150,10609,https://api.github.com/repos/tensorflow/tensorflow/issues/10609,Yc174,5,0,0,0,0,0,i trained faster-rcnn on coco dataset when iter it appears a problem:image coco_train_.jpg iter total loss rpn_loss_cls rpn_loss_box loss_cls loss_box lr speed s iter w tensorflow/core/framework/op_kernel.cc invalid argument reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack w tensorflow/core/framework/op_kernel.cc invalid argument reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack w tensorflow/core/framework/op_kernel.cc invalid argument reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack w tensorflow/core/framework/op_kernel.cc invalid argument reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack w tensorflow/core/framework/op_kernel.cc invalid argument reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack) traceback most recent call last):file home/yanchao/tffrcnn/faster_rcnn/train_net.py line in restore=bool(int(args.restore)))file home/yanchao/tffrcnn/faster_rcnn/../lib/fast_rcnn/train.py line in train_netsw.train_model(sess max_iters restore=restore)file home/yanchao/tffrcnn/faster_rcnn/../lib/fast_rcnn/train.py line in train_modelcls_prob bbox_pred rois sess.run(fetches=fetch_list feed_dict=feed_dict)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in runrun_metadata_ptr)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in runfeed_dict_string options run_metadata)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_runtarget_list options run_metadata)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_callraise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack) caused by op ugradients/topkv_grad/reshape defined at:file home/yanchao/tffrcnn/faster_rcnn/train_net.py line in restore=bool(int(args.restore)))file home/yanchao/tffrcnn/faster_rcnn/../lib/fast_rcnn/train.py line in train_netsw.train_model(sess max_iters restore=restore)file home/yanchao/tffrcnn/faster_rcnn/../lib/fast_rcnn/train.py line in train_modelgrads norm tf.clip_by_global_norm(tf.gradients(loss tvars file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in gradientsgrad_scope op func_call lambda grad_fn(op out_grads))file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in maybecompilereturn grad_fn exit earlyfile home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in grad_scope op func_call lambda grad_fn(op out_grads))file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/nn_grad.py line in topkgradind_d array_ops.reshape(op.outputs array_ops.stack ind_lastdim ))file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in reshapename=name)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_opop_def=op_def)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_oporiginal_op=self._default_original_op op_def=op_def)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in initself._traceback extract_stack()...which was originally created as op utopkv defined at:file home/yanchao/tffrcnn/faster_rcnn/train_net.py line in restore=bool(int(args.restore))) elided identical lines from previous traceback file home/yanchao/tffrcnn/faster_rcnn/../lib/fast_rcnn/train.py line in train_netsw.train_model(sess max_iters restore=restore)file home/yanchao/tffrcnn/faster_rcnn/../lib/fast_rcnn/train.py line in train_modelself.net.build_loss(ohem=cfg.train.ohem)file home/yanchao/tffrcnn/faster_rcnn/../lib/networks/network.py line in build_lossrpn_cross_entropy_n_neg tf.nn.top_k(rpn_cross_entropy_n_neg k=top_k)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/nn_ops.py line in top_kreturn gen_nn_ops._top_kv(input k=k sorted=sorted name=name)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in top_kvname=name)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_opop_def=op_def)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_oporiginal_op=self._default_original_op op_def=op_def)file home/yanchao/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in initself._traceback extract_stack()invalidargumenterror see above for traceback reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero node gradients/topkv_grad/reshape reshape t=dt_int tshape=dt_int device=/job:localhost/replica:/task:/gpu: (topkv gradients/topkv_grad/stack) process finished with exit code does anyone meet the same things or this is a bug of tensorflow its really a strange problem
234877498,10602,https://api.github.com/repos/tensorflow/tensorflow/issues/10602,edgimar,1,0,0,0,0,0,the way tensorboard is currently implemented seems to prevent one from selecting in a web browser the text of tf.summary.text summaries it would be nice if it were possible to copy/paste such text
234783535,10590,https://api.github.com/repos/tensorflow/tensorflow/issues/10590,prannayk,1,0,0,0,0,0,feature request to create an async embedding saving buffer which runs on the processor while the code trains on a gputhe aim would be make sure that training is not affected by saving the model that anyway takes a single thread therefore it is a process that is done using an async buffer it could optimize the training of models system information linux ubuntu installed from source with gpu implementation cuda cudnn gpu nividia titan xthere was a x slowdown to in line saving
234558219,10545,https://api.github.com/repos/tensorflow/tensorflow/issues/10545,gliboc,1,0,0,0,0,0,system informationnot relevant to issue describe the problemthe addresses provided for seeing the code in the code organization chapter of this tutorial are no longer valid i think the right place could be source code logsaddress of the tutorial the addresses provided for the code
234452158,10521,https://api.github.com/repos/tensorflow/tensorflow/issues/10521,meteorcloudy,2,0,0,0,0,0,from i see compiling tensorflow/core/kernels/conv_grad_ops_d.cc s standalone actions running compiling tensorflow/core/kernels/conv_grad_ops_d.cc s standalone actions running compiling tensorflow/core/kernels/conv_grad_ops_d.cc s standalone actions running compiling tensorflow/core/kernels/conv_grad_ops_d.cc s standalone actions running compiling tensorflow/core/kernels/conv_ops_d.cc s standalone as you can see it takes about half an hour to compile these two files.this has been a bottleneck of the tf windows bazel build time for a while.i guess its because we are compiling with o option so compiler was spending too much time on optimizing but is there anything we can do to optimize the build time
234405498,10520,https://api.github.com/repos/tensorflow/tensorflow/issues/10520,Warvito,10,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow) :no os platform and distribution e.g linux ubuntu linux ubuntu lts xenial xerus tensorflow installed from source or binary) :binary tensorflow version use command below) :..-rc bazel version if compiling from source cuda/cudnn version gpu model and memory :geforce gtx exact command to reproduce : import tensorflow as tfx_d tf.placeholder(tf.float,shape= none,,,, )conv_t tf.layers.convd_transpose(x_d describe the problemi am getting a typeerror when i use a placeholder with batch size as none as inputs to the convd_transpose this problem does not happen with the tf.layers.convd_transpose source code logs import tensorflow as tfx_d tf.placeholder(tf.float,shape= none,,, )convd_t tf.layers.convd_transpose(x_d,, , )x_d tf.placeholder(tf.float,shape= none,,,, )conv_t tf.layers.convd_transpose(x_d typeerror traceback most recent call last ipython-input--ebcae in module x_d tf.placeholder(tf.float,shape= none conv_t tf.layers.convd_transpose(x_d usr/local/lib/python./dist-packages/tensorflow/python/layers/convolutional.pyc in convd_transpose(inputs filters kernel_size strides padding data_format activation use_bias kernel_initializer bias_initializer kernel_regularizer bias_regularizer activity_regularizer trainable name reuse reuse=reuse scope=name return layer.apply(inputs usr/local/lib/python./dist-packages/tensorflow/python/layers/base.pyc in apply(self inputs args kwargs output tensor(s return self.__call__(inputs args kwargs def assert_input_compatibility(self inputs usr/local/lib/python./dist-packages/tensorflow/python/layers/base.pyc in call__(self inputs args kwargs check input assumptions set after layer building e.g input shape self._assert_input_compatibility(inputs outputs self.call(inputs args kwargs apply activity regularization usr/local/lib/python./dist-packages/tensorflow/python/layers/convolutional.pyc in call(self inputs outputs_d array_ops.reshape(outputs outputs_shape outputs_shape outputs_shape outputs_shape outputs_shape outputs_d nn.bias_add usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_array_ops.pyc in reshape(tensor shape name result op_def_lib.apply_op(reshape tensor=tensor shape=shape name=name return result usr/local/lib/python./dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self op_type_name name keywords except typeerror as err if dtype is none raise err else raise typeerror typeerror failed to convert object of type type list to tensor contents none consider casting elements to a supported type"
234295835,10498,https://api.github.com/repos/tensorflow/tensorflow/issues/10498,taion,4,0,0,0,0,0,ref system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary docker image tensorflow version use command below bazel version if compiling from source n/a cuda/cudnn version per docker image gpu model and memory k exact command to reproduce n/a describe the problemthe logging configuration in tf_logging is odd and inconsistent with how things are generally done in python.in general its typical to attach handlers to the root logger in python because tf_logging sets up its own stream handler and does not disable the propagate flag anybody who does follow the standard python convention of attaching handlers to the root logger gets duplicated log output from tensorflow.per while this isnt a bug per se its still wrong and undesirable.the cleanest way forward is probably to just set propagate=false on the tf logger
234100172,10482,https://api.github.com/repos/tensorflow/tensorflow/issues/10482,bowang,11,0,0,0,0,0,this pr implements the channel groups in convolutional layers convd convd convd convdtransposed).the grouped convolution was firstly implemented in alexnet as a way to share filter parameters across feature maps a detailed discussion on this feature can be found on here this feature is supported by caffe doc and used in its reference caffenet adding this feature to tensorflow makes it easier to compare models on two different frameworks and migrate from caffe to tensorflow
233553295,10436,https://api.github.com/repos/tensorflow/tensorflow/issues/10436,tillahoffmann,4,0,0,0,0,0,i am trying to compile tensorflow from source but cannot build master the build for the pip package fails with the following error message error users/till/git/tensorflow/tensorflow/tensorboard/components/vz_sorting/build compiling typescript files failed execrooter failed error executing command cd private/var/tmp/_bazel_till/ebccfeafed/execroot/tensorflow exec env bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter bazel-out/local-py-opt/bin/tensorflow/tensorboard/components/vz_sorting/vz_sorting-tsc-execroot.json com.google.devtools.build.lib.shell.badexitstatusexception process exited with status traceback most recent call last file bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter line in module main file bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter line in main raise assertionerror(could not find python binary python_binary)assertionerror could not find python binary python i am running mac osx bazel python in a conda virtual environment apple llvm version clang this might be related to suggestions would be greatly appreciated
233457737,10429,https://api.github.com/repos/tensorflow/tensorflow/issues/10429,cancan101,1,0,0,0,0,0,i suggest to separate tensorboard from tensorflow.this could mean break tensorboard to its own repos under the same organization publish a separate tensorboard wheel package on pypi the advantages of this are simpler building of tensorboard which will make it easier for non core tensorflow members to contribute to the development of the project tensorboard to operate on its own release cycle simpler installation of tensorboard it can be installed on its own without needing full install of tensorflow
233397506,10421,https://api.github.com/repos/tensorflow/tensorflow/issues/10421,orome,1,0,0,0,0,0,the heuristics used by tensorboard to determine which scopes are added to the main graph and which are not by default often results in scopes being included that obscure the underlying network structure.it would be nice to have an additional named parameter for tf.name_scope that indicated whether that scope should be removed or included in the main graph by default
233394072,10419,https://api.github.com/repos/tensorflow/tensorflow/issues/10419,ProGamerGov,1,0,0,0,0,0,the current deepdream guide located here uses the inceptionh model from what i can tell it does not appear very straightforward in terms of how to fine tune the model in order to create different deepdream hallucinations from a custom data set it also does not appear to be relatively easy to change the model that the guide uses i think that an additional guide which shows individuals how to fine tune a model for the purposes of deepdream would be useful for those trying to explore the artistic and visual aspects of tensorflow models i havent been able to find any guide for creating custom deepdream models in tensorflow so i am not sure where to start
233312556,10408,https://api.github.com/repos/tensorflow/tensorflow/issues/10408,Caselles,1,0,0,0,0,0,i have a memory leak with tensorflow i refered to to address my issue and i followed the advices of the answer that seemed to have solved the problem however it does not work here in order to recreate the memory leak i have created a simple example first i use this function that i got here to check the memory use of the python process def memory import os import psutil pid os.getpid py psutil.process(pid memoryuse py.memory_info memory use in gb...i think print(memory use memoryuse)then everytime i call the build_model function the use of memory increases.here is the build_model function that has a memory leak def build_model model tf.reset_default_graph with tf.graph().as_default tf.session as sess tf.contrib.keras.backend.set_session(sess labels tf.placeholder(tf.float shape=(none input tf.placeholder(tf.float shape=(none x tf.contrib.keras.layers.dense activation=relu name=dense)(input x tf.contrib.keras.layers.dropout(.)(x x tf.contrib.keras.layers.dense activation=relu name=dense)(x y tf.contrib.keras.layers.dense activation=sigmoid name=dense)(x loss tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(labels y train_step tf.train.adamoptimizer(.).minimize(loss initialize all variables init_op tf.global_variables_initializer sess.run(init_op sess.close tf.reset_default_graph return i would have thought that using the block with tf.graph().as_default tf.session as sess and then closing the session and calling tf.reset_default_graph would clear all the memory used by tensorflow apparently it does not.the memory leak can be recreated as following memory build_model memory build_model memory()the output of this is for my computer memory use memory use memory use clearly we can see that all the memory used by tensorflow is not freed afterwards why?i hope i made myself clear
233184561,10397,https://api.github.com/repos/tensorflow/tensorflow/issues/10397,drcrook1,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out. heres why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu tensorflow tensorflow installed from source or binary) :from docker image tensorflow/tensorflow tensorflow version use command below) :from docker image tensorflow/tensorflow bazel version if compiling from source) :n/a cuda/cudnn version :n/a gpu model and memory :n/a exact command to reproduce :deployed to a kubernetes cluster below is yaml file.apiversion vkind servicemetadata labels app tensorboard name tensorboardspec ports port targetport selector app tensorboard type loadbalancer---apiversion extensions/vbetakind deploymentmetadata labels app tensorboard name tensorboardspec template metadata labels app tensorboard containers name tensorboard command bin/sh c args tensorboard logdir tensorboard image tensorflow/tensorflow ports containerport you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.! tensorboard_regex_bug just simply is not working source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
232922000,10375,https://api.github.com/repos/tensorflow/tensorflow/issues/10375,danielegrattarola,7,0,0,0,0,0,"would it be possible to implement matrix multiplication between two sparsetensor currently its possible to multiply two dense tensor with many zeros using the x_is_sparse parameter of tf.matmul and a sparsetensor with a tensor using tf.sparse_tensor_dense_matmul however with some datasets it is impossible to store dense matrices in memory and it would be great if we could have the option of fully sparse multiplication.thanks,daniele"
232891556,10373,https://api.github.com/repos/tensorflow/tensorflow/issues/10373,fvisin,2,0,0,0,0,0,when working on dense prediction tasks it is very convenient to show the predictions of the network in tensorboard to evaluate them qualitatively during training even if tensorboard shows only a subset of these images all the summaries that have been saved since the beginning of times are kept on disk the occupied space grows with the size of the dataset the sampling frequency and the number of experiments often resulting in a huge waste of space when the disk space is limited this can heavily limit the number of experiments logs one can run simultaneously and/or keep stored.it would be great to have a way to either limit the number of images saved on disk or to remove some of them e.g for old experiments here is what i suggest add an argument to tf.summary.image to define how many images should be kept on disk e.g buffer_size or steps_to_retain also add an extra argument e.g retain_strategy to select the strategy to define which images should be kept e.g keep latest sample uniformly add a function to remove some of the stored images programmatically.this feature request is partially related to related to this so thread
232808675,10367,https://api.github.com/repos/tensorflow/tensorflow/issues/10367,danqing,1,0,0,0,0,0,system information no custom code ubuntu trying to compile tensorflow r from source bazel cuda cudnn k gpu gb memory describe the problemim trying to build tensorflow from source with cuda support my cuda and cudnn are installed successfully as i can run a tensorflow program fine with pre-built tensorflow gpu image).the command i used was: shbazel build config=opt config=cuda tensorflow/tools/pip_package:build_pip_package cxxopt=-d_glibcxx_use_cxx_abi copt=-mavx copt=-msse copt=-msse copt=-mavx copt=-mfma copt=-mfpmath=both i specified cuda and cudnn explicitly in configure before running the above the error is the following: sherror home/colafly/.cache/bazel/_bazel_colafly/feebffeeefe/external/protobuf/build c compilation of rule protobuf//:js_embed failed crosstool_wrapper_driver_is_not_gcc failed error executing command cd home/colafly/.cache/bazel/_bazel_colafly/feebffeeefe/execroot/tensorflow exec env ld_library_path=:/usr/local/cuda/lib:/usr/local/cuda/extras/cupti/lib:/usr/local/cuda/lib:/usr/local/cuda/extras/cupti/lib:/usr/local/cuda/lib:/usr/local/cuda/extras/cupti/lib path=/home/colafly/.pyenv/plugins/pyenv-virtualenv/shims:/home/colafly/.pyenv/shims:~/.pyenv/bin:/home/colafly/bin:/home/colafly/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda/bin pwd=/proc/self/cwd external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc u_fortify_source d_fortify_source fstack-protector fpie wall wunused-but-set-parameter wno-free-nonheap-object fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections g std=c g md mf bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d frandom-seed=bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o iquote external/protobuf iquote bazel-out/host/genfiles/external/protobuf iquote external/bazel_tools iquote bazel-out/host/genfiles/external/bazel_tools isystem external/bazel_tools/tools/cpp/gcc no-canonical-prefixes wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted fno-canonical-system-headers c external/protobuf/src/google/protobuf/compiler/js/embed.cc o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status python cant open file external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc errno no such file or directorytarget tensorflow/tools/pip_package:build_pip_package failed to build however the file actually exists: shsudo find name crosstool_wrapper_driver_is_not_gcc/home/colafly/.cache/bazel/_bazel_colafly/feebffeeefe/external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc (also exists in bazel-tensorflow/external/local_config_cuda/crosstool )thanks
232722007,10356,https://api.github.com/repos/tensorflow/tensorflow/issues/10356,gunan,1,0,0,0,0,1,fixes
232651657,10350,https://api.github.com/repos/tensorflow/tensorflow/issues/10350,osdamv,1,0,0,0,0,0,"hi while i was building the android demo with the command bazel build c opt tensorflow/examples/android:tensorflow_demo verbose_failures it throw the error error home/damian/ai/tensorflow/tensorflow/examples/android/build extracting java resources from deploy jar for split java resource apk failed resource_extractor failed error executing command cd home/damian/.cache/bazel/_bazel_damian/fcfcdfffbdb/execroot/tensorflow exec env bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor bazel-out/local-opt/bin/tensorflow/examples/android/tensorflow_demo_deploy.jar bazel-out/local-opt/bin/tensorflow/examples/android/_dx/tensorflow_demo/extracted_tensorflow_demo_deploy.jar com.google.devtools.build.lib.shell.badexitstatusexception process exited with status file home/damian/.cache/bazel/_bazel_damian/fcfcdfffbdb/execroot/tensorflow/bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor.runfiles/org_tensorflow/../bazel_tools/tools/android/resource_extractor.py line print usage syntaxerror missing parentheses in call to print i debuged the error and the cause is in the file bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor around the line python_binary pythonif iswindows and not python_binary.endswith(.exe python_binary python_binary exe find a file in a given search path.def searchpath(name search_path os.getenv(path os.defpath).split(os.pathsep for directory in search_path if directory continue path os.path.join(directory name if os.path.isfile(path and os.access(path os.x_ok return path return nonedef isrunningfromzip return false find the real python binary if its not a normal absolute pathdef findpythonbinary if python_binary.startswith case path is a label not supported yet raise assertionerror bazel does not support execution of python interpreters via labels yet elif python_binary.startswith case absolute path return python_binary elif in python_binary case path is relative to current working directory return os.path.join(os.getcwd python_binary else case path has to be looked up in the search path return searchpath(python_binary) the script need and assumes the default usr/bin/python is linked to python x wich in my case is not,my current work around is a simple ln sf usr/bin/python usr/bin/python .i would like to patch but i dont have idea where is the file bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor because is an autogen script i tried look for it in the bazel repo but i cant find it"
232425179,10310,https://api.github.com/repos/tensorflow/tensorflow/issues/10310,taion,1,0,0,0,0,0,fixes
232421855,10309,https://api.github.com/repos/tensorflow/tensorflow/issues/10309,taion,1,0,0,0,0,0,suggests that the demo tensorboard was supposed to go away entirely but tensorboard/development.md still says to use it.better that it shows scalars and histograms than not
232370166,10299,https://api.github.com/repos/tensorflow/tensorflow/issues/10299,Lakedaemon,6,0,0,0,0,0,every developer wants to use his own model most app built for mobile will crash at runtime because those model wont just use the sparse subset of ops shipped with the aar available from jcenter which means that mobile developers need a way to easily and painlessly cross compile tenserflow with the right ops for their custom models the print_header_for_selecrtive_registration.py script is a good step in the right direction for this but it is not documented the documentation is completely lacking please document how to cross-compile tenserflow for mobile with the right types and the right ops with want command what files should we modify where i spent days looking at the tensorflow code/build files trying things and i still could not build a binary that would make that annoying op wasnt registered issue awayfix this this is not an individual problem look at the amount of questions and issues about tenserflow and op wasnt registered on the internet.btw the tensorflow aar from jcenter is completely useless as it can only be used to build the demo app.it would be better to put the demo apk on google play it would not mislead developers into thinking they can easily build apps with it
231772970,10238,https://api.github.com/repos/tensorflow/tensorflow/issues/10238,zklgame,4,0,0,0,0,0,when i run from tensorflow.examples.tutorials.mnist import input_data in my terminal following error occur: ---------------------------------------------------------------------------importerror traceback most recent call last)
231616447,10216,https://api.github.com/repos/tensorflow/tensorflow/issues/10216,Panaetius,1,0,0,0,0,0,there is a bug in the indices returned from tf.nn.max_pool_with_argmax when a padding is applied the indices with be based on the shape supplied to max_pool_with_argmax instead of the shape+padding.simple code example to reproduce: import tensorflow as tfdef main with tf.session as session input tf.get_variable(weights shape initializer=tf.truncated_normal_initializer(stddev dtype=tf.float dtype=tf.float val idx tf.nn.max_pool_with_argmax(input padding=same padding will turn dimensions to x y idx x idx y idx x idx max_x tf.reduce_max(x max_y tf.reduce_max(y max_x tf.reduce_max(x max_y tf.reduce_max(y session.run(tf.global_variables_initializer m_x m_y m_x m_y session.run( max_x max_y max_x max_y print(%d d d d%(m_x m_y m_x m_y))if name main main() this prints as you can see the padding would increase the dimensions of the tensor to x so the maximum y coordinate should be and x should be but if we unravel the argmax indices with a width of we get a maximum x of but maximum y of only if we instead use as width for unraveling the unpadded width of the input tensor we get and respectively which are the correct values for the unpadded input tensor.so the b height y width x channels c formula for tf.nn.max_pool_with_argmax uses the input tensor dimensions for width not the input+padding dimensions.this is relevant if you then use the indices to unpool/reverse the max_pool since often youll multiple the dimensions of the max_pool output by to get the input dimensions which would be off with a naive implementation like this.when using this to implement an unpooling operation for instance for segnet this will cause every line of the image to shift by pixel if padding is applied to the width of an image basically slightly tilting an image its especially obvious with multiple argmax&unpool in succession.it also means that with zero-padding if the whole tensor is negative values in which case the zero-padding would be the highest value in the tensor it wont return the coordinates of the padding basically it doesnt actually add any padding to the input tensor it just pretends it does to make dimensions line up but doesnt consider the actual values/zeros in the padding this might be intended behaviour though it would strike me as odd given the usual understanding of padding if so the documentation should be changed to make it clear that this is the way the operation works but i think this would be against the principle of least astonishment since i assume most people would think that an op talking about padding would actually add padding values.see also for a use-case for this with additional discussion
231602239,10215,https://api.github.com/repos/tensorflow/tensorflow/issues/10215,ipoletaev,8,0,0,0,0,0,it will be great if in the contrib.keras in the nearest future linear crf layer inside the keras wrapper will be added.i think a lot of people will find this very useful
231495859,10204,https://api.github.com/repos/tensorflow/tensorflow/issues/10204,j-min,1,0,0,0,0,0,i got following issues when i use tf.summary.text and view the summaries on tensorboard it shows me text summaries in random order it randomly removes existing summaries and show me only a few is there a configuration for maximum number of summaries to keep i can usually see only around summaries on tensorboard even if i added summaries times other summaries work properly when i use summaries like below. summary_op tf.summary.merge(summaries other scalar distribution histogram summariesvalid_summary_op tf.summary.merge( valid_sentence_summary text summary with tf.summary.text i can reproduce this problem in two different environments ubuntu cuda cudnn tf rc bazel gpu titan x pascal use gpus~gpus mac osx sierra tf rc bazel no gpubelow is sample code to reproduce this issue. import tensorflow as tftext_list this is the first text this is nd text this is random text idsent id:sent for id sent in enumerate(text_list)}sentid sent:id for id sent in idsent.items()}tf.reset_default_graph outer_string tf.convert_to_tensor(this is string outside inner scope.)outer_summary tf.summary.text(outside_summary outer_string)with tf.name_scope(validation_sentences as scope id_list tf.placeholder(tf.int shape name=sent_ids valid_placeholder tf.placeholder(tf.string name=valid_summaries inner_summary tf.summary.text(sent_summary valid_placeholder summaries outer_summary inner_summary summary_op tf.summary.merge(summaries sess tf.session()summary_writer tf.summary.filewriter(logdir=./text_summary graph=sess.graph)for step in range predicted_sents_ids sess.run id_list feed_dict id_list list of string predicted_sents idsent id for id in predicted_sents_ids valid_summary sess.run(summary_op feed_dict valid_placeholder predicted_sents summary_writer.add_summary(valid_summary global_step=step summary_writer.flush summary_writer.flush flush didnt help.. and below is the result on tensorboard.! image
231370622,10195,https://api.github.com/repos/tensorflow/tensorflow/issues/10195,bryant1410,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below rc freeze_graph method from tensorflow.python.tools should be able to work just with an input checkpoint it neednt a graph definition from a protobuf file just restoring the metagraph and using the graph from the session lets you get rid of the graph def file.also as you have provided from rc a method to freeze from code without loading the files freeze_graph_with_def_protos it should be able to work without a checkpoint but just with a session.these will make freezing way simpler
231217442,10179,https://api.github.com/repos/tensorflow/tensorflow/issues/10179,campoy,2,0,0,0,0,0,system informationim running the tensorflow/tensorflow:latest docker image bash docker run it rm tensorflow/tensorflow pythonpython default nov gcc on linuxtype help copyright credits or license for more information import tensorflowtraceback most recent call last file stdin line in module>importerror no module named tensorflow note that the same works perfectly for python bash docker run it rm tensorflow/tensorflow pythonpython default nov gcc on linuxtype help copyright credits or license for more information import tensorflow describe the problemimporting tensorflow from python fails for the tensorflow/tensorflow docker imagethis is surprising because python itself is installed so the solution is:a make tensorflow be importable from pythonb remote python so its obvious you need to use a different tag point at it maybe
231119119,10166,https://api.github.com/repos/tensorflow/tensorflow/issues/10166,oleg-trott,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu ubuntu bit tensorflow installed from source or binary pip install tensorflow-gpu tensorflow version use command below v..-rc--geced bazel version if compiling from source n/a cuda/cudnn version gpu model and memory k gb also gtx gb exact command to reproduce : time cuda_visible_devices python memory_usage.py tf encounters oom when i try to initialize an array half the size of the vram but only when its a float type float float float this happens even when trainable=false .i checked via nvidia-smi that no other process is using the gpu. import tensorflow as tfvram gbdef use_half_vram(dtype n vram dtype.size x tf.get_variable(x shape= n dtype=dtype initializer=tf.constant_initializer trainable=false with tf.session as sess sess.run(tf.global_variables_initializer use_half_vram(tf.uint ok use_half_vram(tf.int ok use_half_vram(tf.int ok use_half_vram(tf.float oomuse_half_vram(tf.float oom use_half_vram(tf.float oom
230989163,10155,https://api.github.com/repos/tensorflow/tensorflow/issues/10155,andreas-eberle,2,0,0,0,0,0,describe the problemwhen training neural networks and experimenting with different architectures or simply adapting a model to a new number of classes it is crucial to be able to reuse an existing trained model as far as possible for example if i want to use the inception-v architecture and train it on instead of classes i need to be able to load all layers but the logit ones.unfortunately this is not possible at least i wasnt able to find a way with the estimator api whenever the size of a variable in my model changes or i add or remove a variable the estimator cannot load an existing checkpoint any more this is a major drawback making the estimator basically unusable for developing a new architecture or adapting an existing one by iteratively adapting the model requested features it should be possible to tell the estimator that its ok if some variables arent found in the checkpoint those should simply be initialized as if no checkpoint would be loaded it should be possible to specify scopes that should not be loaded from the checkpoint or to specify a flag that says something like just dont load variables that have a different shape that you cant load be able to load an existing checkpoint from a different path than the estimators model_dir when there is no checkpoint in the model_dir yet this is helpful to start training from a different checkpoint without manully having to copy those models checkpoints into the new model_dir inspirationthis request has been inspired by the parameters you can specify to the train_image_classifier.py script from the tensorflow-models/slim directory there you have the parameters checkpoint_exclude_scopes ignore_missing_vars and checkpoint_path .of course one could say its possible to implement this manually but i think these are basic functionalities for everyone doing a bit more deeplearning than only the tutorial thats why i think this should be part of the otherwise easy to use estimator api
230984480,10154,https://api.github.com/repos/tensorflow/tensorflow/issues/10154,gwendoline-28,2,0,0,0,0,0,hi i have encountered the following issue when importing the gpu version of tensorflow in python on windows:c:\users\gwendoline>activate tensorflow-gpu(tensorflow-gpu c:\users\gwendoline>pythonpython continuum analytics inc default may msc v bit amd on wintype help copyright credits or license for more information import tensorflowtraceback most recent call last file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed le module spcifi est introuvable.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed le module spcifi est introuvable.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\gwendoline\anaconda\envs\tensorflow-gpu\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help.i have downloaded the version of cuda and cudnn v and have put the cudnn files cudnn_.dll cudnn.h and cudnn.lib respectively in the cuda repositories bin include and lib\x the corresponding environment variable path is set as c:\program files\nvidia gpu computing toolkit\cuda\v.\bin can you help many thanks
230896165,10149,https://api.github.com/repos/tensorflow/tensorflow/issues/10149,goldingn,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu macos sierra tensorflow installed from source or binary binary via pip tensorflow version use command below v..-rc--geced bazel version if compiling from source n/a cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce : pythonimport tensorflow as tfcategorical tf.contrib.distributions.categorical(probs multinomial tf.contrib.distributions.multinomial(total_count probs mvn tf.contrib.distributions.multivariatenormaldiag(loc scale_diag expected values points and the docs for categorical say value should be float or double but it expects an intcategorical.log_prob tf.tensor categorical_/log_prob/neg shape dtype=float>categorical.log_prob typeerror value passed to parameter labels has datatype float not in list of allowed values int int the docs for categorical say value should be float or double which is how it behaves though this is unlike categorical)multinomial.log_prob valueerror tensor conversion requested dtype int for tensor with dtype float tensor(multinomial_/log_prob/log shape dtype=float)multinomial.log_prob tf.tensor multinomial_/log_prob/sub shape dtype=float output shape points and the docs for both say that the output should be a tensor of shape sample_shape(x self.batch_shape with values of type self.dtype though sample_shape doesnt seem to be relevant here its an argument to param_shapes and sample for categorical with int value the result is a vector matching the shape of valuecategorical.log_prob tf.tensor categorical_/log_prob/neg shape dtype=float for multinomial with float value the result is a scalarmultinomial.log_prob tf.tensor multinomial_/log_prob/sub shape dtype=float for multivariate normal the result is a scalarmvn.log_prob tf.tensor multivariatenormaldiag_/log_prob/add shape dtype=float describe the problemthere are four related issues the expected type of value for the log_prob method in tf.contrib.distributions.categorical is inconsistent with the documentation the expected values for tf.contrib.distributions.categorical and tf.contrib.distributions.multinomial are inconsistent with one another which is odd as the categorical distribution is a special case of the multinomial with total_count the output dimensions for tf.contrib.distributions.categorical and tf.contrib.distributions.multinomial are inconsistent with the documentation the output dimensions for tf.contrib.distributions.categorical are a vector which doesnt really make sense for a multivariate distribution and is inconsistent with tf.contrib.distributions.multinomial and tf.contrib.distributions.multivariatenormal* details are in the code snippet above
230824745,10145,https://api.github.com/repos/tensorflow/tensorflow/issues/10145,lw394,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow not really this primarily a copy and paste of the distributed tensorflow example os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below gpu bazel version if compiling from source cuda/cudnn version gpu model and memory titan x pascal g total exact command to reproduce : bashpython async_btwgraph_launcher.py async_btwgraph_launcher.py pythonfrom async_btwgraph_dist_trainer import trainimport osfrom multiprocessing import processimport timefrom tensorflow.contrib.training import hparamsimport tensorflow as tf set up configurations to sweepoutput_dir tfprojects/output_dir_debugcluster_spec ps localhost worker localhost localhost localhost localhost cluster tf.train.clusterspec(cluster_spec)def worker(device params hparams(cluster=cluster job_name device task_index device if device ==worker allow each worker to see only of the gpus os.environ cuda_visible_devices =str(params.task_index else hide all gpus from ps os.environ cuda_visible_devices train(output_dir params)if name main devices ps worker worker worker worker processes for d in devices p process(target=worker args=(d p.start processes.append(p for p in processes p.join() async_btwgraph_dist_trainer.py pythonimport osimport numpy as npimport tensorflow as tfimport timetf.logging.set_verbosity(tf.logging.info enables training error print out during trainingdef model_fn(features,labels,mode,params outputs layers.fully_connected inputs features num_outputs outputs layers.fully_connected inputs outputs num_outputs outputs layers.fully_connected inputs outputs num_outputs loss tf.losses.mean_squared_error(outputs labels train_op tf.contrib.layers.optimize_loss loss none optimizer=adam learning_rate predictions predictions:tf.identity(outputs,name predictions return predictions loss train_opdef dumb_input_fn x tf.random_normal dtype=tf.float y tf.random_normal dtype=tf.float return x,y #def train(output_dir params print( *jobname :,params.job_name cluster params.cluster job_name params.job_name task_index params.task_index gpu task_index create and start a server for the local task server tf.train.server(cluster job_name=job_name task_index=task_index if job_name ps server.join elif job_name worker assigns ops to the local worker by default with tf.device(tf.train.replica_device_setter worker_device=/job:worker/replica:/task:%d task_index cluster=cluster build model x,y dumb_input_fn train_op model_fn(x,y,none,params global_step tf.contrib.framework.get_or_create_global_step the stopatstephook handles stopping after running given steps hooks= tf.train.stopatstephook(last_step step start_time time.time with tf.train.monitoredtrainingsession(master=server.target is_chief=(task_index checkpoint_dir=output_dir hooks=hooks as mon_sess while not mon_sess.should_stop mon_sess.run(train_op if step print(step step,/(time.time()-start_time),steps/sec start_time time.time step describe the problemim trying to use the distributed tensorflow example to do async between graph replication on a titan x machine with gpu per worker without distributed tf and using a single gpu the same code trains at steps/sec as shown at the end of the log below this distributed trainer clocks at steps/sec the gpus are barely utilized,! image plenty of cpu headroom,! image if i simply remove the parameter server from this example but keeping all workers they all grab gpu maxing it out and each worker process running at steps/sec and gpus are unused.! image see that im setting os.environ cuda_visible_devices to enable only unique gpu per worker is this expected behavior?thanks,luke source code logs bashpython async_btwgraph_launcher.py *jobname ps w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. *jobname worker w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. *jobname worker w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. *jobname worker w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. *jobname worker w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations e tensorflow/stream_executor/cuda/cuda_driver.cc failed call to cuinit cuda_error_no_device i tensorflow/stream_executor/cuda/cuda_diagnostics.cc retrieving cuda diagnostic information for host mlearn i tensorflow/stream_executor/cuda/cuda_diagnostics.cc hostname mlearn i tensorflow/stream_executor/cuda/cuda_diagnostics.cc libcuda reported version is i tensorflow/stream_executor/cuda/cuda_diagnostics.cc driver version file contents nvrm version nvidia unix x kernel module tue jan pst gcc version gcc version ubuntu ubuntu i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel reported version is i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel version seems to match dso i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost localhost localhost localhost i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost localhost localhost localhost i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost localhost localhost localhost i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost localhost localhost localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost localhost localhost localhost i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost i tensorflow/core/distributed_runtime/master_session.cc start master session bdbb with config i tensorflow/core/distributed_runtime/master_session.cc start master session ffabf with config i tensorflow/core/distributed_runtime/master_session.cc start master session cccf with config i tensorflow/core/distributed_runtime/master_session.cc start master session cecd with config:step steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/sec i tensorflow/core/distributed_runtime/master_session.cc start master session cfcbae with config i tensorflow/core/distributed_runtime/master_session.cc start master session bcbcbf with config i tensorflow/core/distributed_runtime/master_session.cc start master session bdff with config:step steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/secstep steps/sec"
230685882,10133,https://api.github.com/repos/tensorflow/tensorflow/issues/10133,FrankWork,1,0,0,0,0,0,tensorflow for python failed while ran the following code python python anaconda bit default jul gcc red hat on linux import tensorflow as tf tf.__version help(tf.configproto in english segmentation fault core dumped python python anaconda bit default jul gcc red hat on linux type help copyright credits or license for more information import tensorflow as tf tf.__version rc help(tf.configproto in english segmentation fault core dumped)tensorflow for python worked fine python python default nov gcc on linux import tensorflow as tf tf.__version rc help(tf.configproto
230590241,10128,https://api.github.com/repos/tensorflow/tensorflow/issues/10128,tqangxl,5,0,0,0,0,0,tensorflow release will support python
230589498,10127,https://api.github.com/repos/tensorflow/tensorflow/issues/10127,Czxck001,3,0,0,3,0,3,mentioned in issue and tf is now supporting fft operators however there still remains irfft as a cpu kernel unimplemented in this pr i made up the missing part of fftcpu class and get the cpu kernel for irfft work.i remove the limitation of if test.is_gpu_available(cuda_only=true in fft_ops_test.py and passed all test cases by bazel test config=opt k tensorflow/python/kernel_tests:fft_ops_test .the code is linted by clang-format using google code style
230582974,10125,https://api.github.com/repos/tensorflow/tensorflow/issues/10125,appierys,2,0,0,0,0,0,is it possible to incorporate weight normalization into tensorflow itself
230485519,10111,https://api.github.com/repos/tensorflow/tensorflow/issues/10111,amtagrwl,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary) :using pip tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory :quadro k exact command to reproduce :import tensorflow as tfi have installed everything following the instructions and opened a post on so but havent receieved many responses i have looked at multiple other posts with the same keywords but was unable to solve the issue from there i used the standard installation instructions using pip for windows the following is the stacktrace when i run import tensorflow as tf in python command line microsoft windows version copyright c microsoft corporation all rights reserved.c:\users\aagarwal>pythonpython v..:cba jan msc v bit amd on wintype help copyright credits or license for more information import tensorflow as tftraceback most recent call last file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\aagarwal\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\aagarwal\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(mname file c:\users\aagarwal\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file c:\users\aagarwal\appdata\local\programs\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow_internal file c:\users\aagarwal\appdata\local\programs\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help
230480154,10110,https://api.github.com/repos/tensorflow/tensorflow/issues/10110,sstirlin,1,0,0,0,0,0,in the underlying c code tested here dictionary can be used however the python bindings do not expose the ability to specify a dictionary scorer at all see here
230273785,10090,https://api.github.com/repos/tensorflow/tensorflow/issues/10090,Jake132456,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out. heres why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request.i installed tensorflow at my window it is only version and using python how can i update version and can i use python i already installed anaconda newest version.i installed it using anaconda prompt source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
230268525,10089,https://api.github.com/repos/tensorflow/tensorflow/issues/10089,eaplatanios,46,0,0,0,0,7,"hi,i have open sourced a scala api for tensorflow that contains much more complete functionality than the java api here the readme file in the repository contains information on what features are supported etc i would really appreciate some feedback from interested parties in the community on the library.one main limitation is that the library does not yet support while loops in the graph this is due to being unable to implement gradient backpropagation using the current c api skye do you have any suggestions on how to proceed with respect to that it would be really useful for implementing rnns.other cool stuff such as fetching arbitrarily structured data from sessions e.g lists of tensors or maps of tensors etc are supported and are for the most part strongly typed.cheers,anthony"
230267461,10088,https://api.github.com/repos/tensorflow/tensorflow/issues/10088,migueldeicaza,2,0,0,0,0,0,feature requesti am working on the net bindings for c and as part of this process i am replicating the c api test suite in c to ensure everything works as expected.the register_op capability is available to c developers and i would like to have this capability surfaced so my binding and likely other bindings can roll out these tests as well.this is what i would like to be able to do from c
230235190,10081,https://api.github.com/repos/tensorflow/tensorflow/issues/10081,terrytangyuan,1,0,0,0,0,0,seems like the nightly builds have been failing for a while for example python mac gpu build history here is this something being worked on since its been a month or so
230204543,10073,https://api.github.com/repos/tensorflow/tensorflow/issues/10073,KashiErez,1,0,0,0,0,0,"hi,i ran into this bug while trying to upgrade tensorflow version to using nvidia/cuda:.-cudnn-devel-ubuntu docker image i also tried other docker images as well).note that same code runs fine with tensorflow attached is python script that prints tensorflow version see readme.txt for instruction of how to build run the docker image fail-to-load-tf-bug.zip is the stack trace for tensorflow traceback most recent call last file run.py line in module import tensorflow as tf file usr/local/lib/python./dist-packages/tensorflow/__init__.py line in module from tensorflow.python import file usr/local/lib/python./dist-packages/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file usr/local/lib/python./dist-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow_internal fp pathname description)importerror libcuda.so cannot open shared object file no such file or directoryfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help. here is the logs for tensorflow here it works fine)please pay attention message couldnt open cuda library libcuda.so. i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc couldnt open cuda library libcuda.so ld_library_path usr/local/nvidia/lib:/usr/local/nvidia/libi tensorflow/stream_executor/cuda/cuda_diagnostics.cc hostname fcei tensorflow/stream_executor/cuda/cuda_diagnostics.cc libcuda reported version is not found was unable to find libcuda.so dso loaded into this programi tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel reported version is permission denied could not open driver version path for reading proc/driver/nvidia/versioni tensorflow/stream_executor/cuda/cuda_gpu_executor.cc ld_library_path usr/local/nvidia/lib:/usr/local/nvidia/libi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc failed to find libcuda.so on this system failed precondition could not dlopen dso libcuda.so dlerror libcuda.so cannot open shared object file no such file or directoryi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallytensorflow version tensorflow git version v..-rc--gbba-dirty thanks erezp.s:i built and ran the docker image on machines mac docker version ce build cde no gpu ubuntu lts docker version build bf gpu info: $nvidia-smisun may nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx tit off off n/a c p w w mib mib default geforce gtx tit off off n/a c p w w mib mib default geforce gtx tit off off n/a c p w w mib mib default geforce gtx tit off a off n/a c p w w mib mib default processes gpu memory gpu pid type process name usage no running processes found"
230133893,10059,https://api.github.com/repos/tensorflow/tensorflow/issues/10059,AndreiCostinescu,3,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution windows as well as linux ubuntu tensorflow installed from source or binary installed with pip in root of anaconda like in the documentation not installed from master tensorflow version v cuda/cudnn version cuda version gpu model and memory geforce gtx titan x gb exact command to reproduce python test_summarizing.py describe the problemhi everyone,i was trying to write a function to do data augmentation on images with a probability of the function should do the augmentation and if not return the image unmodified the basic idea of my usage you can extract from the code below. image tf.cond(tf.less(probability lambda do_augmentation(image lambda image) in the augmentation function i want to see how the image changed so i added tf.summary.image after every image-processing step but when running the summary operation after i merged all summaries with tf.summary.merge_all i get the following error: tensorflow.python.framework.errors_impl.invalidargumenterror the tensor returned for merge/mergesummary was not valid. i tried to debug the problem and saw that when i dont use the summaries commenting them in the augmentation function the whole code works.i couldnt find any help on stackoverflow regarding this problem however i saw only one other post which had this kind of error so i took that code to be sure that it was not a problem with tf.summary.image but a problem in general with tf.summary and played around to see where the problem is sadly i couldnt figure it out...in the attached zip file test_summarizing.zip there is the test_summarizing.py file which contains functions summary_not_working_simple() :is a minimal example to replicate the error of my code and of the original problem i had.(i used the scalar summary instead of the image summary because it doesnt make a difference if you comment both summaries in f and f the code always works if you comment one out of the summary-calls in either function f or f the code sometimes works and sometimes produces the traceback found below to replicate try commenting both calls then run the code then comment only one call and run the code possibly multiple times then comment the other call and run the code again you should see that with one commented call to tf.summary.scalar the code sometimes produces the error and sometimes it simply works if you dont comment anything leaving both calls to the summary the code never works and always produces the traceback shown below summary_not_working_stack_overflow() :to replicate the error you must in the function multi_step_decay of class multistepdecay at lines comment resp uncomment the with tf.control_dependencies block and the error will appear.could someone please look into the problem of the summary_not_working_simple and explain to me why the summary is not working i have found a workaround to using the tf.cond but the code is very messy now plus it would make sense to have the possibility of writing summaries from every point of the tensorflow code right?and if you could also explain why the issue in the second function summary_not_working_stack_overflow occurs i would appreciate it very much source code logstraceback for summary_not_working_simple example: traceback most recent call last file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_call return fn(*args file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run_fn status run_metadata file c:\users\andrei\anaconda\lib\contextlib.py line in exit next(self.gen file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\framework\errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.invalidargumenterror the tensor returned for merge/mergesummary was not valid.during handling of the above exception another exception occurred:traceback most recent call last file c:/users/andrei/pycharmprojects/testing/test_summarizing.py line in module summary_not_working_simple file c:/users/andrei/pycharmprojects/testing/test_summarizing.py line in summary_not_working_simple session.run(summary_merge_opt file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run run_metadata_ptr file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\client\session.py line in run feed_dict_string options run_metadata file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_run target_list options run_metadata file c:\users\andrei\anaconda\lib\site-packages\tensorflow\python\client\session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror the tensor returned for merge/mergesummary was not valid. source code of my the original function: def preprocess_for_train_summary_error(image height width fast_mode=true scope=none central_fraction with tf.name_scope(scope train_image image height width if image.dtype tf.float image tf.image.convert_image_dtype(image dtype=tf.float tf.summary.image(original_image tf.expand_dims(image axis max_outputs random_augment tf.random_uniform minval maxval dtype=tf.float def augmentation_pipeline(image_arg random_translate tf.random_uniform minval maxval dtype=tf.float translated_image tf.cond(tf.less(random_translate lambda translate(image_arg lambda image_arg tf.summary.image(translated_image tf.expand_dims(translated_image axis max_outputs random_rotate tf.random_uniform minval maxval rotated_image tf.cond(tf.less(random_rotate lambda rotate(translated_image lambda translated_image tf.summary.image(rotated_image tf.expand_dims(rotated_image axis max_outputs random_flip tf.random_uniform minval maxval flipped_image tf.cond(tf.less(random_flip lambda flip(rotated_image lambda rotated_image tf.summary.image(flipped_image tf.expand_dims(flipped_image axis max_outputs def f(x ordering return distort_color(x color_ordering=ordering fast_mode=fast_mode random_distort_colors tf.random_uniform minval maxval color_distorted_image tf.cond(tf.less(random_distort_colors lambda apply_with_random_selector(flipped_image f lambda flipped_image tf.summary.image(color_distorted_image tf.expand_dims(color_distorted_image axis max_outputs return image_arg image tf.cond(tf.less(random_augment lambda augmentation_pipeline(image lambda image image tf.image.central_crop(image central_fraction=central_fraction tf.summary.image(central_cropped_image tf.expand_dims(image axis max_outputs image tf.expand_dims(image image tf.image.resize_bilinear(image height width align_corners=false image tf.squeeze(image tf.summary.image(resized_image tf.expand_dims(image axis max_outputs subtract off the mean and divide by the variance of the pixels image tf.subtract(image name=sub_mean image tf.multiply(image name=div_var return image"
229857263,10021,https://api.github.com/repos/tensorflow/tensorflow/issues/10021,w4nderlust,15,0,0,0,0,0,feature requestin tf.losses.softmax_cross_entropy theres an optional field weights i assumed this field was used for assigning a different weight to each class but it actually is used to assign a weight to each sample in the batch in my use case i have a batch_size of and classes so i was passing a tensor and got this error: invalidargumenterror see above for traceback incompatible shapes vs node optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/mul_grad/broadcastgradientargs broadcastgradientargs t=dt_int device=/job:localhost/replica:/task:/gpu: (optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/mul_grad/shape optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/mul_grad/shape_) i looked at the implementation confirmed that the function expects batch_size as the dimension of thensor and realized that my expected behavior cannot be achieved easily as tf.nn.softmax_cross_entropy_with_logits doesnt have a weight parameter.my current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that i pass but thats inefficient compared to the optimized implementation of tf.nn.softmax_cross_entropy_with_logits .so my request is provide an optimized tf.nn.softmax_cross_entropy_with_logits that also accepts weights for each class as a parameter use it inside tf.losses.softmax_cross_entropy so that one can pass weights as a scalar a batch_size tensor a num_classes tensor or a batch_size num_classes tensor the same dimension of onehot_labels
229772309,10013,https://api.github.com/repos/tensorflow/tensorflow/issues/10013,ankur2136,4,0,0,0,0,0,this is a possible duplicate of posting here as there is no way to re-open or comment on the previous bug the comments suggests that the updating the driver would most probably fix the issue but its not the case environment info distributor id ubuntudescription ubuntu ltsrelease codename xenial> python c import tensorflow as tf print(tf.git_version tf.version v..-rc--geced nvidia-smi nvidia-smi driver version before starting the service c usr/bin/python mib after deleting the session note that i am creating the session and loading a checkpoint running the session and then closing it explicitly with session.close i am also resetting the graph by calling tf.reset_default_graph after closing the session c usr/bin/python mib device_t=/gpu graph tf.graph soft_config tf.configproto(allow_soft_placement=true soft_config.gpu_options.allow_growth true tried both true and false and doesnt seem to help with graph.as_default graph.device(device_t tf.session(config=soft_config as sess batch_shape batch_size img.shape img_placeholder tf.placeholder(tf.float shape=batch_shape name=img_placeholder preds transform.net(img_placeholder saver tf.train.saver saver.restore(sess checkpoint_dir content np.zeros(batch_shape dtype=np.float content img preds sess.run(preds feed_dict={img_placeholder content result crop_img(_preds .astype(data_in.dtype sess.close explicitly closing the session and deleting does not help as well del sess tf.reset_default_graph return resulthere are some of the relevant logs w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name grid kmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name grid k pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name grid k pci bus id
229603963,9995,https://api.github.com/repos/tensorflow/tensorflow/issues/9995,carefree0910,1,0,0,0,0,0,support returning all states generated in dynamic_rnn if the states are rather simpleunit test could be found here i finished this implementation under api r and i found the latest code of tensorflow.python.ops.rnn.py is quite different from that on my windows machine with api r since i cannot build the source code though ive tested them under api r i havent actually tested the committed codes in the latest environment
229422773,9976,https://api.github.com/repos/tensorflow/tensorflow/issues/9976,jubjamie,7,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow n/a os platform and distribution e.g linux ubuntu win tensorflow installed from source or binary pip tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory :k gb exact command to reproduce launch tensorboard describe the problemtensorboard is ing on a lot of resources see the error messages below i think i saw a similar issue somewhere so this may be a duplicate youre probably aware but thought id file just in case source code logs warning:tensorflow:path external\weblas_weblas_js/file/weblas.map.json not found sending warning:tensorflow:path external\web_animations_js/web-animations-next-lite.min.js.map not found sending warning:tensorflow:path external\weblas_weblas_js/file/weblas.map.json not found sending warning:tensorflow:path external\web_animations_js/web-animations-next-lite.min.js.map not found sending warning:tensorflow:path external\data/plugin/text/runs not found sending warning:tensorflow:path external\data/plugin/text/runs not found sending warning:tensorflow:path external\data/plugin/text/runs not found sending warning:tensorflow:path external\data/plugin/text/runs not found sending
229414504,9974,https://api.github.com/repos/tensorflow/tensorflow/issues/9974,jubjamie,3,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows/ubuntu tensorflow installed from source or binary binary tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory k exact command to reproduce n/a describe the problemis it just me or is there quite a large disconnect between the two versions of estimator the tutorial in the docs guides you through the contrib version but i understand that it has also been moved to tf.estimator however in the docs it appears that almost all functionality e.g canned estimators or fit appears to be missing or altered and there is little documentation to explain this new api.have i misunderstood something here i imagine that we are preferred to use tf.estimator because using the contrib version kicks up all sorts of warning about how it will be deprecated last year although the estimator tutorial still uses it and also fit which i cant clearly see the replacement for in the new api is there going to be any example code or tutorial for the new tf.estimator api as i feel it desperately needs it the contrib version was difficult enough to understand that i gave up but want to try again thanks
229344845,9969,https://api.github.com/repos/tensorflow/tensorflow/issues/9969,vladfi1,6,0,0,0,0,0,im on a multi-machine cluster where not all the machines have gpus previously with i could use one tensorflow-gpu installation either from source or from the provided wheel on all the machines ive now upgraded to and tensorflow crashes at import on the non-gpu machines importerror libcuda.so cannot open shared object file no such file or directorymy current workaround is to have different tensorflows cpu or gpu in different conda environments and load the conda environment based on whether i need to use a gpu or not
229224207,9952,https://api.github.com/repos/tensorflow/tensorflow/issues/9952,lirchi,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow) :yes minor changes see below os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary) :binary tensorflow version use command below bazel version if compiling from source describe the problemi use the high level api to train an estimator specifically tf.contrib.learn.dnnregressor in python i then use export_savedmodel to save it to protobuf when i try to load it from c i get: data loss cant parse saved_model.pb as binary proto source code logs python file import numpy as npimport pandas as pdimport sysimport pickleimport tensorflow as tfimport random os shutilfrom tensorflow.contrib.layers import create_feature_spec_for_parsingfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utilstf.logging.set_verbosity(tf.logging.info)train_filename sys.argv test_filename sys.argv saved_model_directory sys.argv columns x y x y sp features x y x y target sptraining_set pd.read_csv(train_filename skipinitialspace=true names=columns)test_set pd.read_csv(test_filename skipinitialspace=true names=columns)def input_fn(data_set feature_cols k tf.constant(data_set k .values for k in features targets tf.constant(data_set target .values return feature_cols targetsfeature_cols tf.contrib.layers.real_valued_column(k for k in features feature_spec create_feature_spec_for_parsing(feature_cols)serving_input_fn input_fn_utils.build_parsing_serving_input_fn(feature_spec)validation_monitor tf.contrib.learn.monitors.validationmonitor input_fn=lambda input_fn(test_set eval_steps every_n_steps=)estimator tf.contrib.learn.dnnregressor feature_columns=feature_cols hidden_units model_dir=saved_model_directory config=tf.contrib.learn.runconfig(save_checkpoints_secs=))estimator.fit(input_fn=lambda input_fn(training_set steps monitors= validation_monitor )estimator.export_savedmodel(export_dir_base=saved_model_directory serving_input_fn serving_input_fn cpp file #include tensorflow/core/public/session.h#include tensorflow/core/platform/env.husing namespace tensorflow;using std::cout;using std::vector;using std::pair;int main(int argc char argv initialize a tensorflow session session session status status newsession(sessionoptions session if status.ok cout status.tostring n return read in the protobuf graph we exported the path seems to be relative to the cwd keep this in mind when using bazel run since the cwd isnt where you call bazel run but from inside a temp folder cout reading model.\n graphdef graph_def status readbinaryproto(env::default saved_model.pb graph_def if status.ok cout status.tostring n return cout done reading model.\n
229213773,9950,https://api.github.com/repos/tensorflow/tensorflow/issues/9950,bowang,4,0,0,1,0,0,implement a reader op to access records stored in an lmdb database.this facilitates caffe users to switch to tensorflow without rebuilding their potentially very large lmdb datasets
229125889,9939,https://api.github.com/repos/tensorflow/tensorflow/issues/9939,nryant,6,0,0,0,0,0,system information os platform and distribution ubuntu lts tensorflow installed from source tensorflow version v..-rc--gfa rc bazel version cuda/cudnn version gpu model and memory maxwell titan x gib overview of problemsince updating to the most recent version as of yesterday of tensorflow ive started seeing the following ominous warning when serializing a wide resnet variant that ive been using for acoustic modeling warning:tensorflow:error encountered when serializing layer_name_uids type is unsupported or the types of the items dont match field type in collectiondef dict object has no attribute namehowever a metagraphdef does export and i am able to successfully use it to recreate the trained model after playing around with simpler architectures it looks like the problem comes from the average pooling i do at the end which involves a call to tf.contrib.layers.avg_poold for a trivial example that elicits this warning please see the script at
228790793,9918,https://api.github.com/repos/tensorflow/tensorflow/issues/9918,starcrest,2,0,0,0,0,0,savers automatically clean up checkpoints and thats lovely but there are special points during training that i want to be sure to save e.g transitioning from a pre-training phrase to full training which i cant ensure with the current options unless i just keep everything by making max_to_keep keep_checkpoint_every_n_hours huge).two possible approaches saver.save preserve=true) never clean up this checkpoint max_to_keep is defined per save_path i.e whenever i change the save_path argument to save in the middle of the session dont clean up the checkpoints with the previous save_path .this is related to with a little book-keeping on the client you could do yourself
228779127,9917,https://api.github.com/repos/tensorflow/tensorflow/issues/9917,jvlmdr,2,0,0,0,0,0,system information tensorflow installed from source or binary source tensorflow version use command below v..--geced bazel version if compiling from source non-git cuda/cudnn version gpu model and memory nvidia m gb describe the problemi would like to apply a tf.nn.convd_transpose operation to each channel of a feature image independently there is no tf.nn.depthwise_convd_transpose operation.i tried using tf.nn.depthwise_convd_native_backprop_input however when i try to optimize a function that involves one of these operations it results in an error because there is no gradient operation defined: lookuperror no gradient defined for operation depthwiseconvdnativebackpropinput op type depthwiseconvdnativebackpropinput) this is related to is possible to achieve this functionality using convd_transpose by constructing a large filter with many coefficients set to zero however it is relatively inefficient especially for a large number of channels
228603734,9904,https://api.github.com/repos/tensorflow/tensorflow/issues/9904,chenghuige,4,0,0,0,0,0,tf version rcit looks beamsearchdecoder never used next_cell_state but always using the inital cell state.in beam_search_decoder.py next_cell_state nest.map_structure self._maybe_split_batch_beams next_cell_state self._cell.state_size)but next_cell_state is never used later since just pass state to beam_search_step function where state.cell_state is never changed beam_search_output beam_search_state beam_search_step time=time logits=cell_outputs beam_state=state batch_size=batch_size beam_width=beam_width end_token=end_token length_penalty_weight=length_penalty_weight
228586883,9902,https://api.github.com/repos/tensorflow/tensorflow/issues/9902,ruggeri,1,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow yes os platform and distribution e.g linux ubuntu osx tensorflow installed from source or binary binary tensorflow version use command below bazel version if compiling from source na cuda/cudnn version none gpu model and memory none exact command to reproduce see below describe the problemhi tensorflow humans thanks so much for making tensorflow!right now if you feed a floating point array into a integral placeholder type it will be converted implicitly to my knowledge most python operations will not implicitly convert.the implicit conversion potentially creates convenience but of course it also creates the opportunity for a hard-to-see bug in my case i lost about day to find this bug and experienced great sadness that probably says more about me than it does about tf.still my feeling is that it is a more sensible default to require the user to do the conversion explicitly alternatively perhaps it would be logical to log a warning to the user.note that to my knowledge tf operations like tf.equal require both tensors to have the same type so users might have a belief that that tf and session#run require them to be fairly explicit about types.if it would be likely to be accepted i am happy to write a patch for tf that warns the user when they feed a tensor of the wrong type.thanks for reading this issue source code logs import numpy as npimport tensorflow as tfsession tf.session()convert_implicitly tf.placeholder(tf.int none )float_input np.random.uniform(size result session.run convert_implicitly feed_dict convert_implicitly float_input print(result) output python test.py w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine andcould speed up cpu computations
228562870,9895,https://api.github.com/repos/tensorflow/tensorflow/issues/9895,thomasquintana,4,0,0,0,0,0,os ubuntu/linux tensorflow compiled from sourcetensorflow version r.bazel version cuda/cudnn versions gpu model/memory titanx/gbafter turning on xla jit compiling tf fails with a core dump i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gib w tensorflow/stream_executor/cuda/cuda_driver.cc creating context when one is currently active existing xcb i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y y i tensorflow/core/common_runtime/gpu/gpu_device.cc y y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/compiler/xla/service/platform_util.cc platform cuda present with visible devices i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices i tensorflow/compiler/xla/service/service.cc xla service xaab executing computations on platform host devices i tensorflow/compiler/xla/service/service.cc streamexecutor device undefined undefined i tensorflow/compiler/xla/service/platform_util.cc platform cuda present with visible devices i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices i tensorflow/compiler/xla/service/service.cc xla service xac executing computations on platform cuda devices i tensorflow/compiler/xla/service/service.cc streamexecutor device titan x pascal compute capability i tensorflow/compiler/xla/service/service.cc streamexecutor device titan x pascal compute capability f tensorflow/compiler/xla/service/algebraic_simplifier.cc check failed user->operand(reshape_or_broadcast_operand_index reshape_or_broadcast xfbce vs xfbcbe)aborted core dumped
228488185,9881,https://api.github.com/repos/tensorflow/tensorflow/issues/9881,jeansely,0,1,0,0,0,0,i wish to implement clstms network architecture in the tensorflow to facilitate extension and modifications to the network already create in module
228374432,9868,https://api.github.com/repos/tensorflow/tensorflow/issues/9868,stalagmite7,1,0,0,0,0,0,ive been looking at the chrome timeline to profile my keras models but have been unable to find any documentation on how i could use it with my keras models.i am running keras with a tf backend and my sequential models are all built in keras for instance this is how my model is being built model.fit_generator generator data_gen(args steps_per_epoch=tr_steps epochs=args.epochs validation_data=data_gen(args validation_steps=val_steps verbose callbacks= checkpointer i am at a loss for how i would try to generate a timeline trace for this model and wanted to know if there is any related documentation for how i can do this
228374198,9867,https://api.github.com/repos/tensorflow/tensorflow/issues/9867,guschmue,2,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu windows tensorflow installed from source or binary binary tensorflow version use command below bunknown bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce follow describe the problemthe memory footprint for parameter server keep growing.the cause is a memory leak in a windows specific path in grpc which is fixed here filed the issue issue so others dont need to spend the time debugging it and as reason to update the grpc version
228354406,9864,https://api.github.com/repos/tensorflow/tensorflow/issues/9864,jbedorf,2,0,0,0,0,0,this pull request adds an additional communication path to tensorflow that allows tensors to be exchanged using mpi based send and recieve routines when running distributed tensorflow tthe used mpi implementation will pick the most optimal path available between the two communication points allowing the user to take advantage of high performance networks such as infiniband certain mpi implementations allow direct data-transfers from gpu buffers cuda aware gpudirect rdma this option can be enabled using an environment variable see the readme for more details.note be aware of the known problems that results in unpredictable behavior for certain complex networks any help/insight on that from other mpi experts would be appreciated
228288192,9859,https://api.github.com/repos/tensorflow/tensorflow/issues/9859,Hvass-Labs,6,0,0,0,0,0,i hope this question is considered suitable for asking here on github rather than stackoverflow as it relates to a possibly missing feature of the tensorflow api i apologize beforehand if i am wasting your time.this concerns tensorflow and tf.layers the tutorial on tf.layers uses manual reshaping to get from rank to rank tensors search for the word flatten is not very elegant and i would prefer to use a flatten function.the documentation for tf.layers.dense says something about flattening the input but it apparently does something else as discussed in other threads and does not seem to be any tf.layers.flatten function see e.g the api docs there is one for tf.contrib.layers.flatten as shown here wonder if flatten has been omitted for some reason when moving layers to tf core why or perhaps it has been moved somewhere else but i have searched and i cannot find it anywhere?furthermore i would like to ask if i can expect that tf.layers is going to be the standard builder api going forward or will you focus on keras instead i have previously used prettytensor there is also tf.slim and other builder apis i dont want to change builder api every months so id like to know what the tf developers are betting on this time
228012555,9839,https://api.github.com/repos/tensorflow/tensorflow/issues/9839,chris-chris,1,0,0,0,0,0,i applied the layer normalization technique to lstmcell and coupledinputforgetgatelstmcell. reference normalizationjimmy lei ba jamie ryan kiros geoffrey e hintonthese codes need some code reviews.can anybody help me
227920174,9832,https://api.github.com/repos/tensorflow/tensorflow/issues/9832,wirehack,2,0,0,0,0,0,"i intalled tensorflow from nightly binaries the tensorflow version is v..-rc--gfacf rc i have cuda and a tesla kc with g memory my os is linux ubuntu i tried to run the following code to test the latest beamsearchdecoder: lstm rnn.outputprojectionwrapper(rnn.layernormbasiclstmcell(n_hidden dropout_keep_prob=keep_prob n_classes)infer_decoder beamsearchdecoder(lstm embedding=lambda tokens:tf.nn.embedding_lookup(embedding_matrix tokens),start_tokens=start_tokens end_token=eos initial_state=encoder_state beam_width=)decoder_outputs_infer decoder_state_infer decoder_seq_infer dynamic_decode(infer_decoder) but i got traceback most recent call last file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto str_values compat.as_bytes(x for x in proto_values file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in listcomp str_values compat.as_bytes(x for x in proto_values file home/anxf/.local/lib/python./site-packages/tensorflow/python/util/compat.py line in as_bytes bytes_or_text,))typeerror expected binary or unicode string got noneduring handling of the above exception another exception occurred:traceback most recent call last file home/anxf/test_beam.py line in module decoder_outputs_infer decoder_state_infer decoder_seq_infer dynamic_decode(infer_decoder file home/anxf/.local/lib/python./site-packages/tensorflow/contrib/seqseq/python/ops/decoder.py line in dynamic_decode swap_memory=swap_memory file home/anxf/.local/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in while_loop result context.buildloop(cond body loop_vars shape_invariants file home/anxf/.local/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop pred body original_loop_vars loop_vars shape_invariants file home/anxf/.local/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop body_result body(*packed_vars_for_body file home/anxf/.local/lib/python./site-packages/tensorflow/contrib/seqseq/python/ops/decoder.py line in body decoder_finished decoder.step(time inputs state file home/anxf/.local/lib/python./site-packages/tensorflow/contrib/seqseq/python/ops/beam_search_decoder.py line in step length_penalty_weight=length_penalty_weight file home/anxf/.local/lib/python./site-packages/tensorflow/contrib/seqseq/python/ops/beam_search_decoder.py line in beam_search_step final_shape= static_batch_size beam_width file home/anxf/.local/lib/python./site-packages/tensorflow/contrib/seqseq/python/ops/beam_search_decoder.py line in tensor_gather_helper output array_ops.reshape(output final_shape file home/anxf/.local/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in reshape name=name file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op raise err file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op preferred_dtype=default_dtype file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in internal_convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file home/anxf/.local/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto supported type type(values values))typeerror failed to convert object of type class list to tensor contents none consider casting elements to a supported type.process finished with exit code i wonder how i can fix this problem or does this mean beamsearchdecoder is still testing and does not work right now"
227551182,9807,https://api.github.com/repos/tensorflow/tensorflow/issues/9807,terrytangyuan,1,0,0,0,0,0,fixes
227229564,9775,https://api.github.com/repos/tensorflow/tensorflow/issues/9775,nelson-liu,12,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu linux ubuntu tensorflow installed from source or binary binary tensorflow version use command below feature request training argument for contrib.rnn.dropoutwrapper for applying dropout depending on train/inference phase.in tf.layers.dropout the training parameter is a handy setting that lets you apply dropout depending on whether the model is training or doing inference its very convenient to be able to pass a boolean to the model placeholder and have it automatically do the right thing when it comes to dropout.unfortunately tf.contrib.rnn.dropoutwrapper does not have this same parameter and i think it would greatly benefit from it this is a feature request for it.i tried implementing it myself with tf.cond and either returning the dropped-out outputs/states or the untouched ones but i couldnt figure out how to share the variables between them in the cond
227227205,9774,https://api.github.com/repos/tensorflow/tensorflow/issues/9774,concerttttt,1,0,0,0,0,0,not sure if this is the right place for an error...the link to cifar source code on tensorflow webpage reports since yesterday afternoon may kindly help to fix it.this is the page found contains error is the link found to reports
227171250,9770,https://api.github.com/repos/tensorflow/tensorflow/issues/9770,av8ramit,1,0,0,0,0,0,please go to stack overflow for help and support you open a github issue here is our policy it must be a bug or a feature request the form below must be filled out. heres why we have that policy tensorflow developers respond to issues we want to focus on work that benefits the whole community e.g fixing bugs and adding features support only helps individuals github also notifies thousands of people when issues are filed we want them to see you communicating an interesting problem rather than being redirected to stack overflow system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu tensorflow installed from source or binary tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce :you can collect some of this information using our environment capture script can obtain the tensorflow version withpython c import tensorflow as tf print(tf.git_version tf.version describe the problemdescribe the problem clearly here be sure to convey here why its a bug in tensorflow or a feature request source code logsinclude any logs or source code that would be helpful to diagnose the problem if including tracebacks please include the full traceback large logs and files should be attached try to provide a reproducible test case that is the bare minimum necessary to generate the problem
226994008,9751,https://api.github.com/repos/tensorflow/tensorflow/issues/9751,sjperkins,1,0,0,0,0,0,this corrects for me
226947516,9747,https://api.github.com/repos/tensorflow/tensorflow/issues/9747,austinzh,5,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution darwin austins-mbp darwin kernel version fri mar pst root:xnu-..~/release_x x_mac os x tensorflow installed from source or binary) :binary tensorflow version use command below) :v..-rc--geced bazel version if compiling from source cuda/cudnn version :none gpu model and memory :none exact command to reproduce ref to codes tensorflow import tf.version tf.git_version v..-rc--geced tf.compiler_version v..-rc--geced sanity check array dtype=int describe the problemafter export and import a meta graph with uninitialized local variables,you can not inittialize them with sess.run(tf local_variables_initializer causetf do not register variables proto function with key local_variables and when export meta graph to protobuf source code can not find to_proto function from repository source code logs pythonimport tensorflow as tfgraph tf.graph()with graph.as_default x tf.variable collections= tf.graphkeys.local_variables y tf.variable z x yorigin_meta_graph tf.train.export_meta_graph(graph=graph)new_graph tf.graph()with new_graph.as_default tf.train.import_meta_graph(origin_meta_graph init tf.local_variables_initializer()with tf.session as sess sess.run(init) bashtraceback most recent call last file users/austin/workspace/aip/rd/tensorflow/tensorflow/test.py line in module init tf.local_variables_initializer file users/austin/workspace/aip/rd/tensorflow/venv/lib/python./site-packages/tensorflow/python/ops/variables.py line in local_variables_initializer return variables_initializer(local_variables file users/austin/workspace/aip/rd/tensorflow/venv/lib/python./site-packages/tensorflow/python/ops/variables.py line in variables_initializer return control_flow_ops.group(* v.initializer for v in var_list name=name file users/austin/workspace/aip/rd/tensorflow/venv/lib/python./site-packages/tensorflow/python/ops/variables.py line in listcomp return control_flow_ops.group(* v.initializer for v in var_list name=name)attributeerror tensor object has no attribute initializer pythonimport tensorflow as tfgraph tf.graph()with graph.as_default x tf.variable collections= tf.graphkeys.local_variables y tf.variable z x yorigin_meta_graph tf.train.export_meta_graph(graph=graph)new_graph tf.graph()with new_graph.as_default tf.train.import_meta_graph(origin_meta_graph)print(graph.get_collection(tf.graphkeys.local_variables))print(new_graph.get_collection(tf.graphkeys.local_variables)) bash as it show above in origin graph local_variable collection is a list of tf.variable but in the new graph is a list of tf.tensor work aroundadd following registration in your model coreor ref to this pr tensorflow.core.framework import variable_pbfrom tensorflow.python.framework import opsfrom tensorflow.python.ops import variablesfrom tensorflow.python.framework.ops import register_proto_functionregister_proto_function ops.graphkeys.local_variables proto_type=variable_pb.variabledef to_proto=variables.variable.to_proto from_proto=variables.variable.from_proto"
226933344,9745,https://api.github.com/repos/tensorflow/tensorflow/issues/9745,salvador-dali,1,0,0,0,0,0,is it possible to add a function to calculate a distance between each pair of the two collections of inputs scipy has it implemented in distance.sdist and supports many distance metrics.is it possible to have this function in tf i have made an implementation for eucledian distance but have no idea how to implement it for other metrics
226777261,9724,https://api.github.com/repos/tensorflow/tensorflow/issues/9724,kwotsin,3,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu ubuntu lts tensorflow installed from source or binary source tensorflow version use command below bazel version if compiling from source cuda/cudnn version gpu model and memory gtx m describe the problemfrom using the official freeze_graph.py file from tf i am getting a very low accuracy in prediction as compared to manually exporting the graph using a file i wrote called write_pb.py i get a much higher accuracy.to be specific here are the differences: differences using write_pb.py to manually export the graph converted way more variables to constants even with the same checkpoint files it takes a long long time for freeze_graph.py to actually complete the exporting very importantly i get a very low accuracy from using freeze_graph meanwhile by exporting the graph manually i get a nearly identical accuracy as if i predicted an image right from the checkpoint without freezing manually exporting the graph results in a smaller file size just mb of difference the manually exported graph has a faster inference time than the graph obtained from freeze_graph.py .this is the freeze_graph.py file i got from the tf repo is the write_pb file i wrote source code logsusing freeze_graph with this command python freeze_graph.py input_checkpoint=model.ckpt input_graph=graph.pbtxt output_graph=frozen_model_from_freeze_graph.pb output_node_names=inceptionresnetv/logits/predictions i get this output i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx mmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx m pci bus id i tensorflow/compiler/xla/service/platform_util.cc platform cuda present with visible devices i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices i tensorflow/compiler/xla/service/service.cc xla service xced executing computations on platform host devices i tensorflow/compiler/xla/service/service.cc streamexecutor device undefined undefined i tensorflow/compiler/xla/service/platform_util.cc platform cuda present with visible devices i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices i tensorflow/compiler/xla/service/service.cc xla service xce executing computations on platform cuda devices i tensorflow/compiler/xla/service/service.cc streamexecutor device geforce gtx m compute capability converted variables to const ops ops in the final graph. meanwhile if i manually freeze the graph using write_pb.py ,i get this output i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx mmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx m pci bus id i tensorflow/compiler/xla/service/platform_util.cc platform cuda present with visible devices i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices i tensorflow/compiler/xla/service/service.cc xla service xef executing computations on platform host devices i tensorflow/compiler/xla/service/service.cc streamexecutor device undefined undefined i tensorflow/compiler/xla/service/platform_util.cc platform cuda present with visible devices i tensorflow/compiler/xla/service/platform_util.cc platform host present with visible devices i tensorflow/compiler/xla/service/service.cc xla service xef executing computations on platform cuda devices i tensorflow/compiler/xla/service/service.cc streamexecutor device geforce gtx m compute capability exporting graph...converted variables to const ops"
226765955,9722,https://api.github.com/repos/tensorflow/tensorflow/issues/9722,sudhirshahu51,2,0,0,0,0,0,c:\users\sudhit>pip install ignore-installed upgrade is not a supported wheel on this platform.how will i know which will be supported wheel for my platform
226763265,9720,https://api.github.com/repos/tensorflow/tensorflow/issues/9720,carlthome,2,0,0,0,0,0,from the documentation of tf.while_loop for training tensorflow remembers the tensors that are produced in the forward inference but needed in back propagation these tensors can be a main source of memory consumption and often cause oom problems when training on gpus when the flag swap_memory is true we swap out these tensors from gpu to cpu this for example allows us to train rnn models with very long sequences and large batches.but after having trained is tensorflows engine smart enough to avoid swapping even if swap_memory=true or should optimize_for_inference.py flip swap_memory i.e make the equivalent modifications to the graph)?naturally this only applies to running inference on a gpu.@yuanbyu
226528480,9682,https://api.github.com/repos/tensorflow/tensorflow/issues/9682,vinay-hebb,1,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow custom code os platform and distribution e.g linux ubuntu windows bit tensorflow installed from source or binary binary tensorflow version use command below tensorflow-..-cp-cpm-win_amd bazel version if compiling from source cuda/cudnn version gpu model and memory exact command to reproduce run the below code in python idle used pip installation for cpu only describe the problemi have used following code in tf and tf in tf it executes and crashes in tf in tf i dont know any way to get weights(or prior probabilities for each component in a given gmm changes related to weights are added in tf hence i need to run tf from tensorflow.contrib.factorization.python.ops import gmm as gmm_lib import random import numpy as npimport tensorflow as tfmu sigma x_d np.random.randn astype(f gmm gmm_lib.gmm(,random_seed gmm.fit(x_d source code logsi got the below crash in tf warning:tensorflow:using temporary folder as model directory c:\users\hvinay~.cor\appdata\local\temp\tmptpejlidwarning from warnings module file python installed directory>\lib\site-packages\tensorflow\python\util\deprecation.py line equality a bfuturewarning comparison to none will result in an elementwise object comparison in the future.warning:tensorflow:from c:\users\hvinay.corp\documents\tf\gmmest.py calling baseestimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))traceback most recent call last file python installed directory>\lib\site-packages\tensorflow\python\client\session.py line in do_call return fn(*args file python installed directory>\lib\site-packages\tensorflow\python\client\session.py line in run_fn status run_metadata file python installed directory>\lib\contextlib.py line in exit next(self.gen file python installed directory>\lib\site-packages\tensorflow\python\framework\errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.invalidargumenterror you must feed a value for placeholder tensor input with dtype float node input placeholder dtype=dt_float shape device=/job:localhost/replica:/task:/cpu: () during handling of the above exception another exception occurred:traceback most recent call last file c:\users\hvinay.corp\documents\tf\gmmest.py line in module gmm.fit(x_d file python installed directory>\lib\site-packages\tensorflow\python\util\deprecation.py line in new_func return func(*args kwargs file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in fit skcompat(self).fit(x y batch_size steps max_steps monitors file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in fit monitors=all_monitors file python installed directory>\lib\site-packages\tensorflow\python\util\deprecation.py line in new_func return func(*args kwargs file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in fit loss self._train_model(input_fn=input_fn hooks=hooks file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in train_model config=config_pb.configproto(allow_soft_placement=true file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in monitoredtrainingsession stop_grace_period_secs=stop_grace_period_secs file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in init stop_grace_period_secs=stop_grace_period_secs file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in init self._sess recoverablesession(self._coordinated_creator file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in init wrappedsession.__init__(self self._create_session file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in create_session return self._sess_creator.create_session file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in create_session self.tf_sess self._session_creator.create_session file python installed directory>\lib\site-packages\tensorflow\python\training\monitored_session.py line in create_session init_fn=self._scaffold.init_fn file python installed directory>\lib\site-packages\tensorflow\python\training\session_manager.py line in prepare_session sess.run(init_op feed_dict=init_feed_dict file python installed directory>\lib\site-packages\tensorflow\python\client\session.py line in run run_metadata_ptr file python installed directory>\lib\site-packages\tensorflow\python\client\session.py line in run feed_dict_string options run_metadata file python installed directory>\lib\site-packages\tensorflow\python\client\session.py line in do_run target_list options run_metadata file python installed directory>\lib\site-packages\tensorflow\python\client\session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror you must feed a value for placeholder tensor input with dtype float node input placeholder dtype=dt_float shape device=/job:localhost/replica:/task:/cpu: () caused by op input defined at file string line in module file python installed directory>\lib\idlelib\run.py line in main ret method(*args kwargs file python installed directory>\lib\idlelib\run.py line in runcode exec(code self.locals file c:\users\hvinay.corp\documents\tf\gmmest.py line in module gmm.fit(x_d file python installed directory>\lib\site-packages\tensorflow\python\util\deprecation.py line in new_func return func(*args kwargs file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in fit skcompat(self).fit(x y batch_size steps max_steps monitors file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in fit monitors=all_monitors file python installed directory>\lib\site-packages\tensorflow\python\util\deprecation.py line in new_func return func(*args kwargs file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in fit loss self._train_model(input_fn=input_fn hooks=hooks file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py line in train_model features labels input_fn file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\data_feeder.py line in input_builder self._input_dtype input file python installed directory>\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\data_feeder.py line in get_placeholder dtypes.as_dtype(dtype none shape name=name_prepend file python installed directory>\lib\site-packages\tensorflow\python\ops\array_ops.py line in placeholder name=name file python installed directory>\lib\site-packages\tensorflow\python\ops\gen_array_ops.py line in placeholder name=name file python installed directory>\lib\site-packages\tensorflow\python\framework\op_def_library.py line in apply_op op_def=op_def file python installed directory>\lib\site-packages\tensorflow\python\framework\ops.py line in create_op original_op=self._default_original_op op_def=op_def file python installed directory>\lib\site-packages\tensorflow\python\framework\ops.py line in init self._traceback extract_stack()invalidargumenterror see above for traceback you must feed a value for placeholder tensor input with dtype float node input placeholder dtype=dt_float shape device=/job:localhost/replica:/task:/cpu"
226481182,9678,https://api.github.com/repos/tensorflow/tensorflow/issues/9678,haikuoyao,2,0,0,0,0,0,"i try tf stylize on android and it works perfectly also i find the training code of this image stylization model its name is magenta/image_stylization and provides two pre-trained models monet and varied the first one had styles and second has so my idea it to use them to replace image stylization model is what i did save ckpt model to pb save graph import tensorflow as tfimport modelimport opsnum_styles imgwidth imgheight channel checkpoint models/multistyle-pastiche-generator-monet.ckptinputimage tf.placeholder(tf.float,shape= none,imgwidth,imgheight,channel ,name=input)styles tf.placeholder(tf.float,shape= num_styles ,name=style_num)with tf.name_scope transform model.transform(inputimage normalizer_fn=ops.weighted_instance_norm normalizer_params weights tf.constant(mixture weights styles num_categories num_styles center true scale true})model_saver tf.train.saver(tf.global_variables())with tf.session as sess tf.train.write_graph(sess.graph_def models input.pb) freeze graph bazel-bin/tensorflow/python/tools/freeze_graph input_graph=input.pb input_checkpoint=multistyle-pastiche-generator-monet.ckpt output_node_names=transformer/expand/conv/conv/sigmoid input_binary=false output_graph=frozen.pb inference bazel-bin/tensorflow/python/tools/optimize_for_inference input=frozen.pb output=inference.pb input_names=input output_names=transformer/expand/conv/conv/sigmoid frozen_graph=true quantize bazel-bin/tensorflow/tools/quantization/quantize_graph input=inference.pb output=quantize_graph.pb output_node_names=transformer/expand/conv/conv/sigmoid mode=weights_rounded replace model with quantize_graph.pb then i got an issue.i can see there are styles and they display on the screen screenshot the image is not transformed theres no style on the image its the just original image.tf version is android is anyone met the same issue or anyone knew how exactly to convert these two models and use on mobile"
226434645,9672,https://api.github.com/repos/tensorflow/tensorflow/issues/9672,timttate,13,0,0,0,2,0,descriptiontrying to use tf.contrib.image.rotate produces the error tensorflow.python.framework.errors_impl.notfounderror op type not registered imageprojectivetransform this appears to happen on windows using both cpu and gpu and does not appear to happen on linux ubuntu tensorflow test case import tensorflow as tfimages tf.zeros rotated_images tf.contrib.image.rotate(images traceback traceback most recent call last file test.py line in module rotated_images tf.contrib.image.rotate(images file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\contrib\image\python\ops\image_ops.py line in rotate angles_to_projective_transforms(angles image_width image_height file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\contrib\image\python\ops\image_ops.py line in transform output gen_image_ops.image_projective_transform(images transforms file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\contrib\image\ops\gen_image_ops.py line in image_projective_transform transforms=transforms name=name file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\op_def_library.py line in apply_op op_def=op_def file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\ops.py line in create_op set_shapes_for_outputs(ret file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\ops.py line in set_shapes_for_outputs shapes shape_func(op file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\common_shapes.py line in call_cpp_shape_fn debug_python_shape_fn require_shape_fn file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\common_shapes.py line in call_cpp_shape_fn_impl input_tensors_as_shapes status file c:\users\tim\anaconda\lib\contextlib.py line in exit next(self.gen file c:\users\tim\didi-car\tf_env\lib\site-packages\tensorflow\python\framework\errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.notfounderror op type not registered imageprojectivetransform system information windows pro tensorflow installed via pip into virtual environment python anaconda
226290528,9661,https://api.github.com/repos/tensorflow/tensorflow/issues/9661,cancan101,2,0,0,0,0,0,tensorflow using xla is able to aot compile a graph using tfcompile there does not seem to be a way to or it it not documented cross compile the graph ie compile on os x for deployment on ios related so question).i suggest adding a means of performing this cross compilation
226208082,9654,https://api.github.com/repos/tensorflow/tensorflow/issues/9654,JulesKzl,1,0,0,0,0,0,im using tensorflow high-level api estimators to create my neural net but im using it into a class and i have to call an instance of my class to generate the model of the neural network here self.a ) class neuralnetwork(object def init__(self create neural net regressor tf.estimator.estimator(model_fn=self.my_model_fn model_dir=/tmp/data def my_model_fn(self features labels mode generate neural net model self.a a predictions loss train_op return tf.estimator.estimatorspec mode=mode predictions=predictions loss=loss train_op=train_op) but i get the error valueerror model_fn has following not expected args self i tried to remove the self for the args of my model but got another error typeerror got multiple values for keyword argument solution for now as it was suggested on stackoverflow is to use a lambda function to wrap my function my_model_fn see below but it will be nicer without it. class neuralnetwork(object def init__(self create neural net regressor tf.estimator.estimator model_fn=lambda features labels mode self.my_model_fn(features labels mode model_dir=/tmp/data
226155720,9646,https://api.github.com/repos/tensorflow/tensorflow/issues/9646,ZhenghaoFei,0,0,0,0,0,4,"hi,here i find a possible memory leaking bug describe the problemwhen we doing session.run var if we add operator before that variable such as session.run(-var memory will keep growing by the iteration when we call the train function with var thousands of times it will use up all the memory and also let the running time become much slower source code logshere is a minimal example code: normal situation: import tensorflow as tf@profiledef run a tf.constant b tf.constant c a b sess tf.session for i in range b sess.run(c run()line mem usage increment line contents mib mib profile def run mib mib a tf.constant mib mib b tf.constant mib mib c a b mib mib sess tf.session mib mib for i in range mib mib b sess.run(c) memory leaking import tensorflow as tf@profiledef run a tf.constant b tf.constant c a b sess tf.session for i in range b sess.run(-c)run line mem usage increment line contents mib mib profile def run mib mib a tf.constant mib mib b tf.constant mib mib c a b mib mib sess tf.session mib mib for i in range mib mib b sess.run(-c system information os platform and distribution e.g linux ubuntu mac os and linux ubuntu tensorflow installed from source or binary pip version tensorflow version use command below v..--gedf-dirty bazel version if compiling from source cuda/cudnn version n/a gpu model and memory n/a exact command to reproduce see above"
226061814,9638,https://api.github.com/repos/tensorflow/tensorflow/issues/9638,smistad,1,0,0,0,0,0,system info ubuntu bit gcc intel i cpu segmentation fault occurs in eigen when a certain avx instruction is performed see stack trace below this occurs during session run of several convolutional neural network graphs.tensorflow checked out from the master branch today is built using cmake with tensorflow_build_shared_lib enabled which generates a libtensorflow.so library file this library file is linked to another c application which simply loads a graph and executes it.disabling the cmake option tensorflow_optimize_for_native_arch removes the error but probably also reduce performance.below is a nasty long stack trace if you need any other info please let me know.stack trace xfffeeb in mm_store_ps a p=xfffccd at usr/lib/gcc/x_-linux-gnu//include/avxintrin.h eigen::internal::pstore(float float vector const to=xfffccd from at home/smistad/workspace/fast/build_release/external/tensorflow/external/eigen_archive/unsupported/eigen/cxx/../../../eigen/src/core/arch/avx/packetmath.h xfffefdb in eigen::internal::gemm_pack_lhs::operator this=xffffda blocka=xfffccd lhs depth rows stride offset at home/smistad/workspace/fast/build_release/external/tensorflow/external/eigen_archive/unsupported/eigen/cxx/../../../eigen/src/core/products/generalblockpanelkernel.h xfffeff in eigen::tensorevaluator::context::context::context::context(std::tuple std::_index_tuple this=xfffcaaca args=::context::context::workerloop this=xbda thread_id at home/smistad/workspace/fast/build_release/external/tensorflow/external/eigen_archive/unsupported/eigen/cxx/src/threadpool/nonblockingthreadpool.h xfffedebade in eigen::nonblockingthreadpooltempl::nonblockingthreadpooltempl(int bool tensorflow::thread::eigenenvironment)::{lambda()#}::operator const at home/smistad/workspace/fast/build_release/external/tensorflow/external/eigen_archive/unsupported/eigen/cxx/src/threadpool/nonblockingthreadpool.h xfffedebe in std::_function_handler::nonblockingthreadpooltempl(int bool tensorflow::thread::eigenenvironment)::{lambda()#}>::_m_invoke(std::_any_data const functor at usr/include/c++//functional xfffedebac in std::function::_m_invoke(std::_any_data const functor at usr/include/c++//functional xfffedebac in std::function(std::_index_tuple this=x at usr/include/c++//functional xfffedef in std::_bind_simple
226046465,9637,https://api.github.com/repos/tensorflow/tensorflow/issues/9637,BigHopes,1,0,0,0,0,0,i am learning about convolutional neural networks from the tutorial inside it there is a link for getting the code but it is not working can i get the code
226035782,9634,https://api.github.com/repos/tensorflow/tensorflow/issues/9634,momeara,4,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow no os platform and distribution e.g linux ubuntu centos release final x tensorflow installed from source or binary source tensorflow version use command below master git version dfbcadfdcaabcfad checked out may rd bazel version if compiling from source bazel cuda/cudnn version gpu model and memory quadro k gb ddr exact command to reproduce : setenv cc mnt/nfs/home/momeara/opt/bin/gccsetenv cxx mnt/nfs/home/momeara/opt/bin/g++setenv extra_bazel_args verbose_failures jobs=setenv cplus_include_path mnt/nfs/home/momeara/opt/includesetenv c_include_path mnt/nfs/home/momeara/opt/includesetenv library_path mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/libsetenv ld_library_path mnt/nfs/work/momeara/sea/deepsea/cuda/lib:/usr/local/cuda-./lib:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib:/usr/local/cuda-./extras/cupti/lib:$ld_libr\ary_pathsetenv path usr/local/cuda-./bin:/mnt/nfs/work/momeara/sea/deepsea/tensorflow/bazel-../output:$path./configureplease specify the location of python default is mnt/nfs/work/momeara/tools/anaconda/envs/sea/bin/python :found possible python library paths mnt/nfs/work/momeara/tools/anaconda/envs/sea/lib/python./site-packagesplease input the desired python library path to use default is mnt/nfs/work/momeara/tools/anaconda/envs/sea/lib/python./site-packages using python library path mnt/nfs/work/momeara/tools/anaconda/envs/sea/lib/python./site-packagesplease specify optimization flags to use during compilation when bazel option config=opt is specified default is march=native :do you wish to use jemalloc as the malloc implementation y/n njemalloc disableddo you wish to build tensorflow with google cloud platform support y/n nno google cloud platform support will be enabled for tensorflowdo you wish to build tensorflow with hadoop file system support y/n nno hadoop file system support will be enabled for tensorflowdo you wish to build tensorflow with the xla just-in-time compiler experimental y/n yxla jit support will be enabled for tensorflowdo you wish to build tensorflow with verbs support y/n yverbs support will be enabled for tensorflowdo you wish to build tensorflow with opencl support y/n nno opencl support will be enabled for tensorflowdo you wish to build tensorflow with cuda support y/n ycuda support will be enabled for tensorflowdo you want to use clang as cuda compiler y/n nnvcc will be used as cuda compilerplease specify the cuda sdk version you want to use e.g leave empty to use system default please specify the location where cuda toolkit is installed refer to readme.md for more details default is usr/local/cuda :please specify which gcc should be used by nvcc as the host compiler default is mnt/nfs/home/momeara/opt/bin/gcc :please specify the cudnn version you want to use leave empty to use system default please specify the location where cudnn library is installed refer to readme.md for more details default is usr/local/cuda mnt/nfs/work/momeara/sea/deepsea/cudaplease specify a list of comma-separated cuda compute capabilities you want to build with.you can find the compute capability of your device at note that each additional compute capability significantly increases your build time and binary size. default is warning output base mnt/nfs/home/momeara/.cache/bazel/_bazel_momeara/efabebbdc is on nfs this may lead to surprising failures and undetermined behavior.................................................................................____starting clean this may take a while consider using async if the clean takes more than several minutes.configuration finishedbazel output_user_root=/scratch/momeara/.cache/baze build c opt config=cuda tensorflow/tools/pip_package:build_pip_package verbose_failures jobs describe the problemduring configure it tries to make a symlink ln s mnt/nfs/work/momeara/sea/deepsea/cuda/include/cudnn.h scratch/momeara/.cache/baze/efabebbdc/execroot/tensorflow/bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include/cudnn.hbut this fails because the directory scratch/momeara/.cache/baze/efabebbdc/execroot/tensorflow/bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include does not existnotice that it has include/include at the end.if i change this line genrules.append(_symlink_genrule_for_dir(repository_ctx none cudnn-include cudnn_header_dir cudnn.h include/cudnn.h ))to genrules.append(_symlink_genrule_for_dir(repository_ctx none cudnn-include cudnn_header_dir cudnn.h cudnn.h ))the build proceeds without error source code logs writing script external/local_config_cuda/cuda/cuda-include.genrule_script.sh for host error scratch/momeara/.cache/baze/efabebbdc/external/local_config_cuda/cuda/build executing genrule local_config_cuda//cuda:cudnn-include failed bash failed error exec\uting command cd scratch/momeara/.cache/baze/efabebbdc/execroot/tensorflow exec env ld_library_path=/mnt/nfs/work/momeara/sea/deepsea/cuda/lib:/usr/local/cuda-./lib:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib:/usr/local/cuda-./extras/cupti/lib:/mnt/nfs/ho\me/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib path=/usr/local/cuda-./bin:/mnt/nfs/work/momeara/sea/deepsea/tensorflow/tensorflow/bazel-../output:/mnt/nfs/work/momeara/tools/anaconda/envs/sea/bin:/mnt/nfs/work/momeara/tools/anaconda/bin:\/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v..-linux-x/bin:/mnt/nfs/home/momeara/opt/bin:/mnt/nfs/work/momeara/tools/anaconda/envs/sea/bin:/mnt/nfs/work/momeara/tools/anacond\a/bin:/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v..-linux-x/bin:/mnt/nfs/home/momeara/opt/bin:/mnt/nfs/work/momeara/tools/anaconda/envs/sea/bin:/mnt/nfs/work/momeara/tools/\anaconda/bin:/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v..-linux-x/bin:/mnt/nfs/home/momeara/opt/bin:/usr/lib/qt-./bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:\/bin:/usr/bin bin/bash c source external/bazel_tools/tools/genrule/genrule-setup.sh;ln s mnt/nfs/work/momeara/sea/deepsea/cuda/include/cudnn.h bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include/cudnn.h com.google.devtools.build.lib.shell.badexitstatusexcept\ion process exited with status ln creating symbolic link bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include/cudnn.h no such file or directoryblaze leaving directory scratch/momeara/.cache/baze/efabebbdc/execroot/tensorflow/____building complete.target tensorflow/tools/pip_package:build_pip_package failed to build____elapsed time s critical path s
225915286,9620,https://api.github.com/repos/tensorflow/tensorflow/issues/9620,bkovalev,2,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow not use last tf master and last models os platform and distribution e.g linux ubuntu ubuntu tensorflow installed from source or binary from source dramatically less compilation number of files less in files tensorflow version use command below v..-rc--gdf rc bazel version if compiling from source cuda/cudnn version gpu model and memory nvidia p pci gb exact command to reproduce :on ps bazel-bin/inception/imagenet_distributed_train job_name=ps task_id ps_hosts worker_hosts=...:,...:all good info:tensorflow:ps hosts are info:tensorflow:worker hosts are i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps localhost i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost describe the problemon worker cuda_visible_devices bazel-bin/inception/imagenet_distributed_train batch_size job_name=worker ps_hosts worker_hosts data_dir=/data/imagenet_data train_dir=/data/imagenet_train task_id=its failed with info:tensorflow:ps hosts are info:tensorflow:worker hosts are i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job ps i tensorflow/core/distributed_runtime/rpc/grpc_channel.cc initialize grpcchannelcache for job worker localhost i tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc started server with target grpc://localhost traceback most recent call last file root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py line in module tf.app.run file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py line in main inception_distributed_train.train(server.target dataset cluster_spec file root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_distributed_train.py line in train inception.loss(logits labels file root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_model.py line in loss weight file root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/losses.py line in cross_entropy_loss cross_entropy tf.contrib.nn.deprecated_flipped_softmax_cross_entropy_with_logits file usr/local/lib/python./dist-packages/tensorflow/python/util/lazy_loader.py line in getattr module self._load file usr/local/lib/python./dist-packages/tensorflow/python/util/lazy_loader.py line in load module importlib.import_module(self.__name file usr/lib/python./importlib/__init__.py line in import_module import__(name file usr/local/lib/python./dist-packages/tensorflow/contrib/__init__.py line in module from tensorflow.contrib import image file usr/local/lib/python./dist-packages/tensorflow/contrib/image/__init__.py line in module from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms file usr/local/lib/python./dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py line in module single_image_random_dot_stereograms.so file usr/local/lib/python./dist-packages/tensorflow/contrib/util/loader.py line in load_op_library ret load_library.load_op_library(path file usr/local/lib/python./dist-packages/tensorflow/python/framework/load_library.py line in load_op_library none none error_msg error_code)tensorflow.python.framework.errors_impl.notfounderror usr/local/lib/python./dist-packages/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so undefined symbol zngoogleprotobufinternallogmessagecens_loglevelepkcithanks,boris"
225502749,9576,https://api.github.com/repos/tensorflow/tensorflow/issues/9576,weiliu620,1,0,0,0,0,0,"im using tf.nn.convolution to implement the dilated convolution in d i got no algorithm without scratch worked error during training here is the related codeto define the model, def inference(self images is_training keep_prob forward inference return c self._dilation_conv(images self._n_filters scope c dilation_rate score self._conv(c scope score filters filter_size pred tf.nn.softmax(score pdb.set_trace return score pred def conv(self in_tensor filters scope filter_size none if self._wd is none myreg none else myreg tf.contrib.layers.l_regularizer(float(self._wd if filter_size is none filter_size self._filter_size with tf.variable_scope(scope return tf.layers.convd in_tensor filters filters kernel_size filter_size padding valid activation none kernel_initializer tf.truncated_normal_initializer(stddev self._stddev kernel_regularizer myreg name conv def dilation_conv(self in_tensor n_filters scope dilation_rate filter_size none dilated convolution filter with batch norm if self._wd is none myreg none else myreg tf.contrib.layers.l_regularizer(float(self._wd if filter_size is none filter_size self._filter_size batch_size h w d in_channel in_tensor.get_shape().as_list with tf.variable_scope(scope kernel tf.get_variable(weights shape self._filter_size in_channel n_filters dtype tf.float initializer tf.truncated_normal_initializer(stddev=self._stddev output tf.nn.convolution(in_tensor kernel padding same strides dilation_rate dilation_rate name dilation_conv somehow set training true even for testing phase works better if self._bn output tf.layers.batch_normalization output training true name bn return tf.nn.relu(output relu def get_loss(self labels scores beta return total loss of the model including data loss and regularization loss it looks that softmax_cross_entropy_with_logits does not need the input logits dimension as long as the last dimension is for classes it should work so we do not need reshape the input tensor tf has a weighted_cross_entropy_with_logits but this function is for multi-class problem i.e a picture may have both a dog and a truck class_weight tf.constant( beta beta tf suppot numpys broadcasting score array has dim bxy weights array has dim which is broadcast to bxy weighted_logits tf.multiply(scores class_weight both logits and labels are bxy dimension cross_entropy tf.nn.softmax_cross_entropy_with_logits(logits weighted_logits labels labels fromm dim bxy to dim scalar cross_entropy_mean tf.reduce_mean(cross_entropy name cross_entropy reg_loss_list tf.get_collection(tf.graphkeys.regularization_losses if reg_loss_list return cross_entropy_mean tf.add_n(reg_loss_list else return cross_entropy_mean and to train the model i used tf.train.adamoptimizer i didnt paste the training related code since i dont think they are relevant but i can add them later if that helps here is the error logs i saw in train.train(train_dataset test_dataset model model out_ckpt=./ckpt_d/dilation summary_dir=./summary_d/dilation i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name graphics device pci bus id notfounderror traceback most recent call last)/home/weiliu/.local/lib/python./site-packages/tensorflow/python/client/session.py in do_call(self fn args try return fn(*args except errors.operror as e:/home/weiliu/.local/lib/python./site-packages/tensorflow/python/client/session.py in run_fn(session feed_dict fetch_list target_list options run_metadata feed_dict fetch_list target_list status run_metadata usr/lib/python./contextlib.py in exit__(self type value traceback try next(self.gen except stopiteration:/home/weiliu/.local/lib/python./site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status compat.as_text(pywrap_tensorflow.tf_message(status pywrap_tensorflow.tf_getcode(status finally:notfounderror no algorithm without scratch worked node gradients/c/dilation_conv_grad/convdbackpropinputv convdbackpropinputv t=dt_float padding=same strides device=/job:localhost/replica:/task:/gpu: (gradients/c/dilation_conv_grad/shape c/weights/read gradients/addn node adam/update recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__adam/update tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () during handling of the above exception another exception occurred:notfounderror traceback most recent call last)"
225419182,9566,https://api.github.com/repos/tensorflow/tensorflow/issues/9566,ppwwyyxx,3,0,0,0,0,0,curl wc c mb) each page in the documentation now contains a huge left navbar contributing over of the size this would waste a lot of network traffic on loading identical navbar over and over again it creates a big trouble when i tried to build an offline version of the doc the whole html documents used to be mb now they are gb
225309082,9549,https://api.github.com/repos/tensorflow/tensorflow/issues/9549,sachinprabhu007,2,0,0,0,1,0,"hii tried basic program in python shell it fails to create session please assist.thanks python default oct gcc on linuxtype help copyright credits or license for more information import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally hello tf.constant(hi,tensorflow sess tf.session()i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa no de so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_device.cc found device with prop erties:name tesla kcmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibw tensorflow/stream_executor/cuda/cuda_driver.cc creating context when one is currently active existing xefee tensorflow/core/common_runtime/direct_session.cc internal failed initial izing streamexecutor for cuda device ordinal internal failed call to cudevic eprimaryctxretain cuda_error_invalid_devicetraceback most recent call last file stdin line in module file usr/local/lib/python./dist-packages/tensorflow/python/client/session py line in init super(session self).__init__(target graph config=config file usr/local/lib/python./dist-packages/tensorflow/python/client/session py line in init self._session tf_session.tf_newdeprecatedsession(opts status file usr/lib/python./contextlib.py line in exit self.gen.next file usr/local/lib/python./dist-packages/tensorflow/python/framework/error s_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.internalerror failed to create session"
225287288,9545,https://api.github.com/repos/tensorflow/tensorflow/issues/9545,ethanluoyc,8,0,0,0,0,0,system information have i written custom code as opposed to using a stock example script provided in tensorflow os platform and distribution e.g linux ubuntu macos sierra tensorflow installed from source or binary) :pip tensorflow version use command below cpu describe the problemi am trying to implement ec available from basically it is a neural network that is used for learning a transition model using neural networks in the training set i have data of the form x_t x_t where both x_t and x_t needs to be transformed by an encoding network e.g a variational autoencoder i use the following snippet for creating the encoding network adapted from def encode(self x share=none fc tf.contrib.layers.fully_connected with tf.variable_scope(encoder reuse=share l fc(x weights_initializer=tf.orthogonal_initializer activation_fn=tf.nn.relu l fc(l weights_initializer=tf.orthogonal_initializer activation_fn=tf.nn.relu return l def decode(self z share=none fc tf.contrib.layers.fully_connected with tf.variable_scope(decoder reuse=share l fc(z weights_initializer=tf.orthogonal_initializer activation_fn=tf.nn.relu l fc(l weights_initializer=tf.orthogonal_initializer activation_fn=tf.nn.relu return fc(l self.x_dim weights_initializer=tf.orthogonal_initializer activation_fn=tf.nn.sigmoid) then i would use something like pythonh_enc_t encoder(x_t)h_enc_t_next encoder(x_{t share=true) to create the encoded output for the model.the problem is that when visualizing this on tensorboard while it is sharing the variables by setting share=true for the variable scope on the graph visulisation you will have encoder and encoder instead of just a decoder scope of course they took different input since we need to transform x_t and x_t but shouldnt the network be wrapped in the same scope since underneath we are reusing the same weights i wonder if it is a feature to have encoder and encoder separately or it is a limitation of the variable scoping the problem is illustrated in the screenshot below you will see duplicates for encoder sampleqphi etc:! graph-run i would expect something like this as appeared in the paper to be a more reasonable visualization h_enc with input x_t and x_t are the same network:
225263792,9540,https://api.github.com/repos/tensorflow/tensorflow/issues/9540,yongtang,1,0,0,0,0,0,this fix tries to address the issue raised in to support slice of sparse tensors.this fix fixes signed-off-by yong tang yong.tang.github@outlook.com
225233420,9530,https://api.github.com/repos/tensorflow/tensorflow/issues/9530,BDHU,1,0,0,0,0,0,i get the following error when i try to run a python program: importerror libcudnn.so cannot open shared object file no such file or directory i understand this is a simple error most people would encounter when they start tensorflow i did checked the solutions on stackoverflow and i managed to run the same program in terminal what i did is i made an virtualenv and i activated it and when i try using python to run the program it successfully proceeds without any error however the error always appear when i run the program in pycharm even with python interpreter set to the location of virtualenv i created can someone tell me why as i had no idea why it works in terminal but not with pycharm
225221029,9527,https://api.github.com/repos/tensorflow/tensorflow/issues/9527,fxsuper,2,0,0,0,0,0,im running on tf and ive used tf.while_loop tensorarray to implement dynamic unrolling of a type of recurrence that i previously unrolled statically through python code the difference in speed is very dramatic with forward inference being about x slower when dynamically unrolled and backprop about x slower is this expected are there any tricks for optimization that im missing this is on cpu performance gap on gpu is even larger
225164816,9519,https://api.github.com/repos/tensorflow/tensorflow/issues/9519,rightaditya,13,0,0,0,0,0,system information custom code worked fine on os linux ubuntu installed binary via pip tensorflow version v..-rc--geced cuda v cudnn v gtx gb ram to reproduce run tensorboard try to filter runs in web interface nothing happens problem descriptionrunning tensorboard with v gives me the following warnings in the console repeated four times once the web interface is opened: warning:tensorflow:path external/data/plugin/text/runs not found sending at first i ignored it but it turns out that when examining the runs in the web interface the regex filter for the runs doesnt work at all this exact command in the exact same folder with the exact same logs worked without issue with v
225163441,9518,https://api.github.com/repos/tensorflow/tensorflow/issues/9518,Celelibi,1,0,0,0,0,0,system information os platform and distribution e.g linux ubuntu linux rbsylaptop amd smp debian x gnu/linux tensorflow installed from source or binary pip install tensorflow-gpu tensorflow version use command below) :tf.version tf.git_version v..-rc--gecedtf.compiler_version v..-rc--geced gpu model and memory :found device with properties name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gib describe the problemit looks like there is no eigen vector kernel that would run on gpu even the cpu version seems to be serial as it uses only one core for a single matrix source code logsthis code just create a random matrix and try to compute its eigen values and vectors on the gpu mat np.random.random sess tf.session with tf.device(/gpu eigen tf.self_adjoint_eig(mat sess.run(eigen)which fails with the error invalidargumenterror see above for traceback cannot assign a device to node selfadjointeigv could not satisfy explicit device specification device:gpu because no supported kernel for gpu devices is available node selfadjointeigv selfadjointeigv t=dt_double compute_v=true device=/device:gpu: (selfadjointeigv/input) and for a large enough matrix it can be seen that the cpu kernel only uses one core
225146607,9517,https://api.github.com/repos/tensorflow/tensorflow/issues/9517,jhaux,1,0,0,0,0,0,"dear tensorflow team,after constructing my model using the functionality provided by tf.contrib.layers i now want to extend my model over several gpus i learned that it might be beneficary to place variables on the cpu when doing that to reduce data transfer overhead after not seeing an easy way to do this i found a workaround i described on stackoverflow my solution is to generate variable-nodes in the graph where the variable-getter of the fully_connected layer for example would expect the variables to be.as this is not a very nice solution i messed with the fully_connected layer and the build_variable_getter function to basically allow me to specify where i want to place the variabels thus after adding the kwarg variable_device to tf.contrib.layer.fully_connected adding the kwarg variable_device to tf.contrib.layer._build_variable_getter adding the kwarg device to tf.contrib.layer._model_variable_getter and passing this as kwarg to model_variable defined in tensorflow.contrib.framework.python.ops.variables i get the desired functionality when using the fully_connected layer.below you find the modified version of layers.py in my eyes this would be a very useful feature for all layers that contain trainable variables which is why i would like to make a request for this feature.if you think the supplied modification is good enough i can also try to make a pull request after updating the other layers.best johannes system information have i written custom code as opposed to using a stock example script provided in tensorflow as described above i did os platform and distribution linux ubuntu tensorflow installed from source or binary binary tensorflow version v..--gedf-dirty cuda/cudnn version gpu model and memory titan x pascal gb source code logssee stackoverflow modified layers.py"
225103906,9513,https://api.github.com/repos/tensorflow/tensorflow/issues/9513,alexgkendall,7,0,0,0,0,0,are there any plans to implement d or higher convolutional layers
225026383,9509,https://api.github.com/repos/tensorflow/tensorflow/issues/9509,chasep255,3,0,0,0,0,0,i am using the newest tensorflow which i built from source as of yesterday in an attempt to fix this issue originally i had a source build of tensorflow i am running ubuntu with cuda and cudnn my gpu is a gtx the problem i am having is when i try to train my character based translator model using the xla compiler the code makes it all the way through the initialize variables etc up to the first run command which contains my train step and then just freezes both my gpu and cpu are idle i attached gdb to my process and it seems to be stuck waiting for some sort of notification my model builds and runs fine if i am running it without training in predict mode but still with xla it also runs fine if i train it without xla just the combo of xla and training is the issue.i attached to this my code plus some sample training data this problem should be reproducible by running the train.py. charactertranslator.zip
224982614,9506,https://api.github.com/repos/tensorflow/tensorflow/issues/9506,jia-kai,5,0,0,0,0,0,tensorflow version v..-rc--geced code to reproduce py#!/usr/bin/env python coding utf import tensorflow as tfprint(tf.git_version tf.version)for device in cpu gpu print(device with tf.device(/{}:.format(device var tf.get_variable(var{}.format(device shape dtype=int vari tf.assign(var sess tf.session sess.run(vari) error tensorflow.python.framework.errors_impl.invalidargumenterror cannot assign a device to node vargpu could not satisfy explicit device specification device:gpu because no supported kernel for gpu devices is available. this should be caused by tf_call_gpu_number_types only iterating over float types and it is used to generate the assign kernels however tf_call_number_types iterates over all types including integers is this asymmetry a deliberate design choice
224912704,9501,https://api.github.com/repos/tensorflow/tensorflow/issues/9501,thisismohitgupta,1,0,0,0,0,0,describe the problemi am using android to deploy my tensorflow model i am obfuscating names already but that wont stop people stealing and just plugging and playing graph file in their apps feature request for accepting encrypted graph files to prevent our graph files from leaking or any pointers on how to implement it
224889608,9498,https://api.github.com/repos/tensorflow/tensorflow/issues/9498,gongzhitaao,1,0,0,0,0,0,i use the tf.metrics.accuracy however it is a bit counter-intuitive in that it maintains a running accuracy the doc agrees with this the following simple script illustrates the situation pythonimport os supress tensorflow logging other than errorsos.environ tf_cpp_min_log_level import numpy as npimport tensorflow as tfprint(tf.__version x tf.placeholder(tf.int y tf.placeholder(tf.int acc acc_op tf.metrics.accuracy(labels=x predictions=y)sess tf.interactivesession()sess.run(tf.global_variables_initializer())sess.run(tf.local_variables_initializer())v sess.run( acc acc_op feed_dict={x y print(v v sess.run(acc)print(v v sess.run( acc acc_op feed_dict={x y print(v v sess.run(acc)print(v my concerns are the use of accuracy is bit surprising are we supposed to manually construct the normal accuracy imho it is better to implement the normal accuracy behavior or provide a clean way to reset the local variables created by tf.metrics.accuracy i.e the count and total
224763433,9485,https://api.github.com/repos/tensorflow/tensorflow/issues/9485,jonasrauber,1,0,0,0,0,0,i think tf.contrib.distributions.bernoulliwithsigmoidprobs can be removed because bernoulli itself has a logits parameter that does the exact same thing afaik if you agree i can make a pull-request if necessary
224684331,9480,https://api.github.com/repos/tensorflow/tensorflow/issues/9480,pangbochen,7,0,1,0,0,0,i use tensorboard on win.everyting works fine and the cmd shows starting tensorboard b at when i input into chrome nothing happens.i google for it but find limited answer.then i input into chrome the page comes out
224310689,9450,https://api.github.com/repos/tensorflow/tensorflow/issues/9450,mingdachen,1,0,0,0,0,0,i was trying to call opt.compute_gradients inside the while_loop but it failed with the error message: attributeerror whilecontext object has no attribute pred i found a similiar problem in stackoverflow code: pythonbatch_size inputs tf.ones((batch_size labels tf.zeros((batch_size outputs tf.layers.dense(inputs units=)loss outputs labelsloss_ta tf.tensorarray(dtype=tf.float size=batch_size)loss_ta loss_ta.unstack(loss)opt tf.train.adamoptimizer(.)init_grad vars_list tf.trainable_variables()for var in vars_list init_grad.append(tf.zeros_like(var))i tf.constant dtype=tf.int)def condition(i args return tf.less(i batch_size)def loop_fn(i gradients all_loss loss all_loss.read(i grads opt.compute_gradients(loss vars_list for idx grad var in enumerate(grads gradients idx grad return i gradients all_loss final_grad tf.while_loop(condition loop_fn i init_grad loss_ta )train_op opt.apply_gradients(zip(final_grad vars_list)) seems like the problem is in the tensorarray if i do not read loss from the tensorarray it will be fine besides i am using version on cpu
224127767,9437,https://api.github.com/repos/tensorflow/tensorflow/issues/9437,admcrae,1,0,0,0,0,0,system information have i written custom code no os platform and distribution ubuntu tensorflow installed from source tensorflow version bbafaddadaafeeff bazel version exact command to reproduce bazel run tensorflow/tools/docs:generate src_dir=tensorflow/docs_src output_dir=/tmp/tf_docs describe the problemproblem since python namedtuple objects have no dict property see here and here for more details this problem is easily fixed by changing all instances of h.format( info.__dict to h.format( info._asdict in pretty_docs.py the codegen library used by the docs generator is years old and doesnt work with python the problem im getting is identical to and source code logsinitial stack trace: traceback most recent call last file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate.py line in module sys.exit(doc_generator.build(flags file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py line in build write_docs(output_dir parser_config yaml_toc=self.yaml_toc file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py line in write_docs f.write(pretty_docs.build_md_page(page_info file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/pretty_docs.py line in build_md_page return build_class_page(page_info file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/pretty_docs.py line in build_class_page parts.append(h.format( method_info.__dict__))attributeerror methodinfo object has no attribute dict__error non-zero return code from command process exited with status stack trace after fixing dictionary problem: traceback most recent call last file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate.py line in module sys.exit(doc_generator.build(flags file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py line in build write_docs(output_dir parser_config yaml_toc=self.yaml_toc file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py line in write_docs page_info parser.docs_for_object(full_name py_object parser_config file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/parser.py line in docs_for_object page_info.set_signature(py_object parser_config.reverse_index file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/parser.py line in set_signature self._signature generate_signature(function reverse_index file home/amcrae/.cache/bazel/_bazel_amcrae/cdaedffcfbfcc/execroot/tf_docs/bazel-out/local-py-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/parser.py line in generate_signature default_text codegen.to_source(ast_default file home/amcrae/workspace/tf_docs/venv/lib/python./site-packages/codegen.py line in to_source generator.visit(node file usr/lib/python./ast.py line in visit return visitor(node file home/amcrae/workspace/tf_docs/venv/lib/python./site-packages/codegen.py line in visit_call if node.starargs is not none:attributeerror call object has no attribute starargserror non-zero return code from command process exited with status update command for fixing dictionary issue is sed i e s/__dict__/_asdict tensorflow/tools/docs/pretty_docs.py
223897530,9420,https://api.github.com/repos/tensorflow/tensorflow/issues/9420,mdymczyk,1,0,0,0,0,0,describe the problemis there any plan to add distributed mode to the java api i checked the code and it seems to be doable unless i missed something so i was wondering if anyone is already working on it i went through the issue tracker and prs but couldnt find anything related
223568141,9382,https://api.github.com/repos/tensorflow/tensorflow/issues/9382,mikehenninger,3,0,0,0,0,0,"when i went from tensorflow v to v.rc histograms stopped working correctly the data isnt drawn unless that trace is highlighted via mouseover and even then its not using the right plot boundaries:! image an example cursor is not displayed but is at the dot in top left of graph otherwise graph is blank generated from this code: import tensorflow as tfimport numpy as npxx tf.variable(tf.random_normal dtype=np.float),dtype=tf.float)yy tf.variable(np.eye dtype=np.float dtype=tf.float)zz tf.matmul(xx yy)save_location g:\\tmp\\lstm\\dbg\\dbgtf.summary.histogram(tensorboard_no_like zz)merged tf.summary.merge_all()sess tf.interactivesession()train_writer tf.summary.filewriter(save_location sess.graph)sess.run(tf.global_variables_initializer())sess.run(tf.local_variables_initializer())qwert zz.eval()m sess.run(merged)train_writer.add_summary(m) i havent tried this toy repro with v but other models that ive done in and rc exhibit the same behavior the distributions look fine and scalars also display fine.another problem thats new to me between and rc is that graphs sometimes display fine sometimes are blank until i reload browser page a few times apologies if this should have been a separate issue wanted to keep spam volume down i dont have a strong idea of what triggers this but the frequency is pretty high roughly half the time.the console running tensorboard emits the following warning periodically the timing of which i havent correlated with either problem mentioned above warning:tensorflow:path external\data/plugin/text/runs not found sending system information using custom code copied above windows bit fully patched tensorflow rc downloaded from and installed via pip cuda cudnn gtx ti gb ram generate histograms data eg with code above start tensorboard fail to view histogram view graph experience intermittent success"
223471259,9374,https://api.github.com/repos/tensorflow/tensorflow/issues/9374,RuofanKong,4,0,0,0,0,0,i am trying to set up gpu configuration for tensorflow the step is very simple call tensorflow.python.client.device_lib.list_local_devices to detect the number of gpu devices on the machine and then set config for tensorflow the following is the code for reproducing: from logging import getloggerimport tensorflow as tffrom tensorflow.python.client import device_liblog getlogger(__name__)def get_available_gpus get available gpu devices info local_device_protos device_lib.list_local_devices return x.name for x in local_device_protos if x.device_type gpu def test_gpu_memory_usage detect available gpu devices info log.info(on this machine gpu devices get_available_gpus set tensorflow gpu configuration gpu_options tf.gpuoptions(per_process_gpu_memory_fraction tf_config=tf.configproto allow_soft_placement=true device_count={gpu len(get_available_gpus gpu_options=gpu_options log_device_placement=true session tf.session(config=tf_config mimick training process while true pass test_gpu_memory_usage() if you run the above code you could notice that even though you set gpu memory fraction per process to it still allocates the whole gpu memory by looking at command nvidia-smi however if you dont call get_available_gpus the memory allocation works fine that means there might be a bug in device_lib.list_local_devices to prevent setting up tensorflow gpu memory usage.ps my code runs on machine with gpu geforce gtx cuda os ubuntu and python and the above issue could be reproduced using either tensorflow v v or v
223460178,9373,https://api.github.com/repos/tensorflow/tensorflow/issues/9373,zycdragonball,1,0,0,0,0,0,this is according to issue the cpu implementation now support rank i have a trouble in the gpu implementation on current line old tobit(out).device(d tobit(out).constant(t()); nvcc can compile it without error but at runtime it produces assertion failed cudagetlasterror cudasuccess function run file library/python/./site-packages/tensorflow/include/unsupported/eigen/cxx/src/tensor/tensorexecutor.h line abort trap when i try to change it to tobit(out).setzero(); it gives bus error i got the same failure on r source files.any help will be appreciated
223143380,9341,https://api.github.com/repos/tensorflow/tensorflow/issues/9341,StefanoD,1,0,0,0,0,0,os ubuntu tensorflow installed from source tensorflow version rc bazel version if compiling from source cuda/cudnn version gpu model and memory nvidia gtx gb ram exact command to reproduce*:------------------------mnist examples works fine bute mnist example with summaries crashes with: (tensorflow stefano@stefano-linux:~/dokumente/programming/python/tensorflowcoretutorial home/stefano/tensorflow/bin/python home/stefano/dokumente/programming/python/tensorflowcoretutorial/src/mnist_with_summaries.pyextracting home/stefano/dokumente/programming/python/mnist/train-images-idx-ubyte.gzextracting home/stefano/dokumente/programming/python/mnist/train-labels-idx-ubyte.gzextracting home/stefano/dokumente/programming/python/mnist/tk-images-idx-ubyte.gzextracting home/stefano/dokumente/programming/python/mnist/tk-labels-idx-ubyte.gz i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zero i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx gbmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx gb pci bus id accuracy at step accuracy at step accuracy at step accuracy at step accuracy at step accuracy at step accuracy at step accuracy at step accuracy at step accuracy at step i tensorflow/stream_executor/dso_loader.cc couldnt open cuda library libcupti.so ld_library_path usr/local/lib:/usr/local/cuda/lib:/usr/lib/x_-linux-gnu:/usr/lib/nvidia f tensorflow/core/platform/default/gpu/cupti_wrapper.cc non-ok-status tensorflow::env::default()->getsymbolfromlibrary getdsohandle kname f status not found usr/local/lib/python./dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so undefined symbol cuptiactivityregistercallbackscould not find cuptiactivityregistercallbacksin libcupti dso source code is that tensorflow looks for libcupti.so but only libcupti.so is installed: stefano@stefano-linux ls usr/lib/x_-linux-gnu grep libcupti*libcupti.solibcupti.so..libcupti.so... i installed libcupti like described in the install section on the tensorflow website: sudo apt-get install libcupti-dev libcupti-dev is not available for ubuntu
223125461,9339,https://api.github.com/repos/tensorflow/tensorflow/issues/9339,cancan101,2,0,0,0,0,0,right now fused winograd is disabled by default this is even though enabling this speeds up models considerable in the x case see what remains as far as issues etc to get this faster conv enabled by default?/cc yangzihao tfboyd
222967589,9326,https://api.github.com/repos/tensorflow/tensorflow/issues/9326,Currie32,1,0,0,0,0,0,i am hoping that you can make an update to tensorflow/python/framework/op_def_registry.py i discovered this issue when i was building a seqseq model with attention specifically i believe the issue stems from tf.contrib.seqseq.prepare_attention because the error message is valueerror no op named attn_add_fun_fff in defined operations when i use bahdanau as my attention_option when i used luong for the attention_option the error is valueerror no op named attn_mul_fun_ff in defined operations.this issue was also brought up here however with my issue i was using python thanks for your help
222908250,9319,https://api.github.com/repos/tensorflow/tensorflow/issues/9319,cancan101,5,0,0,0,0,0,caffe use can use nnpack for which it says which specifically optimizes convolutions on arm>currently caffe is optimized for arm cpus with neon basically any arm cpu since perhaps surprisingly arm cpus outperform the on-board gpus our nnpack arm cpu implementation outperforms apples mpscnnconvolution for all devices except the iphone for a convolutional implementation it is recommended to use nnpack since thats substantially faster x-x than the standard imcol/sgemm implementation used in most frameworks the readme for nnpack lists tensorflow as a framework that could potentially use it though that has not yet happened believe that tf also avoids using the imcol/sgemm approach on mobile and instead uses the eigen tensorconvolution it would be good to benchmark these two options against each other and see if tf performance can be improved by using the nnpack conv instead of the eigen conv there is an open ticket to do this benchmarking a feature i suggest offering an nnpack backed kernel to allow comparing vs eigen
222615001,9299,https://api.github.com/repos/tensorflow/tensorflow/issues/9299,wangdelp,3,0,0,0,0,0,hi there by default tf takes all gpu memory available on the machine even if very few memory are actually needed this would prevent running other processes and lead to out of memory errors although we can use gpu options to set the gpu amount but sometimes we are running others codes and do not want to bother or forget to add gpu configurations to make it consumes less memory consider most people are using shared machines it would bring difficulties for other users of gpu server very often.the default configuration of using all gpu memory may have some benefits that i am not totally aware of but i wonder if the benefits outweigh the drawbacks of doing that therefore i wonder if tf could make it consume least gpu memory as default
222396683,9285,https://api.github.com/repos/tensorflow/tensorflow/issues/9285,hholst80,3,0,0,0,0,0,system informationdocker image tensorflow/tensorflow:nightly or rc describe the problemstart a tensorboard process tensorboard logdir efs/log/atari and try and filter it does not have any effect.! image
222373574,9284,https://api.github.com/repos/tensorflow/tensorflow/issues/9284,tillahoffmann,15,0,0,0,0,0,tf.where does not support broadcasting like its numpy equivalent at the moment how easy would it be to add broadcasting here are some examples. pythoncondition np.random.normal x np.zeros y np.ones np.where(condition x y).shape tf.where(condition x y invalidargumenterror shapes must be equal rank but are and for select op select with input shapes pythoncondition np.random.normal x np.zeros y np.ones np.where(condition x y).shape tf.where(condition x y invalidargumenterror dimension in both shapes must be equal but are and for select op select with input shapes
222229222,9276,https://api.github.com/repos/tensorflow/tensorflow/issues/9276,admcrae,1,0,0,0,0,0,this addresses since atan is not implemented in eigen this is loosely based on the rint implementation suggestions are welcome
222161446,9270,https://api.github.com/repos/tensorflow/tensorflow/issues/9270,lixiangchun,7,0,0,0,0,0,the svm output layer has been shown to accelerate model training in is the svm output layer available as an alternative for tf.nn.softmax in tensorflow at this moment
222153544,9267,https://api.github.com/repos/tensorflow/tensorflow/issues/9267,umangmehta12,1,0,0,0,0,0,trying to build tf without the support of grpc should not include the create_channel header
221951572,9236,https://api.github.com/repos/tensorflow/tensorflow/issues/9236,Androbin,1,0,0,0,0,0,added op to frame a signal into overlapping frames.inspired by python_speech_features.sigproc.framesig be used in front of spectral functions tf.placeholder(tf.float none frames tf.contrib.signal.frames(pcm magspec tf.abs(tf.spectral.rfft(frames image tf.expand_dims(magspec
221914823,9230,https://api.github.com/repos/tensorflow/tensorflow/issues/9230,jwise,2,0,0,0,0,0,"system information have i written custom code as opposed to using a stock example script provided in tensorflow yes see below os platform and distribution e.g linux ubuntu tensorflow installed from source or binary source tensorflow version use command below gfdf-dirty head bazel version if compiling from source not a build issue cuda/cudnn version cudnn cuda though this runs on cpu gpu model and memory x titan x pascal x titan x though this runs on cpu exact command to reproduce virtualenv/bin/python nnrandom.py broken virtualenv/bin/python nnrandom.py broken describe the problem source code and logswhen using augmentation primitives sometimes they create run-to-run non-determinism by ignoring the system random seed this makes reproducing a run impossible even when using tf.set_random_seed one case in which they seem to do this is inside a tf.device block consider the following snippet: pythonimport tensorflow as tfimport numpy as nptf.app.flags.define_integer(seed rng seed)flags tf.app.flags.flagstf.app.flags.define_float(augment_hue hue augment factor)tf.app.flags.define_boolean(no_augment false whether to disable augmentation entirely)def mk_input np.random.seed as np.random.uniform(size astype(np.float if not flags.no_augment def aug(img img tf.image.random_hue(img flags.augment_hue seed return img as tf.map_fn(aug as return astf.app.flags.define_boolean(broken false def main(argv=none if len(argv print argv was,argv raise runtimeerror(unknown argument print(nn building graph tf.set_random_seed(flags.seed with tf.session(config=tf.configproto(allow_soft_placement true as sess if flags.broken with tf.device(/cpu input mk_input else input mk_input a tf.reduce_mean(input tf.global_variables_initializer().run tf.train.start_queue_runners(sess sess print(nn tf session initialized for step in xrange res sess.run(a print(res)if name main tf.app.run() if you run this without broken then youll get the same result every time on my machine the result without broken starts but if you run this with broken then youll get a different result each time for instance: jwise@jwise-dt:/home/scratch.jwise_dl virtualenv/bin/python nnrandom.py broken tail n.....jwise@jwise-dt:/home/scratch.jwise_dl virtualenv/bin/python nnrandom.py broken tail n..... i have not dug any deeper yet as to where exactly the nondeterminism is coming from but i figured that this is pretty close to a minimal repro case based on tf primitives.thanks,joshua"
221742880,9210,https://api.github.com/repos/tensorflow/tensorflow/issues/9210,supercoderhawk,1,0,0,0,0,0,"system information:windows x tensorflow rc description:the d sparse tensor placeholder multiply with d dense tensor has bug the operation will failed. pythonx tf.sparse_placeholder(tf.float shape= none y tf.constant(np.ones dtype=tf.float)z tf.matmul(x y a_is_sparse=true)indices values dense_shape x_val tf.sparsetensorvalue(indices values dense_shape)with tf.session as sess res sess.run(z feed_dict={x x_val print(res) expected result(xx but output some errors actually pythontraceback most recent call last file d:/learning/master_project/clinicaltext/sourcecode/python/dnn_cws/seg_dnn.py line in module cws segdnn(constant.vocab_size embed_size constant.dnn_skip_window file d:/learning/master_project/clinicaltext/sourcecode/python/dnn_cws/seg_dnn.py line in init self.loss tf.reduce_sum(tf.matmul(self.slim_map_matrix,tf.expand_dims(tf.transpose(self.word_score),),a_is_sparse=true file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\ops\math_ops.py line in matmul a ops.convert_to_tensor(a name=a file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\framework\ops.py line in convert_to_tensor as_ref=false file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\framework\ops.py line in internal_convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\framework\constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\framework\constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\framework\tensor_util.py line in make_tensor_proto tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\framework\tensor_util.py line in listcomp tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file e:\intelpython\envs\tensorflow-intel\lib\site-packages\tensorflow\python\util\compat.py line in as_bytes bytes_or_text,))typeerror expected binary or unicode string got tensorflow.python.framework.sparse_tensor.sparsetensor object at xfabd> change the z to pythonz tf.sparse_tensor_dense_matmul(x,y) also failed because the shape of sparse must d,but x and b has d"
221681884,9201,https://api.github.com/repos/tensorflow/tensorflow/issues/9201,cancan101,4,0,0,0,0,1,system informationusing the tensorflow/tensorflow:..-devel-gpu docker image. (v..--gedf-dirty host driver version generic issueif i set compute mode to exclusive_process on the nvidia device sudo nvidia-smi c then even though i tell the session not to use gpus config=tf.configproto(device_count={gpu tensorflow attempts to use the gpu resulting in an inability to create session: internalerrortraceback most recent call last)
221569625,9189,https://api.github.com/repos/tensorflow/tensorflow/issues/9189,gaohuazuo,2,0,0,0,0,0,this is a prototype for new ops mux and demux are introduced by this patch mux op takes i input input as input and produces input_i as output it is meant to replace tf.case and tf.cond demux op is a generalized version of switch it takes i input as input and produces output output_n where output_i is set to input and other outputs are dead.this patch also contains an algorithm that rewrites the graph before execution so that mux ops are efficient just like tf.cond .example: i tf.placeholder(tf.int a tf.placeholder(tf.int b tf.placeholder(tf.int c tf.select(i a b mux op is renamed to select in python api.sess.run(c feed_dict={i a b returns trace to see that mul node is not executed.sess.run(c feed_dict={i a b returns trace to see that add node is not executed. i wouldnt be surprised if this patch causes problem somewhere in particular the graph rewriting algorithm may produce valid but cyclic graph it also lacks proper handling of special nodes like enter nextiteration etc however i would like to know if this approach is promising i can work on this if there is no fundamental difficulty
221241314,9162,https://api.github.com/repos/tensorflow/tensorflow/issues/9162,JerrikEph,6,0,0,0,0,0,"as far as i know the tf.stop_gradient function can only stop the gradient of a whole tensor i recently wanted to implement a model that requires the stop of gradient for some entry of a tensor not the whole tensor and i have come up with an workaround, def entry_stop_gradients(target mask mask_h tf.logical_not(mask mask tf.cast(mask dtype=target.dtype mask_h tf.cast(mask_h dtype=target.dtype return tf.stop_gradient(mask_h target mask target hope somebody could implement a low level version of this feature"
221112357,9150,https://api.github.com/repos/tensorflow/tensorflow/issues/9150,eaplatanios,2,0,0,0,0,0,i am currently using the c api and building a scala api on top of it it seems that what is done in the python api and the java api is that the tensors fed into sessions are being copied to buffers internal to the native library i am also currently doing that in the scala library but i was wondering if we can do the following:lets assume we can share a pointer to the underlying data structure between c and scala through a java nio directmemorybuffer for example then is there any functionality to obtain a tensor view that is a slice of that tensor directly using that buffer i imagine that since the tf op kernels are implemented in c it should be possible to use the stridedslice op directly on a tensor data structure without needing to use a session the same idea can be extended to other ops so first of all is that true?secondly if it is where is that functionality available in the c api or exposed elsewhere so that i can use it from within my scala library i currently do the indexing on the byte buffer myself but that can be painful for arbitrary slices.one main issue with sharing a pointer is how to deal with the java garbage collector i havent figured that out yet but even if i cant do that the above comment still applies how can i use op kernels directly in order to manipulate tensors outside of the symbolic graph that is useful for languages other than python where a library as powerful as numpy is not available.thank you
221053917,9142,https://api.github.com/repos/tensorflow/tensorflow/issues/9142,Panaetius,3,0,0,0,0,0,right now the bounding box operators tf.image.draw_bounding_boxes tf.image.non_max_suppression and tf.image.sample_distorted_bounding_box expect bounding boxes in the form of ymin xmin ymax xmax with the origin being the lower left corner of the image but images themselves are tensors and the pixel with index in the tensor is in the top left so bounding box coordinates are the opposite in the y direction to tensor indices.additionally the operations tf.image.pad_to_bounding_box and tf.image.crop_to_bounding_box take coordinates in the form of ymin xmin height width with the origin being the top-left corner so the coordinates are inconsistent even within the image ops themselves plus the parametrization of the bounding boxes is different too).and the tf.image.crop_and_resize op doesnt specify what origin it uses though i think its bottom left too)i feel like this sort of inconsistency is unnecessarily confusing and a high risk for introducing errors.its especially bad since if you supply bounding boxes the wrong way around to draw_bounding_boxes itll still draw them correctly all bounding box operators should use the same coordinate system and preferably the same parametrization and preferably the coordinates should be consistent with image tensor indexing
220952991,9138,https://api.github.com/repos/tensorflow/tensorflow/issues/9138,VasilievSerg,5,0,0,0,0,0,"hello,developers of pvs-studio c/c++/c static analyzer present their check report of the source code of tensorflow in the article containing the review of the most suspicious code fragments they discovered.you can read article at the official site have provided links to github for each code fragment to make viewing more comfortable the article doesnt cover all the issues that were found by the analyzer so perhaps it would be interesting for you to review them yourself in case you have questions feel free to ask them.best regards,sergey vasiliev"
220859528,9125,https://api.github.com/repos/tensorflow/tensorflow/issues/9125,poolio,2,0,0,0,0,0,when applying graph_replace to graphs containing ops with the original_op attribute it can fail with a keyerror the error occurs in transformer._copy_ops when trying to copy an op whose original_op has not yet been copied the ordering of ops that are copied is not deterministic so this error pops up somewhat randomly.the original_op attributes appear to be created by tf.gradients to point back to the op from the forward pass.example code snippet note you may need to run this multiple times to get a failure): pythonimport tensorflow as tfgraph_replace tf.contrib.graph_editor.graph_replacew tf.variable name=w)y tf.multiply(tf.multiply(w w name=mul w name=mul)g tf.gradients(y w) g_new graph_replace(g w.value g}) error: /usr/local/lib/python./dist-packages/tensorflow/contrib/graph_editor/transform.py in transform_op_if_inside_handler(info op keep_if_possible if op in info.sgv.ops return info.transformed_ops op else if keep_if_possible and info.graph is info.graph_:keyerror tf.operation mul type=mul> i see three possible fixes remove original_op attributes in the copied graph i dont see anywhere in the tf codebase where it is used move the creation of the original_op attribute from the copy_op_handler function to the end of transformer._copy_ops after all ops have been copied topologically sort the ops being copied so that ops that are original_op attributes are created before their children.my implementation of option seems to fix this problem but i might be missing something about the usage of original_op
220853884,9124,https://api.github.com/repos/tensorflow/tensorflow/issues/9124,vit-stepanovs,8,0,0,0,0,3,currently tf is already built as a shared library that is included in the python package however that library is implicitly linked against python libs and thus expects python to be present on the machine wherever it is used it is undesirable for scenarios not requiring python e.g native application that need tensorflow does not include the tf c api.this pr allows optionally building tf as a stand-alone dll that does not have the above issues i am also working on allowing cmake to link all c tests against that dll and will submit such changes in a separate pr.as a bonus this pr also fixes a build break for tf_tools.cmake when gpu is enabled
220787079,9117,https://api.github.com/repos/tensorflow/tensorflow/issues/9117,lukeiwanski,6,0,0,0,0,0,opencl implementation improvements build use gcc/g as a host compiler to avoid eigen version bump opencl implementation improvements register sycl implementations for random ops simplify by using eigen math functions registers scatter and scatternd ops for sycl registers stack op for sycl fixes no sycl buffer found error for debug ops registers matmul and transpose ops to sycl device for double extends analyzer_cli_test.py test to cover sycl fixes transpose op for double when on sycl bumps eigen version to fix double precision issue on sycl extends sessiondebugtestbase to cover sycl bumps eigen version refactors ops registration introduces workaround for const op related to the difference between cuda which uses pointers and opencl that uses buffers/accessors extends memory types to cover device_sycl as well introduces getsycldevice method that returns list of supported devices with gpu device having the highest priority doesnt include blacklisted devices internal::transpose tensorflow::internal::transpose in order to avoid compilation reported error adds sycl_runtime to bazels array_deps replicates tf_call_gpu_proxy_types for sycl fixes an issue caused by switch to aligned allocator for sycl buffer fix testsimple and testconst in stack_op_test randomgamma has no gpu friendly implementation register batch normalization kernels for opencl compatibility fixes for tensorflow rc fixes scatter op implements batchmatmul op for sycl lowercase the device name when gpu or sycl returned kernel_estimator_test.py assertequal assertalmostequal due to floating point representation on the device
220575788,9099,https://api.github.com/repos/tensorflow/tensorflow/issues/9099,Lescurel,2,0,0,0,0,0,at the moment only bilinear interpolation is supported by crop_and_resize however when working with little images and with labeled images it sometimes makes more sense to use a nearest neighbor interpolation.any plans of adding a nearest neighbor interpolation for the method in the near future
220516369,9094,https://api.github.com/repos/tensorflow/tensorflow/issues/9094,Aetf,1,0,0,0,0,0,have i written custom code as opposed to using a stock example script provided in tensorflow yes i was training a tree based network using tensorflow fold to train word embeddings for sql parse tree but this shouldnt be a problem because the tensorboard can correctly read the checkpoint file tensorflow installed from source or binary binary installed using pip tensorflow version bazel version if compiling from source n/a cuda/cudnn version n/a gpu model and memory cpu only exact command to reproduce open the log folder tflogs.zip using tensorboard seems the absolute path is hardcoded in the checkpoint file the absolute path should be tmp/workspace/tflogs ): tensorboard logdir=tflogs switch to embedding tab enable d label on the top left corner of the projector describe the problem clearlybefore enabling d label! image enabling d label! image result label shownactual result the projector becomes blank while the following error shown in the js console: uncaught typeerror cannot read property tostring of undefined at projectorscatterplotadapter.getlabeltext tf-tensorboard.html at projectorscatterplotadapter.generatedlabelsarray tf-tensorboard.html at projectorscatterplotadapter.createvisualizers tf-tensorboard.html at projectorscatterplotadapter.setdlabelmode tf-tensorboard.html at htmlelement.
220500375,9091,https://api.github.com/repos/tensorflow/tensorflow/issues/9091,JonathanRaiman,4,0,0,0,0,0,in a series simple tensorflow programs i obtain memory leaks unbounded growth of cpu memory).on original program on a computer with gb of ram this leak is about megabytes per hour of total memory plots of computers memory over time long time scale picture:! unknown short time scale picture:! unknown problem descriptionthe original program was more advanced and included rnns/saving/loading etc but i narrowed it down to a simple for loop with no gradient descent where memory grows over time without bound tested on fedora and mac osx issue occurs when running on single gpu titan x pascal or on cpu varying the sizes of the variables in the graph only changes the degree of growth but does not prevent the effect from occurring this issue occurs on tensorflow and on current tensorflow no custom code was used tensorflow was installed using pip in both cases pre-compiled binary each time this was pip install tensorflow-gpu using cuda cudnn v though this should not impact the use-case since no cudnn kernels are being used gpu is a titan x pascal gb of vram not titan xp to reproduce: import argparseimport psutilfrom os import getpidimport tensorflow as tfimport numpy as npdef fc(inputs output_size with tf.variable_scope(fc input_size inputs.get_shape() - .value w tf.get_variable(w shape= input_size output_size b tf.get_variable(b shape= output_size initializer=tf.constant_initializer out tf.nn.xw_plus_b(inputs w b return outdef create_model(input_size output_size model placeholders with tf.variable_scope(inputs input_placeholder tf.placeholder tf.float none input_size name=input_placeholder meaningless function of inputs op tf.reduce_mean(tf.reduce_sum(fc(input_placeholder output_size return input_placeholder opdef parse_args(args=none parser argparse.argumentparser parser.add_argument(--max_epochs type=int default parser.add_argument(--batch_size type=int default parser.add_argument(--input_size type=int default parser.add_argument(--output_size type=int default parser.add_argument(--device type=str default=gpu return parser.parse_args(args=args)def create_batches(inputs input_size batch_size n batches for i in range(n x np.random.uniform size=(batch_size input_size batches.append({inputs x return batchesdef main args parse_args session_conf tf.configproto(allow_soft_placement=true np.random.seed process psutil.process(getpid with tf.session(config=session_conf as session tf.device(args.device inputs op create_model(args.input_size args.output_size session.run(tf.global_variables_initializer batches create_batches(inputs args.input_size args.batch_size for epoch in range(args.max_epochs before process.memory_percent for feed_dict in batches session.run(op feed_dict after process.memory_percent print(memory change f f before after))if name main main() output will be exact numbers are percentages of computers ram so should change based on hardware but main point is that memory continues to grow when the program has no variation between graph runs batches are all the same size no randomness is left in the program etc.): memory change memory change memory change memory change memory change memory change memory change memory change memory change how can i fix this i currently suspect a cpu memory pool issue inside tensorflow since the problem is fairly generic and does not depend on the ops inside the graph much from what ive gathered most likely candidate is the tf.asarray /copying of numpy arrays in feed_dict leading to memory fragmentation etc supposing this were the case ive heard that tcmalloc should alleviate this but no dice note ive also checked that objgraph shows no growth in program over time
220498093,9090,https://api.github.com/repos/tensorflow/tensorflow/issues/9090,krisjobs,12,0,0,0,0,3,hey guys!i have been banging my head for a couple of days with the following:i am using a monitoredtrainingsession with a single local server but bug is identical even if using supervisor and/or distributed architecture). #...sess tf.train.monitoredtrainingsession(...)\while true sess.run(train_op) assuming sequential runs of the python client without changing a single line of code the problem is that i sometimes get the error tensorflow.python.framework.errors_impl.failedpreconditionerror failed to rename graph.pbtxt.tmpedfdceeadce to graph.pbtxt the process cannot access the file because it is being used by another process and the interesting thing is that sometimes this does not occur and training begins but it then happens just as the first before_run call of the checkpoint hook tries to save the initial state as seen in the monitoredsession source code failed to rename model.ckpt-_temp_fcbbcbdb/part--of-.data--of-.tempstate to model.ckpt-_temp_fcbbcbdb/part--of-.data--of the process cannot access the file because it is being used by another process. solved dont ever store log data and checkpoints in dropbox
220491392,9089,https://api.github.com/repos/tensorflow/tensorflow/issues/9089,yaroslavvb,2,0,0,0,0,0,instructions to upgrade to cpp protobuf implementation on mac from dont work work makes tf fails with following stacktrace traceback most recent call last file kronecker_benchmark.py line in module import tensorflow as tf file users/yaroslav/anaconda/envs/mar/lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import file users/yaroslav/anaconda/envs/mar/lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.core.framework.graph_pb import file users/yaroslav/anaconda/envs/mar/lib/python./site-packages/tensorflow/core/framework/graph_pb.py line in module from google.protobuf import descriptor as descriptor file users/yaroslav/anaconda/envs/mar/lib/python./site-packages/google/protobuf/descriptor.py line in module from google.protobuf.pyext import messageimporterror dlopen(/users/yaroslav/anaconda/envs/mar/lib/python./site-packages/google/protobuf/pyext/_message.cpython-m-darwin.so library not loaded usr/local/lib/libprotobuf..dylib referenced from users/yaroslav/anaconda/envs/mar/lib/python./site-packages/google/protobuf/pyext/_message.cpython-m-darwin.so reason image not found work-around is to use older link:pip install upgrade that it works python c from google.protobuf.internal import api_implementation print(api_implementation._default_implementation_type) macos e tensorflow latest nightly from today installed as: pip install upgrade
220462631,9081,https://api.github.com/repos/tensorflow/tensorflow/issues/9081,marcociccone,1,0,0,0,0,0,i just added this parameter that was missing in the convd_transpose layer .if output_shape is none then the layer acts as usual returning the smallest shape possible
220462551,9080,https://api.github.com/repos/tensorflow/tensorflow/issues/9080,elefthei,12,0,0,0,0,0,im experimenting with multiple tensorflow gpu processes and the nvidia multi-process server have the following mnist example as a benchmark neural.py) import tensorflow as tfimport osfrom tensorflow.examples.tutorials.mnist import input_datadata input_data.read_data_sets(mnist_data_%d os.getpid one_hot=true construction phasex tf.placeholder(tf.float shape= none y tf.placeholder(tf.float shape= none with tf.name_scope(fc w tf.variable(tf.truncated_normal stddev b tf.variable(tf.truncated_normal stddev h tf.sigmoid(tf.matmul(x w b)with tf.name_scope(fc w tf.variable(tf.truncated_normal stddev b tf.variable(tf.truncated_normal stddev y_predict tf.nn.softmax(tf.matmul(h w b)with tf.name_scope(eval with tf.name_scope(loss cross_entropy tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_predict reduction_indices= ))learning_rate backprop tf.train.gradientdescentoptimizer(learning_rate).minimize(cross_entropy)correct tf.equal(tf.argmax(y tf.argmax(y_predict accuracy tf.reduce_mean(tf.cast(correct tf.float))#executionsess tf.session()sess.run(tf.initialize_all_variables())train_steps batch_size for i in range(train_steps batch_x batch_y data.train.next_batch(batch_size sess.run(backprop feed_dict={x batch_x y batch_y})print(sess.run(accuracy feed_dict={x data.test.images y data.test.labels})) and im running two processes like this time python neural.py time python neural.py without nvidia-cuda-mps-control running as a daemon this is the output: ..real m.suser m.ssys m.sreal m.suser m.ssys m.s with nvidia-cuda-mps-control running as a daemon im getting an internal error: f tensorflow/core/common_runtime/gpu/gpu_device.cc check failed err cudasuccess vs f tensorflow/core/common_runtime/gpu/gpu_device.cc check failed err cudasuccess vs bash line aborted core dumped python neural.py i can verify from the nvidia-mps logs in var/log/nvidia-mps that the tensorflow cuda context successfully started an nvidia-cuda-mps-server and connected to it. /var/log/nvidia-mps/control.log control start control accepting connection control new client from user server is not ready push client to pending list control starting new server for user the mps server should be compatible with the cuda api which tensorflow uses so im uncertain about why im getting this error. tensorflow version ubuntu cuda cudnn nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m tesla k off off off n/a c p w w mib mib default
220273803,9049,https://api.github.com/repos/tensorflow/tensorflow/issues/9049,vincentbecker,2,0,0,0,0,0,i installed cpu-only tensorflow version on windows using the pip installer i am trying to get the android example to run using cmake as explained in i cloned the newest version of the tensorflow repository and created a new android studio project and followed the instructions from the webpage above by modifying the gradle files i already found out that it should be debugcompile project(path tensorflow-android-inference configuration debug)releasecompile project(path tensorflow-android-inference configuration release) instead of debugcompile project(path tensorflow_inference configuration debug)releasecompile project(path tensorflow_inference configuration release) now however i get a build error stating error:project app declares a dependency from configuration releasecompile to configuration release which is not declared in the descriptor for project tensorflow-android-inference.has anyone tried this or could anyone point me to a proper explanation of how to use cmake to build the project any help would be very much appreciated thanks
220070674,9035,https://api.github.com/repos/tensorflow/tensorflow/issues/9035,briancheung,1,0,0,0,0,0,porter@fattire:~/projects/tensorflow bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkgthu apr pdt using tmpdir tmp/tmp.kcypvwxju~/projects/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles projects/tensorflow~/projects/tensorflow/tmp/tmp.kcypvwxju projects/tensorflowthu apr pdt building wheelerror cant copy tensorflow/python/pywrap_tensorflow_internal.py doesnt exist or not a regular file this happens after building tensorflow with bazel and running the binary build by bazel: bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkg pywrap_tensorflow_internal.py doesnt exist in the source directories:tensorflow commit fdcebazel cuda release v cudnn gpu nvidia titan gb
220064773,9034,https://api.github.com/repos/tensorflow/tensorflow/issues/9034,shaform,1,0,0,0,0,0,describe the problem clearlyif the updates_collections of a batch_norm layer is set other than tf.graphkeys.update_ops it is no longer possible to compute nd-order tf.gradients with respect to the weights of a fully_connected layer.p.s it is okay when updates_collections is set as tf.graphkeys.update_ops i think updates_collections should not affect the ability to compute gradients environments ubuntu bit python anaconda bit default dec gcc red hat on linux tensorflow-gpu installed from pip libcublas.so libcudnn.so libcufft.so libcuda.so libcurand.so source code pythonimport tensorflow as tfwith tf.session as sess x tf.placeholder(tf.float none is_training tf.placeholder(tf.bool name=is_training outputs tf.contrib.layers.fully_connected(inputs=x num_outputs outputs tf.contrib.layers.batch_norm inputs=outputs is_training=is_training updates_collections=bad_collections get gradients of x with respect to outputs values grads tf.gradients(outputs x bad_vars tf.get_collection(tf.graphkeys.trainable_variables get gradients of weights with respect to gradients of x bad_grads tf.gradients(grads bad_vars this line logs traceback most recent call last file test.py line in module bad_grads tf.gradients(grads bad_vars file home/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in gradients out_grads i control_flow_ops.zeroslikeoutsideloop(op i file home/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in zeroslikeoutsideloop pred op_ctxt.predattributeerror nonetype object has no attribute pred
220055757,9033,https://api.github.com/repos/tensorflow/tensorflow/issues/9033,salmiadu,3,0,0,0,0,0,hi guys if you dont wont to read everything go down directly to solution here is the way how i fixed the problem of installing tensorflow on windows i will start from the begining i downloaded the anaconda for windows with python version create a conda environment named tensorflow by invoking the following command:c conda create n tensorflow activate the conda environment by issuing the following command:c activate tensorflow tensorflow)c your prompt should change issue the appropriate command to install tensorflow inside your conda environment to install the cpu-only version of tensorflow enter the following command:(tensorflow)c pip install ignore-installed upgrade message appear can not install this wheel i forgot the message its just couldnt find the point to install the gpu version of tensorflow enter the following command on a single line):(tensorflow)c pip install ignore-installed upgrade same message appear couldnt install so after that i wanted to write my first code c>pythonpython version anaconda import tensorflow as tf it appears this error traceback most recent call last file stdin line in module>modulenotfounderror no module named tensorflow-----------------------------------------------------------------------------solutionfix the problem cp-cpm-win_amd.whltensorflow version cp python version neededwin_amd windows x so the anaconda for windows with python version so we need python i downloaded it from other this web site cause anaconda has only the version for python so this anaconda for python then i followed the other steps everything worked fine i hope everything will work with you
220021391,9029,https://api.github.com/repos/tensorflow/tensorflow/issues/9029,tillahoffmann,0,0,0,1,0,0,this pr adds cpu kernels for ffts cf a few points to note this pr does not yet support rffts these can in principle be implemented by slicing such that the negative frequency components of the fft are removed maybe benoitsteiner has some better ideas though does it make sense to split up the code into multiple files to avoid having if google_cuda half way rryan because the tensor fft in eigen is templated i couldnt avoid bloating the binary a bit as discussed via e-mail
219959592,9024,https://api.github.com/repos/tensorflow/tensorflow/issues/9024,el3ment,10,0,0,2,0,0,it would be nice if there was an export option in tensorboard that would export a latex-ready version of the visible graph perhaps using svg pgfplots or something similar its possible now to export csv and create the graphic by hand but having it baked into tensorboard would be nice
219680921,8994,https://api.github.com/repos/tensorflow/tensorflow/issues/8994,cancan101,2,0,0,0,0,0,in tensorboard rc hitting the reload button image causes the scaling on the charts to reset that was not the case in for example if i zoom in here:! image zoom is lost
219379782,8962,https://api.github.com/repos/tensorflow/tensorflow/issues/8962,eaplatanios,1,0,0,0,0,0,"hi,in python we have graph collections e.g trainable_variables which are stored in the graph class i assume that these are somehow serialized in the graph protobuf so that when a graphdef is imported the relevant collections are imported too however i do not see any option in the c api for defining such collections is there something that i am missing if not why not include that functionality in the c api?thank you,anthony"
218958852,8926,https://api.github.com/repos/tensorflow/tensorflow/issues/8926,jubjamie,4,0,0,0,0,0,the tf.image distortion functions only accept a single image as an input whereas it would be much more useful to be able to place the distortion within your graph flow so any batches of images that pass through it get distorted too i believe a current solution is as suggeested by mrry here on stack overflow however it feels like much like the other ops for images work being able to take batches would be much more useful.is the solution proposed by mrry the accepted method or should the distortion functions be taking batches
218857845,8919,https://api.github.com/repos/tensorflow/tensorflow/issues/8919,meteorcloudy,2,0,0,0,0,0,c:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x_/tensorflow/tools/pip_package/build error loading package tensorflow/python encountered error while reading extension file protobuf.bzl no such package protobuf traceback most recent call last file c:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x_/tensorflow/workspace.bzl line apply_patch(repo_ctx repo_ctx.attr.patch_file file c:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x_/tensorflow/workspace.bzl line in apply_patch execute_and_check_ret_code(repo_ctx patch p d r more arguments file c:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x_/tensorflow/workspace.bzl line in execute_and_check_ret_code fail(non-zero return code when more arguments>))non-zero return code when executing patch p d c:/tmp/_bazel_system/wgutbc/external/protobuf i c:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x_/third_party/protobuf/add_noinlines.patch:stdout stderr java.io.ioexception createprocess the system cannot find the file specified. culprit deafabeaecdedfcbcdbereason patch command is not installed in msyssolution run pacman syuu noconfirm patch @gunan
218760067,8906,https://api.github.com/repos/tensorflow/tensorflow/issues/8906,Namnamseo,5,0,0,0,0,0,when some of the dimensions are unknown tf.nn.sufficient_statistics uses reduce_prod to guess the size.but reduce_prod is not differentiable on gpu this commit avoids the usage of sufficient_statistics in tf.nn.moments() .by this way calling moments wont involve any transfer between cpu and gpu this can solve for example fchollet/keras#.i didnt know how to test things so i edited my source a little downloaded tensorflow/python/ops/nn_test.py imported mine and added lines like: pythontf.nn.moments patched_moments and run the test it passed all the unshifted one had unstable output but i might have done something wrong so please double-check it
218754929,8905,https://api.github.com/repos/tensorflow/tensorflow/issues/8905,Bihaqo,2,0,0,0,0,0,ive encountered a tf.float matrix of size x such that tf.svd of it returns nans whilenp.linalg.svd works fine.converting the matrix into tf.float and then converting back to tf.float makes everything works with tf too while being a tiny perturbation).here is an example jupyter notebook can download pickled matrix here using conda python tried on mac and ubuntu and a fresh version of tensorflow from pip tried both cpu and gpu versions
218734245,8901,https://api.github.com/repos/tensorflow/tensorflow/issues/8901,yongtang,1,0,0,0,0,0,"this fix adds gzip and zlib support for fixedlengthrecordreader,as was discussed in when fixedlengthrecordreader is used it will check forcompression_type flag and use zlibinputstream as needed.the usage of inputbuffer in fixedlengthrecordreader hasalso been changed to bufferedinputstream to match zlibinputstream.this fix fixes"
218712647,8898,https://api.github.com/repos/tensorflow/tensorflow/issues/8898,lunasdejavu,0,0,0,0,2,0,what related github issues or stackoverflow threads have you found by searching the web for your problem?i was installing tensorflow gpu version on ubuntu x-but i found an error import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc couldnt open cuda library libcudnn.so ld_library_path home/lunasdejavu/downloads:/usr/local/cuda-./libi tensorflow/stream_executor/cuda/cuda_dnn.cc unable to load cudnn dsoi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally i tried to install again and again follow the instructions it is still useless.i tried the nvidia_cuda-._samples then make no error after all can somebody help me i was working on this setting for almost hours environment infooperating system:gcc version ubuntu ubuntu bitinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):i cant use the commandbut the packages are cuda-repo-ubuntu---local-ga_..-_amd.debcudnn-.-linux-x-v..tgzif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu mar build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried?i searched the manual of th cd installpath export ld_library_path= pwd :$ld_library_path add installpath to your build and link process by adding i
218656295,8891,https://api.github.com/repos/tensorflow/tensorflow/issues/8891,loliverhennigh,3,0,0,0,0,0,added an implementation of convolutional lstms related to this issue
218385752,8859,https://api.github.com/repos/tensorflow/tensorflow/issues/8859,sunnysuhappy,2,0,0,0,0,0,im using pycharm and cant get code completion when using tf.contrib.but when i write tensorflow.contrib import learn it has a right behavior.i guess it happened because of the lasy loading of tf.contrib.can anyone help me
218384967,8858,https://api.github.com/repos/tensorflow/tensorflow/issues/8858,surmenok,3,0,0,0,0,0,a simple python program which runs a few tensorflow computations consequently crashes when running on gpu.code: from future import print_functionimport numpy as npimport tensorflow as tffrom tensorflow.python.client import timelinedef train_model(run_number image_size num_labels batch_size layer_neuron_count graph tf.graph with graph.as_default tf_valid_dataset tf.constant(valid_dataset variables weights tf.variable tf.truncated_normal( image_size image_size layer_neuron_count biases tf.variable(tf.zeros( layer_neuron_count weights tf.variable tf.truncated_normal( layer_neuron_count num_labels biases tf.variable(tf.zeros( num_labels valid_layer tf.nn.relu(tf.matmul(tf_valid_dataset weights biases valid_prediction tf.matmul(valid_layer weights biases with tf.session(graph=graph as session tf.global_variables_initializer().run print(validation session.run(valid_prediction print(validation done)valid_dataset np.random.uniform astype(dtype=np.float)valid_labels np.random.uniform astype(dtype=np.float)for i in range print(run format(i train_model(i) it should run the same computation times recreating a graph and a session every time works fine when i run it on cpu when running on gpu it fails on running computation for nd rd or th session.console output: i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyrun w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id validationvalidation donerun i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id validationvalidation donerun i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id validationvalidation donerun i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id validation then the machine just restarts.there are no relevant messages in syslog before the restart what related github issues or stackoverflow threads have you found by searching the web for your problem running other tensorflow programs i noticed that sometimes such crashes happen when i use large tensors issues above seem to be related at least symptoms are similar environment infogpu geforce gtx tioperating system ubuntu ltsinstalled version of cuda and cudnn cuda cudnn output of ls l usr/local/cuda/lib/libcud* : -rw-r--r root root mar usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root mar usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root mar usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root mar usr/local/cuda/lib/libcudart.so...-rw-r--r root root mar usr/local/cuda/lib/libcudart_static.alrwxrwxrwx root root mar usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx root root mar usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root mar usr/local/cuda/lib/libcudnn.so...-rw-r--r root root mar usr/local/cuda/lib/libcudnn_static.a tensorflow pip install tensorflow-gpu version the output from python c import tensorflow print(tensorflow.__version__) : i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally what other attempted solutions have you tried?tried to reinstall ubuntu/cuda/cudnn/tensorflow didnt help
218330016,8852,https://api.github.com/repos/tensorflow/tensorflow/issues/8852,rajhans,5,0,0,0,0,0,currently savedmodelbuilder throws an exception when called with a directory that already exists it will be helpful to add a flag overwrite when initializing savedmodelbuilder which allows it to overwrite the contents of the directory when save is called when training a model every couple of hours on fresh data it will be easier to overwrite existing models than having to implement housekeeping code around cleaning old models or writing code to move around new model after training
218138842,8832,https://api.github.com/repos/tensorflow/tensorflow/issues/8832,rongjiecomputer,1,0,0,0,0,0,in a few pages of tensorflow.org like the footer appears at the wrong place.the fix is to move footer class=devsite-utility-footer>...... after ...
218094129,8830,https://api.github.com/repos/tensorflow/tensorflow/issues/8830,stoneyang,2,0,0,0,0,0,"hi,ive installed on my ubuntu machine and deployed tf with success at least before i run the example shipped with tf e.g mnist_with_summaries.py running mnist_with_summaries.py i got the very problem posted in the discussion is mainly about the missing libcupti.so and most of them suggested to install or symlink the lib but both approach didnt work for me since the so file exists when installing cuda under the directory usr/local/cuda/extra/cupti/lib so i thought what i need to do is just link the library under the directory usr/local/cuda/lib however problem still exists after the linking and the libcupti-dev in apt-get is old v which fails also anyone could give me some advice thanks in advance drpngx gunan k-wu"
218065786,8828,https://api.github.com/repos/tensorflow/tensorflow/issues/8828,cancan101,10,0,0,4,0,0,"cudnn v has been released there are some cool new features dilated convolutions dilated convolutions are now supported in cudnn without achange in api cudnnconvolutionbiasactivationforward allows for the execution of a single kernel fusing convolution bias and activation operationsfull release notes:>dilated convolutions dilated convolutions are now supported in cudnn without achange in api previously unused upscale fields in the convolution descriptorhave been repurposed to allow user specification of dilation factors along eachdimension support for dilation is present in the following code paths forward cudnn_convolution_fwd_algo_implicit_gemm,backward data cudnn_convolution_bwd_data_algo andbackward filter cudnn_convolution_bwd_filter_algo_ the new api cudnnconvolutionbiasactivationforward allows for the execution ofa single kernel fusing convolution bias and activation operations at present onlyper channel bias and relu activation are supported. inference on bit integer data is now supported leveraging the element dotproduct instruction idpa of pascal gpus with cuda capabilities two tensorlayouts are supported for this feature cudnn_tensor_nhwc with int datatype and cudnn_tensor_nchw_vect with intx data type. rnn now supports algorithmso cudnn_rnn_algo_standard same functionality as in cudnn v.o cudnn_rnn_algo_persist_static this algorithm relies on the usage of persistent cuda kernels whichare pre-compiled to fit different gpus. this algorithm is available only on pascal gpus.o cudnn_rnn_algo_persist_dynamic this algorithm also relies on the usage of persistent cuda kernelsbut these kernels are compiled at runtime using nvrtc in some casesthis results in a significant performance benefit. this algorithm is also available only on pascal gpus and is supportedonly on linux and windows. support for d-fft convolutions has been added new api routine cudnnreducetensor has been added supporting reductionoperations activation mode cudnn_activation_elu is now supported. a deterministic max pooling mode cudnn_pooling_max_deterministichas been added. significant performance improvement for softmax layers for modecudnn_softmax_mode_channel has been achieved when low batchnumber is used. significant performance improvements have been added for cudnnaddtensorwhen spatial dimensions are set to"
218032322,8820,https://api.github.com/repos/tensorflow/tensorflow/issues/8820,wilderfield,8,0,0,0,0,0,consider the two following hardware scenarios linux running on x w fpga fabric connected via pcie linux running on arm a with axi i/f to fpga fabric think xilinx zynq)how could the tensorflow core be accelerated for these scenarios fpga vendors do offer opencl binaries for running opencl apis to parallelize computations.forgive me i am a bit ignorant still learning in this area but i am intrigued by the future possibility of this and would love to help in anyway i can
217987537,8817,https://api.github.com/repos/tensorflow/tensorflow/issues/8817,awav,2,0,0,0,0,0,i came across with a task where i would like to apply stochastic depth regularization technique using tensorflow tensorflow doesnt provide enough settings to implement this one i found closed issue which is similar to this request where guys finished the discussion with claim that tf.cond tf.select primitives are enough for this task but if you carefully read the paper it says that during training the depth changes for both directions forward and backward propagation steps therefore number of tranable w parameters of the network changes too the core conception of the tensorflow is building computation graph before session of training is run currently i can not create dynamic computation graph so that depending on a boolean value w parameters of a layer were not engaged in optimisation process.if tf.variable accepted trainable parameter as a boolean tensor apart from built-in boolean value it would solve the problem in this case it would mean that tensorflow operates natively with dynamic computational graphs which in fact very powerful tool.i would appreciate any suggestions and ideas so that this question was closed for good and all.@vrv martinwicke aselle
217919515,8809,https://api.github.com/repos/tensorflow/tensorflow/issues/8809,unnir,5,0,0,0,0,0,"hi all,just tried to start the script from here one issue for python users: pythonimport urllibraw urllib.urlopen(iris_training_url).read this returns: bash---------------------------------------------------------------------------attributeerror traceback most recent call last)"
217789469,8793,https://api.github.com/repos/tensorflow/tensorflow/issues/8793,sjperkins,1,0,0,0,0,0,stagingarea and perhaps other structures in data_flow_ops.py are not present in the python documentation they are however mentioned in the and release notes what other attempted solutions have you tried?a search of the docs for stagingarea doesnt reveal any of the docstrings present in data_flow_ops.py and unstage are present in the c documentation
217764905,8791,https://api.github.com/repos/tensorflow/tensorflow/issues/8791,lspvic,1,0,0,0,0,0,"this pr solves as in version rc the changes are made:>tensorboard uses a relative data directory for easier embedding.but the request of project plugin data in embeddings panel e.g data/plugin/projector/runs still requests in a absolute path the code html with the leading slash leads the absolutely path request.this pr remove the leading slash.updated:new version of code is: new tf.dashboard.vzprojectordashboard(/data/plugin/projector), still has a leading slash"
217725159,8787,https://api.github.com/repos/tensorflow/tensorflow/issues/8787,ahundt,9,0,0,0,0,0,since the keras api is now directly in tensorflow i think it would be very useful if there were a mechanism directly in tensorflow to supply tfrecords to keras such as in a call to model.fit or with equivalent functionality to flow_from_directory .one key implementation detail is with the way model.compile works as detailed in a comment on this topic in keras as discussed in the same comment numpy arrays have been determined to be the primary way of interacting with keras so this should not be an upstream keras request because tfrecords currently require tensorflow so other backends couldnt be supported easily.new pull request includes examples starter code keras issues
217517126,8773,https://api.github.com/repos/tensorflow/tensorflow/issues/8773,xiaoguai0992,5,0,0,0,0,0,i wish to use tensorflow on windows with c and gpu support is there any pre build sdk for windows or i need to compile from sources?when i tried to compile on windows with cmake and msvc smaple trainer cannot build correctly because of some header files missing such as graph.pb.h is there any solutions?env:windows vs cmake
217159370,8742,https://api.github.com/repos/tensorflow/tensorflow/issues/8742,seventhmoon,1,0,0,0,0,0,in link of get the recommended bazel version listed in os_setup.html which is pointing to is broken
217083068,8729,https://api.github.com/repos/tensorflow/tensorflow/issues/8729,yaumeg,2,0,0,0,0,0,i would like to perform a convd_transpose but i cant see any implementation in tensorflow.i guess that inserting zeros between each values and then applying a regular convd would do the job principle explained hereafter with convd_tranpose i cant find a way to implement this kind of operation with tensorflow import numpy as nparr np.arange array np.insert(arr slice none array any ideas thank you
217065002,8727,https://api.github.com/repos/tensorflow/tensorflow/issues/8727,nb312,3,0,1,1,1,1,when i run a command in the terminal c:\python import tensorflow as tf it show the error traceback most recent call last file stdin line in module file e:\tools\python\lib\site-packages\tensorflow\__init__.py line in module del corenameerror name core is not defined i have a searching but dont find an answer i would happy that someone gives me the answer
216971245,8713,https://api.github.com/repos/tensorflow/tensorflow/issues/8713,gaohuazuo,1,0,0,0,0,0,currently configure set some environment variables and invoke bazel fetch when the fetch fails due to network issue for example the environment variables are also lost i tried to source configure but the shell died instantly the only way i found to workaround this is to replace bazel fetch with bash in configure then manually run bazel fetch a few times until it succeed imho configure should do the configuration separately instead of as part of bazel fetch
216881558,8701,https://api.github.com/repos/tensorflow/tensorflow/issues/8701,dennybritz,12,0,0,0,0,0,see for details and user logs.tldr im using tf.learn and for some people the evaluation fails with shape errors this seems to be some kind of gpu memory sharing issue as subsequent runs seem to consistently increase the shape size: invalidargumenterror see above for traceback logits and labels must have the same first dimension got logits shape and labels shape invalidargumenterror see above for traceback logits and labels must have the same first dimension got logits shape and labels shape invalidargumenterror see above for traceback logits and labels must have the same first dimension got logits shape and labels shape evaluation works independently when there is no training in progress it also doesnt happen when using the cpu only.i personally have run into similar issues before then multiple processes were trying to share the gpu but that shouldnt be the case here
216592289,8673,https://api.github.com/repos/tensorflow/tensorflow/issues/8673,cancan101,0,0,0,1,0,0,fixes wanted to get this up for feedback comments etc i know the files are not in the right places.i called it convdbnns but perhaps bnnsconvd is a better name?i also have another pr on top of this that adds an attr filter_layout to allow pre-transposing the weights
216501380,8665,https://api.github.com/repos/tensorflow/tensorflow/issues/8665,sseveran,1,0,1,1,0,1,i am not sure if this is an actual bug or if its expected but undocumented behavior.i have a model that uses multiple lookup tables created via string_to_index i freeze the model like so: bazel-bin/tensorflow/python/tools/freeze_graph input_graph=/tmp/tf/graph.pbtxt input_checkpoint=/tmp/tf/model.ckpt output_graph=/tmp/ticker_classifier.pb output_node_names=sigmoid initializer_nodes=init_all_tables however when the model is reloaded and i attempt to run it i get an error table not initialized i get exactly the same resulting file whether i specify initializer_nodes or not the behavior i was expecting was for the model to contain the lookup tables in a ready to use state for inference but i dont know if that is an unreasonable expectation what related github issues or stackoverflow threads have you found by searching the web for your problem?i have not seen any issues related to this i previously posted about this here environment infooperating system macos and linux centos installed version of cuda and cudnn noneif installed from source provide the commit hash git rev-parse head bbeabdbfbecffdbd build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu mar build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)i have been unable to make a small example but i can spend more time on it if needed what other attempted solutions have you tried?the workaround is to add init_all_tables to the output_nodes and then run init_all_tables before feeding the session examples for inference this does have the side effect of needing to distribute the source files for the tables to the same path on all nodes that was originally used for training
216433405,8660,https://api.github.com/repos/tensorflow/tensorflow/issues/8660,reuben,1,0,0,0,0,0,why diagnosing a problem with batch normalized rnns i tried to add a summary of the population statistics in my batch normalized cell but then when i try to run the merged summaries i get the following error:tensorflow.python.framework.errors_impl.invalidargumenterror the node merge/mergesummary has inputs from different frames the input tower_/rnn/while/multi_rnn_cell/cell_/gru_cell/gates/r/rnn/multi_rnn_cell/cell_/gru_cell/gates/r/pop_var is in frame tower_/rnn/while/tower_/rnn/while the input tower_/rnn/while/multi_rnn_cell/cell_/gru_cell/gates/r/rnn/multi_rnn_cell/cell_/gru_cell/gates/r/pop_var is in frame tower_/rnn/while/tower_/rnn/while/.this is on tensorflow ill try to get a reduced testcase
216428338,8658,https://api.github.com/repos/tensorflow/tensorflow/issues/8658,julj,19,0,0,0,0,0,most of the time i want to save the best models instead of the most recent models doing so using tf.train.saver requires to choose when to save a model and to delete the worst model which might not be the oldest manually.a method to save the n best models according to some user defined value would be nice
216412936,8656,https://api.github.com/repos/tensorflow/tensorflow/issues/8656,matthiasreisser,2,0,0,0,0,0,"what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system linux mint rafaelainstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):ls l usr/local/cuda/lib/libcud*/usr/local/cuda/lib/libcudadevrt.a/usr/local/cuda/lib/libcudart.so libcudart.so../usr/local/cuda/lib/libcudart.so libcudart.so usr/local/cuda/lib/libcudart.so.../usr/local/cuda/lib/libcudart_static.a/usr/local/cuda/lib/libcudnn.so/usr/local/cuda/lib/libcudnn.so./usr/local/cuda/lib/libcudnn.so.../usr/local/cuda/lib/libcudnn_static.a the output from python c import tensorflow print(tensorflow.__version__) ...-rcthis bug appeared also on the current tf release when installed via pip install tensorflow-gpuif installed from source provide the commit hash git rev-parse head )git rev-parse head dbdccacadcba the output of bazel version build label if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) import osos.environ cuda_visible_devices import tensorflow as tfa tf.variable(.)loss a-.) optimizer tf.train.gradientdescentoptimizer(.)train_op optimizer.minimize(loss)sess tf.session()sess.run(tf.global_variables_initializer())print(sess.run( train_op,a ))print(sess.run(a)) the two print statements evaluate to none when allowing gpu computation to happen by commenting out the second line above the two print statements evaluate to: none so apparently when using the gpu the variable a is evaluated before the gradient op is executed and the other way around on cpu i am not entirely sure what the desired behaviour is supposed to be but im pretty sure they should not be inconsistent what other attempted solutions have you tried?a few things i have observed: import os os.environ cuda_visible_devices import tensorflow as tfwith tf.device(/cpu a tf.variable(.)loss a-.) optimizer tf.train.gradientdescentoptimizer(.)train_op optimizer.minimize(loss)init_op tf.global_variables_initializer()sess tf.session()sess.run(init_op)print(sess.run( train_op,a ))print(sess.run(a)) evaluates to none the following code import os os.environ cuda_visible_devices import tensorflow as tfa tf.variable(.)loss a-.) optimizer tf.train.gradientdescentoptimizer(.)train_op optimizer.minimize(loss)init_op tf.global_variables_initializer()with tf.control_dependencies( train_op a tf.identity(a)sess tf.session()sess.run(init_op)print(sess.run(a)) evaluates to this seems to enforce the behaviour of cpu-only computation when using the gpu.also the initial example has been run on three different machines all with the same titanx gpu model.what am i missing any help would be greatly appreciated.matthias"
216348673,8652,https://api.github.com/repos/tensorflow/tensorflow/issues/8652,naatje80,2,0,0,0,0,0,in the latest sources the issue is fixed in the thensorflow/python/client/session.py but i now receive the following error instead: exception ignored in bound method basesession.__del of tensorflow.python.client.session.session object at xfbecee>>traceback most recent call last file home/.local/lib/python./site-packages/tensorflow/python/client/session.py line in del__attributeerror nonetype object has no attribute tf_newstatus i compiled the model manually using the latest version sources and using bazel the same seems to be true as with the issue the error is not always popping up and the run seems to have ended normally however now and then to above error message is displayed
216248007,8640,https://api.github.com/repos/tensorflow/tensorflow/issues/8640,loretoparisi,2,0,0,0,0,0,im building a small project using the current java binding of tensorflow tensorflow-java running the examples of inception network i get this warning about the platform cpu instruction set that could be used w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. is it possibile to build the jar here enabling sse avx and fma instruction set
216113509,8626,https://api.github.com/repos/tensorflow/tensorflow/issues/8626,roytseng-tw,1,0,0,0,0,0,for example i defined a simple network pythonimport tensorflow as tfimport tensorflow.contrib.layers as tcldef model(x reuse=none with tf.variable_scope(foo reuse=reuse as scope tcl.convd(x and then i try to call the model function twice without setting reuse=true for second call pythonx tf.placeholder(tf.float x tf.placeholder(tf.float model(x)model(x) when using with tf.get_variable this will cause a error however its not the case for tf.contrib.layers?i tried to print out all the nodes in the graph with following codes pythonfor n in tf.get_default_graph().as_graph_def().node print(n.name) i got this result: placeholderplaceholder_foo/conv/weights/initializer/random_uniform/shapefoo/conv/weights/initializer/random_uniform/minfoo/conv/weights/initializer/random_uniform/maxfoo/conv/weights/initializer/random_uniform/randomuniformfoo/conv/weights/initializer/random_uniform/subfoo/conv/weights/initializer/random_uniform/mulfoo/conv/weights/initializer/random_uniformfoo/conv/weightsfoo/conv/weights/assignfoo/conv/weights/readfoo/conv/biases/initializer/constfoo/conv/biasesfoo/conv/biases/assignfoo/conv/biases/readfoo/conv/convolution/shapefoo/conv/convolution/dilation_ratefoo/conv/convolution/expanddims/dimfoo/conv/convolution/expanddimsfoo/conv/convolution/expanddims_/dimfoo/conv/convolution/expanddims_foo/conv/convolution/convdfoo/conv/convolution/squeezefoo/conv/biasaddfoo/conv/relufoo_/conv/convolution/shapefoo_/conv/convolution/dilation_ratefoo_/conv/convolution/expanddims/dimfoo_/conv/convolution/expanddimsfoo_/conv/convolution/expanddims_/dimfoo_/conv/convolution/expanddims_foo_/conv/convolution/convdfoo_/conv/convolution/squeezefoo_/conv/biasaddfoo_/conv/relu it seems that there is only one copy of weights instead of two.so even if reuse is not set to true the weights can still be shared is this the correct behavior?or did i miss anything?im using python tensorflow ubuntu
216042105,8623,https://api.github.com/repos/tensorflow/tensorflow/issues/8623,upavan,3,0,0,0,0,0,hi team while installing tensorflow on windows im facing the below error pip install ignore-installed upgrade is not a supported wheel on this platform. please let me know how to overcome this issue.thankspavan
215889399,8604,https://api.github.com/repos/tensorflow/tensorflow/issues/8604,malmaud,1,0,0,0,0,0,i understand why this fails given the way control inputs are inserted in non-variable ops created in while loops but maybe the limitations should be documented somewhere: pythonimport tensorflow as tfi tf.constant()def body(i w tf.variable(tf.constant return i+w loop tf.while_loop(lambda i tf.less(i body i )s tf.session()s.run(tf.global_variables_initializer invalidargumenterror the node while/w/assign has inputs from different frames the input while/j is in frame while/while the input while/w is in frame
215737921,8581,https://api.github.com/repos/tensorflow/tensorflow/issues/8581,wangxianliang,1,0,0,0,0,0,windows server r if input file names contain non ascii characters such as chinese exception will raise node readfile readfile _device=/job:localhost/replica:/task:/cpu: (unstack_:) caused by op readfile defined at file train.py line in module tf.app.run file c:\anaconda\lib\site-packages\tensorflow\python\platform\app.py line in run sys.exit(main(_sys.argv flags_passthrough file train.py line in main train file train.py line in train num_preprocess_threads=num_preprocess_threads file input.py line in distorted_inputs num_readers flags.num_readers file input.py line in batch_inputs image_buffer tf.read_file(filename file c:\anaconda\lib\site-packages\tensorflow\python\ops\gen_io_ops.py line in read_file result op_def_lib.apply_op(readfile filename=filename name=name file c:\anaconda\lib\site-packages\tensorflow\python\framework\op_def_library.py line in apply_op op_def=op_def file c:\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in create_op original_op=self._default_original_op op_def=op_def file c:\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in init self._traceback extract_stack()notfounderror see above for traceback can not get size for __.jpg uf\u\udcd\ub\udcbb\udcb\udcbd\ub\udcb\udca\udcb\udcc\udcce\uc\udcfe\udca\udca node readfile readfile _device=/job:localhost/replica:/task:/cpu: (unstack
215642185,8573,https://api.github.com/repos/tensorflow/tensorflow/issues/8573,kushNumberTheory,1,0,0,0,0,0,i am having issues while building pip package.i am building tensorflow with cpu configurations steps i followedbazel release ubuntu version lts bazel clean configure bazel build config op kush@kush-lenovo-b-:~/machine_learning/deeplearning/tensorflow bazel build config=opwarning ignoring ld_preload in environment.warning config values are not defined in any rc file opinfo found targets...info elapsed time s critical path s now when i try to run bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkg kush@kush-lenovo-b-:~/machine_learning/deeplearning/tensorflow bazel-bin/tensorflow/tools/pip_package/build_pip_packagebash bazel-bin/tensorflow/tools/pip_package/build_pip_package no such file or directory but then i tried sudo it gives sudo bazel-bin/tensorflow/tools/pip_package/build_pip_package command not found bazel is installed in my system i have checked it i also tried giving full path in bazel-bin sudo usr/bin/bazel-bin/tensorflow/tools/pip_package/build_pip_package command not found i have bazel release can anyone help me on this
215557799,8566,https://api.github.com/repos/tensorflow/tensorflow/issues/8566,sullivak,1,0,0,0,0,0,im seeing an issue with various networks where depending on image input size it either works crashes with a cuda_error_illegal_address error or most worryingly runs without issue but visually has strange artifacts see attached does not happen if running on cpu.os ubuntu running docker image based on tensorflow/tensorflow:..-gpu with nvidia-docker ce gpus x titan x pascal was also able to replicate running native on similar machine with same os and hardware.possibly related github issues but these seem to be generally fixed by upgrading to same or earlier tensorflow or cudnn versions.ls l usr/local/nvidia/lib/libcud*lrwxrwxrwx nov usr/local/nvidia/lib/libcuda.so libcuda.so..lrwxrwxrwx nov usr/local/nvidia/lib/libcuda.so libcuda.so..-rw-r--r root root sep usr/local/nvidia/lib/libcuda.so..this code can demonstrate the issues changing run_type changes the effect hopefully each option is clear problem here starts after avg_pool but ive seen with other outputs e.g activation functions. pythonimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfrun_type gpu_artifacts gpu_works gpu_artifacts gpu_crashes cpuif run_type gpu_works num_gpus num_rows elif run_type gpu_crashes num_gpus num_rows elif run_type gpu_artifacts num_gpus num_rows elif run_type cpu num_gpus num_rows num_cols xx yy np.meshgrid(np.linspace np.pi num_cols np.linspace np.pi num_rows))faux_img np.zeros((num_rows num_cols faux_img np.sin(xx np.cos(yy)faux_img np.cos(xx np.cos(yy)faux_img np.cos(xx np.sin(yy)plt.figure()plt.imshow(faux_img)plt.title(input)input_shape faux_img.shapeinput_shape np.append input_shape)inputs np.array( faux_img.copy().ravel() )x_in_ravel tf.placeholder(tf.float none np.prod(input_shape name=x_in_ravel)x_in tf.reshape(x_in_ravel input_shape)filt_vals np.random.randn astype(np.float)w tf.variable(tf.constant(filt_vals))strides conv_out tf.nn.convd(x_in w strides=strides padding=valid name=conv)pool_out tf.nn.avg_pool(conv_out ksize strides=strides padding=valid name=pool)config tf.configproto(device_count={gpu num_gpus})sess tf.session(config=config)sess.run(tf.global_variables_initializer())outs_conv outs_pool sess.run( conv_out pool_out feed_dict={x_in_ravel inputs})filt_out_fig filt_out_ax plt.subplots()pool_out_fig pool_out_ax plt.subplots()filt_out_ax.imshow(outs_conv filt_out_ax.set_title(convd output looks normal)pool_out_ax.imshow(outs_pool pool_out_ax.set_title(avg_pool output striping artifacts)plt.show() output for run_type gpu_crashes: i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyw tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gibw tensorflow/stream_executor/cuda/cuda_driver.cc creating context when one is currently active existing xcbbi tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y y i tensorflow/core/common_runtime/gpu/gpu_device.cc y y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id e tensorflow/stream_executor/cuda/cuda_event.cc error polling for event status failed to query event cuda_error_illegal_addressf tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc unexpected event status output plots for run_type gpu_artifacts:! input
215361333,8551,https://api.github.com/repos/tensorflow/tensorflow/issues/8551,TianweiXing,2,0,0,0,0,0,i try to use image.decode_image to read image file as tensor but this function returns a tensor without shape.as said in the doc their input and output are all of variable size if you need fixed size images pass the output of the decode ops to one of the cropping and resizing ops.but actually it cannot be passed to resizing op without shape.is there a way to use import image as a normal tensor?codes i ran it in ipy notebook fn pngimage_contents tf.read_file(fn)image tf.image.decode_image(image_contents channels=)imageimage.shapeimage.eval().shapeimg_resize tf.image.resize_images(image output:tensorshape(none valueerror traceback most recent call last)
215323264,8548,https://api.github.com/repos/tensorflow/tensorflow/issues/8548,td2014,1,0,0,0,0,0,modified mnist.py to use random seeds to allow for repeatable behavior during development and testing this may be especially helpful in the mnist tutorials both graph level seeds via the tf.set_random_seed call like tf.set_random_seed()and op level via something like:mnist input_data.read_data_sets(flags.data_dir one_hot=true seed=)are allowed to be consistent with the overall tensorflow random seed handling
215243623,8527,https://api.github.com/repos/tensorflow/tensorflow/issues/8527,yurivict,1,0,0,0,0,0,it should use the default values
215242396,8526,https://api.github.com/repos/tensorflow/tensorflow/issues/8526,WiseDoge,1,0,0,0,0,0,environment:windows cpu i hqgpu mdriver cuda cudnn python tensorflow-gpu when i try to run this pythonimport tensorflow as tfinput tf.placeholder(tf.float)input tf.placeholder(tf.float)output tf.add(input input with tf.session as sess print(sess.run( output feed_dict={input input:.})) my program will crash and show that python has stopped working this is trace: e:\pycode>python test.pyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cublas_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cudnn_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cufft_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library nvcuda.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library curand_.dll locallye c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op bestsplits device_type cpu for unknown op bestsplitse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op countextremelyrandomstats device_type cpu for unknown op countextremelyrandomstatse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op finishednodes device_type cpu for unknown op finishednodese c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op growtree device_type cpu for unknown op growtreee c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op reinterpretstringtofloat device_type cpu for unknown op reinterpretstringtofloate c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op sampleinputs device_type cpu for unknown op sampleinputse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op scatteraddndim device_type cpu for unknown op scatteraddndime c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topninsert device_type cpu for unknown op topninserte c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topnremove device_type cpu for unknown op topnremovee c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op treepredictions device_type cpu for unknown op treepredictionse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op updatefertileslots device_type cpu for unknown op updatefertileslotsi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties:name geforce gtx mmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc dma i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc yi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device gpu device name geforce gtx m pci bus id failed to get the number of cuda devices cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime versionfailed to initialize cuda device cuda driver version is insufficient for cuda runtime version but if change test.py as this pythonimport tensorflow as tfwith tf.session as sess with tf.device(/cpu input tf.placeholder(tf.float input tf.placeholder(tf.float output tf.add(input input print(sess.run( output feed_dict={input input:.})) it will ok py e:\pycode>python test.pyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cublas_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cudnn_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cufft_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library nvcuda.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library curand_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties:name geforce gtx mmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc dma i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc yi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device gpu device name geforce gtx m pci bus id e c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op bestsplits device_type cpu for unknown op bestsplitse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op countextremelyrandomstats device_type cpu for unknown op countextremelyrandomstatse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op finishednodes device_type cpu for unknown op finishednodese c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op growtree device_type cpu for unknown op growtreee c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op reinterpretstringtofloat device_type cpu for unknown op reinterpretstringtofloate c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op sampleinputs device_type cpu for unknown op sampleinputse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op scatteraddndim device_type cpu for unknown op scatteraddndime c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topninsert device_type cpu for unknown op topninserte c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topnremove device_type cpu for unknown op topnremovee c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op treepredictions device_type cpu for unknown op treepredictionse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op updatefertileslots device_type cpu for unknown op updatefertileslots
215226148,8524,https://api.github.com/repos/tensorflow/tensorflow/issues/8524,Dav-Jay,1,0,0,0,0,0,from the recorded video of tf dev summit held about one month ago i got to know the newest distributed tf is able to achieve x speed up for the inception v model on a server of nodes gpus and the codes will be released since i am far away from getting such a good scaling performance using my own distributed implementation i have been looking forward to seeing and studying from the training codes from tf could anyone tell me the progress now thanks
215127900,8511,https://api.github.com/repos/tensorflow/tensorflow/issues/8511,dale-cooper,5,0,0,0,0,0,hey the documentation on how to serve canned estimators with tensorflow serving is somewhat confusing the docstring for export_savedmodel says to provide a function returning an inputfnops which isnt documented anywhere in the source it says it moved to estimator/export.py and inputfnops was renamed to servinginputreceiver however this does not exist at all i might have been confused by multiple checked out versions here i suppose digging around in the tests for estimator it all is becoming a bit clearer but it would be great if there were more documentation on this topic also from the serving side as i now have my model exported but cannot get any predictions out of it thanks!christianmentions martinwicke and ispirmustafa
214970271,8496,https://api.github.com/repos/tensorflow/tensorflow/issues/8496,persiyanov,5,0,0,0,0,0,tf has no analogue to np.random.choice function that chooses random element from tensor optionally according to provided probabilities its especially useful in rl problems when you use epsilon-greedy exploration strategy what related github issues or stackoverflow threads have you found by searching the web for your problem? these two solve the problem using tf.multinomial but maybe it will be convenient to include tf.random_choice in tf api?it could be implemented through tf.multinomial for example
214880146,8484,https://api.github.com/repos/tensorflow/tensorflow/issues/8484,w4nderlust,2,0,0,0,0,0,"if you provide to tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits labels that are not inside the number of classes that is the length of the logits or the length of the second dimension of the logits if using the first dimension for batching you get a loss of nan and then everything in the model becomes nan i spent quite some time debugging this in my model and i believe that this should not be silent there should be at least a warning at execution time.more detail i just discovered if you run it on cpu an invalidagrumenterror is raised if you run it on gpu you get nan so you may want to fix only the gpu implementation to behave like the cpu one.environment infooperating system ubuntu bitinstalled version of cuda and cudnn cuda cudnn tensorflow version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)run it with cuda_visible_device to see the difference on cpu and gpu. import numpy as npimport tensorflow as tfclasses num_datapoints xs np.random.rand(num_datapoints,)ys np.random.randint(classes size=num_datapoints)graph tf.graph()with graph.as_default x tf.placeholder(tf.float shape= none y tf.placeholder(tf.int shape= none w tf.variable(tf.random_normal classes b tf.variable(tf.random_normal( classes logits tf.matmul(x w b loss tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits labels=y optimizer tf.train.adamoptimizer(.).minimize(loss)with tf.session(graph=graph as session tf.global_variables_initializer().run for step in range loss_val session.run optimizer loss feed_dict={x xs y ys print(step step loss loss_val:.f}.format(step=step loss_val=loss_val"
214690523,8467,https://api.github.com/repos/tensorflow/tensorflow/issues/8467,3h4,3,0,0,0,0,0,im creating a simple lstm in keras and during training i get these warnings:! screenshot from know i can set the tf_cpp_min_log_level according to this question use the tf.logging.set_verbosity(verbosity to control this but for my example i would like to hide the warnings that tensorflow wasnt compiled to use instructions show the logging of device when starting a new session that is very useful to confirm that the gpu support is working hide the pool allocator warnings since they clutter the console output from keras during training.i havent found a way to do this perhaps it could be added as a feature
214652514,8461,https://api.github.com/repos/tensorflow/tensorflow/issues/8461,kdexd,3,0,0,0,0,0,this pr attempts to introduce convdtranspose class and an equivalent functional interface named convd_transpose additions have been done in tensorflow/python/layers/convolution.py inclusion consists of additions of class method documentation tests and aliases.fixes
214350075,8429,https://api.github.com/repos/tensorflow/tensorflow/issues/8429,jubjamie,1,0,0,0,0,0,this really is a small issue but i couldnt submit a pr as i couldnt find the file for it.at the current version is stated as when it should be the docs at root i.e in tensorflow.org/api_docs refer to the most recent stable branch in this case r.) thats it sorry for raising an issue for such a trivial detail thanks
214300314,8423,https://api.github.com/repos/tensorflow/tensorflow/issues/8423,zhaomingxiaomi,4,0,0,0,0,0,when i used android studio build the android demo i got this error:warning ignoring http_proxy in environment.____loading package tensorflow/examples/android____found target...____building...target tensorflow/examples/android:external_assets up-to-date nothing to build)____elapsed time s critical path s:copyexternalassets up-to-date:buildnativebazelwarning ignoring http_proxy in environment.____loading complete analyzing...warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:avgpooling_op.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:bounds_check.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:cwise_ops.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:cwise_ops_common.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:cwise_ops_gradients.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_activations.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_attention.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_backward_spatial_convolutions.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_cuboid_convolution.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_patch_d.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_pooling.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_softmax.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:eigen_spatial_convolutions.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:fifo_queue.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:maxpooling_op.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:ops_util.cc directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:ops_util.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:padding_fifo_queue.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:pooling_ops_common.cc directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:pooling_ops_common.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:queue_base.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:queue_op.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/kernels:typed_queue.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/ctc:ctc_beam_entry.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/ctc:ctc_beam_scorer.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/ctc:ctc_beam_search.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/ctc:ctc_decoder.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/ctc:ctc_loss_util.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/tensor_bundle:naming.cc directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/tensor_bundle:naming.h directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/tensor_bundle:tensor_bundle.cc directly you should either move the file to this package or depend on an appropriate rule there.warning home/zhaoming/tensorflow/tensorflow/tensorflow/core/build in srcs attribute of cc_library rule tensorflow/core:android_tensorflow_lib_lite please do not import tensorflow/core/util/tensor_bundle:tensor_bundle.h directly you should either move the file to this package or depend on an appropriate rule there.____found target...____building...error home/zhaoming/tensorflow/tensorflow/tensorflow/examples/android/build c compilation of rule tensorflow/examples/android:libtensorflow_demo.so failed false failed error executing command bin/false md mf bazel-out/stub_armeabi-va-opt/bin/tensorflow/examples/android/_objs/libtensorflow_demo.so/tensorflow/examples/android/jni/object_tracking/tracked_object.pic.d remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status target tensorflow/examples/android:tensorflow_native_libs failed to builduse verbose_failures to see the command lines of failed build steps.____elapsed time s critical path s failedfailure build failed with an exception what went wrong:execution failed for task buildnativebazel process command home/zhaoming/bin/bazel finished with non-zero exit value try:run with stacktrace option to get the stack trace run with info or debug option to get more log output.build failedtotal time secshow can i fix it thx
214164470,8403,https://api.github.com/repos/tensorflow/tensorflow/issues/8403,dbarnes,1,0,0,0,0,0,summarywhen using libtensorflow.so in a c app a segfault occurs when exiting the app on ubuntu while on osx sierra this does not get thrown both using protobuf the actual execution of the app is fine successfully running data through the graph only the mentioned segfault when exiting from the stacktrace you can see it occurs while a protobuf hastable is cleared by libtensorflow.so environment infoubuntu installed version of cuda and cudnn ls l usr/local/cuda/lib/libcud rw-r--r root k sep usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root sep usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root sep usr/local/cuda/lib/libcudart.so libcudart.so...-rw-r--r root k sep usr/local/cuda/lib/libcudart.so...-rw-r--r root k sep usr/local/cuda/lib/libcudart_static.alrwxrwxrwx mtanner jul usr/local/cuda/lib/libcudnn.so libcudnn.so.*lrwxrwxrwx mtanner jul usr/local/cuda/lib/libcudnn.so libcudnn.so...*-rwxrwxr-x mtanner m jul usr/local/cuda/lib/libcudnn.so...*-rw-rw-r mtanner m jul usr/local/cuda/lib/libcudnn_static.a installed from source the commit hash edcacdffcfbbfa from the r branch the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jar build time wed feb build timestamp build timestamp as int workspace.bzl simply updated to point to a fixed protobuf to match our system version native.http_archive name protobuf urls sha acbccecafdfbaaecadffeafcc strip_prefix protobuf logs or other output that would be helpful protoc version libprotoc gdb stacktrace at the end of app execution) thread xfffdcdc lwp exited program received signal sigsegv segmentation fault.xfffefedea in std::_hashtable
214058953,8396,https://api.github.com/repos/tensorflow/tensorflow/issues/8396,kinDSa,3,0,0,0,0,0,i am following tensorflow for poet instruction for retrain model i have successfully create retrained_graph.pb and retrained_labels.txt while i use imagenet_comp_graph_label_strings.txt and tensorflow_inception_graph.pb then application run without any error but use my created file then i get error that: caused by java.lang.unsupportedoperationexception op batchnormwithglobalnormalization is not available in graphdef version it has been removed in version use tf.nn.batch_normalization at org.tensorflow.graph.importgraphdef(native method at org.tensorflow.graph.importgraphdef(graph.java at org.tensorflow.graph.importgraphdef(graph.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.load(tensorflowinferenceinterface.java at org.tensorflow.contrib.android.tensorflowinferenceinterface.initializetensorflow(tensorflowinferenceinterface.java at org.tensorflow.demo.tensorflowimageclass
214044373,8394,https://api.github.com/repos/tensorflow/tensorflow/issues/8394,ilya-biryukov,1,0,0,0,0,0,issue descriptiontensorflow currently fails with the following error if compiled using clang in c opt mode when trying to import tensorflow.contrib package in python python code reproducing the problem is very simple import tensorflow.contrib program output: libprotobuf error external/protobuf/src/google/protobuf/descriptor_database.cc file already exists in database tensorflow/core/example/example.proto libprotobuf fatal external/protobuf/src/google/protobuf/descriptor.cc check failed generated_database_->add(encoded_file_descriptor size terminate called after throwing an instance of google::protobuf::fatalexception what check failed generated_database_->add(encoded_file_descriptor size the short story is that protobufs are getting statically linked into two shared libraries both of which get loaded at runtime and that causes the error.heres the full breakdown of what happens protobufs tensorflow/core:protos_all_cc get compiled as a static library protobufs tensorflow/core:protos_all_cc get statically linked into two separate shared libraries pywrap_tensorflow_internal.so and pywrap_tensorflow_print_model_analysis_lib.so while compiling those clang inlines the protobuf initialization code( adddescriptors inside example.pb.cc (look for it in bazel-genfiles to the global initialization code of both shared libraries python run.py starts running while processing pythons import statement dynamic linker gets called to load pywrap_tensorflow_internal.so static initialization code inside example.pb.cc is run registering it to the protobuf database of pywrap_tensorflow_internal.so at a later point pywrap_tensorflow_print_model_analysis_lib.so gets loaded since python calls dlopen with rtld_global dynamic linker finds an existing symbols for adddescriptorsimpl in pywrap_tensorflow_internal.so and uses that for all calls to that function later(for calls coming from pywrap_tensorflow_print_model_analysis_lib.so too static initialization code inside example.pb.cc is run again for pywrap_tensorflow_print_model_analysis_lib.so it calls adddescriptorsimpl and gets into the function from pywrap_tensorflow_internal.so which tries to registers the same file again in the protobuf database of pywrap_tensorflow_internal.so leading to the specified error.here are a few observations that may be interesting it works with gcc because gcc doesnt inline adddescriptors to the global initialization code of libraries then dynamic linker merges those two functions into one and that function has a proper check for being called multiple times( adddescriptorsimpl which is getting called after inlining doesnt but note that it may break too if gcc will start inlining adddescriptors in a newer version it works on mac because dynamic linker there doesnt merge corresponding functions into one note that it means we get multiple protobuf databases(one for each loaded shared library that has protobufs in it and can probably lead to other problems environment infooperating system ubuntu installed version of cuda and cudnn none the commit hash git rev-parse head ) ffbfaeaddadbfdef the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed feb build timestamp build timestamp as int repro extract test.zip to the repository root(its a sample python target that fails make sure clang is installed my version is ubuntu~trusty but that shouldnt matter configure with export cc=/usr/bin/clangtf_need_jemalloc tf_need_gcp tf_need_hdfs tf_enable_xla tf_need_opencl tf_need_cuda yes configure build and run with opt bazel run c opt test:run
213990693,8387,https://api.github.com/repos/tensorflow/tensorflow/issues/8387,jaakkopasanen,1,0,0,0,0,0,tensorboard creates a new unique scope for summaries every time existing variable scope is re-entered leading to summaries being split to different groups in the tensorboard this might be due to summaries using name scopes internally and re-entered variable scopes having unique original_name_scope . pythondef print_scope(scope print scope.name format(scope.name print(scope.original_name_scope format(scope.original_name_scope))with tf.variable_scope(parent as parent_scope print_scope(parent_scope with tf.variable_scope(childa as childa_scope print_scope(childa_scope)print()with tf.variable_scope(parent_scope print_scope(parent_scope with tf.variable_scope(childb as childb_scope print_scope(childb_scope) outputs scope.name parentscope.original_name_scope parent scope.name parent/childascope.original_name_scope parent/childa scope.name parentscope.original_name_scope parent scope.name parent/childbscope.original_name_scope parent_/childb/ child b is created in re-entered parent scope and has prefix for parent in the original_name_scope i believe parent prefix is what confuses name scope in tensorboard summaries i think re-entering existing variable scope should not have these unique prefixes for parent scope.i have lemmatizer wrapped in python class and tensorboard summaries are created in different stages of graph build with re-entered variable scopes tensorboard splits graphs like so screenshot on tensorflow
213966606,8381,https://api.github.com/repos/tensorflow/tensorflow/issues/8381,caiqi,1,0,0,0,0,0,dropoutwrapper in rnn_cell do not have a state for training or testing although the keep_prob can be passed as a tensor conditioned on training/testing would it be possible to add a state argument like in tf.nn.dropout
213957210,8378,https://api.github.com/repos/tensorflow/tensorflow/issues/8378,louisquinn,2,0,0,0,0,0,below is the error:exception in thread main java.lang.unsupportedoperationexception op batchnormwithglobalnormalization is not available in graphdef version it has been removed in version use tf.nn.batch_normalization at org.tensorflow.graph.importgraphdef(native method at org.tensorflow.graph.importgraphdef(graph.java at org.tensorflow.graph.importgraphdef(graph.java at org.tensorflow.examples.labelimage.executeinceptiongraph(labelimage.java at org.tensorflow.examples.labelimage.main(labelimage.java:)i cant find the source file in which to use tf.nn.batch_normalizationare the java libraries using older versions of tensorflow
213840152,8364,https://api.github.com/repos/tensorflow/tensorflow/issues/8364,BlackHC,1,0,0,0,0,0,see source code formatting leaks into general text)or returns and raises get folded into parameters).either the doc generator needs to understand python doc comments better or the doc comments need to be updated to work better with markdown extra newlines etc what do you think?thanks andreas
213838209,8363,https://api.github.com/repos/tensorflow/tensorflow/issues/8363,jyegerlehner,1,0,0,0,0,0,dandelionmane this is an alternative to pr since you mentioned you are considering alternative ways forward it replaces the use of a moving average window with a simple first-order lag filter it has a few advantages a bit less code it doesnt require visiting a potentially large number of values in the window to be averaged when computing each smoothed value the degree of smoothing doesnt change when you get near the end of the curve.i left the changes in the html file so you could just run it without having to rebuild.heres what it does on the same data shown in the other pr.! p_alt
213790682,8359,https://api.github.com/repos/tensorflow/tensorflow/issues/8359,weiliu620,3,0,0,0,0,0,in tf.layers there are convd and convd_transpose there is convd but no convd_transpose im wondering if it is possible to have convd_transpose any time soon thanks
213723772,8348,https://api.github.com/repos/tensorflow/tensorflow/issues/8348,hhultin,2,0,0,0,0,0,"what related github issues or stackoverflow threads have you found by searching the web for your problem?found various issues on sparsetensors but nothing about while loops and gradients environment infooperating system:ubuntu installed version of cuda and cudnn noneif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)this code leads to the error when trying to compute the gradients typeerror expected binary or unicode string got tensorflow.python.framework.sparse_tensor.sparsetensor object at xfbf> import tensorflow as tfdef body(a b x i i b b tf.sparse_tensor_dense_matmul(a x return a b x idef cond(a b x i return i sess tf.interactivesession()indices values a tf.sparsetensor(indices values x tf.ones b tf.zeros_like(x b tf.while_loop(cond body a b x tf.constant() )grad tf.gradients(b x)print(sess.run( grad ))sess.close what other attempted solutions have you tried?if a is removed as a loop variable everything works as expected: import tensorflow as tfdef body(b x i i b b tf.sparse_tensor_dense_matmul(a x return b x idef cond(b x i return i sess tf.interactivesession()indices values a tf.sparsetensor(indices values x tf.ones b tf.zeros_like(x) b tf.while_loop(cond body b x tf.constant() )grad tf.gradients(b x)print(sess.run( grad ))sess.close logs or other output that would be helpful(if logs are large please upload as attachment or provide link). traceback most recent call last file sparse_tensor_gradient_error.py line in module grad tf.gradients(b x file home/y/anaconda/lib/python./site-packages/tensorflow/python/ops/gradients_impl.py line in gradients setgrad(grads y loop_state.zeroslikeforexit(y file home/y/anaconda/lib/python./site-packages/tensorflow/python/ops/control_flow_ops.py line in zeroslikeforexit result array_ops.zeros_like(val optimize=false file home/y/anaconda/lib/python./site-packages/tensorflow/python/ops/array_ops.py line in zeros_like tensor ops.convert_to_tensor(tensor name=tensor file home/y/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file home/y/anaconda/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file home/y/anaconda/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file home/y/anaconda/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file home/y/anaconda/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in listcomp tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file home/y/anaconda/lib/python./site-packages/tensorflow/python/util/compat.py line in as_bytes bytes_or_text,))typeerror expected binary or unicode string got tensorflow.python.framework.sparse_tensor.sparsetensor object at xfbf"
213716640,8345,https://api.github.com/repos/tensorflow/tensorflow/issues/8345,Prakashvanapalli,0,0,0,0,1,0,when i trained model for several epochs and want to retrain it again for more epochs how would adam optimizer work will it initialize the time from t or will it save the last time step a the documentation in tensorflow shows the following calculations is there a away i can add these metrics to tensorboard t t lr_t learning_rate sqrt beta^t beta^t m_t beta m_{t beta g v_t beta v_{t beta g g variable variable lr_t m_t sqrt(v_t epsilon)i know this question need to be asked in stackoverflow but there are no answers for a few questions since a long time question and question am actually getting a problem with error rate when re-training the model from the last checkpoint and i was not sure what exactly is happening with adam optimizer in this case
213706484,8344,https://api.github.com/repos/tensorflow/tensorflow/issues/8344,jacob1017,0,0,1,0,1,0,if we want deployed tensorflow on our cluster it is really inefficient in my opinion as the official tutorial shows how many task you have launched then how many times you should run you program file on those nodes as our developers hope tensorflow will be our hadoop in deep learning hadoop to launch an job would be more convenient just execute once your job command maybe i doesnt use this framework correctly if you have any good ideas for this we can discussed an nice solution and make our world beautiful
213681191,8340,https://api.github.com/repos/tensorflow/tensorflow/issues/8340,ClimbsRocks,3,0,0,0,0,0,per heres my current import: pythonimport osos.environ tf_cpp_min_vlog_level os.environ tf_cpp_min_log_level from keras.constraints import maxnormfrom keras.layers import dense dropoutfrom keras.models import sequentialfrom keras.wrappers.scikit_learn import kerasregressor kerasclassifier however as you can see if you look at a recent travis build there are still tens of thousands of lines of informational logs like so: level tensorflow:registering pack function packgrad at xfccabb in gradient.level tensorflow:registering unpack function unpackgrad at xfccd in gradient.level tensorflow:registering concat function concatgrad at xfccd in gradient.level tensorflow:registering concatv function concatgradv at xfccdc in gradient.level tensorflow:registering concatoffset none in gradient.level tensorflow:registering slice function slicegrad at xfccdd in gradient....level tensorflow in gradients_/relu__grad/relugrad:level tensorflow out gradients_/add__grad/reshape gradients_/add__grad/reshape_:level tensorflow:gradient for matmul_level tensorflow in gradients_/add__grad/reshape:level tensorflow out gradients_/matmul__grad/matmul gradients_/matmul__grad/matmul_:level tensorflow:gradient for relu_level tensorflow in gradients_/matmul__grad/matmul:level tensorflow out gradients_/relu__grad/relugrad:level tensorflow:gradient for add_ the docs are not obvious for how to adjust this using python and keras how can i adjust verbosity to ignore these logs environment infooperating system:linux and macusing v on both platforms installed directly from binary urls
213664321,8337,https://api.github.com/repos/tensorflow/tensorflow/issues/8337,zxvix,3,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem?nothing found environment infooperating system:ubuntu installed version of cuda and cudnn cuda cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): -rw-r--r root root usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root usr/local/cuda/lib/libcudart.so libcudart.so...-rw-r--r root root usr/local/cuda/lib/libcudart.so...-rw-r--r root root usr/local/cuda/lib/libcudart_static.a-rwxr-xr-x root root usr/local/cuda/lib/libcudnn.so-rwxr-xr-x root root usr/local/cuda/lib/libcudnn.so.-rwxr-xr-x root root usr/local/cuda/lib/libcudnn.so...-rw-r--r root root usr/local/cuda/lib/libcudnn_static.a if installed from source provide the commit hash git rev-parse head )aeefaecfbacccbd the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed feb build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)i have a model that basically consists of several bidirectional rnns the longest of which has hundreds of timesteps what other attempted solutions have you tried logs or other output that would be helpfulwhen i try to launch the first session.run call in tfdbg using command r i got the following error i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gib i tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id created model with fresh parameters.traceback most recent call last file train.py line in module trainer.train file train.py line in train step_loss summary self.m.train_step(self.sess batch_docs batch_queries batch_answers batch_target file home/zhangx/nn_tool_x/mr_base/model.py line in train_step feed_dict=feed_dict options=run_options run_metadata=run_metadata file home/zhangx/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/debug/wrappers/framework.py line in run run_end_resp self.on_run_end(run_end_req file home/zhangx/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py line in on_run_end self._dump_root partition_graphs=partition_graphs file home/zhangx/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/debug/lib/debug_data.py line in init self._load_partition_graphs(partition_graphs validate file home/zhangx/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/debug/lib/debug_data.py line in load_partition_graphs self._validate_dump_with_graphs file home/zhangx/anaconda/envs/tf/lib/python./site-packages/tensorflow/python/debug/lib/debug_data.py line in validate_dump_with_graphs node datum.timestamp repr(pending_inputs node )))valueerror causality violated in timing relations of debug dumps optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/tensorarrayreadv/enter__grad/b_acc these input(s are not satisfied optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/tensorarrayreadv/enter__grad/b_acc optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/tensorarrayreadv/enter__grad/nextiteration is this a model related error or possibly a tfdbg bug
213589583,8320,https://api.github.com/repos/tensorflow/tensorflow/issues/8320,rosun82,0,1,0,0,0,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem?i am trying to use grpc_util to convert tensorflow::status to grpc::status and found that i cannot build the target because of package visibility issues.here is my recommendation:remove all visibility when open source tensorflow reason it is pointless to have the package visibility as we have the source code and we can modify it if everybody has to modify their own copy of tensorflow to depend on portions of the source code it only makes peoples life harder as there would be millions of different forks all trying to hack the build file and every time a new tensorflow is launched it breaks other peoples build which seems quite counter productive in google we want to make the dependency as accurate as possible in the open source world because of the visibility issue we have to try to pull in much bigger build target to get something working which is again counter productive environment infooperating system:installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
213588217,8319,https://api.github.com/repos/tensorflow/tensorflow/issues/8319,danqing,3,0,0,1,0,0,environment infooperating system macos tensorflow cuda/cudnn i was setting up tensorflow with gpu reading the instructions it seems that a few things can be made clearer im not sure where the source for the website lives so i want to file an issue so it can possibly be updated and people searching for related issues can have a pointer.the issues are with the requirements to run tensorflow with gpu support section it mentioned that one should set ld_library_path according to nvidia docs but nvidia docs only asks to specify dyld_library_path which actually doesnt work can we say on the website how exactly it should be set same for cudnn the linked page actually has no instruction about how to install cudnn it may be useful to put on the website that the include and lib files should be manually copied over to usr/local/cuda the following should be run to avoid segfault sudo ln sf usr/local/cuda/lib/libcuda.dylib usr/local/cuda/lib/libcuda..dylib .the paths for should be: shexport cuda_home=/usr/local/cudaexport ld_library_path=/developer/nvidia/cuda-./lib/export path=/developer/nvidia/cuda-./bin:${path
213216999,8256,https://api.github.com/repos/tensorflow/tensorflow/issues/8256,notnot,1,0,0,0,0,0,is variable being worked on in the go bindings
213185705,8253,https://api.github.com/repos/tensorflow/tensorflow/issues/8253,arcosmin,2,0,0,0,0,0,"hi all,there is still this reoccurring issue with the tensorflow-gpu install with the latest version from pip.when i try and test using the hello world constant i get the below errors even though it works.e c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op bestsplits device_type cpu for unknown op bestsplitse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op countextremelyrandomstats device_type cpu for unknown op countextremelyrandomstatse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op finishednodes device_type cpu for unknown op finishednodese c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op growtree device_type cpu for unknown op growtreee c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op reinterpretstringtofloat device_type cpu for unknown op reinterpretstringtofloate c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op sampleinputs device_type cpu for unknown op sampleinputse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op scatteraddndim device_type cpu for unknown op scatteraddndime c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topninsert device_type cpu for unknown op topninserte c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topnremove device_type cpu for unknown op topnremovee c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op treepredictions device_type cpu for unknown op treepredictionse c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op updatefertileslots device_type cpu for unknown op updatefertileslots"
213181808,8251,https://api.github.com/repos/tensorflow/tensorflow/issues/8251,samodadela,4,0,0,0,0,0,following the installation guide at cuda cuda_.._win.exe cuddn cudnn-.-windows-x-v..zip python x python-..-amd.exe)then issued:c:\>pip install upgrade tensorflow-gpucollecting tensorflow-gpu could not find a version that satisfies the requirement tensorflow-gpu from versions no matching distribution found for tensorflow-gpusame happens with non-gpu version:c:\>pip install upgrade tensorflowcollecting tensorflow could not find a version that satisfies the requirement tensorflow from versions no matching distribution found for tensorflow what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system windows xinstalled version of cuda and cudnn cudnn-.-windows-x-v..zip cuddn/bin cudnn_.dll if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)just follow the provided instructions on what other attempted solutions have you tried?first i tried bit python but found on so that is not supported install guide should state which python is requrired or i wanted specify the correct url for tensorflow-gpu but i dont know which is the correct one for r gpu like this example):pip install provide correct url on storage.googleapis.com it is not possible to browse the directory contents i need tensorflow r x gpu!got the hint from here logs or other output that would be helpful(if logs are large please upload as attachment or provide link
213119932,8246,https://api.github.com/repos/tensorflow/tensorflow/issues/8246,shoyer,8,0,0,0,0,0,this is a popular question on stackoverflow note that the answer so far only works for some use cases the one presented in the question).the best i could come up with for a general solution uses tf.while_loop which is pretty verbose and maybe slower than necessary ill add a link to the implementation i wrote for tf.contrib.training.resample_at_rate after the next internal/github sync
213109516,8244,https://api.github.com/repos/tensorflow/tensorflow/issues/8244,mdymczyk,3,0,0,0,0,0,the current java apis tensor.create(object is really slow for a batch of images of size xx its taking around seconds to put this into perspective runner.run with that data and an inceptionv graph took below second so data prep is x of the runtime here for a batch of images its around sec).is this working as intended when running the python code using simple sess.run(fetches feed_dict=feed_dict with which the graph meta file was generated tf and feeding a python array i dont see such hiccups the speed is the same as the java runner.run() .might it be because of build flags used maybe im missing some optimizations?for now this small part is killing the whole performance bringing it down from obs/sec runner.run time to about obs/sec tensor.create+run()).a bit of a sidenote the performance page states this will result in poor performance sess.run(train_step feed_dict={x batch_xs y batch_ys})but currently theres no other way to feed data from the java api right a queue able to read from a file and from memory i.e from a java structure would be amazing jar build command export cc=/usr/bin/gccexport cxx=/usr/bin/g++export tf_need_cuda=export gcc_host_compiler_path=$ccexport buildflags=--config=cuda copt=-m linkopt=-m copt=-march=nativebazel build c opt tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni buildflags spawn_strategy=standalone genrule_strategy=standalone environment info os ubuntu gpu gpu titan x pascal gb cpu intel xeon processor e v core gpu drivers nvidia cuda driver version cudnn cuda tensorflow version jar file built from current master cecb example javapublic void test random r new random int imagesize int batch float input new float batch imagesize for(int i i batch i for(int j j imagesize j input i j r.nextfloat long start system.nanotime tensor.create(input long end system.nanotime around sec system.out.println(took end start
212916412,8227,https://api.github.com/repos/tensorflow/tensorflow/issues/8227,cancan101,4,0,0,0,0,2,right now the responsibility of choosing image data format i.e the representation of batches of image is that of the data scientist ie model writer i suggest there should a solution in tf to move this to the optimizer xla perhaps or worst cast op writer.for some background:currently tensorflow supports nchw and nhwc though other formats like chwn might be possible down the road many of the ops support both formats that being said the docs say:>the best practice is to build models that work with both nchw and nhwc as it is common to train using nchw on gpu and then do inference with nhwc on cpu.this requires the user to have to do some wrangling e.g loading the checkpoint of weights and re-buidling graph in python to map from one image format to another further this must be done with some knowledge of the platform on which the graph will be executed ie which ops are defined and if both which is faster)?right now model builder must build the model to take in channel order and pass that around ideally the model could be written once with enough meta information attached to the graph to allow optimizers after the fact ie at inference time on other platforms to choose the best representation further even at training time it would be great if the data scientist didnt need to be concerned with image data format ie dimension ordering and could use abstractions for accessing results that took care of data access.i dont have a clear proposal of how to clean this up but this seems like a potential pain point or a the very least results in people leaving performance on the table both when training and at inference tl dr many data scientists just want to write cnns without thinking about tensor layouts
212911053,8224,https://api.github.com/repos/tensorflow/tensorflow/issues/8224,yaroslavvb,2,0,0,0,0,0,"tensorflow does some numeric type promotion.it should do more of it.examples this works is int gets promoted to floattf.pow this fails apply_op promotion logic is not smart enoughtf.pow this fails is converted to int but needs to be inttf.sparse_placeholder(tf.float this works numpy arrays are int by defaulttf.sparse_placeholder(tf.float np.array( , )) this came up in joshb who wrote type promotion logic in opdeflibrary.apply_opcc suharshs who changed the default to treat python integer as int"
212878918,8220,https://api.github.com/repos/tensorflow/tensorflow/issues/8220,rfeinman,19,0,0,0,0,0,issue summaryi am having trouble allocating gpu devices for a multiprocessing pool please see the short code reproduction below i would like to understand why i am getting the cuda_error_not_initialized error in case for this case the program hangs and i have to stop my docker container to exit minimal reproducible example core code: pythonimport tensorflow as tfdef run_session(device gpu_options tf.gpuoptions(allow_growth=true visible_device_list=device sess tf.session(config=tf.configproto(gpu_options=gpu_options print(using device s device a tf.placeholder(tf.int name=a y tf.identity(a name=y print sess.run(y feed_dict={a sess.close print(done.) case this works fine): pythonrun_session() i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id using device done. case this works fine): pythonrun_session()run_session() i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id using device done.w tensorflow/stream_executor/cuda/cuda_driver.cc creating context when one is currently active existing xcbbei tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id using device done. case this works fine): pythonimport multiprocessing as mpp mp.pool()p.map(run_session p.close()p.join() i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id using device using device done.done. case here the program hangs): pythonimport multiprocessing as mprun_session()p mp.pool()p.map(run_session p.close()p.join() i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id using device done.e tensorflow/stream_executor/cuda/cuda_driver.cc could not retrieve cuda device count cuda_error_not_initializedusing device e tensorflow/stream_executor/cuda/cuda_driver.cc could not retrieve cuda device count cuda_error_not_initializedusing device environment infooperating system ubuntu lts gnu/linux generic x_)docker container gcr.io/tensorflow/tensorflow:latest-devel-gpucuda version cudnn version related github issues
212867181,8218,https://api.github.com/repos/tensorflow/tensorflow/issues/8218,jart,3,0,0,1,0,0,rather than having configure mutate one of our bzl files which dirties the git repository this change has configure put define options in bazelrc for jemalloc gcs hdfs and xla inside a bazelrc at the root of the tensorflow repository this file is listed in gitignore.therefore running configure will no longer cause the git repository to be in a modified state.fixes cc girving
212846023,8214,https://api.github.com/repos/tensorflow/tensorflow/issues/8214,finbarrtimbers,1,0,0,0,0,0,as requested in my machine is having errors running the tests so this isnt tested i would be very grateful if one of the maintainers can run the tests in travis and ill fix the results
212834259,8211,https://api.github.com/repos/tensorflow/tensorflow/issues/8211,cancan101,4,0,0,0,0,0,would be great to be able to build for ios using bazel rather than make this would allow more rapid development of ops that can only run on ios eg metal performance shaders over from looks like there might be some progress here cc petewarden aselle
212693791,8192,https://api.github.com/repos/tensorflow/tensorflow/issues/8192,wanjianwei,1,0,0,0,0,0,"hey,everyone!at first,i can run the demo in ios_examples/camera successfully.then i use retrain.py to retrain with my own dataset,and get a output_graph.pb flie and a output_labels.txt file.after that i replace the files in ios_examples/camera/data with them,and run the demo again,but it failed error description are as follow:####error#########:could not create tensorflow graph invalid argument no opkernel was registered to support op decodejpeg with these attrs registered devices cpu registered kernels no registered kernels node decodejpeg decodejpeg acceptable_fraction channels dct_method fancy_upscaling=true ratio try_recover_truncated=false (decodejpeg/contents) ;:couldnt load model invalid argument no opkernel was registered to support op decodejpeg with these attrs registered devices cpu registered kernels no registered kernels node decodejpeg decodejpeg acceptable_fraction channels dct_method fancy_upscaling=true ratio try_recover_truncated=false (decodejpeg/contents) can someone tell me how to fix it"
212666590,8187,https://api.github.com/repos/tensorflow/tensorflow/issues/8187,andykernahan,5,0,0,0,0,0,"hello,the protobuf.cmake is building an old version of protobuf one which specifically does not include the parsing limit bump from mb to gb referenced in and resolved by it is currently building a fork which contains a specific msvc fix this fix has been merged into the main protobuf repository and is included in is it possible to align the repository and tree used by protobuf.cmake to that used by workspace.bzl im happy to submit a pr.cheers"
212525271,8175,https://api.github.com/repos/tensorflow/tensorflow/issues/8175,yaroslavvb,4,0,0,0,0,0,documentation states note if the inputs tensor has a rank greater than then it is flattened prior to the initial matrix multiply by w however the following returns tensor with shape shape as if the input has not been flattened tf.layers.dense(tf.placeholder(tf.float
212218618,8137,https://api.github.com/repos/tensorflow/tensorflow/issues/8137,taion,6,0,0,0,0,0,the functions in tf.layers take a data_format parameter however this parameter has different semantics from the identically named data_format parameter everywhere else in tensorflow its expected to be channels_first or channels_last versus nhwc nchw or ndhwc everywhere else as such its inconvenient from a dx perspective to intersperse tf.layers code with other tensorflow code as it requires passing different values for the identically-named data_format parameter.ideally the functions in tf.layers should support the more explicit data_format strings while channels_first and channels_last are easier to understand theyre less explicit as there do exist cases outside of tensorflow where the tensor layout is chwn given which channels_first meaning nchw is not optimally clear.on the same note its a bit inconvenient that tf.layers.batch_normalization takes axis instead of data_format while this is more correct it makes it annoying to switch back and forth especially that the fused batch norm implementation only supports nhwc and nchw anyway rather than batch norm on an arbitrary axis
211993525,8107,https://api.github.com/repos/tensorflow/tensorflow/issues/8107,yaroslavvb,4,0,0,0,0,0,i get error below trying to import tensorflow built at commit efdc head as of sat i havent made any changes to my build procedure in last month and installing similarly built wheel from mar works.looking at relevant differences between mar and mar i see that jhseu s c refactored python module loading in trace type help copyright credits or license for more information import tensorflow as tftraceback most recent call last file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper fp pathname description imp.find_module(_pywrap_tensorflow_internal dirname(__file file home/yaroslav/.conda/envs/tf-mar/lib/python./imp.py line in find_module raise importerror(_err_msg.format(name name=name)importerror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper import pywrap_tensorflow_internalimporterror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module raise importerror(msg)importerror traceback most recent call last file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper fp pathname description imp.find_module(_pywrap_tensorflow_internal dirname(__file file home/yaroslav/.conda/envs/tf-mar/lib/python./imp.py line in find_module raise importerror(_err_msg.format(name name=name)importerror no module named pywrap_tensorflow_internalduring handling of the above exception another exception occurred:traceback most recent call last file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module from tensorflow.python.pywrap_tensorflow_internal import file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in module pywrap_tensorflow_internal swig_import_helper file home/yaroslav/.conda/envs/tf-mar/lib/python./site-packages/tensorflow/python/pywrap_tensorflow_internal.py line in swig_import_helper import pywrap_tensorflow_internalimporterror no module named pywrap_tensorflow_internalfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help
211933111,8092,https://api.github.com/repos/tensorflow/tensorflow/issues/8092,Cynthia0629,1,0,0,0,0,0,"hello,everyone!i am new about tensorflow,and i am just trying to build tensorflow from source according to the steps described step by step but i get the following error is there anyone who can help on it that would be very kind of you!thank you very much!cynthia@cynthia-pc:~/tensorflow configure..........continue........then the result came with the following......i guess that maybe something wrong with my bazel but im new about this,and im really no idea how to solve it.unexpected pipe read status no such file or directoryserver presumed dead now printing home/cynthia/.cache/bazel/_bazel_cynthia/bccbaeaa/server/jvm.out:java hotspot(tm server vm warning you have loaded library home/cynthia/.cache/bazel/_bazel_cynthia/install/afdeaefefc/_embedded_binaries/libunix.so which might have disabled stack guard the vm will try to fix the stack guard now.its highly recommended that you fix the library with execstack c or link it with z noexecstack.jni initialization failed home/cynthia/.cache/bazel/_bazel_cynthia/install/afdeaefefc/_embedded_binaries/libunix.so home/cynthia/.cache/bazel/_bazel_cynthia/install/afdeaefefc/_embedded_binaries/libunix.so wrong elf class elfclass possible cause architecture word width mismatch possibly your installation has been corrupted if this problem persists try rm fr home/cynthia/.cache/bazel/_bazel_cynthia/install/afdeaefefc.java.lang.unsatisfiedlinkerror home/cynthia/.cache/bazel/_bazel_cynthia/install/afdeaefefc/_embedded_binaries/libunix.so home/cynthia/.cache/bazel/_bazel_cynthia/install/afdeaefefc/_embedded_binaries/libunix.so wrong elf class elfclass possible cause architecture word width mismatch)at java.lang.classloader$nativelibrary.load(native method)at java.lang.classloader.loadlibrary(classloader.java:)at java.lang.classloader.loadlibrary(classloader.java:)at java.lang.runtime.loadlibrary(runtime.java:)at java.lang.system.loadlibrary(system.java:)at com.google.devtools.build.lib.unixjniloader.loadjni(unixjniloader.java:)at com.google.devtools.build.lib.unix.processutils.(processutils.java:)at com.google.devtools.build.lib.util.processutils.getpid(processutils.java:)at com.google.devtools.build.lib.util.osutils.forcejni(osutils.java:)at com.google.devtools.build.lib.util.osutils.maybeforcejni(osutils.java:)at com.google.devtools.build.lib.runtime.blazeruntime.newruntime(blazeruntime.java:)at com.google.devtools.build.lib.runtime.blazeruntime.createblazerpcserver(blazeruntime.java:)at com.google.devtools.build.lib.runtime.blazeruntime.servermain(blazeruntime.java:)at com.google.devtools.build.lib.runtime.blazeruntime.main(blazeruntime.java:)at com.google.devtools.build.lib.bazel.bazelmain.main(bazelmain.java"
211920506,8090,https://api.github.com/repos/tensorflow/tensorflow/issues/8090,adityaramesh,6,0,0,0,0,0,this line in nn_impl.py has the following comment add e to epsilon when epsilon e to prevent cudnn exception however i still get the cudnn exception when using values of epsilon less than e what related github issues or stackoverflow threads have you found by searching the web for your problem?nothing related environment info operating system centos release final installed version of cuda installed version of cudnn v installed tensorflow-gpu from pip python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) import tensorflow as tffrom itertools import chainwith tf.device(/gpu x tf.zeros scale offset tf.ones tf.zeros y tf.nn.fused_batch_norm(x scale offset mean=none variance=none epsilon=e data_format=nchw is_training=true init_op tf.group(*(v.initializer for v in chain(tf.global_variables tf.local_variables sess tf.session sess.run(init_op sess.run(y logs or other output that would be helpful python tests/fused_batch_norm_crash.pyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyw tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id e tensorflow/stream_executor/cuda/cuda_dnn.cc failed to enqueue forward batch normalization on stream cudnn_status_bad_paramtraceback most recent call last file home/ar/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file home/ar/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn status run_metadata file share/apps/python/../intel/lib/python./contextlib.py line in exit next(self.gen file home/ar/.local/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.internalerror cudnn launch failure input shape node fusedbatchnorm fusedbatchnorm t=dt_float data_format=nchw epsilon=.e is_training=true device=/job:localhost/replica:/task:/gpu: (random_normal ones zeros const const_) during handling of the above exception another exception occurred:traceback most recent call last file tests/fused_batch_norm_crash.py line in module sess.run(y file home/ar/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/ar/.local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file home/ar/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file home/ar/.local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.internalerror cudnn launch failure input shape node fusedbatchnorm fusedbatchnorm t=dt_float data_format=nchw epsilon=.e is_training=true device=/job:localhost/replica:/task:/gpu: (random_normal ones zeros const const_) caused by op fusedbatchnorm defined at file tests/fused_batch_norm_crash.py line in module data_format=nchw is_training=true file home/ar/.local/lib/python./site-packages/tensorflow/python/ops/nn_impl.py line in fused_batch_norm name=name file home/ar/.local/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in fused_batch_norm is_training=is_training name=name file home/ar/.local/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file home/ar/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file home/ar/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()internalerror see above for traceback cudnn launch failure input shape node fusedbatchnorm fusedbatchnorm t=dt_float data_format=nchw epsilon=.e is_training=true device=/job:localhost/replica:/task:/gpu: (random_normal ones zeros const const
211607424,8047,https://api.github.com/repos/tensorflow/tensorflow/issues/8047,agupta74,2,0,0,0,0,0,"i ran tensorflow debugger using the command python m pythonmodule debug but got the following error i,e signal only works in main thread):! screenshotpng error is thrown when a session run call is executed in a child thread spawned from main thread is tensorflow debugger only supported for single threaded applications what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system centos installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):! screenshot installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .! screenshot installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link"
211551389,8037,https://api.github.com/repos/tensorflow/tensorflow/issues/8037,ajl412860,8,0,0,0,0,0,just got tensorflow running now running into this error currently using mac yosemite downloaded tensorflow using pip through anaconda using python w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations. w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations. w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations. so since anaconda has a special set of commands how do you get tensorflow to run on sse sse and avx via the anaconda command system i am really confused how to go about this
211540219,8033,https://api.github.com/repos/tensorflow/tensorflow/issues/8033,pdrews,9,0,0,0,0,0,"i would like to request direct access to the tensorflow buffers through the c interface i have commented previously about this on stackoverflow and have gotten it to work by exposing the tensorcapi class this required some fiddling around with the source code.my specific use case is using tensorflow from a ros robot operating system node ros has its own build system so i have to link to the tensorflow library libtensorflow.so externally the c interface is much more convenient than the c interface as i only load and do forward inference on static graphs.because i get an opencv array as input my only other option is to iterate the entire array and copy it to a newly allocated tensor buffer as suggested in this post in my case copying the buffer by iterating it can take ms whereas simply pointing to the memory already allocated by opencv incurs almost no overhead i am doing all of this in real time as part of a tight control loop so this extra time is absolutely critical.i realize this patch is probably not the right way to expose the interface and my example code provides no memory checking or safeguards but it is an example diff against the r tensorflow tag): diff git a/tensorflow/c/c_api.cc b/tensorflow/c/c_api.ccindex cee..aeb a/tensorflow/c/c_api.cc b/tensorflow/c/c_api.cc tf_tensor tf_tensor_encodestrings(const tensor src dimvec.size base size deletearray base class tensorcapi public static tensorbuffer buffer(const tensor tensor return tensor.buf static tensor maketensor(tf_datatype type const tensorshape shape tensorbuffer buf return tensor(static_cast(type shape buf tensorbuffer tensorcapi::buffer(const tensor tensor return tensor.buf tensor tensorcapi::maketensor(tf_datatype type const tensorshape shape tensorbuffer buf return tensor(static_cast(type shape buf create an empty tensor of type dtype shape can be arbitrary but has to result in a zero-sized tensor.diff git a/tensorflow/c/c_api.h b/tensorflow/c/c_api.hindex ed..af a/tensorflow/c/c_api.h b/tensorflow/c/c_api.h limitations under the license include stddef.h include stdint.h>+#include tensorflow/core/framework/tensor.h c api for tensorflow limitations under the license and the api just provides high level controls over the number of devices of each type using tensorflow::tensor;+using tensorflow::tensorbuffer;+using tensorflow::tensorshape ifdef cplusplus extern c endif extern void tf_deletelibraryhandle(tf_library lib_handle in this address space extern tf_buffer tf_getalloplist namespace tensorflow{+class tensorcapi public static tensorbuffer buffer(const tensor tensor static tensor maketensor(tf_datatype type const tensorshape shape tensorbuffer buf ifdef cplusplus end extern c endifdiff git a/tensorflow/core/framework/tensor.h b/tensorflow/core/framework/tensor.hindex cacd..e a/tensorflow/core/framework/tensor.h b/tensorflow/core/framework/tensor.h limitations under the license namespace tensorflow class tensorbuffer forward declaration.-class tensorcapi class tensorcapi ingroup core represents an n-dimensional array of values. and a small snippet of code to use it put an image in the cameraimg matcv::resize(image->image cameraimg cv::size(inputwidth inputheight cv::inter_area create a new tensor pointing to that memory:const int_t tensordims inputheight,inputwidth,};int imnumpt new int();tf_tensor tftensor tf_newtensor(tf_datatype::tf_uint tensordims cameraimg.data inputheight inputwidth null imnumpt);tensor inputimg tensorflow::tensorcapi::maketensor(tftensor->dtype tftensor->shape tftensor->buffer"
211515745,8029,https://api.github.com/repos/tensorflow/tensorflow/issues/8029,ClarkZinzow,1,0,0,0,0,0,there are by my count at least broken gdoc links scattered among documentation code and build scripts caused by this commit migrating gdocs to the docs_src source code documentation generating system so far in relation to this the following issues have been opened and the following pull requests have been made two of the pull requests and take care of the two broken links on the core tensorflow readme and are waiting on the signing of clas.due to the restructuring that took place in the transition from gdoc to docs_src a simple find-replace wont work for example in reference to this line in fully-connected-reader.py tensorflow/gdoc/how_tos/reading_data.md#reading-from-files would have to be changed to tensorflow/docs_src/programmers_guide/reading_data.md#reading-from-files in fact according to the new reference syntax it should be changed to reading_data#reading-from-files} .moreover some gdoc files that are still linked to by existing documentation no longer exist in this repository e.g the adding an op extension guide links to many of the python files contained in this gdoc folder which has now been deleted although these python files should probably be reinstated somewhere this appears to be the only collection of python files in gdoc that were deleted and are still needed.so how best to go about fixing these issues manually fix each broken link in a single pull request and where should the python files pertaining to the adding an op extension guide be placed
211398708,8011,https://api.github.com/repos/tensorflow/tensorflow/issues/8011,pannous,11,0,0,0,0,0,feature request for a better error description or for better summary handling:the following code works fine if some summaries where defined before: ops ops tf.summary.merge_all() session.run(ops) however if there were no summaries we get:typeerror fetch argument none has invalid type class nonetype>which is really saying one of the session.run ops where empty which is forbidden.alternatively let merge_all return a noop if there are no summaries
211301068,7999,https://api.github.com/repos/tensorflow/tensorflow/issues/7999,Bihaqo,1,0,0,0,0,0,the see download and setup link in the readme is broken
211291466,7995,https://api.github.com/repos/tensorflow/tensorflow/issues/7995,flyi,5,0,0,0,0,0,"traceback most recent call last file d:\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(mname file d:\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file d:\python\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file d:\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file d:\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow file d:\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflowduring handling of the above exception another exception occurred:traceback most recent call last file stdin line in module file d:\python\python\lib\site-packages\tensorflow\__init__.py line in module from tensorflow.python import file d:\python\python\lib\site-packages\tensorflow\python\__init__.py line in module raise importerror(msg)importerror traceback most recent call last file d:\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(mname file d:\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level file frozen importlib._bootstrap line in gcd_import file frozen importlib._bootstrap line in find_and_load file frozen importlib._bootstrap line in find_and_load_unlocked file frozen importlib._bootstrap line in load_unlocked file frozen importlib._bootstrap line in module_from_spec file frozen importlib._bootstrap_external line in create_module file frozen importlib._bootstrap line in call_with_frames_removedimporterror dll load failed the specified module could not be found.during handling of the above exception another exception occurred:traceback most recent call last file d:\python\python\lib\site-packages\tensorflow\python\__init__.py line in module from tensorflow.python import pywrap_tensorflow file d:\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file d:\python\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py line in swig_import_helper return importlib.import_module(_pywrap_tensorflow file d:\python\python\lib\importlib\__init__.py line in import_module return bootstrap._gcd_import(name level package level)importerror no module named pywrap_tensorflowfailed to load the native tensorflow runtime.see some common reasons and solutions include the entire stack traceabove this error message when asking for help.====finally,i find the solution :install microsoft visual c redistributable update x"
211021730,7966,https://api.github.com/repos/tensorflow/tensorflow/issues/7966,pavelbulanov,3,0,0,3,0,0,as per agreement in specific comment im creating a new issue for compilation errors for tf under windows for python pre-requisites: windows v.python bit anaconda git version windows.visual studio build tools called build tools for visual studio rc in the download section)cmake-..-rc and swigwin just downloaded latest version at that moment)tensorflow from source master branch from github) reproduction i followed instruction for cmake just running two commands cmake first and msbuild second.to compile i made two changes in sources i found them based on compilation errors.one was adding include intrin.h to tensorflow\core\platform\windows\cpu_info.h inside of ifndef block because of the following error error c cpuidex identifier not found i found commit introduced usage of cpuidex function based on this msdn link i discovered that header file intrin.h has to be included thats why i put it inside cpu_info.h not sure if its exactly perfect place). second issue was commenting out procedures mm_extract_epi and mm_insert_epi in tensorflow\core\platform\windows\intrinsics_port.h due to error error c mm_extract_epi intrinsic function cannot be defined the second issue with mm_extract_epi may be connected with the same reason i dont know in the meantime platform/windows/intrinsics_port.h references immintrin.h but i dont see its actually included anywhere i assume it may correlate with instrih.h inclusion based on this link meanwhile mm_extract_epi can be found in avxintrin.h which may be is included by immintrin.h the following avx intrinsics are not defined on windows in immintrin.h so we define them here.after fixing those two issues that i was able to successfully run msbuild p:configuration=release tf_python_build_pip_package.vcxprojwhich gave me tensorflow-..-cp-cpm-win_amd.whl that i successfully installed into python using pip it seems to be working i was able to run couple of jupyter notebooks including udacity assignments.btw compilation took about hours on my week laptop i-u
210999839,7964,https://api.github.com/repos/tensorflow/tensorflow/issues/7964,CharlesShang,2,0,0,0,0,0,"i want to visualize detection results on images using tf.summary.image and record the original name of this image at the same time.in the old version tf.image_summary i can pass a placeholder for an image name and feed the name in sess.run like this, log_image_data tf.placeholder(tf.uint none none log_image_name tf.placeholder(tf.string)log_image tf.image_summary(log_image_name tf.expand_dims(log_image_data max_images=) but the new version of api only allows to speficy a fixed name in string type instead of a placeholder when building the summry graph tf.summary.image(name tensor max_outputs collections=none args name a name for the generated node which cannot be a placeholder how can i name the visualized image on the fly"
210920003,7951,https://api.github.com/repos/tensorflow/tensorflow/issues/7951,mrry,109,0,0,34,0,25,tl;dr were designing a new input pipeline api for tensorflow and wed like to collect your feature requests on this issue. weve noticed that one of the biggest challenges in getting started with tensorflow is how to load your own data into your programs while tensorflow has several methods that can be used to build complex input pipelines such as tf.train.string_input_producer tf.train.batch etc they were designed for a particular use case processing a static set of files repeatedly and the average user experience with these methods is not great for example once you reach the end of a pipeline it becomes closed and you can never use it again in the same session this requires users to use unnatural workaroundswith control flow or multiple sessionsto get a signal after processing an entire epoch or switch between processing two datasets e.g training and validation data in the same program see and for feature requests about handling multiple epochs see and numerous stack overflow questions for examples of processing different datasets in the same program the current pipelines use tensorflow queues and multiple python threads which can lead to poor performance lock contention in the queues and the python gil and hard-to-understand exceptions tf.errors.outofrangeerror see for a discussion of input pipeline performance see and many more stack overflow questions for an example of the confusing error the pipelines behave poorly if you forget to call tf.train.start_queue_runners(sess in fact they hang indefinitely and deadlock the user program see and many stack overflow questions for some examples of users who have been bitten by this problem.were decided to start from a clean slate and redesign the input pipeline api the existing methods will remain until tf at least but we are planning to add a new set of methods for loading and manipulating datasets were still preparing a detailed design which we plan to share soon but we anticipate that there will be two new apis a dataset represents a collection of data elements each element can be a tuple of one or more tensors e.g an image and its label we will provide methods for creating datasets from tensors and deriving them from another dataset e.g by slicing its elements repeating its elements shuffling its elements batching its elements mapping a function over its elements etc an iterator can be created from a dataset an iterator represents the current position within a dataset and exposes an operation like tf.queuebase.dequeue that can be run to get the next element there will be explicit operations for initializing an iterator so that it can be reused after you have processed all of the elements in a dataset.a similar pattern turns up in many different settings including javas stream api scalas collections and hence sparks rdds and nets language integrated query announcing this plan early because we want to collect feedback on what features youas tensorflow userswould like to see in an input pipeline api what other pain points have we missed what features do you miss from other systems what other suggestions do you have?we look forward to hearing from you
210863162,7947,https://api.github.com/repos/tensorflow/tensorflow/issues/7947,markslwong,2,0,0,0,0,0,problem tensorflow does not build directly from visual studio. cause the cmake scripts were written to assume building from cmake directly due to its usage of the cmake_build_type variable if you generate a visual studio solution and build from directly within that solution the build will fail because visual studio has multiple-configurations rendering this variable blank when generating a solution. solution replace cmake_build_type with configuration which is a visual studio macro during build time visual studio will replace configuration with the currently active configuration being built. cause the cmake build scripts were written to assume release builds would only be built when building in debug some external libraries generate lib files with a d fixed to the end of the name however the reference to these lib files do not account for the d at the end of the name during debug builds linker errors occur when the file is not found. solution use the cmake command debug and optimized to specify references to difference file names upon debug and release builds respectively. cause the cmake build scripts were written to assume release builds would only be built when building in debug linker errors occur because iterator_debug_level is defined and set to zero for all configurations external libraries like eigen build with its own cmake scripts do not define this causing a different code path to be built. solution only define iterator_debug_level in debug builds. cause visual studio compile errors occur functions must always have a return value. solution for function stubs without proper implementation we return null. cause visual studio compile errors occur we define a double code path for eigen that is not available for cpu builds using sse only. solution we remove the double code path if avx and cuda are not available on windows builds this error only shows up on debug builds im assuming because the compiler doesnt strip out unused symbols and tries to link against it
210859905,7946,https://api.github.com/repos/tensorflow/tensorflow/issues/7946,tgy,1,0,0,0,0,0,i was wondering why my checkpoints were being deleted and its only after looking up the behavior of saver.max_to_keep in the docs that i understood what was happening i think a better default setting for this is to keep all checkpoints at least thats what i would have expected
210836577,7945,https://api.github.com/repos/tensorflow/tensorflow/issues/7945,nightrome,0,0,0,0,0,4,"hi,my entire program freezes when i pop an image from an input queue before starting the queue runner the terminal window becomes unresponsive including ctrl+c or ctrl+d signals and the only way to kill the process is to close the terminal window in rare cases zombie processes remain even after closing the terminal i understand that i am not supposed to do that but i think that tensorflow absolutely has to check whether the queue runner is already initialized and throw an exception otherwise several reported issues might be related to that etc.for reproducibility i provide a minimum non-)working example it requires a subfolder images with a handful of jpg images: import tensorflow as tf read images from queuefilename_queue tf.train.string_input_producer(tf.train.match_filenames_once(./images/train/*.jpg shuffle=false)image_reader tf.wholefilereader image_file image_reader.read(filename_queue)image_query tf.image.decode_jpeg(image_file decode the image as a jpeg file resize and assign to a variableimage_query tf.image.resize_images(image_query image_batch tf.variable(tf.zeros trainable=false name=image_batch)next_image tf.assign(image_batch image_query)loss tf.reduce_mean(image_batch start a new session to show example output.with tf.session as sess fix randomness init variables and run threads tf.set_random_seed tf.global_variables_initializer().run go to the next image sess.run( next_image coord tf.train.coordinator threads tf.train.start_queue_runners(coord=coord for i in xrange get an image tensor and print its value image_tensor sess.run( image_batch print(image_tensor finish off the filename queue coordinator coord.request_stop coord.join(threads) and here is the working code the line sess.run( next_image was moved): import tensorflow as tf read images from queuefilename_queue tf.train.string_input_producer(tf.train.match_filenames_once(./images/train/*.jpg shuffle=false)image_reader tf.wholefilereader image_file image_reader.read(filename_queue)image_query tf.image.decode_jpeg(image_file decode the image as a jpeg file resize and assign to a variableimage_query tf.image.resize_images(image_query image_batch tf.variable(tf.zeros trainable=false name=image_batch)next_image tf.assign(image_batch image_query)loss tf.reduce_mean(image_batch start a new session to show example output.with tf.session as sess fix randomness init variables and run threads tf.set_random_seed tf.global_variables_initializer().run coord tf.train.coordinator threads tf.train.start_queue_runners(coord=coord for i in xrange go to the next image sess.run( next_image get an image tensor and print its value image_tensor sess.run( image_batch print(image_tensor finish off the filename queue coordinator coord.request_stop coord.join(threads"
210664202,7935,https://api.github.com/repos/tensorflow/tensorflow/issues/7935,mahatosourav91,1,0,0,0,0,0,for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem?github issue question here environment infooperating system ubuntu windowsinstalled version of cuda and cudnn noif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version if installed from source provide no if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) from tensorflow.contrib.learn.python.learn.estimators import kmeans as kmeans_libfrom tensorflow.contrib.factorization.python.ops import clustering_opsimport randomimport numpy as npimport tensorflow as tfdef input_fn(x):returns an input_fndef fn return tf.constant(x nonereturn fnx np.array( random.random for i in range for j in range dtype=np.float)km kmeans_lib.kmeansclustering(num_clusters initial_clusters=clustering_ops.kmeans_plus_plus_init)km.fit(input_fn=input_fn(x max_steps what other attempted solutions have you tried?hi i found following lines missing in gen_clustering_ops.py i believe they should there in the file if not then please raise a bug i couldnt find the correct place where the change is to be made! image logs or other output that would be helpful(if logs are large please upload as attachment or provide link). file c:\users#####\anaconda\lib\site-packages\tensorflow\python\framework\ops.py line in call_with_requiringreturn getattr(x f)file c:\users#####\anaconda\lib\site-packages\tensorflow\python\framework\common_shapes.py line in call_cpp_shape_fndebug_python_shape_fn require_shape_fn)file c:\users#####\anaconda\lib\site-packages\tensorflow\python\framework\common_shapes.py line in call_cpp_shape_fn_implno c shape function registered for standard op s op.type)runtimeerror no c shape function registered for standard op nearestneighbors
210332399,7902,https://api.github.com/repos/tensorflow/tensorflow/issues/7902,varunkumar3618,0,0,0,0,0,17,a common training pattern is to run an epoch or a few epochs of training and then evaluate on a development set as far as i can tell there does not seem to be an easy way to do this while also using the tfrecords format while it is possible to run through a data split by setting the num_epochs parameter in the functions to read in single examples doing so would require the computation graph to be rebuilt before every validation phase instead it would be preferable to have a symbolic example that could be reset to the beginning of the dataset using a related operation.i feel that the above pattern is sufficiently widely used to make a new feature worthwhile further having control over the number of epochs also becomes important when trying to replicate the performance numbers of other implementations.####the same training-validation pattern using feed dicts:for epoch in num_epochs for batch in get_batches(train epoch sess.run(train_op feed_dict={batch batch num_correct total for batch in get_batches(val epoch acc sess.run(acc_op feed_dict={batch batch total batch.shape num_correct batch.shape acc print dev accuracy after s epochs s epoch num_correct total
210327394,7900,https://api.github.com/repos/tensorflow/tensorflow/issues/7900,geoHeil,1,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem i already set the link to sudo ln s usr/local/cuda/lib/libcuda.dylib usr/local/cuda/lib/libcuda..dylib environment infooperating system:osx installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) . lrwxr-xr-x root wheel feb usr/local/cuda/lib/libcuda..dylib usr/local/cuda/lib/libcuda.dylib-rwxr-xr-x root wheel jan usr/local/cuda/lib/libcuda.dyliblrwxr-xr-x root wheel jan usr/local/cuda/lib/libcudadevrt.a developer/nvidia/cuda-./lib/libcudadevrt.alrwxr-xr-x root wheel jan usr/local/cuda/lib/libcudart...dylib developer/nvidia/cuda-./lib/libcudart...dyliblrwxr-xr-x root wheel jan usr/local/cuda/lib/libcudart.dylib developer/nvidia/cuda-./lib/libcudart.dyliblrwxr-xr-x root wheel jan usr/local/cuda/lib/libcudart_static.a developer/nvidia/cuda-./lib/libcudart_static if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)start python shellimport tensorflowoutput is python anaconda custom x default dec gcc compatible apple llvm clang on darwintype help copyright credits or license for more information import tensorflowi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas...dylib locallysegmentation fault what other attempted solutions have you tried?setting the link as outlined above in the linked issueschecking the cuda sample projects works fine so the library seems to be installed correctly
210303549,7896,https://api.github.com/repos/tensorflow/tensorflow/issues/7896,RiaanZoetmulder,1,0,0,0,0,0,"hello,i have recently updated to the newest version of tensorflow v and am suddenly having some trouble when i use tf.scan.i have already tried setting shapes for all of the variables which didnt work dont really know what else i could do since that is the solution that is recommended for the error in tf.while_loop...here is the code snippet: h z tf.scan self.sample x initializer h tf.expand_dims(z where h is z is expanded to and x is none it worked in the previous version so i am assuming the trouble is related to the tf.scan code.valueerror the shape for scan/while/merge is not an invariant for the loop it enters the loop with shape but has shape after one iteration provide shape invariants using either the shape_invariants argument of tf.while_loop or set_shape on the loop variables"
210286996,7892,https://api.github.com/repos/tensorflow/tensorflow/issues/7892,ShangtongZhang,2,0,0,0,0,0,tf.gradients has a parameter aggregation_method as default it will sum up all the gradients what should i do if i want to sum up all the gradients by weight to be more specific i want to apply a weight to each gradient then sum them up this weight may also comes from a tensorflow variable i asked a question about this in stackoverflow and attached a minimal code snippet
210285350,7891,https://api.github.com/repos/tensorflow/tensorflow/issues/7891,jyegerlehner,1,0,0,0,0,0,the smoothing of training loss in tensorboard was frustrating it would seemingly select a random set of unfiltered points near the end of the loss history leaving one unable to see the most recent trend.i see that at least one other person was annoyed at this as a change has been committed to master that changes the behavior i think the change in this pr is superior the previous one appears to have just truncated the smoothed curve whereas this one will smooth all the way up until the end when im k training iterations in i find myself really wanting to see how the trend has gone in those last k iterations.i also notice that the previous commit didnt update dist/tf-tensorboard.html which i think is the only thing that affects most people running latest master
210278110,7889,https://api.github.com/repos/tensorflow/tensorflow/issues/7889,jtopor,0,0,0,0,1,0,the code provided for the new tf layer api tutorial see does not work if any filter size other than x is specified or if color images are used no matter what other filter size is specified an error is generated indicating a shape mismatch between the specified filter x in the error message shown below and another seemingly hard-coded x filter buried somewhere in the estimator the tensor referred to below as rhs always reports as no matter what the actual filter size is set to within the python code or if a color channel image is used e.g lhs reports as xxx while rhs is xxx).heres a sample code snippet wherein the filter size is set to x instead of x for the first convolution layer conv tf.layers.convd inputs=input_layer filters kernel_size padding=same activation=tf.nn.relu)and heres the error message:invalidargumenterror see above for traceback assign requires shapes of both tensors to match lhs shape rhs shape node save/assign assign t=dt_float class= loc:@convd/kernel use_locking=true validate_shape=true device=/job:localhost/replica:/task:/cpu: (convd/kernel save/restorev what related github issues or stackoverflow threads have you found by searching the web for your problem?none found this seems to be a new problem related to the new tf layer api environment infooperating system windows anaconda python tensorflow
210218622,7876,https://api.github.com/repos/tensorflow/tensorflow/issues/7876,ramnath-k,1,0,0,0,0,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud using cpu version of tensorflow if installed from binary pip package provide a link to the pip package you installed pip install tensorflow the output from python c import tensorflow print(tensorflow.__version if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonimport tensorflow as tfgraph tf.graph()with graph.as_default initializer tf.random_uniform_initializer(minval maxval seed dtype=tf.float var tf.get_variable(var shape dtype=tf.float initializer=initializer saver tf.train.saver(tf.trainable_variables max_to_keep init_op tf.global_variables_initializer graph.finalize()with tf.session(graph=graph as sess sess.run(init_op saver.save(sess sample_graph global_step=)graph tf.graph()with tf.session(graph=graph as sess saver tf.train.import_meta_graph(sample_graph-.meta saver.restore(sess sample_graph saver.save(sess sample_graph global_step print(saver.last_checkpoints lists only sample_graph does not preserve the previous checkpoint sample_graph- essentially i am checkpointing a graph and then importing it on trying to save the next checkpoint the saver overwrites the previous checkpoint in the checkpoint file the actual meta index and data files are not overwritten and only the last saved checkpoint is present in the checkpoint file is this the intended behavior is there any way to preserve the checkpoints across multiple saves of the graph what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
210214673,7873,https://api.github.com/repos/tensorflow/tensorflow/issues/7873,strongrex2001,1,0,0,0,0,0,the url in the command line is cant install from the urlhowever i replaced x with amd and it works
210193208,7869,https://api.github.com/repos/tensorflow/tensorflow/issues/7869,aselle,1,0,0,0,0,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system:installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
210115961,7856,https://api.github.com/repos/tensorflow/tensorflow/issues/7856,enpasos,6,0,0,0,0,0,os windows bit)tensorflow version cuda gpu yesproblem c :>tensorboard logdir= e :\tmp\tensorflow\mnist\logs tensorboard starts without loading data not working and difficult to detect reason) e :>tensorboard logdir= e :\tmp\tensorflow\mnist\logs tensorboard starts with loading data works perfectly
210110643,7855,https://api.github.com/repos/tensorflow/tensorflow/issues/7855,kchen92,3,0,0,0,0,0,i am wondering if someone could add a convd_transpose layer similar to tf.layers.convd_transpose a tf.nn.convd_transpose op already exists thanks
210094359,7853,https://api.github.com/repos/tensorflow/tensorflow/issues/7853,tranvanhoa533,1,0,0,0,0,0,traceback most recent call last file train_image_classifier.py line in module tf.app.run file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv flags_passthrough file train_image_classifier.py line in main clones model_deploy.create_clones(deploy_config clone_fn batch_queue file home/cloud/hoa/workspace/tensorflows/models/slim/deployment/model_deploy.py line in create_clones outputs model_fn(*args kwargs file train_image_classifier.py line in clone_fn logits end_points network_fn(images file home/cloud/hoa/workspace/tensorflows/models/slim/nets/nets_factory.py line in network_fn return func(images num_classes is_training=is_training file home/cloud/hoa/workspace/tensorflows/models/slim/nets/inception_resnet_v.py line in inception_resnet_v tower_conv tower_pool_ )typeerror concat got an unexpected keyword argument axiswhen i run restnet or inception_resnet of slim model this error was raised can you hep me?thanks
210055501,7849,https://api.github.com/repos/tensorflow/tensorflow/issues/7849,skelso068,4,0,0,0,0,0,"im trying to build tensorflow on a machine that has no internet connection ive downloaded thelatest bazel and installed and then run the configure script i downloaded the git repo with git clone recurse-submodues configure then fails with:this then fails with:info unknown host bazel-mirror.storage.googleapis.com error com.google.devtools.build.lib.packages.buildfilecontainserrorsexception error loading package encountered error while reading extension file closure/defs.bzl no such package io_bazel_rules_closure//closure error downloading to tmp/steven/.cache/bazel/_bazel_kelso/aabcdceaddfcdafb/external/io_bazel_rules_closure/cadabdfadfbaefcfd.tar.gz all mirrors are down unknown host bazel-mirror.storage.googleapis.com unknown host github.com . this is to be expected as the machine is on a private network.however i can manually install tarbalss of packages etc on this box but i cant work out where they should be installed within the source tree.is this possible or can tensorflow only be built with an active connection to the internet?many thanks,s"
210047894,7848,https://api.github.com/repos/tensorflow/tensorflow/issues/7848,mahatosourav91,1,0,0,0,0,0,it would be very helpful if method is provided to predict posterior probability of data per each component in gmm.a stackoverflow question has already been raised by longwoo
209931549,7832,https://api.github.com/repos/tensorflow/tensorflow/issues/7832,timanglade,5,0,0,0,0,0,this branch adds minor improvements to tensorflow on android eedef lowers api support to level which is confirmed to work rather than level as indicated right now this setting is commented out by default but could be a useful guideline to most users it might be more interesting to update the ci server to naturally produce nightly builds at that level but it doesnt seem like i can send a pr for that unless i missed it f introduces the use of gc sections when compiling libtensorflow_inference.so which has the effect of reducing final binary size at the time of this pr the following improvements were observed arch master this branch delta armeabi-va bytes bytes mb arm-va bytes bytes mb x bytes bytes mb x bytes bytes mb final observed impact on an app is around mb on a unified release apk
209842607,7823,https://api.github.com/repos/tensorflow/tensorflow/issues/7823,Iolaum,5,0,0,0,0,0,at tensorflow dev summit mr ashish agarwal gave a presentation for ml toolkit and said that gradient boosting decision trees are coming soon i would like to ask two questions is there an approximate timeline for when this feature might be released is there any information if we will be able to leverage tensorflows local gpu computing and distributed capabilities for example as presented by mr jonathan hseu for this particular algorithm
209741828,7816,https://api.github.com/repos/tensorflow/tensorflow/issues/7816,scharron,2,0,0,0,0,0,when using tensorflow debugger the program crashes with the following message f tensorflow/core/framework/tensor_util.cc check failed dt_string other.dtype vs aborted core dumped what related github issues or stackoverflow threads have you found by searching the web for your problem?nothing environment infooperating system linux ip generic ubuntu smp fri dec utc x x x gnu/linuxinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): -rw-r--r root root sept usr/local/cuda-./targets/x_-linux/lib/libcudadevrt.alrwxrwxrwx root root sept usr/local/cuda-./targets/x_-linux/lib/libcudart.so libcudart.so..lrwxrwxrwx root root sept usr/local/cuda-./targets/x_-linux/lib/libcudart.so libcudart.so...-rw-r--r root root sept usr/local/cuda-./targets/x_-linux/lib/libcudart.so...-rw-r--r root root sept usr/local/cuda-./targets/x_-linux/lib/libcudart_static.alrwxrwxrwx root root dc usr/local/cuda-./targets/x_-linux/lib/libcudnn.so libcudnn.so.lrwxrwxrwx root root dc usr/local/cuda-./targets/x_-linux/lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root dc usr/local/cuda-./targets/x_-linux/lib/libcudnn.so...-rw-r--r root root dc usr/local/cuda-./targets/x_-linux/lib/libcudnn_static.a if installed from binary pip package provide pip package you installed pip show tensorflow-gpuname tensorflow-gpuversion summary tensorflow helps the tensors flowhome-page google inc.author-email opensource@google.comlicense apache location home/ubuntu/anaconda/envs/dl/lib/python./site-packagesrequires wheel protobuf numpy six the output from python c import tensorflow print(tensorflow.__version__) . python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)currently cannot find what operation makes the debugger crash what other attempted solutions have you tried?nothing logs or other output that would be helpful (gdb bt xffffae in gi_raise sig=sig@entry at sysdeps/unix/sysv/linux/raise.c xffffaa in gi_abort at abort.c xfffdfcea in tensorflow::internal::logmessagefatal::~logmessagefatal from home/ubuntu/anaconda/envs/dl/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so xfffdff in tensorflow::tensor::deepcopy(tensorflow::tensor const from home/ubuntu/anaconda/envs/dl/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so xfffdda in tensorflow::copyop::compute(tensorflow::opkernelcontext from home/ubuntu/anaconda/envs/dl/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so xfffdf in tensorflow::(anonymous namespace)::executorstate::process(tensorflow::(anonymous namespace)::executorstate::taggednode long long from home/ubuntu/anaconda/envs/dl/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so xfffdfe in std::_function_handler::nonblockingthreadpooltempl(int tensorflow::thread::eigenenvironment)::{lambda()#}>::_m_invoke(std::_any_data const from home/ubuntu/anaconda/envs/dl/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so xfffdfac in std::_function_handler::_m_invoke(std::_any_data const from home/ubuntu/anaconda/envs/dl/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so xfffdcc in from usr/lib/x_-linux-gnu/libstdc++.so xffffdba in start_thread arg=xffffff at pthread_create.c xffffaefd in clone at sysdeps/unix/sysv/linux/x_/clone.s
209665781,7809,https://api.github.com/repos/tensorflow/tensorflow/issues/7809,sahil210695,2,0,0,0,0,0,python bin/run_analysis.pytraceback most recent call last file bin/run_analysis.py line in module from src.run_analysis import analyze render_results_as_images file deeposm/src/run_analysis.py line in module import label_chunks_cnn_cifar file deeposm/src/label_chunks_cnn_cifar.py line in module import tflearn file usr/local/lib/python./dist-packages/tflearn/__init__.py line in module from layers import normalization file usr/local/lib/python./dist-packages/tflearn/layers/__init__.py line in module from recurrent import lstm gru simple_rnn bidirectional_rnn file usr/local/lib/python./dist-packages/tflearn/layers/recurrent.py line in module from tensorflow.contrib.rnn.python.ops.core_rnn import static_rnn as rnn importerror no module named core_rnnwhile running the github project this one i am able to create training data successfully but not able to run_analysis.py and while runnin im getting this error and
209607670,7800,https://api.github.com/repos/tensorflow/tensorflow/issues/7800,xksteven,1,0,0,0,0,0,currently tensorflow will provide the gradients for the entire network i.e getting grads_and_vars will return all of the gradients but there isnt any nice way to specifically get the gradients of a specific layer.an example use case for the paper neural networks are easily fooled they took the gradients from the last layer and added them to the original image.there is no current way to do this in tensorflow as far as i know
209552156,7791,https://api.github.com/repos/tensorflow/tensorflow/issues/7791,taion,5,0,0,0,0,0,many initializers defined in init_ops are not exported in the global tensorflow package these include glorot_uniform_initializer this is even explicitly mentioned in the docs as the default initializer for variables glorot_normal_initializer variance_scaling_initializer it seems like this might be an oversight glorot_uniform_initializer is mentioned in existing documentation and glorot_normal_initializer has no other hits in the code base which makes it seem like its for user consumption what related github issues or stackoverflow threads have you found by searching the web for your problem?searched environment infov docker image what other attempted solutions have you tried?similar initializers are available in tf.contrib.layers but it seems preferable to use the ones that already exist in core
209460351,7778,https://api.github.com/repos/tensorflow/tensorflow/issues/7778,bingq,26,0,0,2,0,0,opening this with reference to installed tensorflow with reference to on windows and hit the same issue discussed in with applying the solution suggested in that thread the original issue disappeared but got the new warnings c:\users\geldqb>pythonpython v..:cba jan msc v bit amd on wintype help copyright credits or license for more information import tensorflow as tf hello tf.constant(hello tensorflow sess tf.session w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations print(sess.run(hello))bhello tensorflow
209391986,7769,https://api.github.com/repos/tensorflow/tensorflow/issues/7769,Hong-Xiang,1,0,0,0,0,0,how to implement a estimator with multiple model_fn so that we can train gan-like models like following: gan my_estimator(model_fn=multi_model_fn params=params)for i in range(nb_train gan.fit(x y model_id=gen gan.fit(x y model_id=cri
209378615,7764,https://api.github.com/repos/tensorflow/tensorflow/issues/7764,MycChiu,0,0,0,1,0,0,correspond to issue here is the comparison between my implementation:! dynamic_stitch_gpu current implementation:! dynamic_stitch_cpu by nvidia visual profiler with this code since the kernel uses cudadevicearraystruct which depends on code from tensorflow/core:lib i cant generate custom_op library and use the custom_op for performance comparison i ended up manually switch the tensorflow directory python uses between runs*currently i still have two problems for some reason tensorflow wont use the gpu kernel automatically i had to force it with tf.device whats even worse tensorflow seems to refuse loading the gpu kernel in test_session even if i tried to force it with tf.device so this code is currently not tested with dynamic_stitch_test.py as yaroslavvb suggested cuda kernels need to be real-valued so i ended up calling tf_call_gpu_number_types_no_half(register_dynamic_stitch_gpu to register the kernel however i feel like this will break some peoples current code if their input data is from the unsupported data type fixed in the latest commit.*finally this implementation is really under-optimized for cases where the size of each input data slice is small because the indices are still processed by the cpu plus cuda reads data in warps of threads however i still dont think it will be worse than processing everything on cpu then copy them back to gpu though
209319588,7755,https://api.github.com/repos/tensorflow/tensorflow/issues/7755,chrisranderson,1,0,0,0,0,0,the issue sess.run(tf.global_variables_initializer intermittently takes a long time since installing i believe this also happened on my python install but only after installing for python this particular run was in python and the initializer took seconds see bottom of this post and the attached cprofile output for more detail.strangely it seems like the more i run the same file the faster it runs what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): ls l usr/local/cuda/lib/libcud*-rw-r--r root root oct usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root oct usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root oct usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root oct usr/local/cuda/lib/libcudart.so...-rw-r--r root root oct usr/local/cuda/lib/libcudart_static.a-rwxr-xr-x root root oct usr/local/cuda/lib/libcudnn.so-rwxr-xr-x root root oct usr/local/cuda/lib/libcudnn.so.-rwxr-xr-x root root oct usr/local/cuda/lib/libcudnn.so...-rw-r--r root root oct usr/local/cuda/lib/libcudnn_static.a if installed from binary pip package provide a link to the pip package you installed sudo pip install tensorflow-gpu on the output from python c import tensorflow print(tensorflow.__version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonfrom future import absolute_importfrom future import divisionfrom future import print_functionimport timefrom collections import dequeimport numpy as npprint(importing tensorflow)import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataxavier tf.contrib.layers.xavier_initializerbatch_norm tf.contrib.layers.batch_normtf.set_random_seed()flags tf.app.flagsflags flags.flagsflags.define_string(data_dir data directory for storing data)print(reading data.)mnist input_data.read_data_sets(flags.data_dir one_hot=true)initialization_variance print(creating a session.)sess tf.session()def fully_connected_layer(inputs output_size scope nonlinearity=true with tf.variable_scope(scope input_size inputs.get_shape() .value weights tf.get_variable(w input_size output_size initializer=tf.constant_initializer bias tf.get_variable(b output_size initializer=tf.constant_initializer multiplied tf.matmul(inputs weights return tf.nn.elu(multiplied bias if nonlinearity else multiplied biasprint(setting up the model.)input_images tf.placeholder(tf.float none labels tf.placeholder(tf.float none labelslearning_rate_placeholder tf.placeholder(tf.float name=learning-rate)layer_a fully_connected_layer((input_images a)for i in range layer_a batch_norm(fully_connected_layer(layer_a str(i)))layer_b fully_connected_layer(layer_a b nonlinearity=false)predictions tf.nn.softmax(layer_b cross_entropy tf.reduce_mean(-tf.reduce_sum(labels tf.log(predictions reduction_indices= ))train_step tf.train.adamoptimizer(learning_rate_placeholder).minimize(cross_entropy)print(initializing variables.)sess.run(tf.global_variables_initializer what other attempted solutions have you tried?none logs or other output that would be helpfulsome output from python m cprofile o out.profile mnist-simple.py this is a call to sess.run(tf.global_variables_initializer()) .! image
209142114,7737,https://api.github.com/repos/tensorflow/tensorflow/issues/7737,linrio,1,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem no environment infooperating system wininstalled version of cuda and cudnn no please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide yes a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link). traceback most recent call last file g:/codes/tensorflow/autopilot-tensorflow/train.py line in module merged_summary_op tf.merge_all_summaries()attributeerror module tensorflow has no attribute tf.merge_all_summaries
209090482,7731,https://api.github.com/repos/tensorflow/tensorflow/issues/7731,eamartin,13,0,0,0,0,0,this is a follow-up of would be nice to be able to create a variable or name scope that does not obey all currently open scopes.as an example: pythonwith tf.variable_scope(foo with tf.variable_scope(bar x tf.constant name=x foo/bar/xwith tf.variable_scope(foo with tf.variable_scope(bar fresh=true x tf.constant name=x bar/x this would be useful for using scopes with object oriented tensorflow graphs.consider a slightly modified version of the example from assume each method pushes a scope for the instance with the class name and for the method. pythonclass foo(object def init__(self self.x tf.get_variable(x initializer=tf.constant def meth(self foo z tf.multiply(foo name=z return self.x.assign(z def boom(self foo return tf.multiple self.meth(foo name=baz)f foo foo/__init__/xf foo foo_/__init__/xf.meth foo/meth/*f.boom foo/boom/baz foo/boom/foo/meth/z id like the variables in boom s call to meth to be prefixed with only foo/meth rather than foo/boom/foo/meth this requires either releasing the variable scope within boom before calling out hard or providing a way for meth to clear the existing variable scope
208992154,7712,https://api.github.com/repos/tensorflow/tensorflow/issues/7712,carlthome,6,0,0,0,0,0,proposalthe exponential linear unit elu is already in tensorflow as tf.nn.elu which is great the new parametric version called pelu shows very promising experimental results so i wonder if it could be added in to tensorflow too in order to encourage more widespread experimentation with it by the deep learning community one problem with it though is that its stateful e.g tf.variable meaning its not clear to me where in tensorflow it fits in implementationheres an implementation of the pelu that ive been using lately im assuming batch_size is the first dimension in x ): pythondef pelu(x parametric exponential linear unit with tf.variable_scope(x.op.name activation initializer=tf.constant_initializer shape x.get_shape().as_list alpha tf.get_variable(alpha shape beta tf.get_variable(beta shape positive tf.nn.relu(x alpha beta e negative alpha tf.exp((-tf.nn.relu(-x beta e return negative positive reference
208976131,7710,https://api.github.com/repos/tensorflow/tensorflow/issues/7710,jbedorf,1,0,0,3,0,0,this adds a second communication path to the distributed tensorflow implementation next to the grpc communication path this new communication path uses the message passing interface mpi api to handle communication between processes allowing tensorflow to take advantage of modern high performance networks such as infiniband.this pull request is used to start a discussion about this implementation any draw backs alternatives and requests for feedback and participation from the open source community. the current pull request makes the following changes a new communication path has been added for sending and receiving tensors between distinct processes this new communication path uses the mpi api to handle setting up the connections and the data transfer when using a cuda-aware and/or gpudirect rdma mpi implementation this path works for both cpu and gpu based tensor data this new path is implemented by modifying the current send and receive operations the original grpc code stays in place and is responsible for setting up connections and all non-tensordata communications although mpi supports one sided rdma operations this example implementation uses plain blocking send/receive operations this because the current tensorflow memory model makes it difficult to efficiently implement the communication using mpi_get/mpi_put see below usage in order to setup the required mpi environment the python instances have to be launched using mpirun an example launch script which does not modify the original tf script is supplied in tensorflow/tools/dist_test/mpi/start-openmpi.shthe launching can be done differently if one handles the task type/id selection inside the tf python script by either reading the environment variables as set by mpirun or by using the mpipy python library to retrieve process ids and hostnames the mpi execution path can be disabled by setting the environment variable mpi_path_disabled to like:export mpi_path_disabled= mpi requirements the mpi implementation should be built with support for mpi_thread_multiple to enable this path for gpu data the mpi implementation should be built with cuda support cuda-aware mpi tested using openmpi currently the path to the openmpi library is hard-coded in the third_party/gpus/crosstool/crosstool.tpl file this should be made an option in the configure code implementation details mpi process id mapping to grpc names to handle communication between processes mpi identifies processes using a unique process-id tensorflow uses grpc which uses names based on things like worker gpu task to enable the mpi path in co-existence with the grpc communication stack the names of the grpc server are mapped to the unique mpi ids during the initialization of the grpc stack this conversion is then available once the tensor-data will be communicated. send details the original send operation places a tensor in a table which is then picked up by the grpc thread once a request for that data arrives in the mpi path we place an mpi_send call which will block until the matching mpi_recv call is made on the receiving side by making a hash of the tensor description we create a unique combination of sending-process/tensor-id which will be matched by the receiving process this enables multiple tensors to be in flight from a single process using different send threads sending is a two-step process the first message contains the properties of the tensor data type shape followed by a message containing the actual data the send call is intercept in the file tensorflow/core/distributed_runtime/base_rendezvous_mgr.ccin this function baseremoterendezvous::sendthe code verifies that the destination is a different process from the sending process if this is the case and the implementation is enabled at run-time then it will enter the new sendtoremote functions otherwise it will progress through the original code.to keep the changes as organized as possible the actual implementation of the sending-functions are in tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.ccthe other file modifications are required to get the path enabled through out the calling stack. receive details the original receive operation requests a named-tensor from a remote process by placing a request through the grpc stack in the mpi path this request has been replaced by a set of receive calls that match the send operations if there is no matching send operation yet then the path will block until the sending process has arrived at the same point the send and receive operations are matched using a hash of the requested tensor name the first message will describe the tensor this description is passed on to the memory allocation functions after which the actual tensor data flows directly into the newly allocated memory buffer.all the modified receive functionality is in tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.ccby inheriting the grpcremoteworker and replacing the recvtensorasync function note it is possible that a tensor with the same name is send received multiple times between the same set of processes this is not a problem as messages are handled as unique units and have unique follow up ids based on thread ids). tensorflow limitations memory is constantly being de)allocated in between iterations this hinders the performance when using infiniband connections as memory buffers must be pinned/mapped before they can be used this can impact the effective bandwidth by a factor as this pinning is a relatively costly operation it would be more efficient if memory buffers for the same tensors would be reused/retained in between iterations this makes the pinning of memory buffers a one-time occurrence it also allows for the removal of the allocation step in the receive functions.minds.aijeroen bdorf
208924249,7703,https://api.github.com/repos/tensorflow/tensorflow/issues/7703,ray-chu,1,0,0,0,0,0,"i try to build tensorflow from the source code as shows but it fails at basel building like following.commands bazel build config=opt config=cuda verbose_failures tensorflow/tools/pip_package:build_pip_packagewarning sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed see for more information you can turn off this warning via ignore_unsupported_sandboxing.info found target...error local/xian_titan/.cache/bazel/bazel_xian_titan/dbdffcfbfaacafec/external/protobuf/build c compilation of rule protobuf//:js_embed failed crosstool_wrapper_driver_is_not_gcc failed error executing command(cd local/xian_titan/.cache/bazel/bazel_xian_titan/dbdffcfbfaacafec/execroot/tensorflow exec env ld_library_path=/usr/local/lib/:/usr/local/cuda-./lib path=/usr/local/cuda-./bin:/usr/lib/qt-./bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/local/xian_titan/bin external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc u_fortify_source d_fortify_source fstack-protector fpie wall wunused-but-set-parameter wno-free-nonheap-object fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections g std=c md mf bazel-out/host/bin/external/protobuf/objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d frandom-seed=bazel-out/host/bin/external/protobuf/objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o iquote external/protobuf iquote bazel-out/host/genfiles/external/protobuf iquote external/bazel_tools iquote bazel-out/host/genfiles/external/bazel_tools isystem external/bazel_tools/tools/cpp/gcc no-canonical-prefixes wno-builtin-macro-redefined d__date=redacted d__timestamp=redacted d__time__=redacted fno-canonical-system-headers c external/protobuf/src/google/protobuf/compiler/js/embed.cc o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status gcc error trying to exec ccplus execvp no such file or directorytarget tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path si found some other people also had similar problems before but i cannot successfully address the problem from previous threads.i tried bazel and releases they have the same issues.my system is red hat enterprise linux workstation release santiago),java version gcc gcc red hat g gcc red hat hope someone can figure out wheres wrong"
208778134,7680,https://api.github.com/repos/tensorflow/tensorflow/issues/7680,gajop,4,0,0,0,0,0,environment infooperating system ubuntu geforce gtx titan xinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):ls usr/local/cuda-./lib/libcud*/usr/local/cuda-./lib/libcudadevrt.a/usr/local/cuda-./lib/libcudart.so/usr/local/cuda-./lib/libcudart.so../usr/local/cuda-./lib/libcudart.so.../usr/local/cuda-./lib/libcudart_static.a/usr/local/cuda-./lib/libcudnn.so/usr/local/cuda-./lib/libcudnn.so./usr/local/cuda-./lib/libcudnn.so.../usr/local/cuda-./lib/libcudnn.so.../usr/local/cuda-./lib/libcudnn_static.a if installed from binary pip package provide installed with pip tensorflow-gpu python c import tensorflow print(tensorflow.__version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)using the provided tf.contrib.learn model in get_started/get_started documentation import tensorflow as tfimport numpy as npfeatures tf.contrib.layers.real_valued_column(x dimension=) estimator tf.contrib.learn.linearregressor(feature_columns=features)x np.array y np.array input_fn tf.contrib.learn.io.numpy_input_fn({x:x y batch_size num_epochs=)estimator.fit(input_fn=input_fn steps=) the core model works fine logs or other output that would be helpful(if logs are large please upload as attachment or provide link). python test-oom.py i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallywarning:tensorflow:using temporary folder as model directory tmp/tmpewuzwarning:tensorflow:rank of input tensor should be the same as output_rank for column will attempt to expand dims it is highly recommended that you resize your input as this behavior may change.warning:tensorflow:from home/gajop/projekti/ubuntu-ranking-dataset-creator/env/local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py scalar_summary from tensorflow.python.ops.logging_ops is deprecated and will be removed after instructions for updating:please switch to tf.summary.scalar note that tf.summary.scalar uses the node name instead of the tag this means that tensorflow will automatically de-duplicate summary names based on the scope they are created in also passing a tensor or list of tags to a scalar summary op is no longer supported.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx titan xmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory
208764585,7679,https://api.github.com/repos/tensorflow/tensorflow/issues/7679,AlvinChen13,8,0,4,0,0,0,tf r claims that it can achieve x performance improving by gpus for inception v are there any guidelines or sample code to help us to reproduce the results
208744006,7677,https://api.github.com/repos/tensorflow/tensorflow/issues/7677,avantol,7,0,0,0,0,0,the first tf.contrib.learn sample at to run.the code is right underbasic usagenotice how much simpler the linear regression program becomes with tf.contrib.learn:output:warning:tensorflow:using temporary folder as model directory c:\users\john\appdata\local\temp\tmpahtntpwarning:tensorflow:rank of input tensor should be the same as output_rank for column will attempt to expand dims it is highly recommended that you resize your input as this behavior may change.warning:tensorflow:from c:\users\john\appdata\local\programs\python\python\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py scalar_summary from tensorflow.python.ops.logging_ops is deprecated and will be removed after instructions for updating:please switch to tf.summary.scalar note that tf.summary.scalar uses the node name instead of the tag this means that tensorflow will automatically de-duplicate summary names based on the scope they are created in also passing a tensor or list of tags to a scalar summary op is no longer supported w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations w c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.warning:tensorflow:rank of input tensor should be the same as output_rank for column will attempt to expand dims it is highly recommended that you resize your input as this behavior may change.warning:tensorflow:from c:\users\john\appdata\local\programs\python\python\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py scalar_summary from tensorflow.python.ops.logging_ops is deprecated and will be removed after instructions for updating:please switch to tf.summary.scalar note that tf.summary.scalar uses the node name instead of the tag this means that tensorflow will automatically de-duplicate summary names based on the scope they are created in also passing a tensor or list of tags to a scalar summary op is no longer supported.warning:tensorflow:skipping summary for global_step must be a float or np.float.------------------(program exited with code what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system windows python v..:cba jan msc v bit amd on wininstalled version of cuda and cudnn if installed from binary pip package provide a link to the pip package you installed:pip install upgrade to fix the bestsplits error the output from python c import tensorflow print(tensorflow.__version__) ...-rc
208725637,7669,https://api.github.com/repos/tensorflow/tensorflow/issues/7669,lienhua34,6,0,0,0,0,0,when i used the code snippet in the section customizing the evaluation metrics with metricspec of the tutorial logging and monitoring basics with tf.contrib.learn the code snippet is pythonvalidation_metrics accuracy tf.contrib.learn.metric_spec.metricspec metric_fn=tf.contrib.metrics.streaming_accuracy prediction_key=tf.contrib.learn.prediction_key.predictionkey classes precision tf.contrib.learn.metric_spec.metricspec metric_fn=tf.contrib.metrics.streaming_precision prediction_key=tf.contrib.learn.prediction_key.predictionkey classes recall tf.contrib.learn.metric_spec.metricspec metric_fn=tf.contrib.metrics.streaming_recall prediction_key=tf.contrib.learn.prediction_key.predictionkey classes)} my tensorflow version is r when i run my program it print the following error: shell python iris.py traceback most recent call last file iris.py line in module tf.app.run file library/python/./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file iris.py line in main accuracy tf.contrib.learn.metric_spec.metricspec(attributeerror module object has no attribute metric_spec i found that the class tf.contrib.learn.metric_spec.metricspec has been renamed to tf.contrib.learn.metricspec the class tf.contrib.learn.prediction_key.predictionkey also has been renamed to tf.contrib.learn.predictionkey
208700740,7662,https://api.github.com/repos/tensorflow/tensorflow/issues/7662,zasdfgbnm,1,0,0,0,0,0,"hi,in my application i need to do operations that dynamically sum some rows of a matrix to get a new matrix there will be an input tensor named index that guides which part of the tensor to be summed an example is if the input matrix is python and the index is python which simply says the output tensor will have two rows because the index have two rows the first row is the sum of rows with row number i that satisfies i in the input and the second row is the sum of rows with row number so the result should be python i dont find any existing operation that does this job so i implement it support only d matrix and gpu by myself since i already have an implementation im not requesting a new feature here but i do want to know that if the tensorflow team is interested in adding this operation as part of tensorflow if the answer is yes i will add cpu support maybe also xla i have no idea on how to add xla support yet and then create a pull request for that"
208660423,7652,https://api.github.com/repos/tensorflow/tensorflow/issues/7652,seerdecker,10,0,0,0,0,0,these messages appear whenever i run my program.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.now i dont care that tensorflow wasnt compiled to use sse instructions or avx or whatever i dont want to see these messages i also dont want to recompile from source to remove the warnings install something else play games with stderr or whatever i just want a clean way to shut up the warnings.heres my initialization code.import tensorflow as tffrom tensorflow.python.framework import opstf.logging.set_verbosity(tf.logging.error)sess tf.session(config=tf.configproto(device_count gpu see i asked for no warnings yet i still get them thats a bug even if its the intended behavior please fix this.version is tensorflow on linux.thank you
208621304,7643,https://api.github.com/repos/tensorflow/tensorflow/issues/7643,daniellevy,1,0,0,0,0,0,"hi,i am using tensorflow v and i have tried on mac os x and centos i am running into an error when running the following code: w tf.get_variable(w x tf.placeholder(tf.float shape=(none h tf.matmul(x w)grads tf.map_fn(lambda x tf.gradients(x w h) i basically want to have the following but without a fixed batch size: grads tf.gradients(h t w for t in range(batch_size) my error is: invalid argument tensorarray map/tensorarray_@map/while/gradients could not write to tensorarray index because it has already been read. ... tensorflow.python.framework.errors.invalidargumenterror tensorarray map/tensorarray_@map/while/gradients could not write to tensorarray index because it has already been read node map/while/gradients/map/while/tensorarrayread_grad/tensorarraywrite tensorarraywrite t=dt_float class= loc:@map/tensorarray device=/job:localhost/replica:/task:/cpu: (map/while/gradients/map/while/tensorarrayread_grad/tensorarraygrad/tensorarraygrad map/while/identity map/while/gradients/fill map/while/gradients/map/while/tensorarrayread_grad/tensorarraygrad/gradient_flow) i have tried the following workaround using scan instead of map_fn with a zero initializer but to no avail: initializer np.zeros astype(float)grads tf.scan lambda a x tf.gradients(x w h initializer) is this a know issue"
208572055,7636,https://api.github.com/repos/tensorflow/tensorflow/issues/7636,anirudh2290,1,0,0,0,0,0,it would be nice to have a snap package for tensorflow advantage is that it is easy to install and supports multiple distributions
208551422,7634,https://api.github.com/repos/tensorflow/tensorflow/issues/7634,matteson,1,0,0,0,0,0,using the attached meta file: model_fn my-model.metagraph tf.graph()sess tf.interactivesession(graph=graph)t_input tf.placeholder(np.float name=images define the input tensort_preprocessed tf.expand_dims(t_input new_saver tf.train.import_meta_graph(model_fn input_map={images t_input import_scope=import)new_saver.restore(sess results in: keyerror the name gradients/discriminator/minibatch/map/while/tensorarraywrite/tensorarraywritev_grad/tensorarrayreadv/refenter refers to a tensor which does not exist the operation gradients/discriminator/minibatch/map/while/tensorarraywrite/tensorarraywritev_grad/tensorarrayreadv/refenter does not exist in the graph. im trying to remap the input so i can do image space optimization with a library that assumes the network input is width height channels loading doesnt error if i load without the input_map and import_scope keyword arguments however this causes problems for the library im interacting with setting import_scope alone does cause an error what related github issues or stackoverflow threads have you found by searching the web for your problem?asked on stack overflow in case this isnt a bug environment infooperating system:mac a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) ... my-model.meta.zip
208533993,7631,https://api.github.com/repos/tensorflow/tensorflow/issues/7631,cancan101,4,0,0,0,0,0,i have a number of scalars that i plot in tensorboard that are non negative values ie accuracy etc it would be cool if there was a quick way to have the y axis start at rather than allocating unused space in the negative y direction.! image
208345553,7610,https://api.github.com/repos/tensorflow/tensorflow/issues/7610,undertherain,4,0,0,0,0,0,d convolutions on cpu seem unnaturally slow i cant use gpu due to memory limit so im looking at cpu execution d convolutions in tensorflow seem to be well-optimized all cpu cores are used performance is just few times below gpu.with d convolutions the difference is orders of magnitude also ive compared with theano theano d convolutions run on single core but still are times faster.strangely tf uses all cores on cpu with dconv so there must be some bug or extreme inefficiency in implementation.with the same small test model just couple of d conv layers i get second epoch time on gpu both tf and theano theano just a bit faster seconds on cpu theano single threaded and seconds with seemingly multi-threded tf environment infoive tried different tf versions from to installed from pip latest one from os is centos linux release core kernel el.x_im using keras back-end so i cant be sure if the problem is not in the way keras translates convolutiond call into tf primitives but it does not seem likely cheersalex
208180749,7585,https://api.github.com/repos/tensorflow/tensorflow/issues/7585,kaufManu,6,0,0,0,0,0,i just upgraded to r and ran into an issue which i find hard to dissect further the issue did not occur before when i was using version r the error message is the following: invalidargumenterror see above for traceback must have updates.shape indices.shape :ixdim params_shape ixdim got updates.shape indices.shape params_shape node gradients/gathernd__grad/scatternd scatternd t=dt_float tindices=dt_int device=/job:localhost/replica:/task:/cpu: (gathernd_/indices gradients/addn gradients/gathernd__grad/shape) and the stack trace tells me it stems from the last of the following lines: def positional(visible param visible a tensor of size batch_size input_dim sequence_length representing the sequence to be optimized feet np.array feet_idx np.array( range(i i for i in feet batch_size visible.get_shape() .value dim visible.get_shape() .value seq_length visible.get_shape() .value idx_x idx_y idx_z feet_idx feet_idx feet_idx idx_x_t j i for i in idx_x for j in range(batch_size idx_y_t j i for i in idx_y for j in range(batch_size v_feet_x tf.gather_nd(visible idx_x_t v_feet_y tf.gather_nd(visible idx_y_t stack trace points here this function is called to calculate a cost function which is used during an optimization procedure see below for a minimum working example the error as far as i can tell from the stack trace does not happen when the graph is built but when the computation is executed which is why im not sure how i can further narrow down the problem environment infooperating system ubuntu xinstalled version of cuda and cudnn and cudnn rw-r--r root root jan usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root jan usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root jan usr/local/cuda/lib/libcudart.so libcudart.so...-rw-r--r root root jan usr/local/cuda/lib/libcudart.so...-rw-r--r root root jan usr/local/cuda/lib/libcudart_static.alrwxrwxrwx root root nov usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx root root nov usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root nov usr/local/cuda/lib/libcudnn.so...-rw-r--r root root nov usr/local/cuda/lib/libcudnn_static.a installed from source from revision afbffcbaaeced using bazel build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed feb build timestamp build timestamp as int minimum working example import tensorflow as tfimport numpy as npdef get_costs(velos seq_length velo_diff tf.subtract(tf.slice(velos seq_length tf.slice(velos seq_length velo_diff_sq tf.multiply(velo_diff velo_diff return tf.reduce_mean(velo_diff_sq)def positional(visible param visible a tensor of size batch_size input_dim sequence_length representing the sequence to be optimized feet np.array feet_idx np.array( range(i i for i in feet batch_size visible.get_shape() .value dim visible.get_shape() .value seq_length visible.get_shape() .value idx_x idx_y idx_z feet_idx feet_idx feet_idx idx_x_t j i for i in idx_x for j in range(batch_size idx_y_t j i for i in idx_y for j in range(batch_size v_feet_x tf.gather_nd(visible idx_x_t v_feet_y tf.gather_nd(visible idx_y_t return get_costs(v_feet_y seq_length)visible tf.variable(np.reshape(np.arange dtype=tf.float)cost_op positional(visible)train_op tf.train.adamoptimizer(.).minimize(cost_op)with tf.session as sess sess.run(tf.global_variables_initializer sess.run( train_op
208157089,7581,https://api.github.com/repos/tensorflow/tensorflow/issues/7581,yaroslavvb,2,0,0,0,0,0,making this issue to track status of allocator c api based on discussion at tfdev conference cc joshb vrv skye keveman zheng-xq keveman yuanbyu currently allocator api is not public this api in tensorflow/core/framework/allocator.h and more specifically bytes_in_use is currently the only practical way to implement user ops that decide what to do based on available memory like foldr map_fn with swap_memory=true option report available memory to user three options are make this api public keep this api non-public but make it possible to create a user-op that uses this api keep this api non-public and do not provide a way to use it from user-op.currently its somewhere between and api is not public and you can build user-op like memory_probe_op with gcc but not with bazel from an op-creator standpoint is preferable to and is preferable to similar issue was where people tried to make a custom reader user op that uses methods from reader_base.h the final solution was the opposite it was possible to make work with bazel by commenting out disallowed_deps line but not with gcc
208116014,7575,https://api.github.com/repos/tensorflow/tensorflow/issues/7575,damienmg,1,0,0,1,0,0,this removes the needs for clean expunge in configure butrequest the incoming bazel this should fix
208105890,7573,https://api.github.com/repos/tensorflow/tensorflow/issues/7573,javdrher,2,0,0,0,1,0,we currently experience an issue with an implementation during the execution of session.run the process suddenly freezes it does not crash but is irresponsive to ctrl+c it isnt consuming any cpu anymore and is not progressing either this occurs on cpu we havent tested gpu the issue seems to be highly related to we ran the script on a machine running an up-to-date ubuntu with gb of ram and x xeon cpu e v processors the issue occured with tensorflow installed through anaconda then we reproduced the issue using a build of the master branch without any cuda support using the system libraries rather than those shipped with anaconda.the build of the master branch shows:print(tensorflow.__version__)..-rc git rev-parse headafaaffbcbfefcf bazel versionbuild label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed feb build timestamp build timestamp as int attached is the output of running thread apply all bt in gdb all threads appear to be waiting we are running a model locally implemented using gpflow not one of the default models of that library though unfortunately this occurs on a confidential data set if required we can look into providing a mwe but i can not guarantee we can reproduce this easily under different circumstances gdb.txt
208048691,7560,https://api.github.com/repos/tensorflow/tensorflow/issues/7560,ammarasmro,4,0,0,0,0,0,this is relating to the how to retrain inceptions final layer for new categories tutorialim trying to unpack the flower_photos.tgz after curling it using curl o tar xzf flower_photos.tgzthis is from the image retraining tutorial for tensorflowresults from curling total received xferd average speed time time time current dload upload total spent left speed then when i try to unpack tar xzf flower_photos.tgztar unrecognized archive formattar error exit delayed from previous errors
208012782,7552,https://api.github.com/repos/tensorflow/tensorflow/issues/7552,GarrickLin,6,0,0,0,0,0,i created an environment with following command on windows: conda create name tensorflow python anaconda and try to install tensorflow pip install ignore-installed upgrade flow_gpu-..-cp-cpm-win_x_.whl but it reported an error as following: tensorflow_gpu-..-cp-cpm-win_x_.whl is not a supported wheel on this platform. my machine is bit win can anyone tell me whats the problem
208007559,7550,https://api.github.com/repos/tensorflow/tensorflow/issues/7550,cdjhz,8,0,0,0,0,0,training images using keras with tf backend.the error shows that tf version==..head has no attribute pack. pythontraceback most recent call last file binary_convnets.py line in module model.add(flatten file usr/local/lib/python./dist-packages/keras/models.py line in add output_tensor layer(self.outputs file usr/local/lib/python./dist-packages/keras/engine/topology.py line in call self.add_inbound_node(inbound_layers node_indices tensor_indices file usr/local/lib/python./dist-packages/keras/engine/topology.py line in add_inbound_node node.create_node(self inbound_layers node_indices tensor_indices file usr/local/lib/python./dist-packages/keras/engine/topology.py line in create_node output_tensors to_list(outbound_layer.call(input_tensors mask=input_masks file usr/local/lib/python./dist-packages/keras/layers/core.py line in call return k.batch_flatten(x file usr/local/lib/python./dist-packages/keras/backend/tensorflow_backend.py line in batch_flatten x tf.reshape(x tf.pack prod(shape(x) : ) ))attributeerror module object has no attribute pack
207938960,7541,https://api.github.com/repos/tensorflow/tensorflow/issues/7541,pwaller,2,0,0,0,0,0,im trying to write mixed c tensorflow code with python tensorflow code byembedding the cpython interpreter in my application.im mainly doing this because defining the model is only really possible inpython at the moment due to the lack of gradients and i want to definenew models from the c side at speed without needing to invoke orcommunicate to an external python process to get a new model.to reproduce the problem is quite straightforward simply import tensorflow in python after the libtensorflow library has already been dynamically linked.here is a quick reproducer in pure python which will not run: pythonimport ctypestf_dll ctypes.cdll(/usr/local/lib/libtensorflow.so)import tensorflow libtensorflow can be obtained like so: tf_type=cpu set to gpu for gpu supporttf_os=linuxcurl l sudo tar c usr/local xz here are two fatal messages i have encountered the first from the python reproducer above the second from a c program): f tensorflow/stream_executor/cuda/cuda_platform.cc check failed perftools::gputools::port::status::ok multiplatformmanager::registerplatform(std::move(platform ok vs internal platform is already registered with name cuda) f tensorflow/core/lib/monitoring/collection_registry.cc cannot register metrics with the same name tensorflow/cc/saved_model/load_attempt_count i assume the problem is that the pywrap_tensorflow.so has tensorflowstatically linked into them so they dont use libtensorflow then you havetwo shared libraries conflicting with one another.is there a way to avoid this conflict
207833875,7531,https://api.github.com/repos/tensorflow/tensorflow/issues/7531,bfis,5,0,0,0,0,0,id like to request support for all hyperbolic ops and their inverse:sinh cosh asinh acosh atanhespecially asinh is interesting as it can be used as an activation function.this would also allow to address problems with their calculation centrally e.g.: asinh(x log(x sqrt(x will yield /nan and break gradients for small x this is caused by the fact that the log argument x sqrt(x will get rounded to due to machine precision
207783789,7525,https://api.github.com/repos/tensorflow/tensorflow/issues/7525,Prakashvanapalli,0,0,0,0,1,0,i m trying to read images using tensorflow tf.train function but have been getting the following issue please help me import tensorflow as tf import glob filelist glob.glob(../train/*.png def read_my_file_format(filename_queue reader tf.wholefilereader key record_string reader.read(filename_queue example tf.image.decode_png(record_string label tf.reshape(key name=none return example label def input_pipeline(filenames batch_size num_epochs none filename_queue tf.train.string_input_producer filenames num_epochs num_epochs shuffle true example label read_my_file_format(filename_queue min_after_dequeue capacity min_after_dequeue+*batch_size example.set_shape label.set_shape example_batch label_batch tf.train.shuffle_batch example label batch_size batch_size capacity capacity min_after_dequeue min_after_dequeue return example_batch label_batch example_batch label_batch input_pipeline(filelist with tf.session as sess required to get the filename matching to run tf.initialize_all_variables().run coordinate the loading of image files coord tf.train.coordinator threads tf.train.start_queue_runners(coord=coord get an image tensor and print its value x sess.run(example_batch print x.shape finish off the filename queue coordinator coord.request_stop coord.join(threads outofrangeerror randomshufflequeue shuffle_batch_/random_shuffle_queue is closed and has insufficient elements requested current size node shuffle_batch queuedequeuemany _class= loc:@shuffle_batch_/random_shuffle_queue component_types= dt_uint dt_string timeout_ms device=/job:localhost/replica:/task:/cpu: (shuffle_batch_/random_shuffle_queue shuffle_batch_/n outofrangeerror see above for traceback randomshufflequeue shuffle_batch_/random_shuffle_queue is closed and has insufficient elements requested current size node shuffle_batch queuedequeuemany _class= loc:@shuffle_batch_/random_shuffle_queue component_types= dt_uint dt_string timeout_ms device=/job:localhost/replica:/task:/cpu: (shuffle_batch_/random_shuffle_queue shuffle_batch_/n
207762925,7522,https://api.github.com/repos/tensorflow/tensorflow/issues/7522,brisker,9,0,0,0,0,0,jcc@jcc pip show tensorflowname tensorflowversion rcsummary tensorflow helps the tensors flowhome-page google inc.author-email opensource@google.comlicense apache location home/jcc/anaconda/lib/python./site-packagesrequires werkzeug six wheel mock numpy protobufi think the lines above has shown that i have installed the tensorflow using anaconda(i installed tensorflow by building from source i have not created a conda virtual environment.any help?why this error i make sure that in bashrc file i have this line:export ld_library_path=/home/jcc/cudnn/lib:$ld_library_path and in the configure process of tensorflow i typed in home/jcc/cudnn for cudnn_pathmy server is ubuntu titan x gpu cuda cudnnthe last command line of my build history is like:pip install tmp/tensorflow_pkg/tensorflow-..rc-cp-cpmu-linux_x_.whlprocessing tmp/tensorflow_pkg/tensorflow-..rc-cp-cpmu-linux_x_.whlrequirement already satisfied werkzeug in anaconda/lib/python./site-packages from tensorflow==..rc)requirement already satisfied six in anaconda/lib/python./site-packages from tensorflow==..rc)requirement already satisfied wheel in anaconda/lib/python./site-packages from tensorflow==..rc)requirement already satisfied mock in anaconda/lib/python./site-packages from tensorflow==..rc)requirement already satisfied numpy in anaconda/lib/python./site-packages from tensorflow==..rc)requirement already satisfied protobuf in anaconda/lib/python./site-packages from tensorflow==..rc)requirement already satisfied funcsigs python_version in anaconda/lib/python./site-packages from mock>=..->tensorflow==..rc)requirement already satisfied pbr in anaconda/lib/python./site-packages from mock>=..->tensorflow==..rc)requirement already satisfied setuptools in anaconda/lib/python./site-packages/setuptools-..-py..egg from protobuf>=..->tensorflow==..rc)installing collected packages tensorflowsuccessfully installed tensorflow-..rcbut when import tensorflow the error reads like traceback most recent call last file stdin line in module file home/jcc/anaconda/lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import file home/jcc/anaconda/lib/python./site-packages/tensorflow/python/__init__.py line in module raise importerror(msg)importerror traceback most recent call last file home/jcc/anaconda/lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python import pywrap_tensorflow file home/jcc/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file home/jcc/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow fp pathname description)importerror libcudnn.so cannot open shared object file no such file or directoryfailed to load the native tensorflow runtime
207753796,7521,https://api.github.com/repos/tensorflow/tensorflow/issues/7521,fabienbaradel,5,0,0,0,0,0,i am having issue with cpu usage i am running my code on a node with gpus and cpus.i am using tfrecords for reading my data and it works well however when i run my model tensorflow uses all the cpus available on my node even when i specify that i want to use only one thread_):
207677942,7511,https://api.github.com/repos/tensorflow/tensorflow/issues/7511,andrei-pokrovsky,5,0,0,0,0,2,it appears that tf.image.non_max_suppression currently takes about ms for about boxes runs on a single cpu thread and doesnt have a gpu implementation environment infooperating system:ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) ...-rc
207534003,7500,https://api.github.com/repos/tensorflow/tensorflow/issues/7500,Franck-Dernoncourt,2,0,0,0,0,0,i installed tensorflow version rc on windows sp x ultimate python anaconda custom bit using pip install upgrade i try running the test script from in eclipse or in the console import tensorflow as tf print(tensorflow version format(tf.__version hello tf.constant(hello tensorflow sess tf.session print(sess.run(hello))i obtain some error message tensorflow version rc hello tensorflow e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflob w\core\framework\op_kernel.cc opkernel op bestsplits device_type cpu for unknown op bestsplits e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op countextremelyrandomstats device_type cpu for unknown op countextremelyrandomstats e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op finishednodes device_type cpu for unknown op finishednodes e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op growtree device_type cpu for unknown op growtree e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op reinterpretstringtofloat device_type cpu for unknown op reinterpretstringtofloat e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op sampleinputs device_type cpu for unknown op sampleinputs e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op scatteraddndim device_type cpu for unknown op scatteraddndim e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topninsert device_type cpu for unknown op topninsert e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op topnremove device_type cpu for unknown op topnremove e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op treepredictions device_type cpu for unknown op treepredictions e c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc opkernel op updatefertileslots device_type cpu for unknown op updatefertileslotswhy?i didnt have such issues with tensorflow installed with pip install tensorflow tensorflow version bhello tensorflow!stack exchange thread tensorflow version rc on windows opkernel op bestsplits device_type cpu for unknown op bestsplits with test code
207499153,7497,https://api.github.com/repos/tensorflow/tensorflow/issues/7497,ambujpd,3,0,0,0,0,0,im compiling tensorflow from source after the configure step im facing the following error error com.google.devtools.build.lib.packages.buildfilecontainserrorsexception error loading package encountered error while reading extension file closure/defs.bzl no such package io_bazel_rules_closure//closure error downloading to home/xyzuser/.cache/bazel/_bazel_xyzuser/cbecbecabafdfbad/external/io_bazel_rules_closure/cadabdfadfbaefcfd.tar.gz all mirrors are down java.lang.nullpointerexception .this stems from the bazel_clean_and_fetch in configure file bazel fetch tensorflow tensorflow/contrib/nccl tensorflow/examples/android/... same error replicates if i do bazel fetch my system is behind a proxy and ive set http_proxy and https_proxy in the env. git rev-parse head: eabafeadfccdaadcf bazel version: build label non-git)build target bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jar operating system: ubuntu ltsany insights would be helpful as im new to bazel and tensorflow.@jart kchodorow
207382153,7482,https://api.github.com/repos/tensorflow/tensorflow/issues/7482,tompfeil,8,0,0,0,0,0,it would be nice to be able to append or remove samples to or from an existing set of samples saved in a tfrecords file otherwise the file has to be duplicated which is unpractical for datasets tb.formulated in a more generalized way this can also be seen as splitting and concatenating tfrecord files
207344569,7476,https://api.github.com/repos/tensorflow/tensorflow/issues/7476,cancan101,10,0,0,0,0,0,see paper implementation
207319629,7472,https://api.github.com/repos/tensorflow/tensorflow/issues/7472,caisq,1,0,0,0,0,0,this leads to python version increase from to do not merge yet pending jenkins configuration update which is blocked by python build failure we are experiencing in nightly
207240431,7468,https://api.github.com/repos/tensorflow/tensorflow/issues/7468,LiningZheng,0,0,0,0,0,1,added hahahaha
207231613,7467,https://api.github.com/repos/tensorflow/tensorflow/issues/7467,yuvval,4,0,0,0,0,0,would it be possible to add early stopping mechanism to slim.evaluation_loop
207123853,7457,https://api.github.com/repos/tensorflow/tensorflow/issues/7457,gesman,1,0,0,0,0,0,the output from python c import tensorflow print(tensorflow.__version python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally.. heres the error :epoch s loss acc val_loss val_acc exception ignored in bound method basesession.__del of tensorflow.python.client.session.session object at xfe>>traceback most recent call last file home/p/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in del__unboundlocalerror local variable status referenced before assignment
207031434,7449,https://api.github.com/repos/tensorflow/tensorflow/issues/7449,ahundt,7,0,0,0,0,0,update the current version of my tensorflow.sh install script has been working well for me and the update from tf to tf required only a one character change! original post: here are the key lines in my install script with a quote from the tensorflow docs to be compatible with as wide a range of machines as possible tensorflow defaults to only using sse simd instructions on x machines most modern pcs and macs support more advanced instructions so if youre building a binary that youll only be running on your own machine you can enable these by using copt=-march=native in your bazel build command.bazel build copt=-march=native c opt config=cuda tensorflow/tools/pip_package:build_pip_package even with copt=-march=native i get the following warnings about the cpu instruction set contradicting the above statement: w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations. here is the exact script i used to build tensorflow what related github issues or stackoverflow threads have you found by searching the web for your problem system:ubuntu the output from python c import tensorflow print(tensorflow.__version__) . python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally.. if installed from source provide the commit hash git rev-parse head )bbeabdbfbecffdbd the output of bazel version bazel versionbuild label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed feb build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) python c import tensorflow as tf print(tf.__version sess tf.interactivesession sess.close();i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally..w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties:name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id what other attempted solutions have you tried?this person tried some other things
206985644,7439,https://api.github.com/repos/tensorflow/tensorflow/issues/7439,kvrd18,11,0,0,0,0,0,where is the batch normalization implementation for multi-gpu scenarios how does one keep track of mean variance offset and scale in the context of the multi-gpu example as given in the cifar tutorial is the question on stackoverflow left unanswered for so long?for all the beauty that it brings with tensorboard etc its kinda appalling to see tensorflow so far behind torch in terms of its modeling capability id be really glad if someone takes up responsibility and comes up with a decent batch normalization implementation for all cases even if it is already there could anyone care enough to make a good documentation out of it?there are so many issues pertaining to batch normalization with tensorflow its important that you guys straighten this out as batch normalization enables super-fast convergence for very deep networks and it is really important for modern day deep learning research.ps please spare my outburst ive been a torch user for more than a year and i had very high hopes on tensorflow
206880034,7422,https://api.github.com/repos/tensorflow/tensorflow/issues/7422,cancan101,3,0,0,0,0,0,numpy has a function average which peforms a weighted mean i suggest adding this function to tensorflow or adding an optional weights argument to reduce_mean
206842494,7419,https://api.github.com/repos/tensorflow/tensorflow/issues/7419,davideb91,0,0,0,0,1,2,i want build tensorflow android camera demo using a custom classifier following this tutorial.when i build the app using bazel build tensorflow/examples/android:tensorflow_demo i get: conflict asset:workspace is provided with ambiguous priority from external/mobile_multibox/workspace external/inceptionh/workspaceconflict asset:workspace is provided with ambiguous priority from external/stylize/workspace external/mobile_multibox/workspace thanks in advance
206714679,7407,https://api.github.com/repos/tensorflow/tensorflow/issues/7407,bxshi,3,0,0,0,0,0,i created a tensor and use feature_a tf.train.limit_epochs(testing_data_a num_epochs name=feature_a_limit)tf.train.batch( feature_a batch_size enqueue_many=true allow_smaller_final_batch=true name=feature_a_batch) to create a batch queue that produces the tensor only once and i then use this in the input_fn function of model.evaluate with steps because the batch size is and the steps is i expect this could use up the batch queue without an outofrange error or the estimator will catch this error and use it as an indicator to stop the evaluation however the estimator does not catch it and the program ends.is this an expected behavior if i remember correctly estimator used to catch such error another question is why the fifo queue still throw the outofrange even if i just request the exact number of elements in the queue what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system: linux domain generic ubuntu smp tue dec utc x x x gnu/linux the commit hash git rev-parse head ) afffeefdfeeeadfeebb the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu dec build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonimport tensorflow as tffrom tensorflow.contrib.layers.python.layers.optimizers import optimize_lossfrom tensorflow.contrib.learn.python.learn.estimators import model_fnfrom tensorflow.contrib.learn.python.learn.estimators.estimator import estimatorfrom tensorflow.python import debug as tf_debugfrom tensorflow.python.framework import opsdef main hooks tf_debug.localclidebughook def func(features targets mode params idx tf.concat( features a features b axis embedding tf.get_variable(embed dtype=tf.float pred tf.reduce_sum(tf.nn.embedding_lookup(embedding idx train_op optimize_loss(loss=pred global_step=tf.train.get_global_step learning_rate optimizer=adam variables=tf.trainable_variables name=training_loss_optimizer eval_metric_dict dict eval_metric_dict metric pred return model_fn.modelfnops(mode=mode predictions=pred loss=pred train_op=train_op eval_metric_ops=eval_metric_dict model estimator(func params model.fit input_fn=lambda a ops.convert_to_tensor b ops.convert_to_tensor none max_steps testing_data_a testing_data_b def test_input_fn print(test_input_fn entered feature_a tf.train.limit_epochs(testing_data_a num_epochs name=feature_a_limit feature_b tf.train.limit_epochs(testing_data_b num_epochs name=feature_b_limit feature_a_producer feature_b_producer tf.train.batch( feature_a feature_b batch_size enqueue_many=true allow_smaller_final_batch=true name=feature_a_batch print(test_input_fn exit return a feature_a_producer b feature_b_producer none for i in range print(model.evaluate(input_fn=test_input_fn steps print(one iteration done)if name main tf.app.run what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link). ssh://bshi@dsg.crc.nd.edu:/data/bshi/pyenv/bin/python u data/bshi/projc/estimator_test.py debugwarning:tensorflow:using temporary folder as model directory tmp/tmpxtkow_oqtest_input_fn enteredtest_input_fn exitw tensorflow/core/framework/op_kernel.cc out of range fifoqueue feature_a_batch/fifo_queue is closed and has insufficient elements requested current size node feature_a_batch queuedequeueuptov component_types= dt_int dt_int timeout_ms device=/job:localhost/replica:/task:/cpu: (feature_a_batch/fifo_queue feature_a_batch/n) traceback most recent call last file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn status run_metadata file usr/lib/python./contextlib.py line in exit next(self.gen file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.outofrangeerror fifoqueue feature_a_batch/fifo_queue is closed and has insufficient elements requested current size node feature_a_batch queuedequeueuptov component_types= dt_int dt_int timeout_ms device=/job:localhost/replica:/task:/cpu: (feature_a_batch/fifo_queue feature_a_batch/n) during handling of the above exception another exception occurred:traceback most recent call last file data/bshi/projc/estimator_test.py line in module tf.app.run file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file data/bshi/projc/estimator_test.py line in main print(model.evaluate(input_fn=test_input_fn steps file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/util/deprecation.py line in new_func return func(*args kwargs file data/bshi/pyenv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in evaluate log_progress=log_progress file data/bshi/pyenv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in evaluate_model hooks=hooks file data/bshi/pyenv/lib/python./site-packages/tensorflow/contrib/training/python/training/evaluation.py line in evaluate_once session.run(eval_ops feed_dict file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in exit self._close_internal(exception_type file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/training/monitored_session.py line in close_internal h.end(self._coordinated_creator.tf_sess file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/training/basic_session_run_hooks.py line in end feed_dict=self._final_ops_feed_dict file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.outofrangeerror fifoqueue feature_a_batch/fifo_queue is closed and has insufficient elements requested current size node feature_a_batch queuedequeueuptov component_types= dt_int dt_int timeout_ms device=/job:localhost/replica:/task:/cpu: (feature_a_batch/fifo_queue feature_a_batch/n) caused by op feature_a_batch defined at file data/bshi/projc/estimator_test.py line in module tf.app.run file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(_sys.argv flags_passthrough file data/bshi/projc/estimator_test.py line in main print(model.evaluate(input_fn=test_input_fn steps file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/util/deprecation.py line in new_func return func(*args kwargs file data/bshi/pyenv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in evaluate log_progress=log_progress file data/bshi/pyenv/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in evaluate_model features labels input_fn file data/bshi/projc/estimator_test.py line in test_input_fn allow_smaller_final_batch=true name=feature_a_batch file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/training/input.py line in batch name=name file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/training/input.py line in batch dequeued queue.dequeue_up_to(batch_size name=name file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/ops/data_flow_ops.py line in dequeue_up_to self._queue_ref n=n component_types=self._dtypes name=name file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/ops/gen_data_flow_ops.py line in queue_dequeue_up_to_v timeout_ms=timeout_ms name=name file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file data/bshi/pyenv/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()outofrangeerror see above for traceback fifoqueue feature_a_batch/fifo_queue is closed and has insufficient elements requested current size node feature_a_batch queuedequeueuptov component_types= dt_int dt_int timeout_ms device=/job:localhost/replica:/task:/cpu: (feature_a_batch/fifo_queue feature_a_batch/n) process finished with exit code
206560022,7389,https://api.github.com/repos/tensorflow/tensorflow/issues/7389,PhilJd,1,0,0,0,0,0,as requested by girving in ill file a separate issue.the sorted segment reduction ops feature prod min mean sqrt_n while the unsorted ops dont include those so i suggest to add the following functions: tf.unsorted_segment_prod(data segment_ids num_segments name=none)tf.unsorted_segment_min(data segment_ids num_segments name=none)tf.unsorted_segment_mean(data segment_ids num_segments name=none)tf.unsorted_segment_sqrt_n(data segment_ids num_segments name=none if you mark this as contributions welcome id start working on this
206491852,7385,https://api.github.com/repos/tensorflow/tensorflow/issues/7385,jyh2986,4,0,0,0,0,0,i think this will be simple but very useful feature for tensorboard.what i want is the ignoring specific subdirectories.tensorboard load all of subdirectries and display them but it become slower as more subfolders are added so i just want to ignore some directories without moving them.theres several options to do this if there is special character(like in folder name just ignore it like git ignore file manage folders to ignore if nolog file in the folder to ignore.(like nomedia file)i think option is best because folder structure can be changed
206487328,7382,https://api.github.com/repos/tensorflow/tensorflow/issues/7382,geromueller,1,0,0,0,0,0,to embed tensorboard the data needs to be loaded from a relative directory not data
206229362,7359,https://api.github.com/repos/tensorflow/tensorflow/issues/7359,cancan101,2,0,0,0,0,0,this is a repost of per comment from mrry cc ziky). resize_image_with_crop_or_pad should be modified to take either a single image or a batch of images right now the signature of the function specifies image d tensor of shape height width channels for a reference of what the api should look like see resize_images(images images d tensor of shape batch height width channels or d tensor of shape height width channels . some commentary docs for resize_images say resized images will be distorted if their original aspect ratio is not the same as size to avoid distortions see resize_image_with_crop_or_pad however the method referenced cannot be currently used in some cases for which resize_images works due to the above mentioned limitations the entire operation can be implemented using slice operations on the tensor batch this is what resize_image_with_crop_or_pad does but it has the following logic which would be nice not to copy and paste there is some subtlety with round down vs up): python width_diff target_width width offset_crop_width max_(-width_diff offset_pad_width max_(width_diff height_diff target_height height offset_crop_height max_(-height_diff offset_pad_height max_(height_diff the fact this one image op method does not handle batches leads to confusion and various so posts people even resort to solving the problem using all sorts of crazy solutions e.g tf.while_loop this method is useful in combination with concatenation operations for example see the lasagne concatlayer
206064940,7347,https://api.github.com/repos/tensorflow/tensorflow/issues/7347,avloss,1,0,0,0,0,0,fixing broken example of tf.strided_slicenegative strides work differently should they?!)tf.strided_slice(input
205651482,7296,https://api.github.com/repos/tensorflow/tensorflow/issues/7296,mbz,1,0,0,0,0,0,based on documentation tf.gradients return the sum(dy/dx for our problem sum doesnt make sense is it possible to get the list of gradients instead
205546441,7286,https://api.github.com/repos/tensorflow/tensorflow/issues/7286,Julian,1,0,0,0,0,0,hi.it appears that pypi only contains wheels for tensorflow for uploading those wheels are great.but sdists are good to have too specifically im not sure whether tensorflow works on pypy im about to try it out edit now ive seen which i might try to help on but not having sdists uploaded makes that harder to try without hunting down this repository
205475426,7278,https://api.github.com/repos/tensorflow/tensorflow/issues/7278,aizvorski,5,0,0,0,0,0,i would like a tf.nn.separable_convd identical to tf.nn.separable_convd except with separability between dimentions and for use with d cnns.rationale separable convolutions perform very well in d see xception architecture in d the number of parameters grows even faster for non-separable convolutions so the reduction in parameters from using separable convolutions would be relatively even bigger this is one of the reasons d networks tend to have a simpler architecture than d necessity there doesnt seem to be any way to implement this other than in the tf core suggestions
205353365,7258,https://api.github.com/repos/tensorflow/tensorflow/issues/7258,luketg8,3,0,0,0,0,0,i have built libtensorflow.so with bazel and used it successfully with the c api in ubuntu however i was wondering if it is possible at all to use bazel to build a dll extension to use for windows as i want to integrate tensorflow into an existing project thanks in advance
205333303,7257,https://api.github.com/repos/tensorflow/tensorflow/issues/7257,dmitry-xnor,13,0,0,0,0,0,i would go out on a limb and guess that the vast majority of tensorflow users on linux at least use fairly modern cpus it would therefore be beneficial for them to have the prebuilt tf binaries support avx/fma these two isa extensions and especially fma tend to speed up gemm-like math pretty significantly.itd be great if tf team provided prebuilt linux release whl that supports avx/fma perhaps as an alternative non-default wheel these should be compatible with haswell and above haswell came out in lots of people have it by now.to be clear this is not a hugely pressing issue whl can be easily rebuilt from source itd just make things faster and easier for people with modern cpus what related github issues or stackoverflow threads have you found by searching the web for your problem?n/a environment infooperating system:linux ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud noneif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)code: import tensorflow as tfsess tf.interactivesession() output: w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations what other attempted solutions have you tried?compiled from source logs or other output that would be helpful(if logs are large please upload as attachment or provide link
205284887,7253,https://api.github.com/repos/tensorflow/tensorflow/issues/7253,akdeoras,2,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system mac cpu)if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) input tf.placeholder(dtype=tf.float shape=(none input_flattened tf.reshape(input shape= input.get_shape() .value with tf.session as sess init tf.global_variables_initializer sess.run(init input_val input_flattened_val sess.run( input input_flattened feed_dict={input np.random.random().reshape print input_val print input_flattened_val typeerrortraceback most recent call last)
204981134,7225,https://api.github.com/repos/tensorflow/tensorflow/issues/7225,yaroslavvb,2,0,0,0,0,0,tf.clip_by_value is missing an important property of np.clip which lets upper/lower bounds to be tensors whereas tf.clip_by_value only takes scalarsa work-around is tf.minimum(upper tf.maximum(lower x but that uses x memory for gradients another potential work-around is to use tf.map_fn tf.clip_by_value but it is orders of magnitude slower some profiling
204598482,7187,https://api.github.com/repos/tensorflow/tensorflow/issues/7187,Randl,2,0,0,0,0,0,based on update of benchmarking state-of-the-art deep learning software tools by shyhuai freemanx xiaowec if i got it right shows some performance issues for example see table alexnet-r is significantly times slower in tf than in other frameworks an its even slower at gtx than at gtx also resnet is times faster in mxnet those are most significant differences in addition lstm is around times faster in cntk and resnet is twice faster in mxnet.version used was tensorflow commit dd with cuda and cudnn cc yaroslavvb annarev
204486305,7178,https://api.github.com/repos/tensorflow/tensorflow/issues/7178,aselle,0,0,0,1,0,0,"ones,zeros}_initializer tf.sparse_split"
204333964,7166,https://api.github.com/repos/tensorflow/tensorflow/issues/7166,dustinvtran,3,0,0,0,0,0,many packages build on tensorflow for example our work in edward uses tensorflow>=..a as an install requirement however this conflicts with tensorflow-gpu which can no longer be installed because of the requirement specifically on tensorflow what do you suggest is the best way to handle this?one option suggested by gokceneraslan is to hack in the dependency according to whether the user has a gpu another option which prettytensor and keras employ is to not even require tensorflow both options sound not good.)also see also looping in gpflow devs jameshensman alexggmatthews in case they have the same problem note im raising this as an issue instead of asking on a mailing list in case this is something that should be changed on tensorflows end and not our end
203934948,7141,https://api.github.com/repos/tensorflow/tensorflow/issues/7141,kwccoin,1,0,0,0,1,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system ios installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud not related it is iosif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
203825528,7128,https://api.github.com/repos/tensorflow/tensorflow/issues/7128,yaroslavvb,2,0,0,0,0,0,we are doing a bunch of of qr decompositions in numpy i did preliminary investigation of moving them to tf but tf version is slow compared to numpy.below is a benchmark script that runs qr decomposition of x matrix it took seconds in tf and in numpy mkl numpy mkl is the default numpy that comes when installing anaconda.version head from last week built with config=cuda config=opt cpu core intel(r xeon(r cpu e v ghz that pip install upgrade tf_binary_url will overwrite mkl numpy with openblas numpy that is actually slower than tf version the way to check is to look at np.__config__.show and look for strings like mkl_intel_lp you can get mkl version back by uninstalling numpy and doing conda install numpy @rmlarsen
203606176,7109,https://api.github.com/repos/tensorflow/tensorflow/issues/7109,mutasem-mattar,1,0,0,0,0,0,hello i am trying to use tf.nn.dynamic_rnn to train an image captioning model however i keep getting this error valueerror variable lstm//basiclstmcell/linear/matrix does not exist or was not created with tf.get_variable did you mean to set reuse=none in varscope?i am using tensorflow version here is the code snippet lstm_cell tf.nn.rnn_cell.basiclstmcell(num_units=self.config.num_lstm_units state_is_tuple=true if self.mode train lstm_cell tf.nn.rnn_cell.dropoutwrapper lstm_cell input_keep_prob=self.config.lstm_dropout_keep_prob output_keep_prob=self.config.lstm_dropout_keep_prob with tf.variable_scope(lstm initializer=self.initializer as lstm_scope zero_state initial_state lstm_cell.zero_state batch_size=self.seq_embeddings.get_shape dtype=tf.float lstm_scope.reuse_variables sequence_length tf.reduce_sum(self.input_mask lstm_outputs tf.nn.dynamic_rnn(cell=lstm_cell inputs=self.seq_embeddings sequence_length=sequence_length initial_state=initial_state dtype=tf.float scope=lstm_scope
203591847,7108,https://api.github.com/repos/tensorflow/tensorflow/issues/7108,vahidk,4,0,0,0,0,0,tensorflow hangs on ios during session::run i have a deep lstm model that requires running session.run many times the program occasionally hangs after running a few sessions without consuming any cpu tensorflow seems to get stuck at directsession::waitfornotification what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system iosgit rev-parse head eefdfebeabcdabuild label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time tue may build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code std::vectorrun(feed out_layer_names outputs if status.ok log(error status.tostring return internal error logs or other output that would be helpfulthis is a stack trace of all of the threads when the program freezes thread tid x xec libsystem_kernel.dylib __psynch_cvwait queue com.apple.main-thread stop reason signal sigstop frame xec libsystem_kernel.dylib __psynch_cvwait frame xcc libsystem_pthread.dylib _pthread_cond_wait frame xcec libc++..dylib std::__::condition_variable::wait(std::__::unique_lock
203345463,7089,https://api.github.com/repos/tensorflow/tensorflow/issues/7089,abred,1,0,0,0,0,0,in i could do this testaccuracy tf.variable name=accuracy)trainaccuracy tf.variable name=accuracy)testaccsum tf.scalar_summary(accuracy testaccuracy)trainaccsum tf.scalar_summary(accuracy trainaccuracy)summaryopstest tf.merge_summary( testaccsum )summaryopstrain tf.merge_summary( trainaccsum )writertest tf.train.summarywriter(out_dir+/test sess.graph)writertrain tf.train.summarywriter(out_dir+/train sess.graph)to view both scalar summaries in a single plot.in i changed it to this testaccuracy tf.variable name=accuracy trainaccuracy tf.variable name=accuracy)testaccsum tf.summary.scalar(accuracy testaccuracy)trainaccsum tf.summary.scalar(accuracy trainaccuracy)summaryopstest tf.summary.merge( testaccsum )summaryopstrain tf.summary.merge( trainaccsum )writertest tf.summary.filewriter(out_dir+/test sess.graph)writertrain tf.summary.filewriter(out_dir+/train sess.graph)due to the issued warnings.but now each summary is drawn in its own plot and the name is made unique is this behavior intended if so is there a new way to achieve two summaries in one plot?thanks
203191991,7069,https://api.github.com/repos/tensorflow/tensorflow/issues/7069,amortazi,1,0,0,0,0,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system:installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
203191274,7068,https://api.github.com/repos/tensorflow/tensorflow/issues/7068,stevendavis,1,0,0,0,0,0,sun grid engine is commonly used to schedule jobs on scientific computing clusters supercomputers please consider adding support for distributed tensorflow on sge
203186829,7065,https://api.github.com/repos/tensorflow/tensorflow/issues/7065,SeguinBe,7,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem?started on so and was told to post here so post environment infooperating system:ubuntu maxwell titan xinstalled version of cuda and cudnn cuda cudnn python ls l usr/local/cuda/lib/libcud*-rw-r--r root root jan usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root jan usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root jan usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root jan usr/local/cuda/lib/libcudart.so...-rw-r--r root root jan usr/local/cuda/lib/libcudart_static.alrwxrwxrwx users jul usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx users jul usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxrwxr-x users jul usr/local/cuda/lib/libcudnn.so...-rw-rw-r users jul usr/local/cuda/lib/libcudnn_static.a installed from binary pip package with an anaconda distribution the output from python c import tensorflow print(tensorflow.__version__) : i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)using the following code to do a forward pass on a pretrained vgg pythonimport tensorflow as tffrom tensorflow.contrib import slimfrom tensorflow.contrib.slim import netstf.reset_default_graph use rng to avoid the feed_dict argumentinput_images tf.random_uniform maxval preds nets.vgg.vgg_(input_images is_training=false) saver tf.train.saver()config tf.configproto(log_device_placement=true)sess tf.interactivesession(config=config)saver.restore(sess vgg_.ckpt with jupyter notebook magic%timeit sess.run(preds) compared to the pytorch version on the same machine pythonimport numpy as npimport torchimport torchvision.models as modelsfrom torch.autograd import variabletorch.backends.cudnn.benchmark truenet models.vgg()net.cuda()_in variable(torch.from_numpy(np.random.randn astype(np.float)).cuda with jupyter notebook magic%timeit net(_in) i get the following results by comparing the frameworks surprisingly there is a small difference with the more complicated resnet while i get a huge gap for the vgg architecture which almost just uses x convolutions.model tf pytorch vgg ms msresnet ms ms
203115196,7060,https://api.github.com/repos/tensorflow/tensorflow/issues/7060,untom,10,0,0,0,0,0,hi!tensorboard currently version has an option to plot graphs with a logarithmic log scale but this functionality doesnt work well whenever plotting very small values e.g the ones that converge to zero like loss values for example compare this plot in tensorboard:! tensorboard_logplot the exact same data plotted using matplotlib/pandas using logy=true runs sorted(os.listdir(tensorboard/logs d pd.read_csv r) value for r in runs res pd.dataframe(d index=runs).t res.plot(ax=ax logy=true)! matplotlib_logplot the tensorboard plot is much less informative the logarithmic plot option doesnt do anything to improve the plot it would be nice if tensorboard could produce logarithmic plots similar to the ones in matplotlib
203111531,7059,https://api.github.com/repos/tensorflow/tensorflow/issues/7059,jplu,3,1,0,0,0,0,"hello,i do have issues to compile tensorflow on my mac in order to use the java version here some useful information about my environment macos gcc from macports bazel python tensorflow commit acdcdecdcadccdcbb cpu onlyonce i have done the configure im trying to compile the java version and here the log i get: bazel build c opt tensorflow/java:tensorflow tensorflow/java:libtensorflow_jniinfo found targets...error private/var/tmp/_bazel_jplu/ccdbfbbcedaef/external/nanopb_git/build c compilation of rule nanopb_git//:nanopb failed cc_wrapper.sh failed error executing command external/local_config_cc/cc_wrapper.sh u_fortify_source d_fortify_source fstack-protector wall wthread-safety wself-assign wunused-but-set-parameter wno-free-nonheap-object remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status gcc error unrecognized command line option wthread-safety did you mean fthread-jumps?gcc error unrecognized command line option wself-assign did you mean wparam-assign?info elapsed time s critical path s i also tried with a gcc version and i get the exact same error.did anyone have an idea of what is going wrong am i using a wrong version of gcc?thanks in advance"
202713440,7031,https://api.github.com/repos/tensorflow/tensorflow/issues/7031,deepankar1994,3,0,0,0,0,0,"hi,i am using tf ver head on ubuntu python and following this for learning inception model in tf slim import numpy as npimport osimport tensorflow as tfimport urllibfrom datasets import imagenetfrom nets import inceptionfrom preprocessing import inception_preprocessingslim tf.contrib.slimbatch_size image_size inception.inception_v.default_image_sizecheckpoints_dir tmp/checkpoints/with tf.graph().as_default url image_string urllib.urlopen(url).read image tf.image.decode_jpeg(image_string channels processed_image inception_preprocessing.preprocess_image(image image_size image_size is_training=false processed_images tf.expand_dims(processed_image create the model use the default arg scope to configure the batch norm parameters with slim.arg_scope(inception.inception_v_arg_scope logits inception.inception_v(processed_images num_classes is_training=false probabilities tf.nn.softmax(logits init_fn slim.assign_from_checkpoint_fn os.path.join(checkpoints_dir inception_v.ckpt slim.get_model_variables(inceptionv with tf.session as sess init_fn(sess np_image probabilities sess.run( image probabilities probabilities probabilities sorted_inds i for i in sorted(enumerate(-probabilities key=lambda x:x plt.figure plt.imshow(np_image.astype(np.uint plt.axis(off plt.show names imagenet.create_readable_names_for_imagenet_labels for i in range index sorted_inds i print(probability f s probabilities index names index )) however i am getting the following issue when running this logits inception.inception_v(processed_images num_classes is_training=false file home/deepankar/desktop/mtp/tensorflowex/tfslim/models/slim/nets/inception_v.py line in inception_v net end_points inception_v_base(inputs scope=scope file home/deepankar/desktop/mtp/tensorflowex/tfslim/models/slim/nets/inception_v.py line in inception_v_base net tf.concat branch branch branch branch file usr/local/lib/python./dist-packages/tensorflow/python/ops/array_ops.py line in concat dtype=dtypes.int).get_shape file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in convert_to_tensor as_ref=false file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in internal_convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file usr/local/lib/python./dist-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file usr/local/lib/python./dist-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file usr/local/lib/python./dist-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto assertcompatible(values dtype file usr/local/lib/python./dist-packages/tensorflow/python/framework/tensor_util.py line in assertcompatible dtype.name repr(mismatch type(mismatch).__name__))typeerror expected int got list containing tensors of type message instead"
202683778,7025,https://api.github.com/repos/tensorflow/tensorflow/issues/7025,yaroslavvb,4,0,0,0,0,0,this is the stack trace we sometimes get when trying to use tensorflow on a gpu thats occupied by another process it would help debugging if the error said something about memory.@zheng-xq tf.version gfcad-dirty(nightly from last week) w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use sse instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use avx instructions but these are available on your machine and could speed up cpu computations.w tensorflow/core/platform/cpu_feature_guard.cc the tensorflow library wasnt compiled to use fma instructions but these are available on your machine and could speed up cpu computations.i tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name titan x pascal)major minor memoryclockrate ghz pcibusid total memory gibfree memory mibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name titan x pascal pci bus id traceback most recent call last file home/yaroslav/.conda/envs/tim-jan/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file home/yaroslav/.conda/envs/tim-jan/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn status run_metadata file home/yaroslav/.conda/envs/tim-jan/lib/python./contextlib.py line in exit next(self.gen file home/yaroslav/.conda/envs/tim-jan/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.internalerror dst tensor is not initialized node zeros const dtype=dt_float value=tensor
202664211,7021,https://api.github.com/repos/tensorflow/tensorflow/issues/7021,foxik,1,0,1,0,0,0,the tf.contrib.losses.sparse_softmax_cross_entropy_loss has an weights parameter which can be used to weight the individual batch elements the weights parameter can have various shape which are all taken care of in compute_weighted_loss .however the sparse_softmax_cross_entropy_loss methods contains an errorneous squeeze of the weights if some dimension of weights is not known the number of dimensions after the squeeze is unknown which causes the compute_weighted_loss method to throw an exception however if the squeeze in sparse_softmax_cross_entropy_loss is not called compute_weighted_loss can deal even with unknown dimensions of weights.note that tf.contrib.losses.softmax_cross_entropy_loss does not contain the squeeze of the weights even if it uses the weights argument in an equal way.should fix
202337276,6999,https://api.github.com/repos/tensorflow/tensorflow/issues/6999,AngledLuffa,55,0,0,0,0,5,the current whl does not support python please update to support the latest version of python thanks!c:\windows\system>python versionpython c:\windows\system>pip install upgrade is not a supported wheel on this platform
202306410,6992,https://api.github.com/repos/tensorflow/tensorflow/issues/6992,gilgils,6,0,0,0,0,0,i suggest adding lu decomposition with partial pivoting to tensorflow the implementation must support rectangular matrices as well and not only square matrices tensorflow already uses lu decomposition for solving linear systems of equations.lu decomposition can be used as an efficient algorithm for finding the range of a matrix and its low rank approximation the advantages over qr and svd are that lu is more computationally efficient and is very suited for gpus giving a significant speed boost to many computational tasks.see the following papers li h linderman g c szlam a stanton k p kluger y tygert m algorithm an implementation of a randomized algorithm for principal component analysis acm transactions on mathematical software toms shabat g shmueli y aizenbud y averbuch a randomized lu decomposition applied and computational harmonic analysis li h kluger y tygert m randomized algorithms for distributed computation of principal component analysis and singular value decomposition arxiv preprint arxiv
202212899,6981,https://api.github.com/repos/tensorflow/tensorflow/issues/6981,s6norit4,1,0,0,0,0,0,"hi there,i have an issue when i am trying to inverse a tensor as shown below import tensorflow as tf a tf.placeholder(tf.float tf.inv(a)traceback most recent call last file stdin line in module>attributeerror module object has no attribute invoperation system mac os x python i installed tensor flow using pip install tensorflow tf.__version__..if someone could help out that would be cool thanks"
202173331,6978,https://api.github.com/repos/tensorflow/tensorflow/issues/6978,shyamalschandra,1,0,0,0,0,0,when will an official build of tensorflow for the latest release be available for use with opencl-enabled graphics cards should i fork a version and then hope to commit is this something the core team is interested in because i have a nvidia-enabled imac that is years old and i cannot accelerate my computation for different projects what is possible within the next year
202150181,6977,https://api.github.com/repos/tensorflow/tensorflow/issues/6977,DolanDack,1,0,0,0,0,0,i am trying to install tensorflow gpu on a different environment than my original anaconda root so i followed the instruction on the website installed cuda cudnn and tensorflow on a new tensorflow environment when i execute the import tensorflow on spyder ipython i get his error:! tsfl when i try to import it from python then it works fine:! tsfl the same happens when i tried to install the simple tensorflow on my original root environment works on python but not on ipython any suggestions please thank you
201824031,6955,https://api.github.com/repos/tensorflow/tensorflow/issues/6955,ppries,7,0,0,0,0,0,this is more of a general feature request for a better way to load data into a tensor in the c api but ill take our specific case as a reference.we currently train a model in python freeze it and load it in a c production pipeline this works fine but it seems the only way of loading data into a tensor is by looping through every single element in the data and copying it to a tensor in our case were dealing with a x fps video input signal each frame coming as an opencv matrix making it infeasible to do if we want to process a video within a reasonable amount of time.it seems there is a way of creating a tensor from a pointer in the c api see which we will test next but it would be nice to also have this functionality in the c api.for reference it takes approximately ms to copy from an xx opencv matrix to a tensor while it takes ms to do session.run which i assume includes transferring between cpu and gpu memory an issue related to this is that we will probably already have the data on the gpu from some earlier preprocessing so we would also be very interested in not having to transfer between cpu and gpu unnecessarily.so i guess it boils down to are there any plans for making it easier to load data already in memory to a tensor how can we contribute
201803881,6954,https://api.github.com/repos/tensorflow/tensorflow/issues/6954,civilman628,2,0,0,0,0,0,nathansilberman sguadai can see there are some pre-trained models in the link below for download these are only checkpoint files we cant use them at all because without pbtxt file which is the graph define we cant restore the any model only base on ckpt file there are no example about how to use these checkpoint file such as for a single image prediction or feature extraction i can only see one example about how to use v pb file for prediction but that one is pb file not ckpt file without pbtxt file we cant freeze the graph at all.i want to use inception v model which is smaller than v and better than v however i can do nothing...even if you fine tune the model you can only evaluate it base on a batch file but not to predict single image in the section of evaluating performance of a model of the page we can only get accuracy however we need to test single image to see the result but not a accuracy number for a batch file
201585238,6929,https://api.github.com/repos/tensorflow/tensorflow/issues/6929,ahhentz,2,0,0,0,0,0,the documentation of ctc_beam_search_decoder describes the input argument as inputs d float tensor size max_time x batch_size x num_classes the logits.implying that they should be the linear projections however by inspecting the unit tests and the implementation it seems these inputs should actually be log-probabilities in this case the documentation should read inputs d float tensor size max_time x batch_size x num_classes the output symbol log-probabilities
201499757,6923,https://api.github.com/repos/tensorflow/tensorflow/issues/6923,nvarini,3,0,0,0,0,0,"hello i am writing with regard compiling tensorflow on linux clusters as far as i understood bazel recompiles everything it needs on out cluster we use spack to compile new software and provide modules to the users with spack its easy to track dependencies and provenance of a given installation.by recompiling everything bazels need the dependencies are messed up and we dont understand how tensorflow has been compiled from a spack point of view why is it stated n.b we provide linux build instructions primarily for the purpose of testing the build we recommend using the standard bazel-based build on linux.?actually how can we tell to bazel to use system-provided libraries like boost?thanks very much,nicola"
201423968,6914,https://api.github.com/repos/tensorflow/tensorflow/issues/6914,benbarsdell,1,0,0,0,0,0,the public docker images and pip packages are not compiled against cuda architecture or which can cause a delay of more than a minute during initialization when executed on corresponding hardware p gtx series etc as the cuda driver runs jit compilation on all of the internal kernels.given that the public binaries are now built against cuda it would make sense to have them target archs and minimal reproducer run with cuda_cache_disable=): import tensorflow as tfsess tf.session()rand_init tf.random_uniform rand_var tf.variable(rand_init)init tf.global_variables_initializer()sess.run(init related issues environment infooperating system linux
201396346,6911,https://api.github.com/repos/tensorflow/tensorflow/issues/6911,yaroslavvb,3,0,0,0,0,0,right now switching branches requires running configure which then makes next bazel test rebuild everything.in particular on core xeon switching branches/ ./configure makes bazel test learning/python run minutes longer because its building stuff from scratch this makes it hard for open-source contributors to work on more than one pull request at a time.in particular the error from bazel test when switching git branches is as follows: error home/yaroslav/tensorflow.git/tensorflow/tensorflow/core/build executing genrule tensorflow/core:version_info_gen failed bash failed error executing command bin/bash c remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status traceback most recent call last file tensorflow/tools/git/gen_git_source.py line in module generate(args.generate file tensorflow/tools/git/gen_git_source.py line in generate old_branch new_branch))runtimeerror run configure again branch was refs/heads/match-once-fix but is now refs/heads/symbolic-gradient-fix
200859497,6861,https://api.github.com/repos/tensorflow/tensorflow/issues/6861,chiphuyen,0,0,1,0,0,0,so for tf.nn.nce_loss for wordvec_basic if i explicitly use keyword arguments such as tf.nn.nce_loss(weights=nce_weights biases=nce_biases labels=train_labels inputs=embed num_sampled=num_sampled num_classes=vocabulary_size))everything is fine but if i get rid of the keywords tf.nn.nce_loss(nce_weights nce_biases train_labels embed num_sampled vocabulary_size))i run into the bug file tensorflow/python/ops/nn.py line in nce_loss name=name file tensorflow/python/ops/nn.py line in compute_sampled_logits array_ops.reshape(true_w new_true_w_shape file tensorflow/python/ops/gen_math_ops.py line in mul result op_def_lib.apply_op(mul x=x y=y name=name file tensorflow/python/framework/op_def_library.py line in apply_op inferred_from input_arg.type_attr ))typeerror input y of mul op has type float that does not match type int of argument x. i looked into the source code for nce_loss but couldnt seem to find the cause of this bug can anyone help me with this thanks a lot
200808687,6850,https://api.github.com/repos/tensorflow/tensorflow/issues/6850,hawkinsp,5,0,0,0,0,0,the official libcudnn-dev package from nvidia developers.nvidia.com version cuda places the cudnn.h header at /usr/include/x_-linux-gnu/cudnn_v.h but tensorflows config script looks for it at: /usr/lib/x_-linux-gnu/include/cudnn.h which means that running tensorflows configure script fails.a workaround is to add a symlink: sudo mkdir usr/lib/x_-linux-gnu/includesudo ln s usr/include/x_-linux-gnu/cudnn_v.h usr/lib/x_-linux-gnu/include/cudnn.h after which tensorflows configure script completes successfully
200790383,6847,https://api.github.com/repos/tensorflow/tensorflow/issues/6847,bodokaiser,6,0,0,0,0,0,in this issue someone requested a reverse operation to tf.extract_image_patches the comments suggest that there exists a gradient operation for this purpose which was added with this pr it is not obvious how to apply this gradient operation and there are no information on this in the api docs therefor i request to add an example to tf.extract_image_patches where one transforms an image tensor of shape image_height image_width channels to patch_num patch_height patch_width channels and vice versa.thank you in advance
199804310,6766,https://api.github.com/repos/tensorflow/tensorflow/issues/6766,jrosti,2,0,0,0,0,0,"environment infooperating system ubuntu installed version of cuda and cudnn cuda cudnn tensorflow version installed from also using tf cuda cudnn minimal reproducible example import tensorflow as tfy tf.placeholder(int none y)one_hot_y=tf.one_hot(y,)ce tf.nn.softmax_cross_entropy_with_logits(one_hot_y one_hot_y)sess tf.session()sess.run(ce y result on gpu e tensorflow/core/common_runtime/bfc_allocator.cc tried to allocate bytesw tensorflow/core/common_runtime/allocator_retry.cc request to allocate bytesf tensorflow/core/common_runtime/gpu/gpu_device.cc eigenallocator for gpu ran out of memory when allocating see error logs for more detailed info.aborted core dumped) result on cpu array dtype=float"
199573239,6742,https://api.github.com/repos/tensorflow/tensorflow/issues/6742,asross,1,0,0,0,0,0,if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonfrom tensorflow.examples.tutorials.mnist import input_datamnist input_data.read_data_sets(/tmp/data one_hot=true) raises http.client.remotedisconnected remote end closed connection without response if you dont already have the data cached i believe this is because the underlying data source_url is currently down
199468899,6730,https://api.github.com/repos/tensorflow/tensorflow/issues/6730,usmcamp0811,1,0,0,0,0,0,"environment infooperating system ubuntu installed version of cuda and cudnn libcudart.so libc,x usr/local/cuda-./targets/x_-linux/lib/libcudart.so libcudart.so libc,x usr/lib/x_-linux-gnu/libcudart.so libcudart.so libc,x usr/local/cuda-./targets/x_-linux/lib/libcudart.so libcudart.so libc,x usr/lib/x_-linux-gnu/libcudart.so libcuda.so libc,x usr/lib/x_-linux-gnu/libcuda.so libcuda.so libc usr/lib/i-linux-gnu/libcuda.so libcuda.so libc,x usr/lib/x_-linux-gnu/libcuda.so libcuda.so libc usr/lib/i-linux-gnu/libcuda.sopip install tensorflow-gpu rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)i posted to the below stackoverflow link with some code and my initial problem that i mostly fixed but it does seem that there is a problem with the tf.map_fn i was trying to use tf.map_fn to distort a batch of images and it throws an error sometimes the batch starts its life as a tfrecords file of flat images the batch is turned into d tensor by tf.reshape then put through tf.map_fn(image distortion function and then reshaped back into a flat d tensor most of the time i get a reshape error but not all the time the times i dont get the error the code will run for all epochs... /home/mcamp/anaconda/bin/python media/mcamp/local sshd/python projects/garagedoor/train_model.pyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_device.cc found device with properties name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_device.cc dma i tensorflow/core/common_runtime/gpu/gpu_device.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id traceback most recent call last file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_call return fn(*args file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in run_fn status run_metadata file home/mcamp/anaconda/lib/python./contextlib.py line in exit next(self.gen file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.invalidargumenterror the tensor returned for reshape was not valid.during handling of the above exception another exception occurred:traceback most recent call last file media/mcamp/local sshd/python projects/garagedoor/train_model.py line in module model.train(training_data epochs=flags.n_epochs file media/mcamp/local sshd/python projects/garagedoor/convnetclass.py line in train training_data_dict y_train_batch file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file home/mcamp/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.invalidargumenterror the tensor returned for reshape was not valid"
199434284,6727,https://api.github.com/repos/tensorflow/tensorflow/issues/6727,krautt,3,0,0,0,0,0,"hello!i have been working my way through the tf.contrib.learn tutorials and have been attempting to integrate the tf.contrib.learn.monitors.validationmonitor into the deep classifier in wide_n_deep.py as shown below what related github issues or stackoverflow threads have you found by searching the web for your problem i searched both github and stakeoverflow with the terms tensorflow input_fn and validationmonitor but wasnt able to find anyone else who reported similar issues environment infooperating system i ran this on ubuntu server on a physical i with a gtx gpu when i noticed the problem i know that i was using the gpu on the original physical box from previous tests and because during the hang the nvidia_smi command showed considerable load on the gpu i was able to replicate the problem with cpu on a vm as well.installed version of cuda and cudnn home/andersonjas/libcudnn-dev_..-+cuda._amd.deb/home/andersonjas/libcudnn_..-+cuda._amd.deb if installed from binary pip package provide a link to the pip package you installed:from anaconda bit package: pip install tensorflow the output from python c import tensorflow print(tensorflow.__version__) . andersonjas@ubuntu python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally..head if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonvalidation_monitor tf.contrib.learn.monitors.validationmonitor(input_fn=lambda:input_fn(df_test every_n_steps=)m.fit(input_fn=lambda input_fn(df_train steps=,monitors= validation_monitor ) doing this in a jupyter notebool causes the code to hang indefinitely to make completely sure that i dont have a bug in my own code i can make the following change: pythonvalidation_monitor tf.contrib.learn.monitors.validationmonitor(input_fn=lambda:input_fn(df_test every_n_steps=)m.fit(input_fn=lambda input_fn(df_train steps monitors= validation_monitor ) and then the code executes fine what other attempted solutions have you tried?i also built an input_fn interface to the iris and boston housing price predictor code code each showed similar hangs logs or other output that would be helpful(if logs are large please upload as attachment or provide link).as a noob im learning that esoteric error messages are a luxury in this case the code just hangs indefinitely"
199410360,6724,https://api.github.com/repos/tensorflow/tensorflow/issues/6724,johny7,4,0,0,0,0,0,pretty much topic tells the whole story.for auto-encoder based on d convolutions opposite operation is required to complete decoding part.this is planned to be used for sound processing
199397359,6720,https://api.github.com/repos/tensorflow/tensorflow/issues/6720,JoelKronander,2,0,0,0,0,0,"the tf.image.resize_images seems to use a strange padding option which one is not clear to me at the moment i tried to replicate the bilinear interpolation with various padding options in for example skimage but cant replicate the behaviour.it would be nice to be able to set the padding option used in tf.images.resize_images or document what is used at least.example code for comparing the results of tf.images.resize_images and skimage transform:looks like tf.images.resize_images does some weird unsymmetrical padding!?using tensorflow import tensorflow as tfimport tensorlayer as tlimport numpy as npimport skimagefrom scipy.misc import imread imresize imsavesess tf.interactivesession()#create simple test imageimsize xa ya np.ogrid :imsize imsize img np.repeat((xa ya np.newaxis float(imsize imsize)x tf.placeholder(tf.float imsize imsize y tf.image.resize_images(x,(imsize imsize*))sess.run(tf.global_variables_initializer())upsampled_tf_result sess.run(y feed_dict={x img })upsampled_skimage_result skimage.transform.rescale(img mode=symmetric cval order preserve_range=false)print(np.allclose(upsampled_tf_result upsampled_skimage_result))imsave(upsampled_tf_result.png np.squeeze(upsampled_tf_result))imsave(upsampled_skimage_result.png upsampled_skimage_result"
199382238,6716,https://api.github.com/repos/tensorflow/tensorflow/issues/6716,yaroslavvb,2,0,0,0,0,0,tldr to debug tensorflow out-of-memory situations one needs to see tensor de-allocation info.you can see allocation stats in timeline but without de-allocation info you cant calculate peak memory currently getting peak memory is possible by hacking tensorflow to print deallocation messages with timestamps as here a bunch of regular expression to parse log_memory messages as in here this needs building your own version of tensorflow this is not accessible to most people perhaps this can be remedied by adding deallocation events to session run timeline michaelisard some recent places this issue came up
199324233,6706,https://api.github.com/repos/tensorflow/tensorflow/issues/6706,raraujosc,0,0,0,0,0,4,when using the head version of tensorflow as an external bazel dependency like tensorflow_serving does we run into an issue in the line: load(@//third_party:common.bzl template_rule) inside tensorflow/third_party/jpeg/jpeg.build that line assumes that tensorflow is the main repository but when tensorflow is included in another project the main repository is the main project so that common.bzl file is not found i think that line needs to read: load(@org_tensorflow//third_party:common.bzl template_rule
199136431,6682,https://api.github.com/repos/tensorflow/tensorflow/issues/6682,jihunchoi,4,0,0,0,0,0,"i frequently use tf.tensordot for its flexibility on axes.however as i multiply two tensors whose shapes are partly known it cannot infer the shape of the result tensor i.e shape is unknown while other alternative functions that do the same operation infer shapes well.for example let a and b be a tf.placeholder(float shape= none b tf.placeholder(float shape we can validate that the below functions infer shapes well. result_matmul tf.matmul(a b)result_matmul.get_shape().as_list none result_einsum tf.einsum(ij,jk->ik a b)result_einsum.get_shape().as_list none however when tf.tensordot is used the weird result occurs: result_tensordot tf.tensordot(a b axes result_tensordot.get_shape tensorshape(none)result_tensordot.get_shape().as_list error thus i have to call set_shape(shape function explicitly to make the output be able to be used for further operations.there also exists an issue that it still uses concat function where the warning that it would be deprecated after occurs instead of concat_v"
199097730,6679,https://api.github.com/repos/tensorflow/tensorflow/issues/6679,jkschin,3,0,0,0,0,0,i reference this post one does not call tf.local_variables_initializer when using tf.train.string_input_producer with num_epochs set as a local variable it will throw an outofrangeerror which might mislead people into thinking that the error lies with tf.train.string_input_producer instead of initializing the local variable.throwing an uninitialized local variable error would be very helpful in this case.@mrry
198874152,6656,https://api.github.com/repos/tensorflow/tensorflow/issues/6656,yaroslavvb,1,0,0,0,0,0,need something like to fix configure since is down
198862012,6651,https://api.github.com/repos/tensorflow/tensorflow/issues/6651,lspvic,1,0,0,0,0,0,tensorboard use a absolute path for data request like data/runs data/graph data/logdir .if someone want to deploy tensorboard with a path the data xmlhttprequest would be thus give not found errors and tensorboard doesnt work properly.to make the data request with a relative xmlhttprequest url for data request should be used the default datadir data will change to datadir data; the default datadir to datadir data will always work whenever tf-tensorboard.html is placed or any context path is set
198680928,6633,https://api.github.com/repos/tensorflow/tensorflow/issues/6633,albertz,5,0,0,0,0,0,according to the cudnn docs the functions cudnnrnnforwardinference cudnnrnnforwardtraining get the argument cudnntensordescriptor_t xdesc where:xdesc array of tensor descriptors each must have the same second dimension the first dimension may decrease from element n to element n but may not increase.the usage of xdesc is a bit non-straight-forward i wrote about that in more detail here to a comment in the cntk code about the dimensions of each xdesc t these dimensions are what cudnn expects the minibatch dimension the data dimension and the number because each descriptor describes one frame of data)tensorflow sets the same minibatch dimension for each xdesc t in cudnnrnnsequencetensordescriptor int dims batch_size data_size int strides dims dims dims status dynload::cudnnsettensornddescriptor parent handle tensordesc data_type datatype sizeof(dims sizeof(dims nbdims dims dima strides stridea cudnn_return_if_fail(status failed to update tensor descriptor replicate handle across the number of steps handles_.assign(seq_length handle);also creaternnsequencetensordescriptor needs a new api to allow for that.and im not sure if there are ways to prepare the input x for cudnnrnnforwardtraining so that it has all sequences contiguously behind each other and the sequences are sorted by sequences length.similar as packsequencesforcudnn in cntk
198610476,6622,https://api.github.com/repos/tensorflow/tensorflow/issues/6622,BaiGang,1,0,0,0,0,0,in section building the graph of gdoc/tutorials/wordvec/index.md: python compute the nce loss using a sample of the negative labels each time.loss tf.reduce_mean tf.nn.nce_loss(nce_weights nce_biases embed train_labels num_sampled vocabulary_size)) whereas the actual order of parameters is as in python/ops/nn_impl.py is defined as: pythondef nce_loss(weights biases labels inputs so the embed and the train_labels parameters in the code excerpt in the wordvec tutorial page should be swapped to match the definition of tf.nn.nce_loss
198566318,6620,https://api.github.com/repos/tensorflow/tensorflow/issues/6620,boche,1,0,0,0,0,0,i am testing how to use cudnnlstm there is not a lot of documentation on this in my own experiment i found when use adamoptimizer with cudnnlstm it always raises the following exception i also found another repository using cudnnlstm and uploaded it here it also raises the same exception file ptb_word_lm.py line in main m ptbmodel(is_training=true config=config debug=flags.debug file ptb_word_lm.py line in init self._train_op optimizer.apply_gradients(zip(allgrads allvars file data/asr/ramons/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/training/optimizer.py line in apply_gradients self._create_slots(var_list file data/asr/ramons/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/training/adam.py line in create_slots self._zeros_slot(v m self._name file data/asr/ramons/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/training/optimizer.py line in zeros_slot named_slots var slot_creator.create_zeros_slot(var op_name file data/asr/ramons/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/training/slot_creator.py line in create_zeros_slot val array_ops.zeros(primary.get_shape().as_list dtype=dtype file data/asr/ramons/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/tensor_shape.py line in as_list raise valueerror(as_list is not defined on an unknown tensorshape valueerror as_list is not defined on an unknown tensorshape.for now using gradientdescentoptimizer is okay but it seems all other fancy optimizers all have similar problems i have looked up this problem on stackoverflow and found a related thread it seems the problem is that parameter buffer used in cudnnlstm doesnt have fixed shape validate_shape=false defined in line of which is required by adamoptimizer cudnn seems to be faster less time per epoch but if we cant use better learning algorithms with it meaning more epochs then the total running time may not be improved so much
198405916,6612,https://api.github.com/repos/tensorflow/tensorflow/issues/6612,ahundt,2,0,0,0,0,0,zlib permalinks including the current version are always in in are removed when no longer current.should resolve if merged into r branch
198382825,6606,https://api.github.com/repos/tensorflow/tensorflow/issues/6606,aclaussen1,1,0,0,0,0,0,im trying to use tensorflow for this project gotten to the point where when i import tensorflow i get the messages that all the cuda libraries are successfully opened locally.i can run the following python code from and it works fine import tensorflow as tf hello tf.constant(hello tensorflow sess tf.session print(sess.run(hello))hello tensorflow a tf.constant b tf.constant print(sess.run(a b))>however when i run the wavenet project i get the following error messages and then python crashes. c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties:name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc dma i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc yi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id warning:tensorflow:from train.py in main initialize_all_variables from tensorflow.python.ops.variables is deprecated and will be removed after instructions for updating:use tf.global_variables_initializer instead.trying to restore saved checkpoints from logdir\train\--t no checkpoint found.e c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc could not create cudnn handle cudnn_status_internal_errore c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc could not destroy cudnn handle cudnn_status_bad_paramf c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\kernels\conv_ops.cc check failed stream->parent()-getconvolvealgorithms(&algorithms what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system:windowsinstalled version of cuda and cudnn cudnn v august for cuda if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cublas_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cudnn_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cufft_.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library nvcuda.dll locallyi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library curand_.dll locallytraceback most recent call last file string line in module>nameerror name tensor is not definedif installed from source provide the commit hash git rev-parse head the output of bazel version what other attempted solutions have you tried?i have tried reinstalling cuda different versions of cudnn looked at different issues with same error messages but nothing seemed to help
198358061,6603,https://api.github.com/repos/tensorflow/tensorflow/issues/6603,foxik,4,0,0,0,0,0,this is a feature request connected with pull request the new summary interface only allows python strings as summary names while the old one could use string tensors.the new behaviour is considerably less flexible consider the case where you have train/devel/test data split and you want to report summaries accuracy for example for each dataset individually i.e having them in three different graphs in tensorboard with the old summary interface this can be accomplished nicely: python dataset_name tf.placeholder(tf.string summary tf.scalar_summary(dataset_name+/accuracy accuracy s session.run( summary dataset_name:train devel test}) with the new interface there has to be three summary nodes in the graph to accomplish this which makes the code repetitive and more complicated to maintain you can create the three nodes with different name scopes but you have to store them somewhere a dictionary and if you want to use these nodes after restoring a metagraph things become unellegant).one possible solution is that pull request adds a prefix to all summary operations the prefix is added to all summary names and can be a string tensor that would allow reporting summaries for different datasets while being applicable to the new summary approach of naming the summary operations according to the summary names i.e with the new approach it is not possible for name to be string tensor but a prefix is fine).the change is opt-in so nobody interested in dynamic summary names is affected while allowing much more flexible names to those interested and i would like to see it merged.however there could be other approaches how the problem could be solved therefore i am opening this issue for discussion and finding the best solution
198280104,6594,https://api.github.com/repos/tensorflow/tensorflow/issues/6594,ahundt,13,0,0,0,0,0,im attempting to install tensorflow from the r branch from source.zlib has been updated from to so it appears the link must be updated from per later comments this has been moved to solution may be to update to is the error im getting: info starting clean this may take a while consider using expunge_async if the clean takes more than several minutes.........error home/ahundt/src/tensorflow/tensorflow/core/build no such package zlib_archive error downloading to home/ahundt/.cache/bazel/_bazel_ahundt/becafbfbbafb/external/zlib_archive/zlib-...tar.gz get returned not found and referenced by tensorflow/core:lib_internal.error home/ahundt/src/tensorflow/tensorflow/core/build no such package zlib_archive error downloading to home/ahundt/.cache/bazel/_bazel_ahundt/becafbfbbafb/external/zlib_archive/zlib-...tar.gz get returned not found and referenced by tensorflow/core:lib_internal.error evaluation of query deps((//tensorflow union bazel_tools//tools/jdk:toolchain failed errors were encountered while computing transitive closure. in addition to the breakage fix perhaps it would also make sense to make a change that would prevent future breakage of this sort
198246632,6585,https://api.github.com/repos/tensorflow/tensorflow/issues/6585,Carmezim,6,0,0,0,0,0,special thanks to all tensorflow engineers for your incredible work this year maintaining and developing so successfully a project of this magnitude isnt an easy work ive personally always seen great attention to the issues and contributions made with an also prompt helpful and nice attitude and believe the community is very grateful for all youve done this year and the excellence of your work cant wait for what is to come in
198242109,6584,https://api.github.com/repos/tensorflow/tensorflow/issues/6584,jeffheaton,5,0,0,0,0,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem?was not able to find any that apply environment infooperating system:macinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .tried on:....-rcif installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)i would like to use skcompat with a dnn classifier but it generates an error a real easy way to see this is to run i run the above example exactly as it is i get this warning: estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...)) so i follow the advice and add this line of code just after the dnnclassifier is created. classifier tf.contrib.learn.skcompat(classifier) this results in the following error how do i use skcompat with this example i would like to use the standard sklearn interface for this application. traceback most recent call last file test.py line in module steps file users/jeff/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in fit loss self._estimator._train_model(attributeerror dnnclassifier object has no attribute train_model what other attempted solutions have you tried?examined source code for tensorflow it appears that skcompat is trying to call a function that dnnclassifier does not have logs or other output that would be helpful(if logs are large please upload as attachment or provide link
198010620,6561,https://api.github.com/repos/tensorflow/tensorflow/issues/6561,AlexandreBriot,1,0,0,0,0,0,hey i am trying tu run the android camera demo with different model.to get a simple use case i started with a retrained inception model produced from the tensorflow for poets codelab tutorial classifying flowers.what i have done i place the new model pb file and the new label txt file in the assets directory.i followed the recommendations specified in the classifieractivity.java file set image_size image_mean image_std set input_name mul and output_name final_result:i bazel build and install the apk file.running the app i got this error in the logcat e/native tensorflow_inference_jni.cc could not create tensorflow graph invalid argument no opkernel was registered to support op decodejpeg with these attrs registered devices cpu registered kernels no registered kernels node decodejpeg decodejpeg acceptable_fraction channels dct_method fancy_upscaling=true ratio try_recover_truncated=false (decodejpeg/contents) _to my understanding this node is preceding my input node and is useless in this case.what is the solution to get rid of it thanks in advance for your answer.alex
197968641,6558,https://api.github.com/repos/tensorflow/tensorflow/issues/6558,zafartahirov,5,0,0,0,0,0,when trying to run configure bazel build copt=-march=native c opt tensorflow/tools/pip_package:build_pip_package verbose_failures it fails with the third_party/eigen/unsupported/eigen/cxx/fixedpoint fatal error src/tensor/tensorcontractionthreadpool.h file not found error.similar problem was seen in i am on mac os x sierra with no cuda and cudnn installed i am using virtual environment on python here is the checked commit and bazel version git rev-parse headacaedfbcfffdfba bazel versionbuild label homebrewbuild target bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu dec build timestamp build timestamp as int this works fine git checkout remotes/origin/r configure with defaults bazel build copt=-march=native c opt tensorflow/tools/pip_package:build_pip_package verbose_failures this finishes correctly here is the full terminal log when bazel is run on the master branch git checkout master configure with defaults bazel build copt=-march=native c opt tensorflow/tools/pip_package:build_pip_package verbose_failuresinfo found target...error users/zafar/github/tensorflow/tensorflow/core/kernels/build c compilation of rule tensorflow/core/kernels:gather_functor failed cc_wrapper.sh failed error executing command cd private/var/tmp/_bazel_zafar/cdccaefdceebae/execroot/tensorflow exec env path=/users/zafar/.virtualenvs/tf-dev-env/bin:/users/zafar/.rbenv/shims:/users/zafar/.node/bin:/users/zafar/bin:/users/zafar/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/x/bin:/usr/local/macgpg/bin:/library/tex/texbin tmpdir=/var/folders/yx/nfnvmpn_rnbzknczhgn/t external/local_config_cc/cc_wrapper.sh u_fortify_source d_fortify_source fstack-protector wall wthread-safety wself-assign fcolor-diagnostics fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections march=native std=c++x md mf bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.d frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o fpic deigen_mpl_only iquote iquote bazel-out/local-opt/genfiles iquote external/bazel_tools iquote bazel-out/local-opt/genfiles/external/bazel_tools iquote external/eigen_archive iquote bazel-out/local-opt/genfiles/external/eigen_archive iquote external/local_config_sycl iquote bazel-out/local-opt/genfiles/external/local_config_sycl isystem external/bazel_tools/tools/cpp/gcc isystem external/eigen_archive isystem bazel-out/local-opt/genfiles/external/eigen_archive deigen_avoid_stl_array iexternal/gemmlowp wno-sign-compare fno-exceptions wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted c tensorflow/core/kernels/gather_functor.cc o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status in file included from tensorflow/core/kernels/gather_functor.cc::in file included from tensorflow/core/kernels/gather_functor.h::in file included from tensorflow/core/framework/type_traits.h::in file included from tensorflow/core/framework/numeric_types.h::./third_party/eigen/unsupported/eigen/cxx/fixedpoint fatal error src/tensor/tensorcontractionthreadpool.h file not found#include src/tensor/tensorcontractionthreadpool.h error generated.target tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path s
197805473,6533,https://api.github.com/repos/tensorflow/tensorflow/issues/6533,VelizarVESSELINOV,59,0,0,4,0,0,im sure that you are aware that python is available and tensorflow is still not available for this version hope next update will cover the cp python version
197627477,6509,https://api.github.com/repos/tensorflow/tensorflow/issues/6509,SergeyBykov1,9,0,0,0,0,0,im using a lenet mnist example from udacitys course link to the source code is below.training works ok on a cpu config tf.configproto(device_count gpu but fails in a gpu mode with the following cuda_error_illegal_address error i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc found device with properties:name geforce gtx gbmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc dma i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc yi c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc creating tensorflow device gpu device name geforce gtx gb pci bus id training...e c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_event.cc error polling for event status failed to query event cuda_error_illegal_address f c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc unexpected event status i have reproduced same error on two setups.environment home pc windows latest anaconda python cuda cudnn for win tensorflow gpu geforce gtx gbenvironment work pc windows latest anaconda python cuda cudnn for win tensorflow gpu geforce gtx gbim sharing two scripts with minor changes that allow a workaround difference:script from example that crashes lablenetbad.py uses raw mnist label data with the tf.one_hot call.the workaround lablenetgood.py reads mnist data with one_hot=true flag and does not use tf.one_hot call on the y placeholder.i think that tf.one_hot does not work properly on the gpu
197616245,6507,https://api.github.com/repos/tensorflow/tensorflow/issues/6507,Warvito,2,0,0,0,0,0,support for tf.float dtype was recently added to a bunch of ops can we add it for convd too please?convd is important to development of videos and medical images systems since both consumes a lot of memory it would be good to have fp support to allow deeper models
197592069,6503,https://api.github.com/repos/tensorflow/tensorflow/issues/6503,kstant0725,15,0,0,0,0,0,the gradient for the svd op would be very useful so that it could be used in networks and cost functions currently when trying to use svd i get the follow:lookuperror no gradient defined for operation svd op type svd)so my request is for the gradient for the svd op
197575471,6500,https://api.github.com/repos/tensorflow/tensorflow/issues/6500,KishoreKarunakaran,1,0,0,0,0,0,"i already have conda installation in my windows i installed tensorflow using pip and it was working fine based on the testing the tensorflow installation im using tensorforestestimator i,m getting the following error lib\site-packages\tensorflow\contrib\tensor_forest\python\ops\_training_ops.so not found complete details hparams tf.contrib.tensor_forest.python.tensor_forest.foresthparams num_trees max_nodes num_classes num_features model_dir=model/fit/) forest_classifier tf.contrib.learn.tensorforestestimator(hparams) warning messages:warning:tensorflow:using temporary folder as model directory c:\users\username\appdata\local\temp\tmpuvjyuinfo:tensorflow:using default config.info:tensorflow:using config tf_random_seed none keep_checkpoint_max cluster_spec tensorflow.python.training.server_lib.clusterspec object at xdce save_checkpoints_steps none save_checkpoints_secs task_id keep_checkpoint_every_n_hours tf_config gpu_options per_process_gpu_memory_fraction is_chief true num_ps_replicas master environment local save_summary_steps task_type none evaluation_master forest_classifier.fit(x=training_data y=y_train batch_size steps=) warning:tensorflow:from d:\program files\anaconda\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\random_forest.py in fit calling baseestimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from d:\program files\anaconda\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\random_forest.py in fit calling baseestimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from d:\program files\anaconda\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\random_forest.py in fit calling baseestimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with batch_size is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))info:tensorflow:constructing forest with params info:tensorflow:{dominate_method hoeffding dominate_fraction bagged_num_features split_initializations_per_input num_trees base_random_seed valid_leaf_threshold bagged_features none min_split_samples bagging_fraction num_classes num_features feature_bagging_fraction num_splits_to_consider split_after_samples model_dir model/fit num_outputs max_nodes num_output_columns regression false max_fertile_nodes info:tensorflow:data path d:\program files\anaconda\lib\site-packages\tensorflow\contrib\tensor_forest\python\ops\_training_ops.so ---------------------------------------------------------------------------notfounderror traceback most recent call last)"
197549409,6497,https://api.github.com/repos/tensorflow/tensorflow/issues/6497,inferrna,0,0,0,0,0,2,"tf is configured with opencl configure media/compressed/drivers_bios/src/dev/tensorflow media/compressed/drivers_bios/src/dev/tensorflowplease specify the location of python default is usr/bin/python usr/bin/pythondo you wish to build tensorflow with google cloud platform support y/n no google cloud platform support will be enabled for tensorflowdo you wish to build tensorflow with hadoop file system support y/n no hadoop file system support will be enabled for tensorflowfound possible python library paths usr/lib/python/dist-packages usr/local/lib/python./dist-packagesplease input the desired python library path to use default is usr/lib/python/dist-packages /usr/local/lib/python./dist-packagesdo you wish to build tensorflow with opencl support y/n yopencl support will be enabled for tensorflowdo you wish to build tensorflow with cuda support y/n nno cuda support will be enabled for tensorflowplease specify which c compiler should be used as the host c compiler default is usr/bin/g++please specify which c compiler should be used as the host c compiler default is usr/bin/gccplease specify the location where computecpp for sycl is installed default is usr/local/computecpp : computecpp is also present ls usr/local/computecpp usr/local/computecpp/bin:compute computecpp_info/usr/local/computecpp/doc:license.text computecpp_error_codes.pdf computecpp_glossary.pdf computecpp_man.pdf computecpp_release_notes.pdfapi_pages computecpp_getting_started.pdf computecpp_info_man.pdf computecpp_platform_support_notes.pdf computecpp_stream_class.pdf/usr/local/computecpp/include:cl sycl/usr/local/computecpp/lib:clang libcomputecpp.so i also has opencl clinfo grep i version platform version opencl amd-app device opencl c version opencl c driver version vm version opencl amd-app device opencl c version opencl c driver version sse,avx,fma version opencl amd-app but it seems that tf dont see any opencl device in my system. python from tensorflow.python.client import device_lib x.name for x in device_lib.list_local_devices() /cpu"
197489914,6490,https://api.github.com/repos/tensorflow/tensorflow/issues/6490,ethereon,1,0,0,0,0,0,this: pythonrudolph tf.zeros prancer tf.zeros comet tf.concat_v( rudolph prancer axis=-)print(tensor shape format(comet.get_shape()))print(evaluated shape format(comet.eval().shape)) produces: tensor shape evaluated shape tested on tensorflow version
197453093,6483,https://api.github.com/repos/tensorflow/tensorflow/issues/6483,suryabhupa,2,0,0,0,0,0,it seems like the seqseq.py library has been deprecated and it would be nice to know where the new code and if the tutorial could point to it
197367133,6474,https://api.github.com/repos/tensorflow/tensorflow/issues/6474,TimZaman,1,0,0,0,0,0,this is a question or feature request regarding the following function tf.image.draw_bounding_boxes(images boxes name=none currentthe required bounding-box format is as follows: batch num_bounding_boxes in which the labels are in y_min x_min y_max x_max format.if i understand correctly this assumes that every image has the same number of bounding boxes suggestiondoesnt it make a lot more sense to change the input format to the following: num_labels in the following format id y_min x_min y_max x_max where id corresponds to the image in the batch
197322249,6469,https://api.github.com/repos/tensorflow/tensorflow/issues/6469,sharod,6,0,0,0,0,0,getting the error:tensorflow.python.framework.errors.invalidargumenterror logits and labels must have the same first dimension got logits shape and labels shape the code is available here am giving the full error log here:traceback most recent call last file ptb_word_lm.py line in module tf.app.run file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv flags_passthrough file ptb_word_lm.py line in main verbose=true file ptb_word_lm.py line in run_epoch vals session.run(fetches feed_dict file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors.invalidargumenterror logits and labels must have the same first dimension got logits shape and labels shape node train/model/sequence_loss_by_example/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits sparsesoftmaxcrossentropywithlogits t=dt_float tlabels=dt_int device=/job:localhost/replica:/task:/gpu: (train/model/add train/model/sequence_loss_by_example/reshape node train/model/truediv recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__train/model/truediv tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () caused by op utrain/model/sequence_loss_by_example/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits defined at file ptb_word_lm.py line in module tf.app.run file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv flags_passthrough file ptb_word_lm.py line in main m ptbmodel(is_training=true config=config input_=train_input file ptb_word_lm.py line in init tf.ones( batch_size num_steps dtype=data_type file usr/local/lib/python./dist-packages/tensorflow/python/ops/seqseq.py line in sequence_loss_by_example logit target file usr/local/lib/python./dist-packages/tensorflow/python/ops/nn_ops.py line in sparse_softmax_cross_entropy_with_logits precise_logits labels name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_nn_ops.py line in sparse_softmax_cross_entropy_with_logits features=features labels=labels name=name file usr/local/lib/python./dist-packages/tensorflow/python/framework/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()invalidargumenterror see above for traceback logits and labels must have the same first dimension got logits shape and labels shape node train/model/sequence_loss_by_example/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits sparsesoftmaxcrossentropywithlogits t=dt_float tlabels=dt_int device=/job:localhost/replica:/task:/gpu: (train/model/add train/model/sequence_loss_by_example/reshape node train/model/truediv recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__train/model/truediv tensor_type=dt_float device=/job:localhost/replica:/task:/cpu
197303947,6466,https://api.github.com/repos/tensorflow/tensorflow/issues/6466,robotnc,1,0,0,0,0,0,"my environment is tensorflow rc in unbuntu cuda cudnn gpu is gtx.i am using the cudnnlstm from tensorflow.contrib.cudnn_rnn package i found that the dropout setting in cudnnlstm seems take no effect and i checked that there is no test for dropout in op unit test so i write a simple code to test it the code is below import tensorflow as tf from tensorflow.contrib.cudnn_rnn import cudnnlstm class cudnn_model def init__(self,dropout self.model cudnnlstm num_layers num_units input_size input_mode skip_input direction unidirectional dropout dropout params_size_t self.model.params_size self.params tf.variable(tf.ones( params_size_t validate_shape=false def run_step(self,rnn_inputs outputs output_h output_c self.model input_data rnn_inputs input_h tf.zeros input_c tf.zeros params self.params is_training true self.outputs outputs return outputs def main inputs tf.pack m cudnn_model(dropout output m.run_step(inputs m cudnn_model(dropout output m.run_step(inputs output tf.nn.dropout(output output m.run_step(tf.nn.dropout(inputs config tf.configproto(allow_soft_placement=true config.gpu_options.allow_growth true sess tf.session(config=config sess.run(tf.initialize_all_variables for i in range out,out,out,out sess.run( output,output,output,output print try time d i print cndnn_dropout out print cudnn_dropout out print tf_out_dropout out print tf_in_dropout out returnand the result is try time cndnn_dropout cudnn_dropout tf_out_dropout tf_in_dropout try time cndnn_dropout cudnn_dropout tf_out_dropout tf_in_dropout try time cndnn_dropout cudnn_dropout tf_out_dropout tf_in_dropout try time cndnn_dropout cudnn_dropout tf_out_dropout tf_in_dropout try time cndnn_dropout cudnn_dropout tf_out_dropout tf_in_dropout from the result i see that the cudnn_dropout takes no effect the result is always same with cudnn_dropout"
197070185,6449,https://api.github.com/repos/tensorflow/tensorflow/issues/6449,k-hashimoto,12,0,0,0,0,0,thank you very much for developing nice framework i have a question about opencl support for macos is there no way to run tensorflow with gpu on recent macbookpro tensorflow support opencl but i think this is for linux not for macos tensorflow opencl need computecpp but computecpp compiler do not support macos(i couldnt find computecpp binary for mac do you have a plot to support recent macbook pros gpu
197066572,6448,https://api.github.com/repos/tensorflow/tensorflow/issues/6448,chrisranderson,3,0,0,0,0,0,before i found it really nice that i could ctrl+f to look up a function really quickly now i either have to google it or use the search bar or worse scan through the hierarchy and click a few times and scroll til i find it not a huge deal but relatively painful.it was nice to get instant search results for even partial bits of function names
196987539,6439,https://api.github.com/repos/tensorflow/tensorflow/issues/6439,carlthome,1,0,0,0,0,0,when plotting scalars in tensorboard the smoothed curve overfits the latest value it means the curve jumps around with subsequent updates and for a line fit it just plainly looks bad and feels wrong.for example when progress reporting neural networks training and plotting the loss over the training data surely the blue line will continue with a slight downwards slope given a stable learning rate but instead this happens at the end of the curve:! image
196958530,6438,https://api.github.com/repos/tensorflow/tensorflow/issues/6438,friskit-china,2,0,0,0,0,0,"hello,could you please have a look about this.i am using tf and jupyter but what makes me confuse is that the log text cannot be shown in jupyter output cell but it output correctly in ipython i think it is because of the stderr this issue have been discussed before in you add several lines to determine whether or not current context is in an interactive environment however even if i use jupyter the return value of sys.flags.interactive is still zero and the logger lever can never be setted to info and use stdout instead of stderr.thanks a lot"
196944506,6437,https://api.github.com/repos/tensorflow/tensorflow/issues/6437,sankethvedula,1,0,0,0,0,0,is there a function to implement separable convolutions with d convolutions we have a similar function in the case of d called as separable_convd()is there a similar implementations in d as well thanks in advance
196850793,6432,https://api.github.com/repos/tensorflow/tensorflow/issues/6432,xu-song,4,0,0,0,0,0,lstm_cell tf.contrib.rnn.basiclstmcell(attributeerror module object has no attribute basiclstmcell.is it been replaced by other cell
196633923,6417,https://api.github.com/repos/tensorflow/tensorflow/issues/6417,HWiese1980,1,0,0,0,0,0,"hey everyone,it seems to me like at least on windows the tf saver cant save model files whose path consists only of the files name with no parent path relative nor absolute the issue lies at or around saver.py where it tries to check whether the parent directory of the file is actually a directory there is no parent directory given i.e an empty string and as such gfile.isdirectory cant check anything it fails and raises a valueerror that the parent directory does not exist expected behavior in my opinion would be that the current working directory is used as the parent path when using no path/just a filename i.e a relative path some details about my system specs windows python tf in a virtual environment pip install tensorflow upgrade just executed issue persists)so im wondering whether this is an expected behavior and how to deal with it or if it is an actual bug that needs to be addressed"
196489807,6405,https://api.github.com/repos/tensorflow/tensorflow/issues/6405,wookayin,1,0,0,0,0,0,a bit of history until tf.split has the following signature: tf.split(split_dim num_split value name=split in commit fbdc since a new op split_v is introduced: tf.split_v(value size_splits split_dim num=none name=split_v in commit abd master tf.split_v is finally renamed to tf.split tf.split(value num_or_size_splits axis num=none name=split) due to these changes the signature of tf.split has been changed afaik tensorflow will having some of breaking changes after after the end of is this backward-incompatible api change intended?if so my suggestion is that tf.split_v which has been introduced in should not be removed in the newer versions as well in the current master tf.split_v is non-existent. attributeerror module object has no attribute split_v i am reporthing this minor issue because i am frequently switching the tensorflow versions from r stable branch to master the breaking future and thus i need a way to write a code that is both compatible in those two versions however due to the change of tf.split it seems that i cannot achieve it at the moment.thanks
196399624,6397,https://api.github.com/repos/tensorflow/tensorflow/issues/6397,Hvass-Labs,1,0,0,0,0,0,i would like to suggest the following feature be added to tensorflow and discuss it before making the pull-request as far as i can tell the feature does not exist in tensorflow motivationtotal variation denoising tvd is sometimes used as a regularizer in image processing to suppress noise it is commonly used in style transfer implementations here is my recent tutorial which uses it researching this tutorial i saw some bizarre implementations of tvd in reality it is just a simple array-slicing operation i thought it would be useful to include it in tensorflow so it is easy to use for everyone math formulathe code below implements the anisotropic version of the formula because it should be easier to optimize the formula can be seen here codethis is the code i propose to add to tensorflow def total_variation(images calculate the total variation for one or more images for use in denoising args images tensor with one or more images the shape is batch height width channel returns a scalar tensor representing the value value tf.reduce_sum(tf.abs(images images tf.reduce_sum(tf.abs(images images return valuestyle note for more complicated expressions like this i prefer to assign the value before i return it it makes the code cleaner and is helpful in debugging discussioni studied this tensorflow module but did not fully understand the design-philosophy and how to make this new function fit in.before making the pull-request i have some questions and issues to discuss would this belong under losses or regularizers in tensorflow where specifically should i add a scope-block i made an implementation that only reduced the sum for axis thus calculating the value on a per-image basis in case there are multiple images in the batch but i dont know if it would ever be used in that way and it made the implementation more complicated perhaps it is best to have the simple version above and extend it if necessary
196321463,6393,https://api.github.com/repos/tensorflow/tensorflow/issues/6393,soliders,2,0,0,0,0,0,"hi,when am run a tensorflow demo model as this have the problem descripte as follows python usr/local/lib/python./dist-packages/tensorflow/models/image/mnist/convolutional.pytraceback most recent call last file usr/local/lib/python./dist-packages/tensorflow/models/image/mnist/convolutional.py line in module tf.app.run(main=main argv= sys.argv unparsed file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv flags_passthrough file usr/local/lib/python./dist-packages/tensorflow/models/image/mnist/convolutional.py line in main train_data_filename maybe_download(train-images-idx-ubyte.gz file usr/local/lib/python./dist-packages/tensorflow/models/image/mnist/convolutional.py line in maybe_download tf.gfile.makedirs(work_directory file usr/local/lib/python./dist-packages/tensorflow/python/lib/io/file_io.py line in recursive_create_dir pywrap_tensorflow.recursivelycreatedir(compat.as_bytes(dirname status file usr/lib/python./contextlib.py line in exit self.gen.next file usr/local/lib/python./dist-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.permissiondeniederror datahow can i fix it?the tensorflow install environment as follows:ubuntu python numpy six protobuf i installed tensorflow as follows sudo h pip install tensorflow"
196280693,6387,https://api.github.com/repos/tensorflow/tensorflow/issues/6387,AndreasMadsen,1,0,0,0,0,0,the sparsemax op is an alternative to the softmax op that allows theoutput to be sparse zero properbility while stil sharing manymathematical properties with softmax.the cross entropy loss doesnt work with sparsemax as log is notdefined thus there is also a sparsemax loss function this lossfunction have a gradient equivalent to that of cross entropy whenusing softmax.original sparsemax article had some issues with getting the numerical precision good enough for assertallcloseaccordingtotype subtracting mean(logits helped a bit but not enough thus i have expanded the assert method such that the tolerance can be specified.----_this code was developed by me andreasmadsen frederikwr and marcodalfarra the original code can be found in the project was supervised by alrojo
196272912,6385,https://api.github.com/repos/tensorflow/tensorflow/issues/6385,AndreyRub,1,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system windowsinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):cuda cudnn if installed from source provide the commit hash git rev-parse head )i downloaded the latest zip on dec sorry i have no idea how to get the version the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed dec build timestamp build timestamp as int my goal is to build an android app which loads pre-trained tensorflow model and runs it on an android device i am working on android studio on windows unfortunately using android studio on linux is currently not an option.following previous advice from this forum i am trying to build an android example using bazel on windows ive successfully installed bazel using chocolatey.following the build instructions i changed the sdk and ndk paths in workspace file and ran: bazel build tensorflow/examples/android:tensorflow_demo so far i did the following to fix errors copied aapt.exe to aapt zipalign.exe to zipalign since their name is different on windows/linux installed using pacman gcc but i am stuck again i am getting the following error which i have no idea how to fix: error c:/users/andrey/appdata/local/temp/_bazel_andrey/gd-gswg/external/protobuf/build c compilation of rule protobuf//:protobuf_lite failed arm-linux-androideabi-gcc failed error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-./prebuilt/windows-x_/bin/arm-linux-androideabi-gcc fstack-protector-strong fpic ffunction-sections funwind-tables remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status external/protobuf/src/google/protobuf/stubs/structurally_valid.cc fatal error opening dependency file bazel-out/android-arm-linux-androideabi-.-va-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/stubs/structurally_valid.d no such file or directory namespace google compilation terminated.error c:/tools/tensorflow-master/tensorflow/examples/android/build output tensorflow/examples/android/tensorflow_demo_symbols/r.txt was not created.error c:/tools/tensorflow-master/tensorflow/examples/android/build output tensorflow/examples/android/tensorflow_demo.srcjar was not created.error c:/tools/tensorflow-master/tensorflow/examples/android/build output tensorflow/examples/android/proguard/tensorflow_demo/_tensorflow_demo_proguard.cfg was not created.error c:/tools/tensorflow-master/tensorflow/examples/android/build output tensorflow/examples/android/tensorflow_demo_processed_manifest/androidmanifest.xml was not created.error c:/tools/tensorflow-master/tensorflow/examples/android/build output tensorflow/examples/android/tensorflow_demo_files/resource_files.zip was not created.error c:/tools/tensorflow-master/tensorflow/examples/android/build output tensorflow/examples/android/tensorflow_demo.ap was not created.error c:/users/andrey/appdata/local/temp/_bazel_andrey/gd-gswg/external/protobuf/build output external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/stubs/stringprintf.o was not created.error c:/users/andrey/appdata/local/temp/_bazel_andrey/gd-gswg/external/protobuf/build output external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/arenastring.o was not created.error c:/users/andrey/appdata/local/temp/_bazel_andrey/gd-gswg/external/protobuf/build output external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/strtod.o was not created.error c:/users/andrey/appdata/local/temp/_bazel_andrey/gd-gswg/external/protobuf/build output external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.o was not created.target tensorflow/examples/android:tensorflow_demo failed to builduse verbose_failures to see the command lines of failed build steps.info elapsed time s critical path s please advice how to proceed isnt there an easier way to achieve what i want i just need pre-built tensorflow binaries for android studio on windows to build an example application so far i found only pre-built python distribution and bazel installation on windows
196272459,6384,https://api.github.com/repos/tensorflow/tensorflow/issues/6384,noammor,1,0,0,0,0,0,"code import tensorflow as tf x tf.placeholder(dtype=tf.int a tf.random_normal x b tf.random_normal c tf.einsum(ijk,kl->ijl a b with tf.session as sess print(sess.run(c x output traceback most recent call last file home/noam/code/test/test_einsum.py line in module c tf.einsum(ijk,kl->ijl a b file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/ops/special_math_ops.py line in einsum axes_to_sum file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/ops/special_math_ops.py line in einsum_reduction product reshape_if_necessary(product uncompacted_shape file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/ops/special_math_ops.py line in reshape_if_necessary return array_ops.reshape(tensor new_shape file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in reshape name=name file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op raise err file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op preferred_dtype=default_dtype file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in listcomp tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file home/noam/miniconda/envs/ml/lib/python./site-packages/tensorflow/python/util/compat.py line in as_bytes bytes_or_text typeerror expected binary or unicode string got using tensorflow version rc also tested and fails the same way on rc"
196237829,6379,https://api.github.com/repos/tensorflow/tensorflow/issues/6379,nmiculinic,2,0,0,0,0,0,tf.nn.convd(value filters stride padding use_cudnn_on_gpu=none data_format=none name=none)tf.nn.convd(input filter strides padding use_cudnn_on_gpu=none data_format=none name=none)tf.nn.convd(input filter strides padding name=none) theres discrepancy between parameter naming between convd and d counterparts firstly value vs input and then singular vs plural filter(s singular would be correct as in d/s case since function accepts only single filter if i understood documentation correctly
196062467,6360,https://api.github.com/repos/tensorflow/tensorflow/issues/6360,danijar,1,2,0,0,0,0,especially when integrating tensorflow into an exiting multi-threaded application its not always easy to use queues for synchronization currently we must use python locks to lock the sess.run calls from different threads exposing a tensorflow lock interface could allow to synchronize access only to needed values of the session: pythonx tf.placeholder(tf.float none y tf.placeholder(tf.float none w tf.variable(tf.zeros b tf.variable(tf.zeros( ))lock tf.lock()with lock pred tf.nn.softmax(tf.matmul(x w b)loss tf.reduce_mean(tf.reduce_sum(y tf.log(pred with lock train_step tf.train.gradientdescentoptimizer(.).minimize(loss)def inference_thread while true generate data sess.run(pred data)def training_thread while true generate data sess.run(train_step data) or simpler: pythonwith tf.lock w tf.variable(tf.zeros
195960228,6349,https://api.github.com/repos/tensorflow/tensorflow/issues/6349,craftlk,0,0,0,0,1,0,"hi,all,thanks for your time.im guest of a server with intel xeon phi processors(knl with centos.i set my own env at first and compile bazel and tensorflow(v..rc successfully(cc=gcc i set my own env as follows(most of it):export home=/home/guestexport home_usr=$home/usrexport java_home=$home/jdk.._export path=$home/intel/bin:$home/bazel_bin:$home_usr/bin:$java_home/bin:$pathexport classpath=.:$java_home/lib/dt.jar:$java_home/lib/tools.jarexport cpath=$home_usr/include:$home/intel/includeexport c_include_path=$cpathexport cplus_include_path=$cpathexport library_path=$home_usr/lib:$home_usr/lib:$home/intel/lib/intel_lin_mic/export ld_library_path=$library_pathexport ld_run_path=$library_pathexport compiler_path=$pathexport cc=gccexport pkg_config_path=$home_usr/lib/pkgconfig:$pkg_config_pathexport perllib=$home_usr()bazel is ok compile.sh()tensorflow is ok configure bazel build c opt tensorflow/tools/pip_package:build_pip_package()then the problem comes,i change the env cc=icpc and run:bazel build c opt copt=-xmic-avx tensorflow/tools/pip_package:build_pip_package error home/guest/.cache/bazel/_bazel_guest/acaabfbebeae/external/gif_archive/build c compilation of rule gif_archive//:gif failed icpc failed error executing command cd home/guest/.cache/bazel/_bazel_guest/acaabfbebeae/execroot/tensorflow exec env ld_library_path=/home/guest/usr/lib:/home/guest/usr/lib:/home/guest/cuda/lib:/home/guest/cuda/lib:/home/guest/cudnn:/home/guest/intel/lib/intel_lin_mic path=/home/guest/intel/bin:/home/guest/bazel_bin:/home/guest/usr/bin:/home/guest/cuda/bin:/home/guest/jdk.._/bin:/usr/lib/mpich/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/guest/.local/bin:/home/guest/bin home/guest/intel/bin/icpc u_fortify_source d_fortify_source fstack-protector wall wl,-z,-relro,-z,now b/home/guest/intel/bin b/usr/bin wunused-but-set-parameter fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections g md mf bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/gif_font.d iquote external/gif_archive iquote bazel-out/host/genfiles/external/gif_archive iquote external/bazel_tools iquote bazel-out/host/genfiles/external/bazel_tools isystem external/gif_archive isystem bazel-out/host/genfiles/external/gif_archive isystem external/bazel_tools/tools/cpp/gcc wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted c external/gif_archive/gif_font.c o bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/gif_font.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status icpc command line warning ignoring unknown option wno-builtin-macro-redefined warning predefined meaning of date discarded warning predefined meaning of timestamp discarded warning predefined meaning of time discarded external/gif_archive/gif_font.c error a value of type void cannot be assigned to an entity of type char dup malloc(strlen(legend compilation aborted for external/gif_archive/gif_font.c code target tensorflow/tools/pip_package:build_pip_package failed to build()ive found a similar problem here i dont know how to solve this problem?i update bazel by git checkout tags under bazel and try to compile.sh again(cc=icpc),but it doesnt work"
195953206,6345,https://api.github.com/repos/tensorflow/tensorflow/issues/6345,KeithYJohnson,1,0,0,0,0,0,environment infooperating system osx the output from python c import tensorflow print(tensorflow.__version__) . python c import tensorflow print(tensorflow.__version__) .zsh command not found rc. im going through the udacity deep learning class and getting an error when evaluating logits tf.nn.xw_plus_b(tf.concat_v(outputs w b) which was changed in import tensorflow tensorflow.__version__..-rc tensorflow.concat_vtraceback most recent call last file stdin line in module>attributeerror module tensorflow has no attribute concat_v tensorflow.concat>>> i installed and am running the latest release rc where concat_v was added in has anyone else seen this thanks in advance
195903881,6342,https://api.github.com/repos/tensorflow/tensorflow/issues/6342,amirj,2,0,0,0,0,0,operating system mac os installed version of cuda and cudnn cuda cudnn-.-osx-x-v please attach the output of ls l path/to/cuda/lib/libcud lrwxr-xr-x root admin b dec usr/local/cuda/lib/libcuda..dylib libcuda.dylib rwxr-xr-x root wheel k nov usr/local/cuda/lib/libcuda.dylib lrwxr-xr-x root wheel b nov usr/local/cuda/lib/libcudadevrt.a developer/nvidia/cuda-./lib/libcudadevrt.a lrwxr-xr-x root wheel b nov usr/local/cuda/lib/libcudart...dylib developer/nvidia/cuda-./lib/libcudart...dylib lrwxr-xr-x root wheel b nov usr/local/cuda/lib/libcudart.dylib developer/nvidia/cuda-./lib/libcudart.dylib lrwxr-xr-x root wheel b nov usr/local/cuda/lib/libcudart_static.a developer/nvidia/cuda-./lib/libcudart_static.a rwxr-xr-x amirhj admin m jul usr/local/cuda/lib/libcudnn..dylib lrwxr-xr-x amirhj admin b jul usr/local/cuda/lib/libcudnn.dylib libcudnn..dylib rw-r--r amirhj admin m jul usr/local/cuda/lib/libcudnn_static.a if installed from binary pip package provide a link to the pip package you installed tensorflow-gpu==..rc the output from python c import tensorflow print(tensorflow.__version rcrunning text classification using convolutional neural networks on characters python test.py i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.dylib locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.dylib locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.dylib locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda..dylib locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.dylib locally warning:tensorflow:using temporary folder as model directory var/folders/gy/wbynkqlwvtcphgn/t/tmpuwjvq warning:tensorflow:from test.py in main calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after instructions for updating estimator is decoupled from scikit learn interface by moving into separate class skcompat arguments x y and batch_size are only available in the skcompat class estimator will only accept input_fn example conversion est estimator est skcompat(estimator warning:tensorflow:from test.py in main calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after instructions for updating estimator is decoupled from scikit learn interface by moving into separate class skcompat arguments x y and batch_size are only available in the skcompat class estimator will only accept input_fn example conversion est estimator est skcompat(estimator traceback most recent call last file test.py line in module tf.app.run(main=main argv= sys.argv unparsed file usr/local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv flags_passthrough file test.py line in main classifier.fit(x_train y_train steps file usr/local/lib/python./site-packages/tensorflow/python/util/deprecation.py line in new_func return func(*args kwargs file usr/local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in fit max_steps=max_steps file usr/local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in train_model train_ops self._get_train_ops(features labels file usr/local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in get_train_ops return self._call_model_fn(features labels model_fn_lib.modekeys.train file usr/local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in call_model_fn model_fn_results self._model_fn(features labels file test.py line in char_cnn_model byte_list n_filters filter_shape padding=valid file usr/local/lib/python./site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py line in func_with_args return func(*args current_args file usr/local/lib/python./site-packages/tensorflow/contrib/layers/python/layers/layers.py line in convolution trainable=trainable file usr/local/lib/python./site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py line in func_with_args return func(*args current_args file usr/local/lib/python./site-packages/tensorflow/contrib/framework/python/ops/variables.py line in model_variable caching_device=caching_device device=device file usr/local/lib/python./site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py line in func_with_args return func(*args current_args file usr/local/lib/python./site-packages/tensorflow/contrib/framework/python/ops/variables.py line in variable caching_device=caching_device file usr/local/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in get_variable custom_getter=custom_getter file usr/local/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in get_variable custom_getter=custom_getter file usr/local/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in get_variable validate_shape=validate_shape file usr/local/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in true_getter caching_device=caching_device validate_shape=validate_shape file usr/local/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in get_single_variable expected_shape=shape file usr/local/lib/python./site-packages/tensorflow/python/ops/variables.py line in init expected_shape=expected_shape file usr/local/lib/python./site-packages/tensorflow/python/ops/variables.py line in init_from_args initial_value name=initial_value dtype=dtype file usr/local/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in lambda shape.as_list dtype=dtype partition_info=partition_info file usr/local/lib/python./site-packages/tensorflow/contrib/layers/python/layers/initializers.py line in initializer raise typeerror(cannot create initializer for non-floating point type typeerror cannot create initializer for non-floating point type
195819010,6336,https://api.github.com/repos/tensorflow/tensorflow/issues/6336,lienhua34,1,0,0,0,0,0,"i used example/export_half_plus_two.py to test session_bundle i run the below command to write v checkpoint files in export path, shellrootbdeeae:/tensorflow/model-export/session_bundle python export_half_plus_two.py use_checkpoint_v=truecopying asset files to tmp/half_plus_two/-tmp/assetscopying asset file hello.txtcopying asset file hello.txt the export directory is shellroot@bdeeae:/tmp/half_plus_two ls ltotal drwxr-xr-x root root dec assets-rw-r--r root root dec checkpoint-rw-r--r root root dec export.data--of--rw-r--r root root dec export.index-rw-r--r root root dec export.meta then i write a import python file the content is pythonimport tensorflow as tfimport numpy as npfrom tensorflow.contrib.session_bundle import session_bundlefrom tensorflow.contrib.session_bundle import constantsfrom tensorflow.contrib.session_bundle import manifest_pbexport_dir tmp/half_plus_two/tf.reset_default_graph()sess meta_graph_def session_bundle.load_session_bundle_from_path export_dir target config=tf.configproto(device_count={cpu with sess.as_default collection_def meta_graph_def.collection_def signatures_any collection_def constants.signatures_key .any_list.value print(signatures length d\n len(signatures_any signatures manifest_pb.signatures signatures_any .unpack(signatures named_signatures signatures.named_signatures input_name named_signatures inputs .generic_signature.map x .tensor_name output_name named_signatures outputs .generic_signature.map y .tensor_name y sess.run( output_name input_name np.array print(y ) it will tell me the variables are uninitialized, shellrootbdeeae:/tensorflow/model-export/session_bundle python import_half_plus_two.py signatures length traceback most recent call last file import_half_plus_two.py line in module y sess.run(output_name input_name np.array( ) file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors_impl.failedpreconditionerror attempting to use uninitialized value b node b/read identityt=dt_float class=loc:b device=/job:localhost/replica:/task:/cpu:(b)caused by op ub/read defined at file import_half_plus_two.py line in module export_dir target config=tf.configproto(device_count=cpu file usr/local/lib/python./dist-packages/tensorflow/contrib/session_bundle/session_bundle.py line in load_session_bundle_from_path saver tf.train.import_meta_graph(meta_graph_def file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in import_meta_graph kwargs file usr/local/lib/python./dist-packages/tensorflow/python/framework/meta_graph.py line in import_scoped_meta_graph producer_op_list=producer_op_list file usr/local/lib/python./dist-packages/tensorflow/python/framework/importer.py line in import_graph_def op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()failedpreconditionerror see above for traceback attempting to use uninitialized value b node b/read identityt=dt_float class=loc:b device=/job:localhost/replica:/task:/cpu:(b) if i changed the use_checkpoint_v option to false there would be no error. shellrootbdeeae:/tensorflow/model-export/session_bundle python export_half_plus_two.py use_checkpoint_v=falsewarning:tensorflow: *warning:tensorflow:tensorflows v checkpoint format has been deprecated.warning:tensorflow:consider switching to the more efficient v format:warning:tensorflow tf.train.saver(write_version=tf.train.saverdef.v) warning:tensorflow:now on by default.warning:tensorflow: *copying asset files to tmp/half_plus_two/-tmp/assetscopying asset file hello.txtcopying asset file hello.txtrootbdeeae:/tensorflow/model-export/session_bundle rootbdeeae:/tensorflow/model-export/session_bundle python import_half_plus_two.py signatures length rootbdeeae:/tensorflow/model-export/session_bundle at this time the export directory is shellroot@bdeeae:/tmp/half_plus_two ls ltotal drwxr-xr-x root root dec assets-rw-r--r root root dec checkpoint-rw-r--r root root dec export--of--rw-r--r root root dec export.meta i found that the diff between v checkpoint file and v checkpoint is a substring data in v checkpoint filename just see the above example the v checkpoint filename is export.data--of and the v checkpoint filename is export--of so i moved to read the load function load_session_bundle_from_path i found a bug at l see following code snippet pythonif not file_io.file_exists(variables_filename variables_filename os.path.join export_dir constants.variables_filename_pattern if not file_io.get_matching_files(variables_filename if graph_util.convert_variables_to_constants is called on a model it wont have any variables and thats ok todo(yxshi verify that the graph_def in fact does not have any reachable variables variables_filename none where the constant constants.variables_filename_pattern defined in constant.py is export-????-of it cannot match the v checkpoint filename so the load function cannot restore the v checkpoint file"
195805324,6334,https://api.github.com/repos/tensorflow/tensorflow/issues/6334,carlthome,2,0,0,0,0,0,i noticed in that some attempts to reduce the pypi package have been made but since the package has grown quite a bit.are there any ongoing attempts of creating a smaller tensorflow runtime that can be used for putting graphs into production for example the most common use case for tensorflow is probably running a machine learning models forward pass for that all of the gradient related code could be excluded from the package no?preferably id love a utility that takes in a graph protobuf and spits out a pip installable tensorflow version with only the required parts of the codebase is this at all doable
195778655,6331,https://api.github.com/repos/tensorflow/tensorflow/issues/6331,zshareef,11,0,0,0,0,0,"hi,i want to install tensorflow on mac currently i am using macos sierra version python was already installed in this current version my python version is i followed the instructions given on the webpage of tensorflow to install it on my mac when i entered the command pip install tensorflow then i have the following error pip install tensorflowcollecting tensorflow downloading tensorflow-..rc-cp-cpm-macosx___x_.whl mb mb kb/s collecting numpy from tensorflow downloading numpy-..-cp-cpm-macosx___intel.macosx___intel.macosx___x_.macosx___intel.macosx___x_.whl mb mb kb/s requirement already satisfied six in library/python/./site-packages/six-..-py..egg from tensorflow)collecting mock from tensorflow downloading mock-..-py.py-none-any.whl kb kb mb/s collecting wheel from tensorflow downloading wheel-..-py.py-none-any.whl kb kb mb/s collecting protobuf from tensorflow downloading protobuf-..-py.py-none-any.whl kb kb mb/s collecting funcsigs python_version from mock>=..->tensorflow downloading funcsigs-..-py.py-none-any.whlcollecting pbr from mock>=..->tensorflow downloading pbr-..-py.py-none-any.whl kb kb mb/s requirement already satisfied setuptools in system/library/frameworks/python.framework/versions/./extras/lib/python from protobuf==..->tensorflow)installing collected packages numpy funcsigs pbr mock wheel protobuf tensorflow found existing installation numpy rc deprecation uninstalling a distutils installed project numpy has been deprecated and will be removed in a future version this is due to the fact that uninstalling a distutils project will only partially uninstall the project uninstalling numpy-..rc:exception:traceback most recent call last file library/python/./site-packages/pip-..-py..egg/pip/basecommand.py line in main status self.run(options args file library/python/./site-packages/pip-..-py..egg/pip/commands/install.py line in run prefix=options.prefix_path file library/python/./site-packages/pip-..-py..egg/pip/req/req_set.py line in install requirement.uninstall(auto_confirm=true file library/python/./site-packages/pip-..-py..egg/pip/req/req_install.py line in uninstall paths_to_remove.remove(auto_confirm file library/python/./site-packages/pip-..-py..egg/pip/req/req_uninstall.py line in remove renames(path new_path file library/python/./site-packages/pip-..-py..egg/pip/utils/__init__.py line in renames shutil.move(old new file system/library/frameworks/python.framework/versions/./lib/python./shutil.py line in move copy(src real_dst file system/library/frameworks/python.framework/versions/./lib/python./shutil.py line in copy copystat(src dst file system/library/frameworks/python.framework/versions/./lib/python./shutil.py line in copystat os.chflags(dst st.st_flags)oserror errno operation not permitted var/folders/jk/sngqqlsvvjzpwgn/t/pip-hpgfve-uninstall/system/library/frameworks/python.framework/versions/./extras/lib/python/numpy-..rc-py..egg-infoi think there is some issue with numpy my current numpy version is rc and it was already installed with macos sierra i tried to uninstall it so that tensorflow install the new version by itself but no success.i tried different solutions available on the web to solve this issue but no success.could anybody tell me that how i can solve this issue and install tensorfow.thanks.zeeshan"
195737159,6327,https://api.github.com/repos/tensorflow/tensorflow/issues/6327,micahstubbs,1,0,0,0,0,0,a feature request for the embedding projector show vector labels as plot axis labels on custom projection viewminimal example mocked up on a screenshot:
195671799,6322,https://api.github.com/repos/tensorflow/tensorflow/issues/6322,danielgordon10,42,0,0,0,0,0,i spent a long time yesterday trying to understand how to get something to show up in the embedding tab on tensorboard it is not very well documented and there are no code examples except for the tiny incomplete snippets in the tutorial i was finally able to get something working but i think it would be helpful for future users to have a real code tutorial i could pretty easily write one for the mnist example since i already have my own working is that something that people will want
195457761,6299,https://api.github.com/repos/tensorflow/tensorflow/issues/6299,AnishShah,0,0,0,1,0,0,fixes fixes this is still not a complete i wanted to know whether this is the correct way i ran the following code and it worked without any error.if it is correct then i will add the same for avg_pool too. in import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyin x tf.placeholder(tf.float shape in y tf.nn.max_pool(x ksize strides padding=valid)in dy tf.gradients(y x)in ddy tf.gradients(dy x
195249722,6281,https://api.github.com/repos/tensorflow/tensorflow/issues/6281,PatWie,1,0,0,0,0,0,environment infooperating system:ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): /path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudadevrt.a/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart.so libcudart.so../path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart.so libcudart.so.../path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart.so.../path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart_static.a/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so libcudnn.so./path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so libcudnn.so.../path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so.../path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so.../path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn_static.a if installed from source provide the commit hash git rev-parse head )cdfefedfdeccfeac the output of bazel version build label fdd)build target bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time tue oct build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonimport tensorflow as tf@tf.contrib.framework.add_arg_scopedef my_func(*args kwargs pass logs or other output that would be helpful(if logs are large please upload as attachment or provide link). traceback most recent call last file stdin line in module file home/user/.local/lib/python./site-packages/tensorflow/contrib/__init__.py line in module from tensorflow.contrib import factorization file home/user/.local/lib/python./site-packages/tensorflow/contrib/factorization/__init__.py line in module from tensorflow.contrib.factorization.python.ops.gmm import file home/user/.local/lib/python./site-packages/tensorflow/contrib/factorization/python/ops/gmm.py line in module from tensorflow.contrib.learn.python.learn.estimators import estimator file home/user/.local/lib/python./site-packages/tensorflow/contrib/learn/__init__.py line in module from tensorflow.contrib.learn.python.learn import file home/user/.local/lib/python./site-packages/tensorflow/contrib/learn/python/__init__.py line in module from tensorflow.contrib.learn.python.learn import file home/user/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/__init__.py line in module from tensorflow.contrib.learn.python.learn import datasets file home/user/.local/lib/python./site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py line in module from tensorflow.contrib.learn.python.learn.datasets import syntheticimporterror cannot import name synthetic
195216748,6279,https://api.github.com/repos/tensorflow/tensorflow/issues/6279,ludovic-pl,2,0,0,0,0,0,"hello,when i run this following python code i get the error module object has no attribute batch_matmul a tf.batch_matmul(none,none) nb:i use the bazel build command for cpu optimization environment infooperating system debian jessyif installed from source provide the commit hash git rev-parse head )cdfefedfdeccfeac"
195101751,6269,https://api.github.com/repos/tensorflow/tensorflow/issues/6269,jramapuram,3,0,0,0,0,0,there doesnt seem to be a tf.random_shuffle impl for the gpu some of the work i do can utilize a gpu shuffle.running tf.rc cuda cudnn titan x non-pascal) bashinvalidargumenterror see above for traceback cannot assign a device to node randomshuffle could not satisfy explicit device specification device:gpu because no supported kernel for gpu devices is available node randomshuffle randomshuffle t=dt_float seed seed device=/device:gpu: (transpose
195080234,6268,https://api.github.com/repos/tensorflow/tensorflow/issues/6268,kmalakoff,2,0,0,0,0,0,i have started porting tensorflow to node.js and was wondering about the status of gradients support in the c and c apis could i ask for some help and or offer some assistance a couple of ideas i would be happy to test the work-in-progress api in node.js using either of both c or c i have tried both apis and would be happy to maintain both as a test case i would be happy to receive some guidance from someone on the tensorflow team on how to implement gradients training without an official api i see from this issue that it might be possible but it has been hard to study the python and ocaml code to understand what to do some pseudocode or links to existing code that would need to replicated could be enough something else for example i could help move the gradient api forward by working with someone on the tensorflow team.let me know if you have any suggestions for a path forward thank you
194991229,6266,https://api.github.com/repos/tensorflow/tensorflow/issues/6266,muneeb699,0,0,0,0,1,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system:using docker and tensorflow is installed in a container with ipython notebook installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) ...if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried?i tried the solution provided on stack overflow i also uninstall pillow package and the install it but no result logs or other output that would be helpful(if logs are large please upload as attachment or provide link).! capture
194990354,6265,https://api.github.com/repos/tensorflow/tensorflow/issues/6265,victorcampos7,4,0,0,0,0,0,currently there is support for audio decoding using ffmmpeg through tf.contrib.ffmpeg.decode_audio would it be possible to add something similar for decoding video files?thanks
194960506,6264,https://api.github.com/repos/tensorflow/tensorflow/issues/6264,BingzheWu,5,0,0,0,0,0,is there any solution to freeze weights for example i want to keep some weights unchanged when them satisfy some condition(like weight during training
194827795,6252,https://api.github.com/repos/tensorflow/tensorflow/issues/6252,MattKleinsmith,2,0,0,0,0,0,the bugan error box pops up whenever jupyter tries to save the hello_tensorflow notebook environment infoubuntu nvidia driver geforce gtx minimal reproducible example nvidia-docker run it p e password=hi gcr.io/tensorflow/tensorflow:latest-gpu logs nvidia-docker run it p e password=hi gcr.io/tensorflow/tensorflow:latest-gpu ... e notebookapp notebook json is invalid additional properties are not allowed umetadata was unexpected failed validating uadditionalproperties in stream on instance ucells uoutputs umetadata uname ustdout uoutput_type ustream utext uresult n} ! screenshot from why i posted this herethe bug occurs on an official tensorflow docker image dockerless version of the bug minimal reproducible example wget notebook port hello_tensorflow.ipynb
194738431,6233,https://api.github.com/repos/tensorflow/tensorflow/issues/6233,zsxgb,1,0,0,0,0,0,i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc couldnt open cuda library libcuda.so ld_library_path usr/local/cuda/lib:/usr/local/cuda/extras/cupti/lib:i tensorflow/stream_executor/cuda/cuda_diagnostics.cc hostname zsx-all-seriesi tensorflow/stream_executor/cuda/cuda_diagnostics.cc libcuda reported version is not found was unable to find libcuda.so dso loaded into this programi tensorflow/stream_executor/cuda/cuda_diagnostics.cc driver version file contents nvrm version nvidia unix x kernel module mon oct pdt gcc version gcc version ubuntu ubuntu i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel reported version is i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc ld_library_path usr/local/cuda/lib:/usr/local/cuda/extras/cupti/lib:i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc failed to find libcuda.so on this system failed precondition could not dlopen dso libcuda.so dlerror libnvidia-fatbinaryloader.so cannot open shared object file no such file or directoryi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally
194731109,6230,https://api.github.com/repos/tensorflow/tensorflow/issues/6230,mzduchon,2,0,0,0,0,0,the second and third colors available in color by are extremely similar when there is overlap the second color orangish appears very similar to third redish one this prevents any pop-out effect
194522211,6207,https://api.github.com/repos/tensorflow/tensorflow/issues/6207,Conchylicultor,3,0,0,0,0,0,i had a look at the issues and i dont think this has been submitted yet sorry if i missed one.the cropping operators like tf.image.resize_image_with_crop_or_pad tf.image.crop_to_bounding_box etc only support d tensors it would be useful to support d tensors for those operators specially when there are used after the resizing operators which support d tensor.edit i just saw there was extract_glimpse or crop_and_resize which could works for d tensors its confusing to have different functions with different name which seems to perform the same operation with just a different input rankedit its not exactly the same functions extract_glimpse and crop_and_resize both require normalized coordinates crop_to_bounding_box use pixel values it would be good to have a function which allows both pixel based coordinates and d input tensors but anyway at least there is a way to do what i wanted
194496541,6202,https://api.github.com/repos/tensorflow/tensorflow/issues/6202,yaroslavvb,5,0,1,0,0,0,this cl has removed shape argument from ones_initializer however this argument remains for zeros_initializer .this is a breaking change so maybe it should be mentioned in release.md with instructions on how code should transition ie ones_initializer should be replaced with ones_initializer currently older code that works in fails in with obscure messageie update tf.get_variable(name=update shape= params_size dtype=dtype initializer=tf.ones_initializer) fails with initial_value name=initial_value dtype=dtype file home/yaroslav/.conda/envs/openai/lib/python./site-packages/tensorflow/python/ops/variable_scope.py line in lambda shape.as_list dtype=dtype partition_info=partition_info)typeerror ones_initializer got multiple values for argument dtype @itsmeolivia
194303257,6189,https://api.github.com/repos/tensorflow/tensorflow/issues/6189,nasimrahaman,6,0,0,0,0,0,problem:tensorflow doesnt place ops e.g mul in pre-existing variable scopes and automatically creates a new scope instead minimal reproducible example pythonwith tf.variable_scope(layer v tf.get_variable(v initializer=tf.constant_initializer tf.float w v print(w.name prints layer/mul: however pythonwith tf.variable_scope(layer v tf.get_variable(v initializer=tf.constant_initializer tf.float))with tf.variable_scope(layer w v print(w.name prints layer_/mul: observe that for the latter the op w is placed in a different variable scope auto-named layer_ .ive tried the following to the same effect: pythonwith tf.variable_scope(layer as scope v tf.get_variable(v initializer=tf.constant_initializer tf.float))with tf.variable_scope(scope w v print(w.name prints layer_/mul: pythonwith tf.variable_scope(layer v tf.get_variable(v initializer=tf.constant_initializer tf.float))with tf.variable_scope(layer reuse=true w v print(w.name prints layer_/mul versionspectensorflow version gpu)os ubuntu w cuda
193989289,6150,https://api.github.com/repos/tensorflow/tensorflow/issues/6150,ppwwyyxx,2,0,0,0,0,0,with the new tf.summary module the name shown in tensorboard has to be equal to the summary op in the graph.as a result the name shown in tensorboard cannot be the same as any existing op in the graph.this can create a bit of problem before this change i could have some op named loss and the summary of its output also named loss which is very consistent and easy to play with i can just look for the name of the tensor in tensorboard.now it looks like i have to add a prefix/suffix to everything because they cannot use the same name otherwise i got loss in tensorboard which is confusing especially to new users.is there some changes that could be done to the new summary module that keeps the nice old feature
193963257,6143,https://api.github.com/repos/tensorflow/tensorflow/issues/6143,kohpangwei,4,0,0,0,0,0,currently neither tf.nn.avg_pool or tf.nn.max_pool support automatically taking second derivatives making it difficult to experiment with second-order methods on neural networks that use any sort of pooling which unfortunately includes most modern networks trying to take a second derivative gives a lookuperror e.g lookuperror no gradient defined for operation gradients_...avgpoolgrad op type avgpoolgrad do you have any plans for implementing second derivatives for pooling operations or any suggested workarounds like rewriting the pooling operations in terms of elementary functions that have second derivatives defined thank you
193962803,6142,https://api.github.com/repos/tensorflow/tensorflow/issues/6142,rohitgirdhar,1,0,0,0,0,0,unable to read the new checkpoints v format that became default from r release using newcheckpointreader this breaks the tf slim api specifically what related github issues or stackoverflow threads have you found by searching the web for your problem started that thread and was suggested to post an issue here environment infooperating system centos installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud cuda cudnn if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) pythonimport tensorflow as tfw tf.variable(tf.truncated_normal(shape name=w)saver tf.train.saver()sess tf.session()sess.run(tf.initialize_all_variables())saver.save(sess my-model)reader tf.train.newcheckpointreader(my-model.index errorprint reader.get_variable_to_shape_map() the error is: txttraceback most recent call last file save_and_restore.py line in module reader tf.train.newcheckpointreader(my-model.index file anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in newcheckpointreader return checkpointreader(compat.as_bytes(filepattern status file anaconda/lib/python./contextlib.py line in exit self.gen.next file anaconda/lib/python./site-packages/tensorflow/python/framework/errors_impl.py line in raise_exception_on_not_ok_status pywrap_tensorflow.tf_getcode(status))tensorflow.python.framework.errors_impl.notfounderror unsuccessful tensorslicereader constructor failed to find any matching files for my-model.index
193958118,6141,https://api.github.com/repos/tensorflow/tensorflow/issues/6141,rw,1,0,0,0,0,0,are there plans to implement directional statistical distributions in particular im looking to get a von mises-fisher distribution into contrib/distributions/python .im beginning work on a vmf distribution myself but as a new tf user my progress is likely to be slow.(note that scipy.stats implements a basic von mises distribution.)references: wikipedia von misesfisher distribution directional statistics in machine learning a brief review
193815198,6119,https://api.github.com/repos/tensorflow/tensorflow/issues/6119,jacktang,5,0,0,0,0,0,"hello,today i upgrade tensorflow package and read the tutorials from the beginning find the result of classifier prediction is different it is i remember that the old version tf run out so whats the problem?the os is osx ei capitan and tensoflow is rc python c import tensorflow print(tensorflow.__version__) -bash rc command not found if i turn on the info log level the log lists below python iris_classifier.pywarning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in fit calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in fit calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in fit calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with batch_size is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow: *warning:tensorflow:tensorflows v checkpoint format has been deprecated.warning:tensorflow:consider switching to the more efficient v format:warning:tensorflow tf.train.saver(write_version=tf.train.saverdef.v) warning:tensorflow:now on by default.warning:tensorflow: *warning:tensorflow: *warning:tensorflow:tensorflows v checkpoint format has been deprecated.warning:tensorflow:consider switching to the more efficient v format:warning:tensorflow tf.train.saver(write_version=tf.train.saverdef.v) warning:tensorflow:now on by default.warning:tensorflow: *warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in evaluate calling evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in evaluate calling evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in evaluate calling evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with batch_size is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))accuracy warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in predict calling predict from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in predict calling predict from tensorflow.contrib.learn.python.learn.estimators.estimator with batch_size is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))warning:tensorflow:from library/python/./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py in predict calling predict from tensorflow.contrib.learn.python.learn.estimators.estimator with as_iterable is deprecated and will be removed after instructions for updating:estimator is decoupled from scikit learn interface by moving intoseparate class skcompat arguments x y and batch_size are onlyavailable in the skcompat class estimator will only accept input_fn.example conversion est estimator est skcompat(estimator(...))predictions"
193677245,6111,https://api.github.com/repos/tensorflow/tensorflow/issues/6111,xuancong84,2,0,0,0,0,0,to prevent tensorflow tf from allocating the totality of graphic memory i always use the following options when creating sessions: config tf.configproto()config.gpu_options.allow_growth truesess tf.session(config=config) however doing so causes some experiments to run out of memory while not doing so will not cause memory overflow for example when running experiments involving rnn such as translate.py or ptb_word_lm.py in the sample code if i specify allow_growth=true i always encounter the following: training epoch learning_rate i tensorflow/core/common_runtime/gpu/pool_allocator.cc poolallocator after get requests put_count evicted_count eviction_rate and unsatisfied allocation rate i tensorflow/core/common_runtime/gpu/pool_allocator.cc raising pool_size_limit from to i tensorflow/core/common_runtime/gpu/pool_allocator.cc poolallocator after get requests put_count evicted_count eviction_rate and unsatisfied allocation rate i tensorflow/core/common_runtime/gpu/pool_allocator.cc raising pool_size_limit from to i tensorflow/core/common_runtime/gpu/pool_allocator.cc poolallocator after get requests put_count evicted_count eviction_rate and unsatisfied allocation rate i tensorflow/core/common_runtime/gpu/pool_allocator.cc raising pool_size_limit from to e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory e tensorflow/stream_executor/cuda/cuda_driver.cc failed to allocate g bytes from device cuda_error_out_of_memory however without specifying allow_growth=true i can run it successfully moreover the oom occurs only after going through some epoches in the training data not right from the beginning.in principle for an ideal memory manager whether oom will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically thus tensorflows low-level memory management code must be flawed in one way or another.below are my system info: operating system:ubuntu ltsinstalled version of cuda and cudnn:/usr/local/cuda-.cudnn-.-linux-x-v..tgzit is installed from binary pip packagea link to the pip package you installed output from python c import tensorflow print(tensorflow.version)xuancong@wxc-ir:~/projects/tf-rnnlm python c import tensorflow print(tensorflow.version)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally
193604950,6095,https://api.github.com/repos/tensorflow/tensorflow/issues/6095,jphalip,6,0,0,0,0,0,tensorflow has a number of built-in trigonometry functions like atan but it doesnt have atan yet.this can be achieved with some custom code as pointed in this comment actually a small mistake on the first line where np.pi should not be added the corrected code is presented below: pythondef atan(y x angle tf.select(tf.greater(x tf.atan(y/x tf.zeros_like(x angle tf.select(tf.logical_and(tf.less(x tf.greater_equal(y tf.atan(y/x np.pi angle angle tf.select(tf.logical_and(tf.less(x tf.less(y tf.atan(y/x np.pi angle angle tf.select(tf.logical_and(tf.equal(x tf.greater(y np.pi tf.ones_like(x angle angle tf.select(tf.logical_and(tf.equal(x tf.less(y np.pi tf.ones_like(x angle angle tf.select(tf.logical_and(tf.equal(x tf.equal(y np.nan tf.zeros_like(x angle return angle while this custom code performs reasonably well itd be both faster and more convenient if tensorflow had it built-in like the other existing functions
193567906,6092,https://api.github.com/repos/tensorflow/tensorflow/issues/6092,mrry,1,0,0,0,0,0,fixes
193551296,6088,https://api.github.com/repos/tensorflow/tensorflow/issues/6088,mrry,1,0,0,0,0,0,two reports of a check failure in the strided slice op have surfaced on stack overflow tensorflow print value of a tensor tensorflow evaluate aborted core dumped both cases the error message is: f tensorflow/core/kernels/strided_slice_op.cc check failed tmp.copyfrom(input.slice(begin end final_shape aborted core dumped) ...which appears to match the check here looks like both questions might be using the same model code so ill ask the questioner to post additional details if possible
193432276,6081,https://api.github.com/repos/tensorflow/tensorflow/issues/6081,taochenshh,0,0,0,1,0,0,i ran cifar_train.py from master branch my question is how to restore the checkpoint file in case that monitoredtrainingsession is being used instead of session i have modified the code like this: pythondef train train cifar for a number of steps with tf.graph().as_default global_step tf.contrib.framework.get_or_create_global_step get images and labels for cifar images labels cifar.distorted_inputs build a graph that computes the logits predictions from the inference model logits cifar.inference(images calculate loss loss cifar.loss(logits labels build a graph that trains the model with one batch of examples and updates the model parameters train_op cifar.train(loss global_step class loggerhook(tf.train.sessionrunhook logs loss and runtime def begin(self self._step def before_run(self run_context self._step self._start_time time.time return tf.train.sessionrunargs(loss asks for loss value def after_run(self run_context run_values duration time.time self._start_time loss_value run_values.results if self._step num_examples_per_step flags.batch_size examples_per_sec num_examples_per_step duration sec_per_batch float(duration format_str s step d loss f f examples/sec f sec/batch print format_str datetime.now self._step loss_value examples_per_sec sec_per_batch saver tf.train.saver with tf.train.monitoredtrainingsession(checkpoint_dir=flags.train_dir hooks= tf.train.stopatstephook(last_step=flags.max_steps tf.train.nantensorhook(loss loggerhook config=tf.configproto(log_device_placement=flags.log_device_placement save_checkpoint_secs save_summaries_steps as mon_sess ckpt tf.train.get_checkpoint_state(flags.train_dir if ckpt and ckpt.model_checkpoint_path restores from checkpoint saver.restore(mon_sess ckpt.model_checkpoint_path assuming model_checkpoint_path looks something like my-favorite-path/cifar_train/model.ckpt extract global_step from it global_step ckpt.model_checkpoint_path.split(/) - .split while not mon_sess.should_stop mon_sess.run(train_op)def main(argv=none pylint disable=unused-argument cifar.maybe_download_and_extract if tf.gfile.exists(flags.train_dir tf.gfile.deleterecursively(flags.train_dir tf.gfile.makedirs(flags.train_dir train cifar.maybe_download_and_extract if not tf.gfile.exists(flags.train_dir tf.gfile.makedirs(flags.train_dir train() the training variables seem to be restored but the step variable still started from how to make the step increases from the restored checkpoint step
193397269,6071,https://api.github.com/repos/tensorflow/tensorflow/issues/6071,b0noI,2,0,0,0,0,0,right now the script translate.py does not allow to use own data for training at the moment it works only with the data that the script downloads from the internet en/fr also there is not any documentation on the format of the input data that it expects in the input in order to have the ability to train the model with own data changes need to be made to the script to document the format of the input data enable the ability to pass an own input file path via the script arguments the file path should be used for training instead of downloading the data from the internet
193340215,6063,https://api.github.com/repos/tensorflow/tensorflow/issues/6063,FCInter,3,0,0,0,0,0,environment infooperating system ubuntu ltsinstalled version of cuda and cudnn no cuda i use cpu-only pip version pip python version operating system ubuntu lts tensorflow version tensorflow-..rc-cp-none-linux_x cpu-only description:i was testing the tutorial example of lstm here i followed the instructions to download the code and dataset and run the program using the command but the code cannot execute and reports the error message file ptb_word_lm.py line in main tf.contrib.deprecated.scalar_summary(training loss m.cost) attributeerror module object has no attribute deprecated what ive run:i just ran the following commands: wget xvf simple-examples.tgz python ptb_word_lm.py data_path=simple-examples/data/ i download the code from the github repository my tensorflow version is the very latest tensorflow-..rc-cp-none-linux_x_.why cant i run this example?thank you all for your kind help
193054375,6035,https://api.github.com/repos/tensorflow/tensorflow/issues/6035,0bserver07,2,0,0,0,0,0,running tf.nn.max_pool_with_argmax on cpu gives a very obscure error: tensorflow.python.framework.errors_impl.invalidargumenterror no opkernel was registered to support op maxpoolwithargmax with these attrs registered devices cpu registered kernels no registered kernels>from this line think its useful to mention tf.nn.max_pool_with_argmax is only implemented for gpu instead
192948599,6026,https://api.github.com/repos/tensorflow/tensorflow/issues/6026,qingyili,5,0,0,0,0,0,environment infooperating system linux gaia genericif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) ...-rcfollowed the instructions on add labels to my embedding my metadata.tsv file is just a single column with the ith row being the label corresponing to the ith row of embedding tensorhowever after i did this and ran tensorboard everything is fine until i click embeddings then i get this error message exception happened during processing of request from traceback most recent call last file usr/local/lib/python./socketserver.py line in process_request_thread self.finish_request(request client_address file usr/local/lib/python./socketserver.py line in finish_request self.requesthandlerclass(request client_address self file usr/local/lib/python./site-packages/tensorflow/tensorboard/backend/handler.py line in init basehttpserver.basehttprequesthandler.__init__(self args file usr/local/lib/python./socketserver.py line in init self.handle file usr/local/lib/python./http/server.py line in handle self.handle_one_request file usr/local/lib/python./http/server.py line in handle_one_request method file usr/local/lib/python./site-packages/tensorflow/tensorboard/backend/handler.py line in do_get self.data_handlers clean_path (query_params file usr/local/lib/python./site-packages/tensorflow/tensorboard/plugins/projector/plugin.py line in serve_runs request.respond(list(self.configs.keys application/json file usr/local/lib/python./site-packages/tensorflow/tensorboard/plugins/projector/plugin.py line in configs latest_checkpoints_changed(self._configs run_path_pairs file usr/local/lib/python./site-packages/tensorflow/tensorboard/plugins/projector/plugin.py line in latest_checkpoints_changed if run_name not in configs:typeerror argument of type nonetype is not iterable if i removed the projector_config.pbtxt from the logdir tensoboard runs properly again also found on the documentation page embedding config.embeddings.add should be embedding config.embedding.add
192943195,6025,https://api.github.com/repos/tensorflow/tensorflow/issues/6025,arunmallya,2,0,0,0,0,0,"what is a good way to determine which inputs of a tensorflow op are backpropagated through?for example consider the crop_and_resize op the description sounds rather straightforward extracts crops from the input image tensor and bilinearly resizes them however if you dig through the code this function pops up cropandresizegrad which states we back-propagate to the image only when the input image tensor has floating point dtype but we always back-propagate to the input boxes tensor.,which indicates that the input bounding box coordinates will get gradients.it might just be me but i have a hard time telling which input tensor variables will get gradients and which will not is there a standard way should this be part of the documentation thanks"
192901003,6015,https://api.github.com/repos/tensorflow/tensorflow/issues/6015,abred,1,0,0,0,0,0,change introduced in this commit not sure if the other files have to be changed as well init_op tf.global_variables_initializer() -> init_op tf.group(tf.global_variables_initializer tf.local_variables_initializer()) without the change the loop is exited immediately and this error occus tensorflow.python.framework.errors_impl.failedpreconditionerror attempting to use uninitialized value input/input_producer/input_producer/fraction_of__full/limit_epochs/epochs node input/input_producer/input_producer/fraction_of__full/limit_epochs/countupto countupto t=dt_int class= loc:@input/input_producer/input_producer/fraction_of__full/limit_epochs/epochs limit device=/job:localhost/replica:/task:/cpu: (input/input_producer/input_producer/fraction_of__full/limit_epochs/epochs
192876690,6012,https://api.github.com/repos/tensorflow/tensorflow/issues/6012,vedhas,1,0,0,0,0,0,thanks for the powerful package equipped with an awesome tensorboard like visualization environment.there is just one issue graphs generated under tensorboard occupy too much space than necessary which makes it difficult to understand what is going on thanks to consequent relative reduction in font size for all the blocks it would be really helpful if user could move the nodes around such that rearranged view is printer friendly and also user friendly please implement the feature soonest you can as it will help newbies like me and anyone in general since this package too is quite new relatively to ramp up quickly and also for debugging the code thank you.vedhas
192829544,6008,https://api.github.com/repos/tensorflow/tensorflow/issues/6008,ExpectationMax,8,0,0,0,0,0,"multiple tensorflow functions i tested tf.argmax,tf.argmin that require axis parameters to be provided show very unintuitive error messages if the axis parameter is omitted.it would probably be more convenient to either throw exceptions earlier with more intuitive error descriptions axis parameter must be passed/must not be none or to define the axis parameters as a required positional parameters environment infooperating system:os x running with cpu backend a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .:..-rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) import tensorflow as tft tf.get_variable(test shape= )maxt tf.argmax(t logs or other output that would be helpfulexception: traceback most recent call last file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op preferred_dtype=default_dtype file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto raise valueerror(none values not supported.)valueerror none values not supported.during handling of the above exception another exception occurred:traceback most recent call last file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/ipython/core/interactiveshell.py line in run_code exec(code_obj self.user_global_ns self.user_ns file ipython-input--fdbfef line in module max tf.argmax(t file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/ops/math_ops.py line in argmax return gen_math_ops.arg_max(input axis name file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/ops/gen_math_ops.py line in arg_max name=name file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/op_def_library.py line in apply_op as_ref=input_arg.is_ref).dtype.name file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape verify_shape=verify_shape file users/maexlich/.virtualenvs/tensorflow/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto raise valueerror(none values not supported.)valueerror none values not supported"
192800636,6007,https://api.github.com/repos/tensorflow/tensorflow/issues/6007,albertz,2,0,0,0,0,0,i want to reuse a name scope which was created earlier with tf.name_scope normally name_scope will create a new unique name in the current namespace if it ends with it will ignore the current namespace and use it as an absolute name scope but then it doesnt make it unique i want a way that it just uses the name as i provide it in the current namespace.current code contextlib.contextmanager def name_scope(self name old_stack self._name_stack if not name both for name=none and name we re-set to empty scope new_stack none elif name and name new_stack name else new_stack self.unique_name(name self._name_stack new_stack yield if new_stack is none else new_stack i suggest something like contextlib.contextmanager def name_scope(self name reuse=false if reuse new_stack self._name_stack name else
192626454,5987,https://api.github.com/repos/tensorflow/tensorflow/issues/5987,kmalakoff,49,0,0,0,0,8,the examples in the slim readme.md give basic documentation for training and evaluating models when used separately however there is guidance missing on how to do the classic cycle of mini-batch gradient descent using shuffled subsets of the training set periodically evaluating validation set and then evaluating on the test set post-training using the mnist tutorial and this tutorial for reference the best i came up with was something like this where im effectively monkey-patching the train_step_fn to periodically output accuracies from tensorflow.contrib.slim.python.slim.learning import train_stepgraph tf.graph()with graph.as_default image label input(train flags.dataset_dir images labels tf.train.shuffle_batch( image label batch_size=flags.batch_size capacity flags.batch_size min_after_dequeue images_validation labels_validation inputs(validation flags.dataset_dir images_test labels_test inputs(test flags.dataset_dir with tf.variable_scope(model as scope predictions model(images flags scope.reuse_variables predictions_validation model(images_validation flags predictions_test model(images_test flags slim.losses.softmax_cross_entropy(predictions labels optimizer tf.train.adamoptimizer(flags.learning_rate train_op slim.learning.create_train_op(slim.losses.get_total_loss optimizer accuracy_validation slim.metrics.accuracy(tf.to_int(tf.argmax(predictions_validation tf.to_int(tf.argmax(labels_validation accuracy_test slim.metrics.accuracy(tf.to_int(tf.argmax(predictions_test tf.to_int(tf.argmax(labels_test def train_step_fn(session args kwargs total_loss should_stop train_step(session args kwargs if train_step_fn.step flags.validation_check accuracy session.run(train_step_fn.accuracy_validation print(step s loss f accuracy f str(train_step_fn.step).rjust total_loss accuracy if train_step_fn.step flags.max_steps accuracy session.run(accuracy_test print(%s loss f accuracy f final test total_loss accuracy train_step_fn.step return total_loss should_stop train_step_fn.step train_step_fn.accuracy_validation accuracy_validationslim.learning.train train_op flags.logs_dir train_step_fn=train_step_fn graph=graph number_of_steps=flags.max_steps) note one problem with this implementation is that the final test set is not guaranteed to be run in the case of early exit ive posted in the slack channel and googled around but havent been able to find any examples for this basic use case accordingly i would like to propose that an example providing the best practice to periodically evaluate batch trained models using the validate set and the trained model against the test set to be added to the slim readme.md.i think it would really help the community to have a clearer idea on the intentions of the slim team on how the batch training and evaluation paths were designed to be used together during and after training
192612459,5985,https://api.github.com/repos/tensorflow/tensorflow/issues/5985,petrbel,7,0,0,0,0,0,"hello everybody, introduction: i am trying to reproduce the work of szegedy et al intriguing properties of neural networks and dezfooli et al universal adversarial perturbations what they do briefly is that they take the gradient g of the network with respect to the input image x then slightly modify the input image x against the obtained gradient i.e x-.g the modified images are consequently considered as another network input hence new gradients g might be computed and obtained the result gradients are manually set to the average of those two set obtains gradients i.e g g i hope i explained that clearly.i successfully implemented it for images in tf working on rc as well now i want to do something similar with recurrent networks sentiment analysis task so i have a input sequence of tokens numbers a variable embeddings which is used by tf.nn.embedding_lookup after the lookup the sequence of proper embeddings is traversed by gru dynamically its last state is considered as the representation of the sequence then a mlp with a single hidden layer and relu activation is applied resulting in neurons representing the positive and negative logits respectively the loss is traditional categorical cross entropy. bug similarly to the papers mentioned above i obtain gradients of the embedding and modify the current embeddings slightly against the obtained gradient than i compute the new loss and new gradients here is the problem and update the network by their mean.the problem is that the returned new gradients are all none s i originally worked with tf however i got an error explaining that second order derivatives are impossible to obtain from scan i upgraded to recently released tf rc which doesnt throw this error but returns none s instead. related issues and sos is somewhat similar but not the same this question deals with very the same problem but the solution isnt sufficient as replacing the none value with zero is really worthless in my case since i need to modify the variable against the gradient. minimal not-working example i am sorry the code is so long it is partially caused by the fact that the problem is non-trivial and requires few lines and partially by the amount of comments i tried to write in order to make the code more readable all platform/version information is stored at beginning of the file.the main function constructs two simplesentiment s as the models that are trained each one is trained for few epochs and batches first instance is without the adversarial hence we perform no trick regarding modifying the gradients for a sanity check the second uses the complicated if branch in the constructor and obtains none s as described above.i believe this is a bug and not my mistake however it cant be ruled out as i am not as experienced with tf as id like to be. python usr/bin/env python python anaconda tf rc from python c import tensorflow print(tensorflow.__version rc os redhat cpu version only no gpu no cuda no cudnnimport numpy as npimport tensorflow as tffrom tensorflow.python.ops.rnn_cell import grucellbatch batch sizemax_len max length of the sequencemlp_hidden_dim number of hidden neurons in the mlpembedding_dim embedding dimensionvocab_size vocabulary sizethreads number of threads to be usedstd standard deviation of ariable initializersclass simplesentiment def init__(self adversarial=false device=/cpu self.embeddings tf.get_variable(word_embeddings initializer=tf.random_uniform( vocab_size embedding_dim with tf.variable_scope(sentiment as scope with tf.device(device inputs self.text tf.placeholder(tf.int batch max_len self.text_len tf.placeholder(tf.int batch self.sentiment tf.placeholder(tf.int batch normal loss loss_normal self._loss(self.text self.text_len self.sentiment define the optimizer note ive tried multiple of optimizers and none helped optimizer tf.train.adamoptimizer(learning_rate if adversarial define adversarial loss lets acces already defined variables scope.reuse_variables gradients of all variable according to normal loss gradients optimizer.compute_gradients(loss_normal print(len(gradients gradients gradients of the embeddings emb_gradient optimizer.compute_gradients(loss_normal self.embeddings this how much we want to shift the embeddings i.e going against the gradient delta tf.sign(emb_gradient lets compute the loss once again but this time we add the delta to the embeddings loss_adversarial self._loss(self.text self.text_len self.sentiment delta new gradient of the whole computational graph adversarial_gradients optimizer.compute_gradients(loss_adversarial print(len(adversarial_gradients adversarial_gradients everything is none now we compute an average of old and new gradients new_gradients g ag vg for g vg ag avg in zip(gradients adversarial_gradients and apply them self.training optimizer.apply_gradients(new_gradients btw this doesnt work either self.training optimizer.apply_gradients(adversarial_gradients self.loss_final loss_normal loss_adversarial else normal loss simply minimize according to the gradients self.loss_final loss_normal self.training optimizer.minimize(loss_normal create the session self.session tf.session(config=tf.configproto(inter_op_parallelism_threads=threads intra_op_parallelism_threads=threads allow_soft_placement=true init everything still deprecated way self.session.run(tf.initialize_all_variables def loss(self text text_len sentiment emb_delta use embedding note that emb_delta is zero as long as adversarial=false if adversarial=false then each embedding is shifted by appropriate emb_delta text tf.nn.embedding_lookup(self.embeddings emb_delta text run gru gru_cell grucell(mlp_hidden_dim outputs state tf.nn.dynamic_rnn(cell=gru_cell inputs=text sequence_length=text_len dtype=tf.float define mlp w tf.get_variable(name=mlp_w shape= state.get_shape mlp_hidden_dim initializer=tf.random_normal_initializer(mean stddev=std w tf.get_variable(name=mlp_w shape= mlp_hidden_dim initializer=tf.random_normal_initializer(mean stddev=std h tf.get_variable(name=mlp_h shape= mlp_hidden_dim initializer=tf.random_normal_initializer(mean stddev=std h tf.get_variable(name=mlp_h shape initializer=tf.random_normal_initializer(mean stddev=std apply mlp of the last gru state after_first_layer tf.nn.relu(tf.matmul(state w h logits tf.matmul(after_first_layer w h compute loss via categorial cross entropy loss tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits sentiment return lossdef main for adversarial in false true print(\n if adversarial print(using the adversarial loss else print(using the standard loss net simplesentiment(adversarial=adversarial for epoch in range print(epoch format(epoch for batch in range loss_final net.session.run( net.training net.loss_final net.text np.array dtype=int net.text_len np.array dtype=int net.sentiment np.array dtype=int print(\tbatch format(batch loss_final))if name main main() thanks for helppetr"
192603756,5983,https://api.github.com/repos/tensorflow/tensorflow/issues/5983,wkcw,4,0,0,0,0,0,i installed the tensorflow for windows with pip install upgrade ignore-installed added the ignore-installed for the official one didnt work on my pc)then i tried some code and ran tensorboard however the browser shows nothing when running with debug there appears some i also tried tensorboard on mac os it worked well so the problem is not caused by the tfevent fileive read through a bunch of related issues and found that maybe the pip install didnt install tensorflow completely and i searched the tensorflow file and did not found some certain files appears in mac os tensorflow such as paper-toolkit
192530086,5974,https://api.github.com/repos/tensorflow/tensorflow/issues/5974,ylhsieh,2,0,0,0,0,0,env tensorflow v rcfollowing the instructions i use tensorboard to visualize embeddings without any metadata and comes the following error file home/morphe/miniconda/envs/tensorflow/lib/python./site-packages/tensorflow/tensorboard/plugins/projector/plugin.py line in configs run_path_pairs.append self.logdir))attributeerror dict_items object has no attribute appendit seems to work if i attempt to change tensorflow/tensorboard/plugins/projector/plugin.py run_path_pairs.append self.logdir run_path_pairs self.logdir) not sure if this is the right way to go is this a bug
192464124,5965,https://api.github.com/repos/tensorflow/tensorflow/issues/5965,liuyipei,1,0,0,0,0,0,this is a feature request as far as i know all three of them currently do not have gpu ops.it seems that if we can at least get a gpu implementation of tf.unique for integers then the user can make tf.where and tf.dynamic_partition manually for those of us who are trying to build models that want to mess around with indices rather frequently this would be incredibly helpful
192410482,5953,https://api.github.com/repos/tensorflow/tensorflow/issues/5953,adamb70,5,0,0,0,0,0,im following through the tutorial and it seems that the tensorflow/models directory is missing from the tensorflow rc build for windows at least when downloaded using pip.i installed tensorflow using pip install upgrade the initial test of tensorflow ran fine.additionally running python m tensorflow.models.image.mnist.convolutional produces the follwing error i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cublas_.dll locally i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cudnn_.dll locally i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library cufft_.dll locally i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library nvcuda.dll locally i c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc successfully opened cuda library curand_.dll locally c:\program files\python\python.exe error while finding spec for tensorflow.models.image.mnist.convolutional importerror no module named tensorflow.models)im new to tensorflow so please correct me if the models module is not supposed to be packaged with the pip download
192215002,5930,https://api.github.com/repos/tensorflow/tensorflow/issues/5930,dustinvtran,1,0,0,0,0,1,this is a feature request for tf.contrib.distributions on master i.e beyond version r tensorflow distributions require that the first parent class is distribution is a problem for our work with random variables in edward which requires that random variable classes are first and distribution is second chatted with ebrevdo and jvdillon about this a few weeks ago they said its possible to change im adding a github issue to formalize the request can i help in any way it is a high priority to fix for edward as it breaks all random variable support
192211373,5929,https://api.github.com/repos/tensorflow/tensorflow/issues/5929,ericyue,1,0,0,0,1,0,what related github issues or stackoverflow threads have you found by searching the web for your problem?cannot find any related post in the web environment infooperating system:centos tensorflow without gpu(cpu only if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) tf.train.saver(max_to_keep thensaver.save(sess checkpoint_file global_step=step what other attempted solutions have you tried? *important i run the same code in my laptop its right with only latest checkponits but in the centos server it generate unlimited checkponits files and the disk almost full logs or other output that would be helpful(if logs are large please upload as attachment or provide link).no error log
192206398,5927,https://api.github.com/repos/tensorflow/tensorflow/issues/5927,KishoreKarunakaran,3,0,0,0,0,0,i was following the documentation of the tensorflow with respect to installing tensorflow in windows when i execute pip command im getting following error pip install upgrade tensorflow==..rc from http error while getting could not install requirement tensorflow==..rc from because of error client error not found for url not install requirement tensorflow==..rc from because of http error client error not found for url for url
192191055,5924,https://api.github.com/repos/tensorflow/tensorflow/issues/5924,craftlk,1,0,0,0,0,0,on a knl server i tried to install tensorflow by compiling way im not root so i manually set the env in my directory and there is no network on the server so i use ssh forward.here is the problem ive met:error home/guest/tensorflow/tensorflow/tensorboard/bower/build no such package iron_resizable_behavior error downloading from to home/guest/.cache/bazel/_bazel_guest/acaabfbebeae/external/iron_resizable_behavior error downloading to home/guest/.cache/bazel/_bazel_guest/acaabfbebeae/external/iron_resizable_behavior/v...tar.gz timed out connecting to read timed out and referenced by tensorflow/tensorboard/bower:bower.error home/guest/tensorflow/tensorflow/tensorboard/bower/build no such package iron_resizable_behavior error downloading from to home/guest/.cache/bazel/_bazel_guest/acaabfbebeae/external/iron_resizable_behavior error downloading to home/guest/.cache/bazel/_bazel_guest/acaabfbebeae/external/iron_resizable_behavior/v...tar.gz timed out connecting to read timed out and referenced by tensorflow/tensorboard/bower:bower.error evaluation of query deps((//tensorflow union bazel_tools//tools/jdk:toolchain failed errors were encountered while computing transitive closure
192148821,5919,https://api.github.com/repos/tensorflow/tensorflow/issues/5919,cancan101,1,0,0,0,0,0,strip_unused does not appear to remove a switch node with constant input
192130428,5914,https://api.github.com/repos/tensorflow/tensorflow/issues/5914,vasantivmahajan,1,0,0,0,0,0,"i am running the sample iris program in tensorflow serving since it is a tf.learn model i am exporting the model using the following classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn) and the signature_fn is defined as shown below: def my_classification_signature_fn(examples unused_features predictions creates classification signature from given examples and predictions args examples tensor unused_features dict of tensor s predictions tensor or dict of tensors that contains the classes tensor as in classes tensor returns tuple of default classification signature and empty named signatures raises valueerror if examples is none if examples is none raise valueerror(examples cannot be none when using this signature fn if isinstance(predictions dict default_signature exporter.classification_signature examples classes_tensor=predictions classes else default_signature exporter.classification_signature examples classes_tensor=predictions named_graph_signatures inputs exporter.generic_signature({x_values examples outputs exporter.generic_signature({preds predictions return default_signature named_graph_signatures the model gets successfully exported using the following piece of code.i have created a client which makes real-time predictions using tensorflow serving.the following is the code for the client: flags.define_string(model_dir tmp/iris_model_dir base directory for output models.)tf.app.flags.define_integer(concurrency maximum number of concurrent inference requests)tf.app.flags.define_string(server predictionservice host:port)#connectionhost port flags.server.split(:)channel implementations.insecure_channel(host int(port))stub prediction_service_pb.beta_create_predictionservice_stub(channel classify two new flower samples.new_samples np.array dtype=float)request predict_pb.predictrequest()request.model_spec.name irisrequest.inputs x_values .copyfrom tf.contrib.util.make_tensor_proto(new_samples))result stub.predict(request secs timeout however on making the predictions the following error is displayed: grpc.framework.interfaces.face.face.abortionerror abortionerror(code=statuscode.internal details=output of type double does not match declared output type string for node recv_input_example_tensor recv client_terminated=true recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/cpu send_device_incarnation tensor_name=input_example_tensor tensor_type=dt_string device=/job:localhost/replica:/task:/cpu: ()) here is the entire stack trace.! image iris model is defined in the following manner specify that all features have real-value datafeature_columns tf.contrib.layers.real_valued_column dimension build layer dnn with units respectively.classifier tf.contrib.learn.dnnclassifier(feature_columns=feature_columns hidden_units n_classes model_dir=model_dir fit model.classifier.fit(x=training_set.data y=training_set.target steps=) can someone provide some documentation for creating the client program which sends the request to tensorflow serving"
192006661,5902,https://api.github.com/repos/tensorflow/tensorflow/issues/5902,larsmennen,2,0,0,0,0,0,i am trying to feed a tensor using the c api that has memory allocated on the gpu using gpubfcallocator into a network.now the placeholder in the network is on the gpu i checked this in tensorboard and the memory allocated for the input tensor is on the gpu but whenever i run the network nvprof print-gpu-trace shows me cuda memcpy htod and cuda memcpy dtoh before the computations i.e convolutions etc start.this suggests to me that the input tensor is being copied to cpu memory and then back to gpu memory.while debugging this i found multiple hints in the source that seem to suggest the cpu is always used as device to feed tensors from.see e.g is this analysis correct how can one feed in a tensor that has memory allocated on gpu memory without copying back and forth to cpu memory if this is currently not possible then i think this would be a good feature to add.especially when one wants to combine tensorflow input/output with other algorithms not in tf one might want to keep data on the gpu to avoid host to device and device to host transfers.thanks in advance environment infooperating system ubuntu ltsinstalled version of cuda and cudnn cuda cudnn output of ls l usr/local/cuda/lib/libcud* : -rw-r--r root root sep usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root sep usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root sep usr/local/cuda/lib/libcudart.so libcudart.so...-rw-r--r root root sep usr/local/cuda/lib/libcudart.so...-rw-r--r root root sep usr/local/cuda/lib/libcudart_static.alrwxrwxrwx root root oct usr/local/cuda/lib/libcudnn.so usr/local/cudnn-.-v./lib/libcudnn.solrwxrwxrwx root root oct usr/local/cuda/lib/libcudnn.so usr/local/cudnn-.-v./lib/libcudnn.so.lrwxrwxrwx root root oct usr/local/cuda/lib/libcudnn.so usr/local/cudnn-.-v./lib/libcudnn.so...lrwxrwxrwx root root oct usr/local/cuda/lib/libcudnn_static.a usr/local/cudnn-.-v./lib/libcudnn_static.a tensorflow installed from source the commit hash git rev-parse head aafadddbffbfb the output of bazel version : build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed nov build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)this should give the general idea. cpptensorflow::gpubfcallocator allocator new tensorflow::gpubfcallocator sizeof(float height width tensorflow::tensor input_tensor tensorflow::tensor(allocator tensorflow::datatype::dt_float tensorflow::tensorshape height width std::vector;run input_layer input_tensor output_layer outputs logs or other output that would be helpfulpartial output of nvprof print-gpu-trace start duration grid size block size regs ssmem dsmem size throughput device context stream name.ms us kb mb/s geforce gtx cuda memset .ms us kb gb/s geforce gtx cuda memcpy htod .ms us b b geforce gtx void cv::cudev::grid_transform_detail::transformsmart(cv::cudev::globptr(int float const tensorflow::functor::dimension(int float const tensorflow::functor::dimension
191775622,5869,https://api.github.com/repos/tensorflow/tensorflow/issues/5869,cancan101,1,0,0,0,0,0,looking at the code in master it looks like neither tensorflow.python.training.saver nor tensorflow.contrib.session_bundle.exporter use optimize_for_inference_lib which is described as:>there are several common transformations that help reduce the amount of computation needed when the network is used only for inference.further these optimizations are not mentioned suggested in the tensorflow serving docs the serving logic could be changed to use frozen models see freeze_graph which calls convert_variables_to_constants
191749359,5867,https://api.github.com/repos/tensorflow/tensorflow/issues/5867,mattrajca,9,0,0,0,0,0,when i first tried using an exported mnist model with tensorflow on ios i got the following error invalid argument no opkernel was registered to support op randomuniform with these attrs registered devices cpu registered kernels no registered kernels node dropout/random_uniform/randomuniform randomuniform t=dt_int dtype=dt_float seed seed= (dropout/shape) since dropout operations are no-ops during inference we pass in a keep probability of it would be nice if they were removed or turned into no-ops of some kind that can be parsed and ignored by the ios library).while i was able to work around this by explicitly exporting a separate graph that does not contain dropout it was pretty tedious and it would be nice if the optimize_for_inference.py script did this automatically environment infooperating system macos installed version of cuda and cudnn nonesource:this weeks tip-of-tree around ddcbccaceebc
191635010,5845,https://api.github.com/repos/tensorflow/tensorflow/issues/5845,duanLH,2,0,0,0,0,0,i use tensoflow r+titan x+cuda+cudnnvbut the speed of training is much slower than caffe it can run about w epochs with caffe in a night but only epochs with tensorflow i dont know why the gap of speed is so big.if anywhere need i to change i follow the cifar example to suit my experimentthanks
191626138,5844,https://api.github.com/repos/tensorflow/tensorflow/issues/5844,vit-stepanovs,5,0,0,0,0,0,currently tensorboard external dependencies js scripts css images etc are not part of pip package built by cmake as a result when you navigate to tensorboard on windows it shows a blank screen added cmake scripts to download tensorboard dependencies and make them part of pip package
191232312,5804,https://api.github.com/repos/tensorflow/tensorflow/issues/5804,jheymann85,2,0,0,0,0,0,to decode the input the function writes the content to a temporary file its name is generated by the function gettempfilename found in tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc the template for the filename is tmp_dir/tmp_file_%pid.%ext . when using multiple decoders in parallel this causes an undetermined behaviour since all decoders want to write and afterwards delete the same file. a possible solution would be to use the thread id instead of the process id i.e. #include sys/syscall.h>#define gettid syscall(sys_gettid)...return io::joinpath(dir strcat(tmp_file gettid extension)); the first two lines are necessary because glibc does not wrap the call solution works for me on linux im however not sure if it works on all supported platforms if thats fine i can make a pull request
191166417,5796,https://api.github.com/repos/tensorflow/tensorflow/issues/5796,shepherd001,2,0,0,0,0,0,i am working on machine translation using seqseq model in tensorflow i am aware that once the graph has been established it cannot be modified during training.what if i want to change the optimizer from sgd to adam after certain global steps?how should the code be?thanks a lot
191111908,5790,https://api.github.com/repos/tensorflow/tensorflow/issues/5790,warmspringwinds,4,0,0,0,0,0,"hello,i have been recently implementing the fcn network for image segmentation found out that there is no bilinear upsampling op in tensorflow or tf-slim.that would be great if there will be an op that will do that because that will make itpossible to bilinearly upsample blobs in a differentiable way or initialize upsamplingfilter with bilinear kernel.i have already implemented that and checked correctness.you can see more if you follow the link if you are interested i can make a pull request.thank you"
191074208,5787,https://api.github.com/repos/tensorflow/tensorflow/issues/5787,mrry,1,0,0,0,0,0,added some instructions for installing/building tensorflow on windows and made minor changes to other docs
190925620,5777,https://api.github.com/repos/tensorflow/tensorflow/issues/5777,Sadrpour,1,0,0,0,0,0,i noticed many have issues with gpu being unavailable with message e.g issue tensorflow/stream_executor/cuda/cuda_driver.cc failed call to cuinit cuda_error_unknown some suggested sudo apt-get install nvidia-modprobe but it does not work for all including me my gpu works until i put the computer to sleep/suspense but after waking up the computer i always get the message above and the gpu gtx is no longer available in execution of the code only cpu is used in nvidia docker i also noticed if prior to suspending the computer i exit the docker and then restart it when i wake the computer the gpu is still available in docker so the problem happens if i suspend the computer while the ipython-notebook session is up and running i am using nvidia-docker nvidia-docker run it p v data/docker:/docker name tensorflow gcr.io/tensorflow/tensorflow:latest-gpu bin/bash nvidia-smi and nvidia-debugdump l both show the gpu is installed and driver is up to date within docker and in the host when i run nvidia-smi in docker the output is nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx off on n/a c p w w mib mib default processes gpu memory gpu pid type process name usage i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallye tensorflow/stream_executor/cuda/cuda_driver.cc failed call to cuinit cuda_error_unknowni tensorflow/stream_executor/cuda/cuda_diagnostics.cc retrieving cuda diagnostic information for host casffi tensorflow/stream_executor/cuda/cuda_diagnostics.cc hostname casffi tensorflow/stream_executor/cuda/cuda_diagnostics.cc libcuda reported version is i tensorflow/stream_executor/cuda/cuda_diagnostics.cc driver version file contents nvrm version nvidia unix x kernel module mon oct pdt gcc version gcc version ubuntu ubuntu i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel reported version is i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel version seems to match dso software specs:os ubuntu lts bitgpu driver nvidia cuda
190902068,5774,https://api.github.com/repos/tensorflow/tensorflow/issues/5774,hamidb,1,0,0,0,0,0,hi i installed tensorflow with python using the pip installation instructions from tensorflow official site.the version i installed is rc and i tried to use per_image_standardization tensor for normalization purposes however it turned out that is not recognized giving the following error: attributeerror module tensorflow.python.ops.image_ops has no attribute per_image_standardization has this error ever observed by someone else
190832735,5757,https://api.github.com/repos/tensorflow/tensorflow/issues/5757,vade,1,0,0,0,0,0,in attempts to highly optimize my tensorflow client application and tf install for consumer desktop hardware ive noticed and noticed in other bug reports that quantized eight bit graphs appear to run very slow my goal is to match the realtime batch x x x ios performance that the camera example gets yet i cant get a desktop cpu compile of tf to get lower than roughly ms per frame where in reality close to ms per frame is needed for roughly hz or ms for hz performance it appears somehow the ios armv build is able to achieve this performance unless i am missing something!from the discussion group i was asked by petewarden to start a bug based on findingsthread here ive added stat tracing to the image label example modified source is here downloaded compiled tensor flow with the following bazel build commands referenced from the makefile for ios which speeds things up just a bit more than the standard compile bazel build c opt copt=-mavx cxxopt=-fno-exceptions cxxopt=--std=c cxxopt=-dndebug cxxopt=-dnotfdbg cxxopt=-o cxxopt=-duse_gemm_for_conv tensorflow:libtensorflow_cc.so i then compiled image label via the standard bazel command bazel build tensorflow/examples/label_image/... and ran it with graphs standard inceptionv inceptionv run through inference optimizer script inceptionv run through inference optimizer and quantizer in weighted rounding mode inceptionv run through inference optimizer and quantizer in eight bit modethe output of the runs are documented here in order the time for the quantized eight bit mode is roughly x longer than previous runs.as a second set of data my custom c app which uses the same lib_tensorfow_cc.so nets similar results to the benchmark inceptionv no optimizations or quantizations frames took seconds inceptionv inference optimizations no quantizations frames took seconds inceptionv inference optimizations quantizations rounded frames took seconds inceptionv inference optimizations quantizations eightbit frames took seconds system: mac os x ghz intel core igb ramxcode command line tools from and enabled via xcselect if installed from source provide the commit hash cfafaaceadbeadcecbad the output of build label homebrewbuild target bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time wed nov build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code) see above for source code for minimally modified label image source what other attempted solutions have you tried?attempted to run cuda but am targeting consumer desktop systems and would like similar performance to ios targets for realtime or better than realtime performance for inceptionv pool feature vector determination and possibly labelling classification.for my system cuda compilation netted similar results to cpu although admittedly i did not enable batch sizes larger than however i think this is moot because ios appears to be able to get realtime labelling and desktop cant is roughly x slower logs or other output that would be helpfullogs and links provided in preamble description
190825495,5755,https://api.github.com/repos/tensorflow/tensorflow/issues/5755,decentralion,0,0,0,1,0,0,fixes
190547248,5731,https://api.github.com/repos/tensorflow/tensorflow/issues/5731,Sraw,2,0,0,0,0,0,to implement a attention machine i need the intermediate state to calculate the attention but dynamic_rnn doesnt return the intermediate states but only final state.hope the dynamic_rnn could return intermediate states so that we can implement a attention machine with dynamic network
190499357,5719,https://api.github.com/repos/tensorflow/tensorflow/issues/5719,lagoon0o0,9,0,0,1,0,0,"hi,i am currently using tf.nn.top_k in my code however i found this operation is calculated on cpu and it is quite slow.so i am wondering if it is possible to calculate topk on gpu?here are the papers and cuda codes about k-selection on gpu someday this feature will be added to tensorflow"
190441260,5705,https://api.github.com/repos/tensorflow/tensorflow/issues/5705,yagnasrinath,1,0,0,0,0,0,i am trying to build the tensorflow from source i get errors like bazel-out/host/bin/external/grpc/grpc_cpp_plugin program not found or is not executable but builds successfully if bazel is not supported its a documentation bug here to mention that bazel latest version should be downloaded i could successfully build with bazel x
189535245,5632,https://api.github.com/repos/tensorflow/tensorflow/issues/5632,bcarpenter-pub,1,0,0,0,0,0,currently image processing has some image manipulation functions like brightness adjustment or contrast adjustment it would be quite useful to have perspective transforms or other affine transformations to help distort training data
189281953,5609,https://api.github.com/repos/tensorflow/tensorflow/issues/5609,sonalgupta,0,0,0,0,1,0,can someone please add documentation on how to use tensorflow.contrib.training.bucket and tensorflow.contrib.training.bucket_by_sequence_length functions?note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system mac os x sierrainstalled version of cuda and cudnn none(please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed mac os x python cpu the output from python c import tensorflow print(tensorflow.__version rcif installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
189133641,5598,https://api.github.com/repos/tensorflow/tensorflow/issues/5598,fventer,1,0,0,0,0,0,i have run into into performance and portability issues with tf.conrtib.learn estimators that use high level feature columns and have done extensive code traversal to figure out a possible solution to make predictions faster.at the core of the problem is that high level feature column tensors created in the input_fn are transformed and added as input tensors to the actual neural network and/or logistic model after that the hidden layers and logit output layer is added to the graph and the logits for the linear model if used are also created this happens every time you run predict on the estimator at the cost an overhead of seconds.one solution i tried is based on posts that i read about the x parameter of the predict method vs input_fn if you create an iterator that feeds examples that never sends the stopiteration exception then you could keep the model resident avoiding the graph being built every time you call predict.however the x parameter as iterable does not work because it seems that you can only provide it low level inputs such as numpy arrays or pandas dataframe of type float or int only it seems that this solution precludes the nice feature columns such as sparse tensors for categoricals and embeddings to get inputs to the neural net etc.another problem that arises with the above issues with the estimator design is that although it has an export method to create a model that tensorflow serving loads nicely it will not work because the graph needs to be built at predict time to extract model inputs from the higher level feature column tensors and of course tensor flow serving does not work like that it loads a graph and runs inputs to create outputs similar to the predict method using the x parameter it does not create graphs on the fly every time a predict call is made.what is needed as a new feature is that the x and y parameters of the predict method should accept raw inputs like strings for categorical features/embeddings that is then passed into the feed_dict parameter of the session run call.similarly for tensorflow serving it should be possible to export the estimator and write the model client so that the raw feature column values strings can be sent to the model instantiated in tensorflow serving this feature is quite urgent in my opinion as models based on the cool abstractions available in the contrib.learn estimator api are not very useful beyond training and evaluation without this requested feature
188731767,5539,https://api.github.com/repos/tensorflow/tensorflow/issues/5539,ssegvic,5,0,0,0,0,0,"there are two issues here tf.nn.convd does not support float tensors on cpu although the documentation states it should after the first occurrence of the situation above tf.nn.convd keeps crashing with the same error even when we provide float tensors what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system linuxinstalled version of cuda and cudnn noneif installed from binary pip package provide a link to the pip package you installed sure though the output from python c import tensorflow print(tensorflow.__version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code issue this never works although the documentation states that it shouldimport tensorflow as tfimport numpy as npinput tf.constant(np.zeros( ,,, ))filter tf.constant(np.zeros( ,,, ))op tf.nn.convd(input filter strides padding=valid)tf.session().run(op issue this works if executed before the part above this does not work if exectuted after the part aboveimport tensorflow as tfimport numpy as npinput tf.constant(np.zeros dtype=np.float)filter tf.constant(np.zeros dtype=np.float)op tf.nn.convd(input filter strides padding=valid)tf.session().run(op"
188601398,5526,https://api.github.com/repos/tensorflow/tensorflow/issues/5526,tillahoffmann,1,0,0,0,0,0,this pr adds functionality to tensorshape to allow for broadcasting in particular tensorshape.broadcast_with(other returns the shape resulting from broadcasting a tensor of the given shape with a tensor of shape other it raises a valueerror if the shapes are not broadcastable tensorshape.is_broadcastable_with(other determines whether two shapes are broadcastable tensorshape.assert_is_broadcastable_with(other asserts that two shapes are broadcastable
188585806,5523,https://api.github.com/repos/tensorflow/tensorflow/issues/5523,shoyer,3,0,0,0,0,0,"i notice that the xla dot operation copies outer-product style broadcast semantics from numpy.dot input output semantics array p x q x r dot array s x r x t array p x q x s x t array dot product read below in brief i think this is a mistake it would be better to follow the matmul style style broadcasting semantics of pythons operation and numpys matmul .matmuls broadcasting is much more general and in my opinion also easier to understand for example it can do batch matrix-multiplication but also can still do outer product style broadcasting if you insert dummy dimensions of length the axes do end up in a different order e.g.,batch matmul p x q x r matmul p x r x t p x q x t outer product matmul p x x q x r matmul x s x r x t p x s x q x t if we could go back in time as numpy developers we assuredly would change dot to work this way now we cannot because of backwards compatibility concerns so it would be nice to change this for xla before we lock in this behavior"
188494731,5516,https://api.github.com/repos/tensorflow/tensorflow/issues/5516,wjaskowski,2,0,0,0,0,0,i created a small benchmark which shows that depending on the model architecture tensorflow is times slower than theano depending on the model architecture the question is whether this effect is due to a design decision or a performance bug which may be solved in the future?related
188440570,5514,https://api.github.com/repos/tensorflow/tensorflow/issues/5514,zakizhou,10,0,0,0,1,0,when i was trying a multi gpu training using python cifar_multi_gpu_train.py num_gpus i got an error: usr@linux:~/tensorflow_source/tensorflow/tensorflow/models/image/cifar python cifar_multi_gpu_train.py num_gpus=i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyfilling queue with cifar images before starting to train this will take a few minutes.filling queue with cifar images before starting to train this will take a few minutes.traceback most recent call last file cifar_multi_gpu_train.py line in module tf.app.run file usr/local/lib/python./dist-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv flags_passthrough file cifar_multi_gpu_train.py line in main train file cifar_multi_gpu_train.py line in train init tf.global_variables_initializer()attributeerror module object has no attribute global_variables_initializer seemingly theres no global_variables_initializer of tf or in other module
188437339,5513,https://api.github.com/repos/tensorflow/tensorflow/issues/5513,errcw,3,0,0,0,0,0,"the current process of building an android app that uses tensorflow is unwieldy at best the only available documentation is baked into the sole example app which itself is embedded in the tensorflow build system having prebuilt drop-in android libraries or even the ability to build these libraries from source would i believe significantly simplify the process.i added my own build definitions to build libraries i can easily drop into an android studio project having experience with blaze made this much more tractable inside tensorflow/contrib/android/build build the jar ends up in my-app/libs/libandroid_tensorflow_inference_java.jarandroid_library name android_tensorflow_inference_java srcs android_tensorflow_inference_java_srcs build the so bazel build tensorflow/contrib/android:libtensorflow.so crosstool_top=//external:android/crosstool host_crosstool_top=@bazel_tools//tools/cpp:toolchain cpu=armeabi-va ends up in my-app/src/main/jnilibs/armeabi-va/libtensorflow.solinker_script tensorflow/contrib/android:jni/version_script.ldscc_binary name libtensorflow.so srcs copts tf_copts linkopts landroid ljnigraphics llog lm z defs s wl,--version-script this line must be directly followed by linker_script linker_script linkshared linkstatic deps android_tensorflow_inference_jni tensorflow/core:android_tensorflow_lib linker_script moreover if the android example could rely on these libraries and therefore exist as a standalone project buildable by android studio i think the barrier to entry for tensorflow on android would be materially lower"
188219915,5493,https://api.github.com/repos/tensorflow/tensorflow/issues/5493,CeliaPhoebe,2,0,0,0,0,0,each time after typing tensorboard logdir=logsonly starting tensorboard on port is shownso i need to type myself.then the graph of neural network and histograms are well displayed but events and distributions are all black like this:! capture d ecran a os enivrement python version im sure that the code is good because someone else runs it correctly on his mac.what should be the problem
188195671,5492,https://api.github.com/repos/tensorflow/tensorflow/issues/5492,sjperkins,4,0,0,0,0,0,unless the variable is initially created with tf.variable validate_shape=false updating a variable using tf.assign validate_shape=false does not update the variable shape what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system ubuntu installed version of cuda and cudnn cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud bash ls l usr/local/cuda-./lib/libcud*-rw-r--r root root sep usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root sep usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root sep usr/local/cuda-./lib/libcudart.so libcudart.so...-rw-r--r root root sep usr/local/cuda-./lib/libcudart.so...-rw-r--r root root sep usr/local/cuda-./lib/libcudart_static.a ls l usr/local/cudnn-.-cuda-./lib/total lrwxrwxrwx root root oct libcudnn.so libcudnn.so.lrwxrwxrwx root root oct libcudnn.so libcudnn.so...-rwxr-xr-x root root oct libcudnn.so...-rw-r--r root root oct libcudnn_static.a if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) . bash python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally..rc if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)in the following script if var is created with validate_shape=true subsequent tf.assign operations dont update the shape they remain as but they do update the data*.however if the var is created with validate_shape=false subsequent tf.assign operations do update both data and the shape setting the shape to and respectively. pythonimport numpy as npimport tensorflow as tfdtype np.floatshape ph tf.placeholder(dtype=dtype)var tf.variable(tf.ones(shape dtype=dtype validate_shape=true)op tf.assign(var ph validate_shape=false)init_op tf.initialize_all_variables()with tf.session as s s.run(init_op print s.run(tf.shape(var print s.run(var s.run(op feed_dict={ph np.ones(shape dtype=dtype print s.run(tf.shape(var should be print s.run(var s.run(op feed_dict={ph np.ones(shape dtype=dtype print s.run(tf.shape(var should be print s.run(var what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link
187939000,5474,https://api.github.com/repos/tensorflow/tensorflow/issues/5474,meteorcloudy,1,0,0,0,0,0,gunan
187935180,5473,https://api.github.com/repos/tensorflow/tensorflow/issues/5473,keithyin,1,0,0,0,0,0,here is the details:modprobe error libkmod/libkmod-module.c kmod_module_insert_module could not find module by name=nvidia__uvmmodprobe error could not insert nvidia__uvm unknown symbol in module or unknown parameter see dmesg)e tensorflow/stream_executor/cuda/cuda_driver.cc failed call to cuinit cuda_error_unknowni tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel driver does not appear to be running on this host gzhao-b proc/driver/nvidia/version does not existi tensorflow/core/common_runtime/gpu/gpu_init.cc no gpu devices available on machine.it worked fine yesterday
187876438,5468,https://api.github.com/repos/tensorflow/tensorflow/issues/5468,AlvinChen13,2,0,0,0,0,0,nvidia releases p p and p gpus and it also releases tensorrt for inference to support fp and int any plans for tensorflow
187772339,5457,https://api.github.com/repos/tensorflow/tensorflow/issues/5457,girving,1,0,0,0,0,0,here you go vrv for some reason my previous edit to tools/git/.gitignore just vanished this adds it to the right place
187666994,5443,https://api.github.com/repos/tensorflow/tensorflow/issues/5443,isabeaups,1,0,0,0,0,0,"thank you very much for providing the numpy einsum feature in tensorflow that is really great the documentation for einsum says to look at the numpy documentation as it provides the same api it is not exactly the same one difference is the possibility of using in numpy which seems not to be implemented in tensorflow this would definitely be a nice feature to have in tensorflow also since that way one could build various function/transformations in tensorflow which are not dependent on the number of axis the tensor has i.e the specific use case here is the error i encountered when trying to do that: d tf.einsum(i...,ij->j...,c,b assertionerror traceback most recent call last ipython-input--dceca in module d tf.einsum(i...,ij->j...,c,b anaconda/envs/chaos/lib/python./site-packages/tensorflow/python/ops/special_math_ops.py in einsum(axes inputs match re.match(( a-z, +)->( a-z axes assert match indices have incorrect format s axes inputs list(inputs assertionerror indices have incorrect format i...,ij->j"
187486266,5411,https://api.github.com/repos/tensorflow/tensorflow/issues/5411,snnn,1,0,0,0,0,0,add resource_variable_ops to cmake
187041314,5375,https://api.github.com/repos/tensorflow/tensorflow/issues/5375,Dannyzen,2,0,0,0,0,0,this is a helpful note for anyone who might come across the following error building tensorflow on arch linux i may also have some funky ssh-agent settings causing this issue): error home/user/testing/tensorflow/tensorflow/tools/tfprof/build no such package linenoise error cloning repository ssh://git@github.com/antirez/linenoise/archive/..tar.gz auth cancel caused by ssh://git@github.com/antirez/linenoise/archive/..tar.gz auth cancel caused by auth cancel and referenced by tensorflow/tools/tfprof:tfprof.error home/user/testing/tensorflow/tensorflow/tools/tfprof/build no such package linenoise error cloning repository ssh://git@github.com/antirez/linenoise/archive/..tar.gz auth cancel caused by ssh://git@github.com/antirez/linenoise/archive/..tar.gz auth cancel caused by auth cancel and referenced by tensorflow/tools/tfprof:tfprof.error evaluation of query deps((//tensorflow union bazel_tools//tools/jdk:toolchain failed errors were encountered while computing transitive closure. i was able to remedy this problem by changing the block for linenoise tensorflow/workspace.bzl to native.new_local_repository name linenoise path path/to/local_repository build_file str(label(//:linenoise.build
186860816,5357,https://api.github.com/repos/tensorflow/tensorflow/issues/5357,Randl,1,0,0,0,0,0,bazel fails to clean with following message warning output base home/username/.cache/bazel/_bazel_username/acfafedfdeddda is on nfs this may lead to surprising failures and undetermined behavior..info starting clean this may take a while consider using expunge_async if the clean takes more than several minutes.error home/username/.cache/bazel/_bazel_username/acfafedfdeddda/server directory not empty). replacement of bazel clean expunge with bazel clean expunge_async here fixes the issue environment infooperating system:ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud ls l usr/local/cuda-./targets/x_-linux/lib/libcud*-rw-r--r root root sep usr/local/cuda-./targets/x_-linux/lib/libcudadevrt.alrwxrwxrwx root root sep usr/local/cuda-./targets/x_-linux/lib/libcudart.so libcudart.so..lrwxrwxrwx root root sep usr/local/cuda-./targets/x_-linux/lib/libcudart.so libcudart.so...-rw-r--r root root sep usr/local/cuda-./targets/x_-linux/lib/libcudart.so...-rw-r--r root root sep usr/local/cuda-./targets/x_-linux/lib/libcudart_static.a the commit hash git rev-parse head git rev-parse headcbdcbfdbbaebc the output of bazel version bazel versionwarning output base home/username/.cache/bazel/_bazel_chaimb/acfafedfdeddda is on nfs this may lead to surprising failures and undetermined behavior.build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri oct build timestamp build timestamp as int
186749207,5350,https://api.github.com/repos/tensorflow/tensorflow/issues/5350,DavidNemeskey,1,0,0,0,0,0,as it is now the softmax samplers sampled_softmax_loss and nce_loss require a c|x|h weight matrix while the logits from softmax come from a h|x|c| -sized one this discrepancy is problematic on two levels it makes for an inconsistent api it results in a performance hit because if one uses a sampled method for training and softmax for testing as is usual when the number of classes is huge one has to call tf.transpose somewhere which does not work well with sparse input see in my case i can choose between or train test wps depending on whether the tf.transpose happens on the sampled softmax loss during training or on regular softmax during testing in either case the performance is suboptimal what related github issues or stackoverflow threads have you found by searching the web for your problem?as mentioned above the fix there however is in the client code not the api in question environment infooperating system linux amd smp debian ckt-+debu x gnu/linux installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): /usr/local/cuda-./lib/libcudart.so i dont have cudnn at the momentif installed from binary pip package provide a link to the pip package you installed the official gpu install for python the output from python c import tensorflow print(tensorflow.__version rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)i cannot do that right now but one can experiment with e.g the ptb example what other attempted solutions have you tried?n/a
186741456,5348,https://api.github.com/repos/tensorflow/tensorflow/issues/5348,llhe,1,0,0,0,0,0,this change address this issue
186739505,5347,https://api.github.com/repos/tensorflow/tensorflow/issues/5347,llhe,3,0,0,0,0,0,tensorflow build dependens on external links for example: python native.new_http_archive name gif_archive url sha abadbeebeacfdfecfccbaafd strip_prefix giflib-../lib build_file str(label(//:gif.build looks like is just a sf mirror located in brazil and not quite stable see i also encountered this some times considering change to another stable one for example list of sf sites/mirrors
186692117,5342,https://api.github.com/repos/tensorflow/tensorflow/issues/5342,lan2720,1,0,0,0,0,0,"hi all,when i use tf.gather_nd it gives me the error notimplementederror gradient for gather_nd is not implemented.i can temporarily solve this problem to flatten both of params and indices and then use tf.gather but it isnt the best way to do so.is there any plan to support that thanks"
186605041,5333,https://api.github.com/repos/tensorflow/tensorflow/issues/5333,spiantino,5,0,0,0,0,0,with xcode-select pointed to xcode xcode-select p/applications/xcode.app/contents/developer i get the following compile error with build_all_ios.sh on sierra: checking whether we are cross compiling configure error in users/serkan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf:configure error cannot run c compiled programs.if you meant to cross compile use host.see config.log for more details not sure if this is related to xcode or to code signing on sierra
186544537,5326,https://api.github.com/repos/tensorflow/tensorflow/issues/5326,ethancaballero,3,0,0,0,0,0,context request to add optimizer that can perform sparse updates so that scaling memory-augmented neural networks with sparse reads and writes from deepmind can be implemented in tensorflow.@alrojo
186479590,5319,https://api.github.com/repos/tensorflow/tensorflow/issues/5319,AFAgarap,1,0,0,0,0,0,i upgraded to rc as you have advised tatatodd in issue i did a sudo configure and this was the result sudo configure~/tensorflow tensorflowplease specify the location of python default is usr/bin/python usr/bin/pythondo you wish to build tensorflow with google cloud platform support y/n nno google cloud platform support will be enabled for tensorflowdo you wish to build tensorflow with hadoop file system support y/n nno hadoop file system support will be enabled for tensorflowfound possible python library paths usr/local/lib/python./dist-packages usr/lib/python/dist-packagesplease input the desired python library path to use default is usr/local/lib/python./dist-packages /usr/local/lib/python./dist-packagesdo you wish to build tensorflow with gpu support y/n ygpu support will be enabled for tensorflowplease specify which gcc should be used by nvcc as the host compiler default is usr/bin/gcc please specify the cuda sdk version you want to use e.g leave empty to use system default please specify the location where cuda toolkit is installed refer to readme.md for more details default is usr/local/cuda usr/local/cuda-.please specify the cudnn version you want to use leave empty to use system default please specify the location where cudnn library is installed refer to readme.md for more details default is usr/local/cuda libcudnn.so resolves to libcudnn.please specify a list of comma-separated cuda compute capabilities you want to build with.you can find the compute capability of your device at note that each additional compute capability significantly increases your build time and binary size. default is info starting clean this may take a while consider using expunge_async if the clean takes more than several minutes..error com.google.devtools.build.lib.packages.buildfilecontainserrorsexception error loading package encountered error while reading extension file cuda/build_defs.bzl no such package local_config_cuda//cuda traceback most recent call last file home/darth/tensorflow/third_party/gpus/cuda_configure.bzl line create_cuda_repository(repository_ctx file home/darth/tensorflow/third_party/gpus/cuda_configure.bzl line in create_cuda_repository cuda_toolkit_path(repository_ctx cuda_version file home/darth/tensorflow/third_party/gpus/cuda_configure.bzl line in cuda_toolkit_path str(repository_ctx.path(cuda_toolkit file home/darth/tensorflow/third_party/gpus/cuda_configure.bzl line in str repository_ctx.path(cuda_toolkit_path).realpathobject of type path has no field realpath. thanks in advance for your response
186365993,5307,https://api.github.com/repos/tensorflow/tensorflow/issues/5307,ghost,0,0,0,1,0,0,with eigen-latest following error occurs:./third_party/eigen/unsupported/eigen/cxx/tensor fatal error unsupported/eigen/cxx/tensor no such file or directorywhen changed to eigen build completes without errors.other mentions of this error
186355765,5305,https://api.github.com/repos/tensorflow/tensorflow/issues/5305,eamartin,2,0,0,0,0,0,im repeatedly making small tweaks to graphs and visualizing them in tensorboard.here are two feature requests for tensorboard that would make this workflow significantly smoother:()allow the user to specify that a node should always be drawn in the main graph or as an auxiliary node alternatively add an option to draw all nodes in main graph.currently tensorboard chooses to put nodes in a subgraph as auxiliary nodes within the subgraph adding a node to the main graph consists of open subgraph click auxiliary node to add to main graph which needs to be repeated for each auxiliary node to move because add to main graph collapses all subgraphs this pain could also be partially alleviated by an option to shift click to add/remove multiple nodes from the main graph at once.()allow the user to force edges to be drawn when both nodes are in the main graph.example case:let op x be in the top level graph let sg be a subgraph and let sg contain subgraphs f_i for i that all take x as input.tensorboard will draw an edge from x to sg but each f_i will have its dependence on x noted as an auxiliary input there is no way to add x to the main graph because it is already in the main graph but visualized as an aux input it would nice for there to be a way for there to be one edge from x to sg and then edges connecting the entry point of x into sg with each of the f_i ideally this option would also have a programmatic interface or at least a default to set
186184212,5293,https://api.github.com/repos/tensorflow/tensorflow/issues/5293,llhe,3,0,0,0,0,0,mr/spark are commonly used for etl and feature generation its better to support close integration with such systems more specifically supporting the following tfrecord file mapreduce inputformat/outputformat integrating feature/example proto classes
186130127,5284,https://api.github.com/repos/tensorflow/tensorflow/issues/5284,LaceyChen17,2,0,0,0,0,0,"i tried to train a binary dnnclassifier similar as the example on got the following traceback traceback.txt explored the tensorflow source code and found that it may relate to get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py! image doing a binary classification,y_shape is something like batch_size, .line changes it into line change it into and finally the output_shape is batch_size, .however the correct ouput shape should be batch_size, .to sum up,we do not need to skip st dimension if it is and len(y_shape"
185724062,5243,https://api.github.com/repos/tensorflow/tensorflow/issues/5243,kushia,2,0,0,0,0,0,"hi everyone,im trying to make prediction with the cifar model in tensorflow/models/image/cifarbut it dont work.here is the code i tried from pil import imageimport tensorflow as tffrom tensorflow.models.image.cifar import cifarimport itertoolswidth height categories airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck filename toto.jpg absolute path to input imageim image.open(filename)im.save(filename format=jpeg subsampling quality=)x tf.placeholder(tf.float none logits cifar.inference(x top_k_pred tf.nn.top_k(logits k=)init_op tf.initialize_all_variables()with tf.session as sess restore variables from training checkpoint input_img tf.image.decode_jpeg(tf.read_file(filename channels tf_cast tf.cast(input_img tf.float float_image tf.image.resize_image_with_crop_or_pad(tf_cast height width images tf.expand_dims(float_image i images.eval print i sess.run(init_op feed_dict={x i variable_averages tf.train.exponentialmovingaverage cifar.moving_average_decay variables_to_restore variable_averages.variables_to_restore saver tf.train.saver(variables_to_restore ckpt tf.train.get_checkpoint_state(/tmp/cifar_train if ckpt and ckpt.model_checkpoint_path print(ckpt.model_checkpoint_path ckpt.model_checkpoint_path saver.restore(sess ckpt.model_checkpoint_path else print(no checkpoint file found exit top_indices sess.run top_k_pred for key value in enumerate(top_indices print categories value str(_ key )) and here is the error i got invalidargumenterror see above for traceback assign requires shapes of both tensors to match lhs shape rhs shape node save/assign assign t=dt_float class= loc:@local/weights use_locking=true validate_shape=true device=/job:localhost/replica:/task:/cpu: (local/weights save/restorev_) so it seems than tensorflow is not happy because the shape of the single image i want to predict dont fit with the shape of a normal batch but for one single image its necessary like this.maybe i miss something but i dont get it or maybe its a bug i already asked the question on stackoverflow in first place here since it could be a bug and nobody answered i post here..thanks in advance"
185138447,5194,https://api.github.com/repos/tensorflow/tensorflow/issues/5194,Jihadik,1,0,0,0,0,0,from the docs it seems that ctc decoder have problems and it cant handle labelings like a b b but if you will look at the code it is not the case everything is ok just typo in the docs
184772397,5163,https://api.github.com/repos/tensorflow/tensorflow/issues/5163,abhishekbanthia,0,0,0,0,0,1,environment infooperating system macos running tensorflow rc im trying to retrain the inceptions final layer with my own set of images and im running into this error:
184702364,5151,https://api.github.com/repos/tensorflow/tensorflow/issues/5151,aashish-kumar,1,0,0,0,0,0,i am not able to run the example for mnist python fully_connected_feed.py extracting data/train-images-idx-ubyte.gzextracting data/train-labels-idx-ubyte.gzextracting data/tk-images-idx-ubyte.gzextracting data/tk-labels-idx-ubyte.gzerror log traceback most recent call last file fully_connected_feed.py line in module tf.app.run file home/aashishk/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv file fully_connected_feed.py line in main run_training file fully_connected_feed.py line in run_training summary tf.summary.merge_all()attributeerror module object has no attribute merge_all what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system ubuntuinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud none tensorflow installed using conda i am only taking the examples folder from here to test with the above version
184664768,5143,https://api.github.com/repos/tensorflow/tensorflow/issues/5143,mihail911,1,0,0,0,0,0,hi all im trying to build tf from source so that i can run it in development mode and be able to expand the functionality of some of the modules but am running into some snags with the bazel building environment infooperating system:mac osx yosemiteif installed from source provide the commit hash git rev-parse head ecfefedbbfbeefafff the output of bazel version build label bbc build target bazel-out/local opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jar build time sun oct build timestamp build timestamp as int running bazel build c opt tensorflow/tools/pip_package:build_pip_package verbose_failures gives warning sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed see for more information you can turn off this warning via ignore_unsupported_sandboxing.error users/mihaileric/documents/research/tensorflow/tensorflow/python/build in cc_library rule tensorflow/python:tf_session_helper non-test target tensorflow/python:tf_session_helper depends on testonly target tensorflow/python:construction_fails_op and doesnt have testonly attribute set.error analysis of target tensorflow/tools/pip_package:build_pip_package failed build aborted.info elapsed time s im having a lot of difficulty interpreting these error messages and am not finding too much on stack overflow or other tf git threads any help would be greatly appreciated
184655425,5141,https://api.github.com/repos/tensorflow/tensorflow/issues/5141,ohadle,1,0,0,0,0,0,i just installed rc from pip tf_binary_url with a brand-new cuda installation and got this error: in import tensorflow as tf---------------------------------------------------------------------------importerror traceback most recent call last)
184619608,5134,https://api.github.com/repos/tensorflow/tensorflow/issues/5134,beastlovezxc,5,0,0,0,0,0,"hello,when i install tensorflow with the version of os x python gpu it brings me the error as http error while getting could not install requirement tensorflow==..rc from because of error client error not found for url to solve it"
184484101,5114,https://api.github.com/repos/tensorflow/tensorflow/issues/5114,odellus,0,0,1,0,0,0,environment ubuntu installed version of cuda and cudnn rw-r--r root root sep usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root sep usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root sep usr/local/cuda-./lib/libcudart.so libcudart.so...-rw-r--r root root sep usr/local/cuda-./lib/libcudart.so...-rw-r--r root root sep usr/local/cuda-./lib/libcudart_static.alrwxrwxrwx root root oct usr/local/cuda-./lib/libcudnn.so libcudnn.so...lrwxrwxrwx root root oct usr/local/cuda-./lib/libcudnn.so libcudnn.so...lrwxrwxrwx root root oct usr/local/cuda-./lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root oct usr/local/cuda-./lib/libcudnn.so...-rw-r--r root root oct usr/local/cuda-./lib/libcudnn_static.a the commit hash git rev-parse head bacbccfefadabc the output of bazel version bazel versionbuild label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri oct build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried?i tried to use tf.gather_nd but it doesnt work on the gpu at this time so im using an alternative logs or other output that would be helpful(if logs are large please upload as attachment or provide link
184449924,5112,https://api.github.com/repos/tensorflow/tensorflow/issues/5112,qingzew,1,0,0,0,0,0,now tensorflow can read data from hdfs so how can i write checkpoint to hdfs for example pythonsaver.save(save_path=hdfs://some_dir
184050953,5073,https://api.github.com/repos/tensorflow/tensorflow/issues/5073,cancan101,1,0,0,0,0,0,"now that i am using the rc i am seeing a lot of this printed out: info in saver output//export/-tmp/export-?????-of is not in all_model_checkpoint_paths manually adding it. i believe when i call done once:saver tf_saver.saver(sharded=true)model_exporter exporter.exporter(saver)model_exporter.init done each epoch step:model_exporter.export(export_path tf.constant(i sess environment infooperating system:using nvidia docker, uname alinux bdceac generic ubuntu smp fri jun utc x x x gnu/linux installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud ls l usr/local/cuda/lib/libcud*-rw-r--r root root aug usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root aug usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root aug usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root aug usr/local/cuda/lib/libcudart.so...-rw-r--r root root aug usr/local/cuda/lib/libcudart_static.a the commit hash git rev-parse head ffcaefedfffbcaddd the output of bazel version bazel versioninfo reading startup options from root/.bazelrc batchextracting bazel installation...build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri jul build timestamp build timestamp as int"
184050195,5072,https://api.github.com/repos/tensorflow/tensorflow/issues/5072,mkolod,2,0,0,0,0,0,please refer to issue for details
184031965,5071,https://api.github.com/repos/tensorflow/tensorflow/issues/5071,guschmue,8,0,0,2,0,6,code changes to enable gpu support for windows
183977880,5069,https://api.github.com/repos/tensorflow/tensorflow/issues/5069,levelvc,1,0,0,0,0,0,using xcode and osx sierra the tensorflow ios build fails with: undefined symbols for architecture armv google::protobuf::any::mergefrom(google::protobuf::any const referenced from tensorflow::metagraphdef_metainfodef::unsafemergefrom(tensorflow::metagraphdef_metainfodef const in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::internal::generictypehandler::merge(google::protobuf::any const google::protobuf::any in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::internal::wireformatlite::writebytesmaybealiased(int std::__::basic_string::new(google::protobuf::arena in libtensorflow-core-armv.a(test_log.pb.o note a missing vtable usually means the first non-inline virtual member function has no definition typeinfo for google::protobuf::any referenced from google::protobuf::any google::protobuf::arena::createmaybemessage(google::protobuf::arena in libtensorflow-core-armv.a(test_log.pb.o tensorflow::metagraphdef_metainfodef::unsafemergefrom(tensorflow::metagraphdef_metainfodef const in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::metagraphdef_metainfodef::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::internal::initemptystring referenced from tensorflow::protobuf_initdefaults_tensorflow_fcore_futil_ftest_flog_eproto_impl in libtensorflow-core-armv.a(test_log.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_futil_fsaved_ftensor_fslice_eproto_impl in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_futil_fmemmapped_ffile_fsystem_eproto_impl in libtensorflow-core-armv.a(memmapped_file_system.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_futil_fevent_eproto_impl in libtensorflow-core-armv.a(event.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_fprotobuf_ftensorflow_fserver_eproto_impl in libtensorflow-core-armv.a(tensorflow_server.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_fprotobuf_fsaver_eproto_impl in libtensorflow-core-armv.a(saver.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_fprotobuf_fqueue_frunner_eproto_impl in libtensorflow-core-armv.a(queue_runner.pb.o google::protobuf::io::codedoutputstream::default_serialization_deterministic referenced from tensorflow::benchmarkentry::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::cpuinfo::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::jobdef::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(tensorflow_server.pb.o tensorflow::metagraphdef::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::signaturedef::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::configproto::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(config.pb.o tensorflow::nodedef::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(node_def.pb.o google::protobuf::internal::mapfieldbase::spaceusedexcludingselfnolock const referenced from vtable for google::protobuf::internal::typedefinedmapfieldbase(google::protobuf::arena in libtensorflow-core-armv.a(test_log.pb.o tensorflow::metagraphdef_metainfodef::unsafemergefrom(tensorflow::metagraphdef_metainfodef const in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::metagraphdef_metainfodef::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::any_default_instance referenced from tensorflow::protobuf_initdefaults_tensorflow_fcore_fprotobuf_fmeta_fgraph_eproto_impl in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::metagraphdef_metainfodef::unsafemergefrom(tensorflow::metagraphdef_metainfodef const in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::any::bytesizelong const referenced from tensorflow::machineconfiguration::bytesizelong const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::metagraphdef_metainfodef::bytesizelong const in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::collectiondef_anylist::bytesizelong const in libtensorflow-core-armv.a(meta_graph.pb.o tensorflow::collectiondef::bytesizelong const in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::protobuf_adddesc_google_fprotobuf_fany_eproto referenced from tensorflow::protobuf_adddesc_tensorflow_fcore_futil_ftest_flog_eproto_impl in libtensorflow-core-armv.a(test_log.pb.o tensorflow::protobuf_adddesc_tensorflow_fcore_fprotobuf_fmeta_fgraph_eproto_impl in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::io::codedinputstream::readlengthandpushlimit referenced from tensorflow::benchmarkentries::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::machineconfiguration::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::savedslicemeta::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::savedtensorslicemeta::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::memmappedfilesystemdirectory::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(memmapped_file_system.pb.o tensorflow::clusterdef::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(tensorflow_server.pb.o tensorflow::metagraphdef::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::internal::repeatedptrfieldbase::internalextend(int referenced from tensorflow::benchmarkentries::unsafemergefrom(tensorflow::benchmarkentries const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::machineconfiguration::unsafemergefrom(tensorflow::machineconfiguration const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::savedslicemeta::unsafemergefrom(tensorflow::savedslicemeta const in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::savedtensorslicemeta::unsafemergefrom(tensorflow::savedtensorslicemeta const in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::memmappedfilesystemdirectory::unsafemergefrom(tensorflow::memmappedfilesystemdirectory const in libtensorflow-core-armv.a(memmapped_file_system.pb.o tensorflow::clusterdef::unsafemergefrom(tensorflow::clusterdef const in libtensorflow-core-armv.a(tensorflow_server.pb.o tensorflow::queuerunnerdef::unsafemergefrom(tensorflow::queuerunnerdef const in libtensorflow-core-armv.a(queue_runner.pb.o google::protobuf::io::codedinputstream::checkentiremessageconsumedandpoplimit(int referenced from tensorflow::benchmarkentries::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::machineconfiguration::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::savedslicemeta::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::savedtensorslicemeta::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::memmappedfilesystemdirectory::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(memmapped_file_system.pb.o tensorflow::clusterdef::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(tensorflow_server.pb.o tensorflow::metagraphdef::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::io::codedinputstream::readvarintfallback referenced from tensorflow::benchmarkentry::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::commitid::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::cpuinfo::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::memoryinfo::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::availabledeviceinfo::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::testresults::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o google::protobuf::internal::mapentrylite(google::protobuf::arena in libtensorflow-core-armv.a(test_log.pb.o google::protobuf::internal::mapfield::init in libtensorflow-core-armv.a(test_log.pb.o void google::protobuf::arena::own(google::protobuf::internal::mutex in libtensorflow-core-armv.a(test_log.pb.o google::protobuf::internal::wireformatlite::verifyutfstring(char const int google::protobuf::internal::wireformatlite::operation char const referenced from tensorflow::entryvalue::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::entryvalue::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::entryvalue::internalserializewithcachedsizestoarray(bool unsigned char const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::benchmarkentry::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o tensorflow::benchmarkentry::serializewithcachedsizes(google::protobuf::io::codedoutputstream const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::benchmarkentry::internalserializewithcachedsizestoarray(bool unsigned char const in libtensorflow-core-armv.a(test_log.pb.o tensorflow::buildconfiguration::mergepartialfromcodedstream(google::protobuf::io::codedinputstream in libtensorflow-core-armv.a(test_log.pb.o google::protobuf::protobuf_initdefaults_google_fprotobuf_fany_eproto referenced from tensorflow::protobuf_initdefaults_tensorflow_fcore_futil_ftest_flog_eproto_impl in libtensorflow-core-armv.a(test_log.pb.o tensorflow::protobuf_initdefaults_tensorflow_fcore_fprotobuf_fmeta_fgraph_eproto_impl in libtensorflow-core-armv.a(meta_graph.pb.o google::protobuf::internal::mergefromfail(char const int referenced from tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(test_log.pb.o tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(saved_tensor_slice.pb.o tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(memmapped_file_system.pb.o tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(event.pb.o tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(tensorflow_server.pb.o tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(saver.pb.o tensorflow::(anonymous namespace)::mergefromfail(int in libtensorflow-core-armv.a(queue_runner.pb.o google::protobuf::internal::generatedmessagereflection::generatedmessagereflection(google::protobuf::descriptor const google::protobuf::message const int const int int int google::protobuf::descriptorpool const google::protobuf::messagefactory int int referenced from tensorflow::(anonymous namespace)::protobuf_registertypes(std::__::basic_string
183937489,5068,https://api.github.com/repos/tensorflow/tensorflow/issues/5068,yuvval,1,0,0,0,0,0,with slim.learning.train tf i would like to restore a model from a checkpoint and continue the training the model had a successful training session and i would like to fine tune it however when i do that tf crash with an error init operations did not make model ready. i do the training with: tf.contrib.slim.learning.train train_op train_dir log_every_n_steps=flags.log_every_n_steps graph=g global_step=model.global_step number_of_steps=flags.number_of_steps init_fn=model.init_fn saver=model.saver session_config=session_config) i tried alternatives following this doc none with g.as_default model_path tf.train.latest_checkpoint(train_dir if model_path def restore_fn(sess tf.logging.info restoring sa&t variables from checkpoint file s restore_fn.model_path model.saver.restore(sess restore_fn.model_path restore_fn.model_path model_path model.init_fn restore_fn else model.init_fn none following slim doc g.as_default model_path tf.train.latest_checkpoint(train_dir if model_path variables_to_restore tf.contrib.slim.get_variables_to_restore model.init_fn tensorflow.contrib.framework.assign_from_checkpoint_fn model_path variables_to_restore else model.init_fn none what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system:linuxinstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally rc logs or other output that would be helpful(if logs are large please upload as attachment or provide link). alternative.txt
183877607,5066,https://api.github.com/repos/tensorflow/tensorflow/issues/5066,acrosson,2,0,0,0,0,0,i am unable to run a tf script on a single gpu both of my gtx s memory are being fully absorbed by tensorflow when the model is initialized but only one of the gpu is being used for computations based on what im seeing in nvidia-smi).because both gpus memory are fully occupied i cannot run two models at once.
183799925,5052,https://api.github.com/repos/tensorflow/tensorflow/issues/5052,schmuell,1,0,0,0,0,0,cmake changes to build gpu support for windows for this to work you need to have the code changes from
183797467,5051,https://api.github.com/repos/tensorflow/tensorflow/issues/5051,schmuell,2,0,0,0,0,0,code changes to enable gpu support under windows tested on windows cuda sdk and cudnn cmake changes to build this come in a nd pull request
183680254,5039,https://api.github.com/repos/tensorflow/tensorflow/issues/5039,timmeinhardt,5,0,0,0,0,0,since many of us train with tensorflow on a remote server and use tensorboard to monitor and evaluate the training i think it would be useful to add a delete option for specific runs on the tensorboard dashboard.when i debug a training process i monitor what the network is doing and sometimes restart the process very quickly like this my data directory easily gets filled with a lot of fail runs deleting them from command is a little tedious cause you always have to check in tensorboard which ones you might want to keep and usually you have to close tensorboard to be able to delete them at all.maybe you guys have a suggestion for an improved workflow otherwise i think being able to delete runs directly from tensorboard would be a nice addition as part of this an active flag for whether a run is still running might be interesting as well obviously this flag would disable the deletion option.i am looking forward to your feedback!edit if this is really a feature worth integrating i would be happy to look into it and submit a pr
183620163,5036,https://api.github.com/repos/tensorflow/tensorflow/issues/5036,Hvass-Labs,78,0,0,0,0,0,would it be possible for the tensorflow developers to put a tar-ball online with the inception v saved as a metagraph i cant find it anywhere.im currently using the following tar-ball with a frozen graph for inception v problem is that i cannot continue training that graph because it is frozen so all the variables have been converted to constants before it was saved i cant find a way to convert the constants back to variables so i dont think that is possible there are also some deprecation warnings regarding batchnormwithglobalnormalization so it will presumably stop working at some point in the future).after searching for a solution for days i found that you have released a newer checkpoint-file for inception v downloaded it but its only the checkpoint-file not the graph-definition so in my python code i would apparently have to create the inception graph using this function first this apparently requires building tensorflow from source as far as i could understand from the readme theres also several options for using the function and it apparently has to be wrapped in arg_scopes and what-not it be possible to update the above tar-ball dated so it also contains the metagraph-files so i can load it more easily and use it in my own python program i have another data-set so i replace the softmax-layer of the inception-graph and i also want to continue optimizing the rest of the variables of the inception-graph.please also consider including a small example program in the tar-ball or a link to some python-code as it would make it a lot easier for everyone who wants to use it or at least make a list of all the relevant tensor-names input output etc.)thanks
183447150,5010,https://api.github.com/repos/tensorflow/tensorflow/issues/5010,c0g,1,0,0,0,0,0,adds a gpu version of the triangular solver op using cublas trsm
183393206,5005,https://api.github.com/repos/tensorflow/tensorflow/issues/5005,alexshires,5,0,0,0,0,0,"hi all,the changes to the metric creation in estimator in tflearn dont match the current documentation on monitors for example accuracy tf.contrib.metrics.streaming_accuracy precision tf.contrib.metrics.streaming_precision recall tf.contrib.metrics.streaming_recall}validation_monitor tf.contrib.learn.monitors.validationmonitor test_set.data test_set.target every_n_steps metrics=validation_metrics) wheras single head metrics if isinstance(predictions dict raise valueerror metrics passed provide only name no prediction but predictions are dict metrics s targets s metrics targets what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system:max osx el capitaninstalled version of cuda and cudnn:n/a cpu version the output from python c import tensorflow print(tensorflow.__version__) ...rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried?looking up metricspec configuration no examples or help available logs or other output that would be helpful(if logs are large please upload as attachment or provide link"
183297552,5000,https://api.github.com/repos/tensorflow/tensorflow/issues/5000,abhitopia,1,0,0,0,0,0,problemthe estimator.fit function takes as argument either x y and batch_size where x and y could be numpy arrays or iterators pros easy to use allows feeding data from arbitrary source as long as problem can be decomposed into x and y cons no provision to provide epoch in case x and y are arrays the data aggregate must be available as opposed to reading on the fly say from database whether array or iterator x and y cant be dictionaries most complex problems cannot be reduced to input matrix and output matrix and may require multiple input features matrices input_fn this is callback function which must return features and target tensors or dictionary of tensors pros allows feeding data from arbitrary source in theory returned features and targets can be dictionary thus allowing to solve complex problems which takes multiple inputs cons only found support for reading files using read_batch_examples read_batch_features read_batch_record_features etc no support for passing placeholder and feed_fn to allow for arbitrary source of input data which dont require queue relevant discussions additional problemcurrently the self.feature_info feature signature must be same for training as well as evaluation however there can be cases where evaluation if done differently than training for example dualencoderlstm model where training just requires context and utterance while evaluation validation requires context and multiple utterances solutionthis pr basically combines the pros of using x y and batch_size with pros of input_fn by allowing x y to be dictionary of multiple arrays this also works for x and y iterators returning dictionary of streaming data also added support for different feature/target signature for training and evaluation
183248664,4991,https://api.github.com/repos/tensorflow/tensorflow/issues/4991,tolga-b,1,0,0,0,0,0,hi sorry i am a bit new to tensorflow and could not find this error anywhere else.my system is:ubuntu ltscuda cudnn on titan x pascal -rw-r--r root root oct usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root oct usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root oct usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root oct usr/local/cuda/lib/libcudart.so...-rw-r--r root root oct usr/local/cuda/lib/libcudart_static.alrwxrwxrwx root root oct usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx root root oct usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root oct usr/local/cuda/lib/libcudnn.so...-rw-r--r root root oct usr/local/cuda/lib/libcudnn_static.a tensorflow fcdbefcccdceebafbf)bazel my problem is: import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallysess tf.session()a tf.constant shape name=a)b tf.matmul(a a)print sess.run(b)e tensorflow/core/framework/op_kernel.cc opkernel op round device_type gpu constraint name t allowed_values list type dt_int for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type gpu constraint name t allowed_values list type dt_int for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type gpu constraint name t allowed_values list type dt_double for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type gpu constraint name t allowed_values list type dt_float for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type gpu constraint name t allowed_values list type dt_half for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type cpu constraint name t allowed_values list type dt_int for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type cpu constraint name t allowed_values list type dt_int for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type cpu constraint name t allowed_values list type dt_double for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type cpu constraint name t allowed_values list type dt_float for unknown op rounde tensorflow/core/framework/op_kernel.cc opkernel op round device_type cpu constraint name t allowed_values list type dt_half for unknown op round this error only comes up once could you please guide me on what is going on here?thank you
183209939,4987,https://api.github.com/repos/tensorflow/tensorflow/issues/4987,huzqatpku,1,0,0,0,0,0,since its convenient to support varied batchsize in tf.placeholder by setting the head element of argument shape to none tf.nn.dropout does not allow none in its argument noise_shape leading to models using dropout with noise_shape cumbersome to implement varied batchsize i wonder is it possible to allow none in noise_shape for better support for varied batchsize thank you
183194372,4984,https://api.github.com/repos/tensorflow/tensorflow/issues/4984,AndreasMadsen,3,0,0,0,0,0,"i think the problem is that as wasnt installed with gcc and thus gcc print-prog-name=as just prints as as is in my path but not in the same location as gcc which gcc/appl/gcc/../bin/gcc which as/appl/binutils/../bin/as what related github issues or stackoverflow threads have you found by searching the web for your problem?there are a couple gcc error trying to exec as execvp no such file or directory gcc error trying to exec as execvp no such file or directory building tensorflow with custom gcc requires hardcoded ld,nm and as i tried setting build action_env=path but that did change anything environment infooperating system uname alinux n el.x smp tue mar cdt x x x gnu/linux installed version of cuda and cudnn cuda cudnn please attach the output of ls l path/to/cuda/lib/libcud ls l appl/cuda/./lib/libcud*-rw-r--r sebo root sep appl/cuda/./lib/libcudadevrt.alrwxrwxrwx sebo root sep appl/cuda/./lib/libcudart.so libcudart.so..lrwxrwxrwx sebo root sep appl/cuda/./lib/libcudart.so libcudart.so...-rwxr-xr-x sebo root sep appl/cuda/./lib/libcudart.so...-rw-r--r sebo root sep appl/cuda/./lib/libcudart_static.a if installed from source provide the commit hash git rev-parse head fcdbefcccdceebafbf the output of bazel version bazel batch versioninfo reading startup options from zhome/ff///.bazelrc batch output_user_root=/work/s/.bazelbuild label ec)build target bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu oct build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code install tensorflowgit clone tensorflowcurl l git am set configuration parametersexport python_bin_path= which python stdpy/bin/pythonexport tf_need_gcp=export tf_need_hdfs=export tf_need_cuda=export gcc_host_compiler_path= which gcc appl/gcc/../bin/gccexport tf_cuda_version=.export cuda_toolkit_path=/appl/cuda/./export tf_cudnn_version=export cudnn_install_path=$home/cuda the cudnn version provide it is too old so i downloaded the latest and unpacked it in homeexport tf_cuda_compute_capabilities=.,.cat home/.bazelrc eof batch always run in batch mode since there are some firewall issues output_user_root home is nfs filesystem this will not work with bazel use workdir insteadstartup batch output_user_root=$workdir/.bazeleof one python configuration cant be set directly use yes to accept automaticallyyes cc=gcc cxx=g configure build tensorflowcc=gcc cxx=g bazel build copt=-w ignore_unsupported_sandboxing spawn_strategy=standalone verbose_failures c opt config=cuda tensorflow/tools/pip_package:build_pip_package what other attempted solutions have you tried ive tried symlinking gcc and as into the same path but then cc and the gcc header files cant be found ive tried searching for how bazel or tensorflow finds as but i cant find that code logs or other output that would be helpful(if logs are large please upload as attachment or provide link).full log error trying to exec as execvp no such file or directoryerror zhome/ff///tensorflow/tensorflow/contrib/rnn/build output tensorflow/contrib/rnn/_objs/python/ops/_gru_ops_gpu/tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.pic.o was not created.error zhome/ff///tensorflow/tensorflow/contrib/rnn/build not all outputs were created.target tensorflow/tools/pip_package:build_pip_package failed to buildinfo elapsed time s critical path s"
183019752,4965,https://api.github.com/repos/tensorflow/tensorflow/issues/4965,alexggmatthews,9,0,0,0,0,0,hamiltonian monte carlo hmc also known as hybrid monte carlo is an efficient markov chain monte carlo mcmc method that exploits gradient information to improve on the simpler mcmc methods see this freely available book chapter by radford neal has been successfully applied to bayesian inference in neural networks again by neal see for instance neals thesis which later became a book is heavily used in modern bayesian modelling for instance hmc and its variants are the primary inference method for stan a popular probabilistic programming language would be useful to be able to use hmc in tensorflow much as one is currently able to use optimizers such as adam or momentum optimization much of the requisite code would be similar to the optimizer code which can be found here raised this feature request as a place to discuss the potential addition of hmc to tensorflow
182995092,4964,https://api.github.com/repos/tensorflow/tensorflow/issues/4964,Hvass-Labs,1,0,0,0,0,0,in another thread i asked about consolidation of the builder apis for tensorflow response was that contrib/layers was going to be one of the official builder apis to be included in tensorflow core along with contrib/learn so i wanted to see how it works and learn how to use it but the only documentation i could find for contrib/layers was this this really all the documentation there is for contrib/layers surely there must be more you couldnt possibly expect people to be able to learn how to use it from these two documents alone the readme doesnt even make any sense am i missing something?please allow me a short rant.im sure youre familiar with scikit-learns beautifully designed api and extensive polished documentation it might serve as a good inspiration for tensorflow you may think that you dont have time to streamline the tensorflow api and improve the documentation because theres more important things that must be done but i believe this is actually the single most important thing you could do to move the project forward heres why:if each new person wastes hours trying to learn tensorflow and theres people who are learning to use it then its wasted developer-hours i actually think those numbers are very conservative personally ive probably wasted more than hours trying to figure out how the complicated and poorly documented tensorflow api works and it seems theres many more than people using tensorflow so maybe its more than million wasted developer-hours in total that is developer-years assuming a month is about work-hours imagine if this developer-time was put into more constructive use all it takes is for the tensorflow api and docs to be more polished so it would be easier to learn it would be a tiny investment in time and effort from the dev-team compared to the return youll get in productivity from the community i really wish this is something you would prioritize highly.thanks
182693259,4935,https://api.github.com/repos/tensorflow/tensorflow/issues/4935,AlvinChen13,1,0,0,0,0,0,based on stackoverflow the answer is no.if it is no does tf have plans to support infiniband
182385295,4903,https://api.github.com/repos/tensorflow/tensorflow/issues/4903,ror6ax,4,0,0,0,0,0,running tensorboard results in this logdir os.path.expanduser(flags.logdir)attributeerror nonetype object has no attribute logdir best practice in this case is arg(opt)parse
182347112,4897,https://api.github.com/repos/tensorflow/tensorflow/issues/4897,seerdecker,11,0,0,0,0,0,as described here tf is inflexible when it comes to access to the gradients provide a method where the user can retrieve the raw gradients not the averaged gradients requiring the user to compute their own gradients is impractical the framework should work for the user not the other way around.use case this is needed for reinforcement learning where the gradients of one net needs to be backpropagated through another net in separate steps
182093063,4880,https://api.github.com/repos/tensorflow/tensorflow/issues/4880,daj,2,0,0,0,0,0,"i was following the pip installation instructions when i ran the test the tensorflow installation steps i hit two errors listed below the fix!to fix i ran pip install pbr funcsigs then the test python program worked environment infooperating system mac osx if installed from binary pip package provide a link to the pip package you installed i installed this one mac os x cpu only python export tf_binary_url the output from python c import tensorflow print(tensorflow.__version__) . ..rc if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)i was following the existing instructions i dont think i missed any steps logs or other output that would be helpfulhere are the two errors that i saw import tensorflow as tftraceback most recent call last file stdin line in module file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python.platform import test file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/python/platform/test.py line in module import mock pylint disable=g-import-not-at-top,unused-import file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/mock/__init__.py line in module import mock.mock as mock file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/mock/mock.py line in module from pbr.version import versioninfoimporterror no module named pbr.version import tensorflow as tftraceback most recent call last file stdin line in module file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/__init__.py line in module from tensorflow.python import file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/python/__init__.py line in module from tensorflow.python.platform import test file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/python/platform/test.py line in module import mock pylint disable=g-import-not-at-top,unused-import file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/mock/__init__.py line in module import mock.mock as mock file usr/local/cellar/python/../frameworks/python.framework/versions/./lib/python./site-packages/mock/mock.py line in module import funcsigsimporterror no module named funcsigs"
181916425,4865,https://api.github.com/repos/tensorflow/tensorflow/issues/4865,eric-czech,3,0,0,0,0,0,this is a little difficult to explain and im guessing someone has to have run into it before but it would be nice to at least get an explanation for whats going on here.the issue im finding is that when you specify a model_dir argument for any tflearn model the first time you fit that model the tensorboard warning like the following will always appear: warning:tensorflow:found more than one graph event per run overwriting the graph with the newest event. if you clear the contents of that directory and rerun the same model the warning no longer appears.for example here is a piece of code to reproduce: import tensorflow as tfimport randomfrom tensorflow.contrib.learn.python import learnfrom tensorflow.contrib.learn.python.learn import datasetsfrom tensorflow.contrib.learn.python.learn.estimators._sklearn import accuracy_scorefrom tensorflow.contrib.learn.python.learn.estimators._sklearn import train_test_splitrandom.seed()iris datasets.load_iris()x_train x_test y_train y_test train_test_split(iris.data iris.target test_size random_state=)def custom_optimizer(learning_rate return tf.train.momentumoptimizer(learning_rate classifier learn.tensorflowdnnclassifier hidden_units n_classes steps learning_rate optimizer=custom_optimizer model_dir=/tmp/tf/bug_test_)classifier.fit(x_train y_train) if you run this in a jupyter notebook wait until it is done cd into tmp/tf/bug_test and run tensorboard logdir the warning will appear if you clear the contents and run again it will no longer appear if you in the same notebook within the same kernel change the directory to something else e.g tmp/tf/bug_test and run again the warning will again appear and the warning will again go away if you clear the contents and run again with the same directory.i also tried pre-creating the directory but that appears to make no difference.i wouldnt imagine this indicates a major problem but i try to take those warnings seriously since they have indicated more problematic issues in the past with running multiple models in the same kernel so it would be great to know what this means environment infooperating system os x yosemite tensorflow version
181901973,4860,https://api.github.com/repos/tensorflow/tensorflow/issues/4860,haosdent,1,0,0,0,0,0,this fixes
181880200,4854,https://api.github.com/repos/tensorflow/tensorflow/issues/4854,chenghuige,1,0,0,0,0,0,i will face this problem when reading tf records on hdfs may be running one hour or two hours then my program will crash down due to this assertion fail from tensorflow c code.can we just ignore this ie ignore the batch with datalosserro so can continue training next batch without having to stop the program
181790801,4836,https://api.github.com/repos/tensorflow/tensorflow/issues/4836,ppwwyyxx,5,0,0,0,0,0,currently it seems like the encouraged way to read data is either through the provided reader/producer ops which are not quite flexible or through python feed_dict which is slower.is there a plan to support something like a zeromqreaderop which keeps reading and deserializing tensors from a zmq socket and produce them this way we can use any comfortable way to process data perhaps in an independent process and send them through zmq for training update wrote my own at
181616829,4815,https://api.github.com/repos/tensorflow/tensorflow/issues/4815,bhack,3,0,0,0,0,0,do you want to support and contribute to the standardizzation of provisional nnef effort by kronos group google is still in the promoter member list
181616802,4814,https://api.github.com/repos/tensorflow/tensorflow/issues/4814,untom,27,0,0,0,0,0,hi currently the streaming metrics are as far as i know not resettable id like to be able to e.g reset the counter after each epoch this way having e.g a very bad accuracy in the beginning of training will not still influence the accuracy value ten epochs later it makes it easier to compare my results to runs obtained outside tensorflow.the only workaround i found is to do sess.run(tf.initialize_local_variables after each epoch but of course this can have bad side effects if i have other local variables that i dont want to reset.or is there a way to achieve what i want that i didnt think of
181557054,4809,https://api.github.com/repos/tensorflow/tensorflow/issues/4809,yaroslavvb,1,0,0,0,0,0,im trying to troubleshoot some slowness in our distributed models and it would be useful to have access to timing of send/recv ops across graph partition.cc suharshs because maybe statspublisherinterface is relevant?for instance a toy benchmark here adds mb vectors of s in one process to variable in another process on local machine if i look at timeline/stepstats i see ms of emptiness followed by ms in addop followed by another ms of missing time.because its a toy benchmark i can figure out that ms is spent in transferring mb from one tf runtime to another and ms is spent making the result available to python client but its harder to do this on a large model
181553295,4807,https://api.github.com/repos/tensorflow/tensorflow/issues/4807,mkolod,1,0,0,0,0,0,tensorflow seems to be using the classic libjpeg v but libjpeg-turbo is heavily optimized to leverage simd on various platforms x arm power etc and is both api and abi compatible with libjpeg at least up to libjpeg given tensorflows goal of portability across a wide varierty of platforms im wondering if libjpeg was decided upon instead of libjpeg-turbo for portability or was this just an omission libjpeg-turbo speeds up jpeg decoding by about x and since that reduces decoding latency and increases throughput it not only improves performance of tensorflow on the cpu but especially on the gpu which tends to wait for the cpu to provide decoded images moreover that could result in a major performance improvement on more cpu-underpowered platforms such as arm e.g via neon simd on the nvidia tx platform which uses the neon-capable cortex a or on arm cortex a on a samsung galaxy s
181136890,4776,https://api.github.com/repos/tensorflow/tensorflow/issues/4776,admcrae,1,0,0,0,0,0,this fixes the python issue found by gunan in
181133138,4775,https://api.github.com/repos/tensorflow/tensorflow/issues/4775,sjlee7748,1,0,0,0,0,0,hi alli want to use avx with tensorflow because vectorization can increase performance.in addition building any program with icpc(intel compiler and compile option xmic-avx can vectorize automatically in state-of-art intel machine.so i use the command as below: cc=icpc bazel build c opt copt=-xmic-avx tensorflow/tools/pip_package:build_pip_package but when i type the command build does not work what related github issues or stackoverflow threads have you found by searching the web for your problem nothing exist related my issue in github or stackoverflow environment info operating system centos tensorflow version r source code version only use cpu what other attempted solutions have you tried?as far as i know linear algebra library for tensorflow is eigen library and simd vectorizations(like sse avx etc are applied to the eigen library.so i changed eigen source code to apply the avx and gcc compile option mavxf referring two links( eigen bug report benoit steiners bitbucket but i didnt apply avx because there was little information about that.if someone know the method about applying avx or the date to release eigen version with avx please tell me about that.thank you very much
180969447,4762,https://api.github.com/repos/tensorflow/tensorflow/issues/4762,gibiansky,3,0,0,0,0,0,i am trying to understand the implementation of tf.while_loop and everything that is built on top of it because i am implementing a custom tf.graph subclass and finding that the way tf.while_loop is handled during gradient computation is important for what i am doing however i cannot find any documentation on the ops that comprise tf.while_loop is there any internal documentation on this implementation?i am finding myself confused about some of the following concepts whilecontext and in general the stack of contexts flows vs tensors framesso far ive gotten quite a bit by just reading the source code but its pretty hard to build yourself a good mental model by going completely bottom up without having any high level picture of how the entire thing is organized.if there is no intention to document these things which would be completely understandable please feel free to close this issue although i would definitely appreciate some help or pointers as to where i can figure out the high-level overview of looping implementation
180969014,4761,https://api.github.com/repos/tensorflow/tensorflow/issues/4761,alrojo,2,0,0,1,0,0,as advised by lukaszkaiser from initial commit of the tf.contrib.seqseq folder.the purpose of this pull request is to agree on a skeleton for the tf.contrib.seqseq folder i took inspiration from the tf.contrib.layers folder and the tf.contrib.losses folder.after this skeleton has been accepted i will start by filling out the tf.contrib.seqseq.loss with the tf.contrib.seqseq.rnn_decoder with and a tf.contrib.seqseq.rnn_decoder_attention including kernel_test.loss_ops_test.py and kernel_test.layers_ops_test.py .please note as some of the calls build for the contrib section are scattered across various folders i might be missing some calls or definitions somewhere
180904095,4754,https://api.github.com/repos/tensorflow/tensorflow/issues/4754,chenghuige,2,0,0,0,0,0,not sure it is a tf bug i have also posted to stackoverflow so if consider not proper to be here close this i find one similar question on stackoverlfow but no reply yet. nan-in-summary-histogram program will face this some times(not every run will face this then if face this i can always reproduce this error loading from the last model i have saved before program crash due to nan when rerun from this model first train process seems fine using the model to generate loss(i have printed loss and shows no problem but after applying gradients the values of embedding variables will turn to nan so nan in embedding will casue histogram collecting info crash if not use histogram program will not crash(assertion fail but since nan exists the model is corrupted.so what is the root cause of the nan problem confused as not know how to debug further and this program with same data and params will mostly run ok and only face this problem during some run..loading existing model from home/gezi/temp/image-caption//model.flickr.rnn.nan/model.ckpt train from restored model home/gezi/temp/image-caption//model.flickr.rnn.nan/model.ckpt i tensorflow/core/common_runtime/gpu/pool_allocator.cc poolallocator after get requests put_count evicted_count eviction_rate and unsatisfied allocation rate i tensorflow/core/common_runtime/gpu/pool_allocator.cc raising pool_size_limit from to epoch train_step duration elapsed train_avg_metrics: loss loss epoch eval_step duration elapsed ratio w tensorflow/core/framework/op_kernel.cc invalid argument nan in summary histogram for rnn/histogramsummary node rnn/histogramsummary histogramsummary t=dt_float device=/job:localhost/replica:/task:/cpu: (rnn/histogramsummary_/tag rnn/image_text_sim/image_mlp/w_h/read w tensorflow/core/framework/op_kernel.cc invalid argument nan in summary histogram for rnn/histogramsummary node rnn/histogramsummary histogramsummary t=dt_float device=/job:localhost/replica:/task:/cpu: (rnn/histogramsummary_/tag rnn/image_text_sim/image_mlp/w_h/read w tensorflow/core/framework/op_kernel.cc invalid argument nan in summary histogram for rnn/histogramsummary node rnn/histogramsummary histogramsummary t=dt_float device=/job:localhost/replica:/task:/cpu: (rnn/histogramsummary_/tag rnn/image_text_sim/image_mlp/w_h/read w tensorflow/core/framework/op_kernel.cc invalid argument nan in summary histogram for rnn/histogramsummary node rnn/histogramsummary histogramsummary t=dt_float device=/job:localhost/replica:/task:/cpu: (rnn/histogramsummary_/tag rnn/image_text_sim/image_mlp/w_h/read w tensorflow/core/framework/op_kernel.cc invalid argument nan in summary histogram for rnn/histogramsummary node rnn/histogramsummary histogramsummary t=dt_float device=/job:localhost/replica:/task:/cpu: (rnn/histogramsummary_/tag rnn/image_text_sim/image_mlp/w_h/read traceback most recent call last file train.py line in tf.app.run
180794662,4742,https://api.github.com/repos/tensorflow/tensorflow/issues/4742,ethereon,1,0,0,0,0,0,for an input with an undefined batch size atrous_convd emits tensors where all except the final dimension are undefined pythoninput tf.placeholder(tf.float none conv tf.nn.convd(input tf.zeros strides padding=same)print(conv.get_shape correctly displays dilated tf.nn.atrous_convd(input tf.zeros rate padding=same)print(dilated.get_shape displays for concrete batch sizes everything works as expected.)tested on rc
180730786,4738,https://api.github.com/repos/tensorflow/tensorflow/issues/4738,markpwoodward,1,0,0,0,0,0,i have recently started using functions from tf.contrib they speed things up a lot thank you i did notice that some of the prototypes in the documents have their arguments replaced with args kwargs see tf.contrib.layers perhaps this is automatically done if the number of arguments exceeds some number unfortunately the prototype is the only place where the argument defaults are shown i have been looking at the source file layers.py to see the full prototype for the defaults which is fine but probably not idea my request is to include the argument defaults somewhere in the documentation i wouldnt mind long prototypes i.e getting rid of the args kwargs or if people want the shorter prototypes maybe the argument defaults could be automatically included somewhere in the description
180551314,4723,https://api.github.com/repos/tensorflow/tensorflow/issues/4723,EdwinGerman,1,0,0,0,0,0,hello everybody i need to convert a csv file to tfrecord for tensorflow i really appreciate your help.an example of csv file that i need to convert is:col col col col target thank you very much
180548412,4722,https://api.github.com/repos/tensorflow/tensorflow/issues/4722,xuancong84,3,0,0,0,0,0,"i am glad to see the newly added einsum function the documentation claims that its usage is the same as numpy however it can do almost nothing as compared to numpy for example it only supports subscripts in the form of unfortunately even matrix transpose does not work i.e ij->ji numpy works aarray einsum(ij->ji,a)array tensorflow does not work: pseudo-code:m tf.variable(tf.random_normal( , ))n tf.einsum(ij->ji,m print m n_ output: array dtype=float array dtype=float) i want to multiply a matrix with every frame vector in every batch or similar operations which can be done by a simple tensor product it seems that i still have to duplicate the matrix so many times and perform a batch_matmul which is very inconvenient and slow and memory consuming.i suggest tensorflow to implement either the tensordot or einsum function which can perform tensor product.it is quite a shame that tensorflow cannot even perform basic tensor product so far"
180516920,4721,https://api.github.com/repos/tensorflow/tensorflow/issues/4721,TimZaman,12,0,0,0,0,0,the image summary in tensorboard is very nice its the most static tab on tensorboard.for example when i am doing a visual test of my auto-encoder for example mnist i see this:! image i wait a few minutes and i refresh and it shows:! image it works but i would like to be able to somehow review the progress other than my current f-stategy.i recon all image summaries are saved inside but i cannot obtain them through tensorboard
180487726,4715,https://api.github.com/repos/tensorflow/tensorflow/issues/4715,jharting,2,0,0,0,0,0,after upgrade from to rc my code no longer runs warning:tensorflow:float is not supported by many models consider casting to float.warning:tensorflow:float is not supported by many models consider casting to float.traceback most recent call last file brain.py line in module classifier.fit(x=trainx y=trainy steps file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py line in fit max_steps=max_steps file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in fit max_steps=max_steps file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in train_model train_op loss_op self._get_train_ops(features targets file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in get_train_ops loss train_op self._call_model_fn(features targets modekeys.train file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in call_model_fn return self._model_fn(features targets mode=mode params=self.params file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py line in dnn_classifier_model_fn weight=_get_weight_tensor(features weight_column_name file usr/lib/python./site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py line in sigmoid_cross_entropy logits.get_shape().assert_is_compatible_with(multi_class_labels.get_shape file usr/lib/python./site-packages/tensorflow/python/framework/tensor_shape.py line in assert_is_compatible_with raise valueerror(shapes s and s are incompatible self other))valueerror shapes and are incompatible my code: ...print shape(trainx print shape(trainy classifier.fit(x=trainx y=trainy steps=)... this worked fine on notice the shapes in the beginning of output looks like a valid combination to me what related github issues or stackoverflow threads have you found by searching the web for your problem?none environment infooperating system fedora tf rc
180487608,4714,https://api.github.com/repos/tensorflow/tensorflow/issues/4714,Mazecreator,3,0,0,0,0,0,"i do a lot of different runs which contain descriptions as well as different meta parameters it would be nice to have a tab on tensorboard so i can tell which experiment i am viewing.my thought would be to simply add static variables from the graph which might include strings ints floats maybe matrix in table format if possible like pca parameters maybe confusion matrix etc these may include a text description current learning rate network meta parameters etc it would be good to simple present the current value of these variables so if there is a search on discount rates for rl or variation such as learning rate those can be viewed this would be different from simply graphing the learning rate as that shows the history rather than current i think of this as a tensorflow dashboard that i can setup.something like:tf.dashboard_summary(tags tensor/value collections=none name=none)eg:tf.dashboard_summary(description:,description_string_tensor_or_python_string)tf.dashboard_summary(learning rate:,lr_tensor)tf.dashboard_summary(discountrate:,discountrate_python_variable)in this case a dashboard tab on tensorflow would contain the labels above and the tensor/value if the value is a python variable then it should be considered constant and will not change over the graph lifecycle if it is a tensor/variable then it should be pulled from the graph at each iteration.i can help but not sure where to jump in to get this started"
180436695,4701,https://api.github.com/repos/tensorflow/tensorflow/issues/4701,atwj,1,0,0,0,0,0,hi i was going through the re-trainer tutorial but i keep encountering the following error.executing genrule tensorflow/core:version_info_gen failed bash failed error executing command bin/bash c remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status error could not expand include path gitcincludefatal bad config file line in usr/local/git/etc/gitconfigtraceback most recent call last file tensorflow/tools/git/gen_git_source.py line in module generate(args.generate file tensorflow/tools/git/gen_git_source.py line in generate write_version_info(dest_file git_version file tensorflow/tools/git/gen_git_source.py line in write_version_info if b in git_version or b in git_version:typeerror in string requires string as left operand not bytestarget tensorflow/examples/image_retraining:retrain failed to builduse verbose_failures to see the command lines of failed build steps code>am i doing something incorrectly
180297953,4680,https://api.github.com/repos/tensorflow/tensorflow/issues/4680,tusharsoni08,4,0,0,0,0,0,environment infooperating system ubuntu im trying to build the pi-examples by building the tensorflow via makefile on linux(ubuntu so st ive done building on linux successfully from here at im trying to build camera example but when ive run make f tensorflow/contrib/pi_examples/camera/makefile command the result came out as follows on terminal: gcc std=c o i/usr/local/include i i/home/tushar/desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/downloads i/home/tushar/desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/downloads/eigen-latest i/home/tushar/desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/gen/proto i/home/tushar/desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/gen/proto_text c tensorflow/contrib/pi_examples/camera/camera.cc o home/tushar/desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/gen/obj/tensorflow/contrib/pi_examples/camera/camera.oin file included from tensorflow/core/framework/tensor.h from tensorflow/contrib/pi_examples/camera/camera.cc::./third_party/eigen/unsupported/eigen/cxx/tensor fatal error unsupported/eigen/cxx/tensor no such file or directory include unsupported/eigen/cxx/tensor compilation terminated.make home/tushar/desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/gen/obj/tensorflow/contrib/pi_examples/camera/camera.o error why could anyone confirm me that third-party library specially eigen is complete or something missing.ive also tried with replacing its third-party eigen library by separately downloaded library but that create new error but similar to this i.e xyz no such file or directory
180191179,4663,https://api.github.com/repos/tensorflow/tensorflow/issues/4663,eamartin,1,0,0,0,0,0,im running tensorflow the following code pythonimport tensorflow as tfx tf.variable dtype=tf.int)old_val tf.identity(x)with tf.control_dependencies( old_val new_val tf.assign(x x with tf.session as sess sess.run(tf.initialize_all_variables for i in xrange print sess.run( old_val new_val x ) outputs from reading the docs on control_dependencies and identity as well as stackoverflow i expected output where indicates that the variable value is unspecified.is this a bug if this is not a bug what is the correct way to refer to the value of variable before and after assignment in a single graph
179892964,4639,https://api.github.com/repos/tensorflow/tensorflow/issues/4639,aselle,12,0,0,0,0,0,see numpys documentation currently support basic indexing off of
179892542,4638,https://api.github.com/repos/tensorflow/tensorflow/issues/4638,aselle,8,0,0,0,0,0,"numpy style advanced indexing is documented here currently support basic indexing stridedslice.e.g.foo some tensoridx= ,, idx= ,, foo idx idx"
179743984,4624,https://api.github.com/repos/tensorflow/tensorflow/issues/4624,bfreskura,4,0,0,0,0,0,os info distrib_id=ubuntudistrib_release=.distrib_codename=wilydistrib_description=ubuntu name=ubuntuversion wily werewolf)id=ubuntuid_like=debianpretty_name=ubuntu version_id=.kernel linux generic x cuda version: -rw-r--r root root nov usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root nov usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root nov usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root nov usr/local/cuda/lib/libcudart.so...-rw-r--r root root nov usr/local/cuda/lib/libcudart_static.a tensorflow pip version python tensorflow.__version output minimal reproducible code import tensorflow as tffrom tensorflow.contrib import slimtf_chk model_ckpt-tf_meta model_ckpt-.metaimages tf.constant tf.float shape with slim.arg_scope slim.fully_connected outputs_collections= tf.graphkeys.activations not working outputs_collections this works logits slim.fully_connected(images scope=logits)init tf.initialize_all_variables()with tf.session as sess sess.run(init saver tf.train.saver saver.save(sess tf_chk)with tf.graph().as_default with tf.session as sess new_saver tf.train.import_meta_graph(tf_meta new_saver.restore(sess tf_chk descriptioncode above will crash the program with the following stack trace: traceback most recent call last file test.py line in module new_saver tf.train.import_meta_graph(tf_meta file home/x/.local/lib/python./site-packages/tensorflow/python/training/saver.py line in import_meta_graph return import_meta_graph_def(read_meta_graph_file(meta_graph_or_file file home/x/.local/lib/python./site-packages/tensorflow/python/training/saver.py line in import_meta_graph_def col_op ops.get_default_graph().as_graph_element(value file home/x/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in as_graph_element return self._as_graph_element_locked(obj allow_tensor allow_operation file home/x/.local/lib/python./site-packages/tensorflow/python/framework/ops.py line in as_graph_element_locked graph repr(name))keyerror the name logits refers to an operation not in the graph. however if i create outputs_collections with the empty list the code willwork also the code will work if i dont specify the outputs_collections_argument
179690696,4614,https://api.github.com/repos/tensorflow/tensorflow/issues/4614,yzsatgithub,5,0,0,0,0,0,the function tf.unique(x name=none finds the unique elements in a d tensor and it now returns two value y and idx y contains all of the unique elements of x sorted inthe same order that they occur in x idx contains the index of each value of x in the unique output y tensor x is y idx tf.unique(x)y idx but i think a third return value which contains the first index of each value of y in the original tensor x is also needed it might work like this tensor x is y idx idx_ori tf.unique(x)y idx idx_ori just like its equivalent in numpy does: array x is y idx_ori np.unique(x return_index=true)y idx_ori
179324918,4589,https://api.github.com/repos/tensorflow/tensorflow/issues/4589,benbarsdell,10,0,0,0,0,0,added support for fp storage but there is currently no support for native fp computation which is available on some hardware such as pascal gpus.in particular the convd and matmul ops could take a new parameter along the lines of compute_dtype which would be plumbed through to cudnn convolution descriptor and cublas hgemm in the backend with the potential for up to a x speedup.related issues
179067399,4571,https://api.github.com/repos/tensorflow/tensorflow/issues/4571,bibhashthakur,1,0,0,0,0,0,that cudnn v is required so i tried to compile the basic example this error was shown._loaded runtime cudnn library compatibility version but source was compiled with compatibility version if using a binary install upgrade your cudnn library to match if building from sources make sure the library loaded at runtime matches a compatible version specified during compile configuration._it worked when i upgraded cudnn to v please update the said webpage if possible operating system ubuntu linux ltsinstalled version of cuda and cudnn rw-r--r root root aug usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root aug usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root aug usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root aug usr/local/cuda/lib/libcudart.so...-rw-r--r root root aug usr/local/cuda/lib/libcudart_static.alrwxrwxrwx root root sep usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx root root sep usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root sep usr/local/cuda/lib/libcudnn.so...-rwxr-xr-x root root jun usr/local/cuda/lib/libcudnn.so...-rw-r--r root root jun usr/local/cuda/lib/libcudnn_static.a tensorflow version: i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally
179043745,4567,https://api.github.com/repos/tensorflow/tensorflow/issues/4567,tomasreimers,3,0,0,0,0,0,"hi in download_dependencies.sh we grep tensorflow/workspace.bzl in order to determine which eigen to download we do this using the command grep o http.*bitbucket.org/eigen/eigen/.*tar\.gz tensorflow/workspace.bzl which we then proceed to try and curl and untar ln due to the new format of workspace.bzl that grep returns eigen_version tar.gz which will download a from bitbucket can we please update the makefile to account for this?thanks,tomas"
178861823,4552,https://api.github.com/repos/tensorflow/tensorflow/issues/4552,alquraishi,2,0,0,0,0,0,i have a model in which one operation happens to be very memory intensive and cannot be executed at once i thought that by splitting the operation manually and executing parts of it individually all on the gpu i would get around the oom problem unfortunately it appears that tf aggressively tries to schedule as many ops to run in parallel as possible even if that results in an oom failure i describe my specific issue further in this stack overflow post seems to me that this problem should be reasonably easy to solve and is rather a serious current limitation of tf the basic principle is that a computation should not fail if its possible in some sequential order to execute it in memory im not talking about situations in which tf has to be clever about splitting a seemingly atomic operations in parts that i understand can be quite difficult and requires art on the part of the programmer what im describing is a situation in which the computation graph as described can in fact be executed without running out of memory but because tf is overly aggressive in scheduling independent ops concurrently it runs out of memory.the solution seems rather straight-forward before putting yet another op onto the gpu or whatever hardware tf should check if that would cause the memory to run out if so it should check if theres a cyclic dependency on the current op getting executed i.e it would be impossible for it to move forward unless the current op is executed if so then thats a legitimate problem and it would fail with an oom error on the other hand if its possible for it to wait until other ops are executed before trying again then it should simply wait until more memory is available i.e it should have a way to run independent operations sequentially.im not familiar enough with the internals of tf to know how difficult a change like this would be but as it currently stands it seems like its preventing a large swath of models from getting executed that otherwise could get executed.on a related note the dynamic_rnn op already seems to do this where its able to shuffle memory back and forth until the computation is done and so perhaps theres an existing partial solution that im not aware of
178833690,4550,https://api.github.com/repos/tensorflow/tensorflow/issues/4550,zorancupic,4,0,0,0,0,0,i want to perform a multilabel image classification task for n classes.ive got sparse label vectors for each image and each dimension of each label vector is currently encoded in this way label true image belongs to this class label false image does not contain to this class missing value/labele.g v for this example v the model should learn that the corresponding image should be classified in the first and third class.my problem is currently how to handle the missing values/labels ive searched through the issues and found this issue could do multilable image classification with: tf.nn.sigmoid_cross_entropy_with_logits(logits targets name=none tensorflow has this error function for sparse softmax which is used for exclusive classification: tf.nn.sparse_softmax_cross_entropy_with_logits(logits labels name=none is there something like sparse sigmoid cross entropy couldnt find something or any suggestions how can i handle my multilabel classification problem with sparse labels
178662717,4536,https://api.github.com/repos/tensorflow/tensorflow/issues/4536,loliverhennigh,19,0,0,0,0,0,"hi all,this is a feature request for convolutional rnns such as convolutional lstms is there any plan to support them in tensorflow?i have an implementation working in here and some basic results indicating how well they do i went about implementing them by making a new class called convrnncell and basicvonvlstmcell and following what is seen in the tensorflow rnn_cell.py file it seems i could redo the whole rnn_cell.py file like this if this is of interest i would be more then happy to implement it for all the rnn functions"
178652537,4535,https://api.github.com/repos/tensorflow/tensorflow/issues/4535,TimZaman,14,0,0,0,0,0,taking into account what mrry said on its not possible to reset a queue unless you have multiple sessions how about resetting it when you have a single session? use case: in a single session if one would want to run exactly one epoch where epoch_size%batch_size of a model x times and report its result exactly after one full and single epoch it would be convenient if one could restart the queues
178649009,4534,https://api.github.com/repos/tensorflow/tensorflow/issues/4534,jeandut,10,0,0,0,0,0,what related github issues or stackoverflow threads have you found by searching the web for your problem need to implement a network with locally connected layers convolution without weight sharing with d inputs images).i was wondering if the implementation of this feature is on the roadmap or if there exists some clever hacks using existing tf functions to do it. edit it seems this kind of layer is often used for face classification for instance in deepface
178499435,4526,https://api.github.com/repos/tensorflow/tensorflow/issues/4526,benbarsdell,3,0,0,0,0,0,it would improve performance in some cases to be able to asynchronously prefetch data over the pcie bus while gpu computation is taking place a gpu-resident queue seems like the natural way to achieve this.in the so thread below yaroslavvb mentions using variables pinned to the gpu to achieve the same effect but i was unable to find a way to get this to work related threads
178435593,4518,https://api.github.com/repos/tensorflow/tensorflow/issues/4518,alexggmatthews,2,0,0,0,0,0,a feature request this should be possible without too much disruption because it relies on a cublas call and therefore would not require changes to stream_executor see section of the linked document requisite blas function is called cublasstrsm and seems to be already in stream executor in this file know that rmlarsen has been interested in this sort of area in the past i dont think it is possible to do this as a user op added at run time because stream executor is not part of the tf includes in the binary but id be happy to be wrong
178000357,4485,https://api.github.com/repos/tensorflow/tensorflow/issues/4485,mayukhg,1,0,0,0,0,0,note only file github issues for bugs and feature requests all other topics will be closed.for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system:ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):i have installed tensorflow using docker by running the following command docker run it p gcr.io/tensorflow/tensorflowif installed from binary pip package provide a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried logs or other output that would be helpful(if logs are large please upload as attachment or provide link).i am trying to retrieve the set of images for tensorflow for poets using the following command curl o but the below mentioned error is being thrown~/tf_files curl version encoding=utf-?>nosuchkeythe specified key does not exist.
177844258,4467,https://api.github.com/repos/tensorflow/tensorflow/issues/4467,ericyue,5,0,0,0,0,0,im going to write to tfrecord file using this code the problem is that this process is very slow such that its not feasible to write a large dataset even in days its just a writer that serialize to disk why its so slow another problem is that the size of the output file is times greater than the original file!does anyone know any way to speed up the process of tfrecordwriter and compress the result
177764050,4456,https://api.github.com/repos/tensorflow/tensorflow/issues/4456,owengbs,5,0,0,0,0,0,the current version of tensorflow still use a greedy algorithm for seqseq decoder i wonder when will beam search be implemented in future versions since this has already been discussed in other issues
177633836,4434,https://api.github.com/repos/tensorflow/tensorflow/issues/4434,yossibiton,3,0,0,0,0,0,i followed this tutorial in order to quantize my graph into bit.i cant share the exact graph here but i can say its a simple convolutional neural network.when i run the benchmark tool over the original and quantized networks its clear that the quantized network is much much slower ms vs ms).slowest nodes in original network time average ms cdf op name matmul fc/fc/matmul convd conv/convd convd conv/convd convd conv/convd convd conv/convd convd conv/convd convd conv_/convd maxpool pool slowest nodes in quantized network time average ms cdf op name quantizedmatmul fc/fc/matmul_eightbit_quantized_bias_add quantizedconvd conv/convd_eightbit_quantized_conv quantizedconvd conv/convd_eightbit_quantized_conv quantizedconvd conv/convd_eightbit_quantized_conv quantizedconvd conv/convd_eightbit_quantized_conv quantizedconvd conv_/convd_eightbit_quantized_conv quantizedconvd conv_/convd_eightbit_quantized_conv quantizedmatmul fc/matmul_eightbit_quantized_bias_add what is the reason for that is it the expected behavior for quantized network environment infooperating system ubuntu installed from source without gpu support commit hash f bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri jul build timestamp build timestamp as int
177625592,4431,https://api.github.com/repos/tensorflow/tensorflow/issues/4431,vladfi1,1,0,0,0,0,0,say i have outputs f(inputs and g of the same shape as inputs id like to compute the directional derivative of outputs with respect to inputs in the direction g in other words the derivative of f(inputs alpha g with respect to alpha at the point alpha= .this is a straightforward application of forward-mode automatic differentiation which should be pretty easy to implement much easier than the already-implemented reverse-mode ad are there any plans to add this feature
177607386,4427,https://api.github.com/repos/tensorflow/tensorflow/issues/4427,alquraishi,6,0,0,0,0,0,the documentation for attentioncellwrapper in contrib states that its based on this paper however the attention mechanism in the paper appears to be distinct from that in attentioncellwrapper for example the tf implementation involves convolutions and requires a fixed attention window neither of which are a feature of the referenced paper i suggest that either the correct paper is referenced or the description is updated to reflect the actual math being implemented the biggest issue right now is that its unclear what the various options really mean attn_length attn_size attn_vec_size etc as their descriptions are very concise and without a paper reference to ground what the documentation is referring to its impossible to know what the attention mechanism is doing short of going through the code line-by-line
177594917,4425,https://api.github.com/repos/tensorflow/tensorflow/issues/4425,cesc-park,3,0,0,0,0,0,"environment infooperating system:distributor id ubuntudescription ubuntu ltsrelease gpu infogpu gtx titan x driver driver version installed version of cuda and cudnn cuda cudnn if installed from binary pip package provide if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code)this code makes errors. import tensorflow as tfa=tf.constant(.,shape b=tf.nn.softmax(a)sess=tf.interactivesession()b.eval()sess.close() this code doesnt make errors. import tensorflow as tfa=tf.constant(.,shape b=tf.nn.softmax(a)sess=tf.interactivesession()b.eval()sess.close() i think this caused by input size of the softmax function.but i dont understand why logs or other output that would be helpful(if logs are large please upload as attachment or provide link). e tensorflow/stream_executor/cuda/cuda_driver.cc could not synchronize on cuda context cuda_error_illegal_address no stack trace availablee tensorflow/stream_executor/cuda/cuda_event.cc error polling for event status failed to query event cuda_error_illegal_addressf tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc unexpected event status f tensorflow/core/common_runtime/gpu/gpu_util.cc gpu sync failed abort core dumped cuda_visible_devices ipython"
176966048,4386,https://api.github.com/repos/tensorflow/tensorflow/issues/4386,aselle,2,0,0,0,0,0,the errors when not running configure are not terribly intuitive see e.g
176898938,4382,https://api.github.com/repos/tensorflow/tensorflow/issues/4382,alrojo,1,0,0,0,0,0,use full tensor and reshape instead of evaluating at every timestep computationally more efficient and faster further handles none in sequence length and raw_rnn style rnn classes does not work with a list of tensors.implemented handling of list as input so it does not break backwards compatibility
176852382,4379,https://api.github.com/repos/tensorflow/tensorflow/issues/4379,keithyin,1,0,0,0,0,0,hi all.i am new to tensorflow and spent a lot of days to install tensorflow but it didnt work need help sudo configure tensorflow tensorflowplease specify the location of python default is usr/bin/python do you wish to build tensorflow with google cloud platform support y/n nno google cloud platform support will be enabled for tensorflowfound possible python library paths usr/local/lib/python./dist-packages usr/lib/python./dist-packagesplease input the desired python library path to use default is usr/local/lib/python./dist-packages /usr/local/lib/python./dist-packagesdo you wish to build tensorflow with gpu support y/n ygpu support will be enabled for tensorflowplease specify which gcc should be used by nvcc as the host compiler default is usr/bin/gcc please specify the cuda sdk version you want to use e.g leave empty to use system default please specify the location where cuda toolkit is installed refer to readme.md for more details default is usr/local/cuda please specify the cudnn version you want to use leave empty to use system default please specify the location where cudnn library is installed refer to readme.md for more details default is usr/local/cuda please specify a list of comma-separated cuda compute capabilities you want to build with.you can find the compute capability of your device at note that each additional compute capability significantly increases your build time and binary size.found stale pid file pid server probably died abruptly continuing.....info starting clean this may take a while consider using expunge_async if the clean takes more than several minutes..error home/keithyin/tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:android_lib_lite target android_lib_lite not declared in package tensorflow/core defined by home/keithyin/tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:session_bundle_lite.error home/keithyin/tensorflow/tensorflow/core/platform/default/build_config/build no such package jpeg_archive error downloading from to home/keithyin/.cache/bazel/_bazel_root/dbddbeabac/external/jpeg_archive error downloading to home/keithyin/.cache/bazel/_bazel_root/dbddbeabac/external/jpeg_archive/jpegsrc.va.tar.gz connection timed out and referenced by tensorflow/core/platform/default/build_config:platformlib.error evaluation of query deps union bazel_tools//tools/jdk:toolchain failed errors were encountered while computing transitive closure.configuration finished
176841918,4378,https://api.github.com/repos/tensorflow/tensorflow/issues/4378,nikhilmishra000,2,0,0,1,0,2,this was meant to solve i think it addresses this too closing
176764315,4363,https://api.github.com/repos/tensorflow/tensorflow/issues/4363,flx42,1,0,0,0,0,0,commit fix nightly docker builds broke the nightly builds nvidia-docker run rm ti tensorflow/tensorflow:nightly-devel-gpu nvidia-sminvidia-smi couldnt find libnvidia-ml.so library in your system please make sure that the nvidia display driver is properly installed and present in your system.please also try adding directory that contains libnvidia-ml.so to your system path. you shouldnt override the existing value of ld_library_path we use it to point to the path where we mount the driver libraries mounted by nvidia-docker see here xx caisq
176718221,4361,https://api.github.com/repos/tensorflow/tensorflow/issues/4361,bsautermeister,10,0,0,0,0,0,"tensorflow version that i use pip package)_---i took heavy use of tf.contrib.layers.batch_norm the last weeks after facing some problems on how to use it correctly i figured out that there are many devs out there who are confused as well such as here would suggest to do following improvements to make it more clear update example in doc-string: the example tells in case we use update_collections on its defaults we have to include this: update_ops tf.get_collection(tf.graphkeys.update_ops)if update_ops updates tf.group(update_ops total_loss control_flow_ops.with_dependencies( updates total_loss) but this is actually not working or deprecated as it throws errors instead we have to do some tiny changes i would suggest to update the docs as follows: from tensorflow.python import control_flow_opsupdate_ops tf.get_collection(tf.graphkeys.update_ops)if update_ops updates tf.tuple(update_ops total_loss control_flow_ops.with_dependencies(updates total_loss) as a side question why do we apply it to the total_loss and not to the train_op directly as described in the doc-string text added a dependency to total_loss works but grouping it with the train_op would make the example more clear in my opinion because we do batch-statistic updates only during training update_ops in combination with reuse varscope: this is related to the question above lets say we have a model with which reuses an convolutional encoder and also its batch-norm-layers several times even when we reuse these layers the update operation for the batch-statistics is added to update_ops nevertheless personally im not sure if this is a bug or if this is really what should be done?or is it required to filter the update-ops after collecting them with update_ops tf.get_collection(tf.graphkeys.update_ops so that each one is executed just once?to sum this up am i wrong that lines should not be executed when reuse=true so changing it to: if not reuse collect the updates to be computed later ops.add_to_collections(updates_collections update_moving_mean ops.add_to_collections(updates_collections update_moving_variance) in my case im using a conv-lstm-conv_tp architecture where i reuse the conv/conv_tp for each timestep when i increase the number of timesteps in the lstm the number of update-ops increases in proportionally while the number of model-parameters stays constant because they get reused currently im getting update-ops when calling tf.get_collection(tf.graphkeys.update_ops as the performance feels super slow when i use batch-norm i guess this high number of update-ops cannot be right handling of is_training parameter: i have seen a lot of examples people doing something like this in their code to handle the is_training parameter: def batch_norm_layer(x,train_phase,scope_bn bn_train batch_norm(x decay center=true scale=true updates_collections=none is_training=true bn_inference batch_norm(x decay center=true scale=true updates_collections=none is_training=false bn tf.cond(train_phase lambda bn_train lambda bn_inference return bn as far as i know this was really required in the past because is_training was just a boolean but since the param can be a bool-tensor as well this is not required anymore since many devs are still ding this workaound added a comment to the doc-string that this is not required anymore could be helpful usage on multi-gpu configuration a when i optimize my code for multi-gpu systems as in the cifar example the number of update-ops increases with the factor of num_gpus might be related to b when i use tf.contrib.batch_norm within a multi-gpu system i get an error like this: invalidargumenterror cannot assign a device to node tower_/inference/convstack/x_bn_/moments/sufficient_statistics/sparsetodense could not satisfy explicit device specification device:gpu because no supported kernel for gpu devices is available.... hence to we have to wrap evey batch_norm call with tf.device(/cpu i guess this might have bad impacts on performance right?thanks!_ps sorry in case this question would fits better to stackoverflow as it is a combination of suggested improvements and questions just let me know"
176561723,4348,https://api.github.com/repos/tensorflow/tensorflow/issues/4348,MaeThird,4,0,0,0,0,0,"when i launch bazel build c opt jobs tensorflow/tools/pip_package:build_pip_package to compile tensorflow from source it crashed at: external/com_googlesource_code_re/build c compilation of rule com_googlesource_code_re//:re failed gcc failed error executing command usr/bin/gcc u_fortify_source d_fortify_source fstack-protector wall wl,-z,-relro,-z,now b/usr/bin b/usr/bin wunused-but-set-parameter wno-free-nonheap-object fno-canonical-system-headers remaining argument(s skipped com.google.devtools.build.lib.shell.badexitstatusexception process exited with status external/com_googlesource_code_re/re/dfa.cc in constructor re::dfa::state::state():external/com_googlesource_code_re/re/dfa.cc error unknown array size in delete... here are some relative infos bazel label non-git gcc gcc tensorflow is the latest master branchcan you tell me how to configure bazel with clang(not cross compile thanks"
176517049,4343,https://api.github.com/repos/tensorflow/tensorflow/issues/4343,gustavla,1,0,0,0,0,0,if i try to configure the lastest master branch dbdbfcdbeeebca using bazel i get: error tensorflow/tensorflow/tensorflow.bzl traceback most recent call last file tensorflow/tensorflow/tensorflow.bzl line rule(attrs srcs attr.label_list more arguments more arguments file tensorflow/tensorflow/tensorflow.bzl line in rule attr.label_list(cfg data allow_files true)expected configurationtransition or nonetype for cfg while calling label_list but got string instead data.error com.google.devtools.build.lib.packages.buildfilecontainserrorsexception error loading package extension file tensorflow/tensorflow.bzl has errors. using avoids this and reverting the offending commit bcdcbbffcfdafbb also fixes it environment infosetup centos bazel cuda cudnn tensorflow master dbdbfcdbeeebca)i run configure and set it up for gpu support i see the error above if i use bazel fixupgrade to obviously however since is still officially supported i thought this deserves to be an issue.another fix is to revert the commit bcdcbbffcfdafbb note i didnt have to revert to its parent undoing that commit alone is enough: git show r bcdcbbf git apply
176188616,4319,https://api.github.com/repos/tensorflow/tensorflow/issues/4319,shkr,2,0,0,0,0,0,i am trying to build and install tensorflow from source however when building with bazel it returns an error stating extension file tensorflow/tensorflow.bzl has errors.here are the commands that i ran and the logs are included bashmkvirtualenv tensorflow_devbrew install bazel swigworkon tensorflow_devpip install six numpy wheel ipythonexport tf_dir=/users/shashank/documents/repositories/tensorflowcd users/shashank/documents/repositories/git clone git@github.com:tensorflow/tensorflow.gitcd tf_dir configure~/documents/repositories/tensorflow documents/repositories/tensorflowplease specify the location of python default is users/shashank/.virtualenvs/tensorflow_dev/bin/python :do you wish to build tensorflow with google cloud platform support y/n no google cloud platform support will be enabled for tensorflowfound possible python library paths users/shashank/documents/py_config users/shashank/.virtualenvs/tensorflow_dev/lib/python./site-packagesplease input the desired python library path to use default is users/shashank/documents/py_config/ /users/shashank/documents/py_config/do you wish to build tensorflow with gpu support y/n no gpu support will be enabled for tensorflowconfiguration finishedcd tf_dir bazel build c opt tensorflow/tools/pip_package:build_pip_packageerror users/shashank/documents/repositories/tensorflow/tensorflow/tensorflow.bzl traceback most recent call last file users/shashank/documents/repositories/tensorflow/tensorflow/tensorflow.bzl line rule(attrs srcs attr.label_list more arguments more arguments file users/shashank/documents/repositories/tensorflow/tensorflow/tensorflow.bzl line in rule attr.label_list(cfg data allow_files true)expected configurationtransition or nonetype for cfg while calling label_list but got string instead data.error com.google.devtools.build.lib.packages.buildfilecontainserrorsexception error loading package extension file tensorflow/tensorflow.bzl has errors.info elapsed time s thanks for your help
176152825,4312,https://api.github.com/repos/tensorflow/tensorflow/issues/4312,gustavla,5,0,0,0,0,0,i am having trouble configuring the latest master branch dbeeedfaeabffcb when i run configure i get: error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:android_lib_lite target android_lib_lite not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:session_bundle.error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:android_lib_lite target android_lib_lite not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:signature.error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:meta_graph_portable_proto target meta_graph_portable_proto not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:session_bundle.error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:meta_graph_portable_proto target meta_graph_portable_proto not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:signature.error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:meta_graph_portable_proto target meta_graph_portable_proto not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:signature.error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:meta_graph_portable_proto target meta_graph_portable_proto not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:session_bundle.error tensorflow/tensorflow/contrib/session_bundle/build no such target tensorflow/core:android_lib_lite target android_lib_lite not declared in package tensorflow/core defined by tensorflow/tensorflow/core/build and referenced by tensorflow/contrib/session_bundle:session_bundle.error evaluation of query deps union bazel_tools//tools/jdk:toolchain failed errors were encountered while computing transitive closure.configuration finished to summarize the dependencies that are included inside the if_android and if_ios are not found they dont exist in the repository so that is not that surprising what is more surprising though is that my vanilla installation is not returning empty lists when if_android is called i havent looked into how those functions work so not sure why that is happening environment infosetup centos bazel cuda cudnn tensorflow master dbeeedfaeabffcb)i run configure and set it up for gpu support actually i dont think this is criticial but first i had to open up configure and add output_base on the two calls to bazel since my setup requires a custom cache directory fixthe if lines were added in edeeafdcbdcaffbee so a fix that i know works is to use its parent commit feeabefbe
175854624,4283,https://api.github.com/repos/tensorflow/tensorflow/issues/4283,mkolod,1,0,0,0,0,0,problemthe name_scope api has changed between last week and last nights latest changes in the branch the previous api was: name_scope(*args kwds) the current api is: name_scope(name) the api was changed in a breaking way rather than the new api being added and the use of the old api triggering a warning.i was wondering what the tensorflow gatekeepers policy is on breaking api changes is there an official policy is the breaking change without a deprecation warning and phase-out intentional
175835369,4281,https://api.github.com/repos/tensorflow/tensorflow/issues/4281,marekmodry,1,0,0,0,0,0,when i try to run the tutorial available at it works great the model is training and the proximity is decreasing for training i used the default data and default command: python translate.py data_dir your_data_directory train_dir checkpoints_directory en_vocab_size fr_vocab_size substituting directory paths of course)however when i try using the model by running python translate.py decode data_dir your_data_directory train_dir checkpoints_directory i get the following error: traceback most recent call last file translate.py line in module tf.app.run file home/user/venv/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv file translate.py line in main decode file translate.py line in decode target_weights bucket_id true file www/data/user/tensorflow/tensorflow/models/rnn/translate/seqseq_model.py line in step outputs session.run(output_feed input_feed file home/user/venv/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/user/venv/lib/python./site-packages/tensorflow/python/client/session.py line in run np_val.shape subfeed_t.name str(subfeed_t.get_shape())))valueerror cannot feed value of shape for tensor weight which has shape it looks like the script is expecting the whole batch which is set to by default but when i type any input it creates only sample of data.version of tensorflow tensorflow==..rc git commit i use fccbfefebdcfffcce )thank you for any ideas
175820685,4279,https://api.github.com/repos/tensorflow/tensorflow/issues/4279,hillegass,1,0,0,0,0,0,trying to build libtensorflow.so from source on a mac using bazel i get an error log is attached below environment infooperating system macos x xcode installed version of cuda and cudnn no cudaif installed from source provide the commit hash git rev-parse head ) adfafcfdda the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jar build time fri jul build timestamp build timestamp as int build log bazel build tensorflow:libtensorflow.soextracting bazel installation...sending sigterm to previous bazel server pid done..error users/projects/tensorflow/tensorflow/core/build no such target tensorflow/tools/git:gen/spec.json target gen/spec.json not declared in package tensorflow/tools/git defined by users/projects/tensorflow/tensorflow/tools/git/build and referenced by tensorflow/core:version_info_gen.error users/projects/tensorflow/tensorflow/core/build no such target tensorflow/tools/git:gen/head target gen/head not declared in package tensorflow/tools/git defined by users/projects/tensorflow/tensorflow/tools/git/build and referenced by tensorflow/core:version_info_gen.error users/projects/tensorflow/tensorflow/core/build no such target tensorflow/tools/git:gen/branch_ref target gen/branch_ref not declared in package tensorflow/tools/git defined by users/projects/tensorflow/tensorflow/tools/git/build and referenced by tensorflow/core:version_info_gen.error analysis of target tensorflow:libtensorflow.so failed build aborted.info elapsed time s
175517908,4251,https://api.github.com/repos/tensorflow/tensorflow/issues/4251,jakubsimanek,1,0,0,0,0,0,"download and setup pip installation instructions for say that ubuntu/linux bit gpu enabled python version requires cuda toolkit and cudnn v. ,that setup results in: e tensorflow/stream_executor/cuda/cuda_dnn.cc loaded runtime cudnn library compatibility version but source was compiled with compatibility version if using a binary install upgrade your cudnn library to match if building from sources make sure the library loaded at runtime matches a compatible version specified during compile configuration. edit sep as vinaynath pointed out cudnn-.-linux-x-v..tgz should be installed for cuda installation of cudnn v cudnn-.-linux-x-v..tgz ~~ cudnn-.-linux-x-v resolves the problem. i think the instructions need an update to requires cuda toolkit and cudnn v environment infooperating system ubuntu lts gnu/linux generic x_)installed version of cuda and cudnn cuda cudnn v cudnn-.-linux-x-v. )(please attach the output of ls l path/to/cuda/lib/libcud ls l usr/local/cuda/lib/libcud*-rw-r--r root root jul usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root jul usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root jul usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root jul usr/local/cuda/lib/libcudart.so...-rw-r--r root root jul usr/local/cuda/lib/libcudart_static.a a link to the pip package you installed the output from python c import tensorflow print(tensorflow.__version python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code python m tensorflow.models.image.mnist.convolutionali tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyextracting data/train-images-idx-ubyte.gzextracting data/train-labels-idx-ubyte.gzextracting data/tk-images-idx-ubyte.gzextracting data/tk-labels-idx-ubyte.gzi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties:name grid kmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name grid k pci bus id initialized!e tensorflow/stream_executor/cuda/cuda_dnn.cc loaded runtime cudnn library compatibility version but source was compiled with compatibility version if using a binary install upgrade your cudnn library to match if building from sources make sure the library loaded at runtime matches a compatible version specified during compile configuration.f tensorflow/core/kernels/conv_ops.cc check failed stream->parent()->getconvolvealgorithms(&algorithms)aborted core dumped"
175133479,4213,https://api.github.com/repos/tensorflow/tensorflow/issues/4213,Huxwell,2,0,0,0,0,1,yann lecun pointed convnets dont need to have a fixed-size input.one can train fully-convolutional network using small images and than use the model without scaling or cropping and feed it with large images for segmentation.it works right away with keras+theano.if one trains the same model with tensorflow as kerases backend during application of pretrained model on image larger than the images used for training he will get a valueerror due to tensor shape mismatch same for manual model definition directly in tf).the shape check needs to be disabled and the convolution layers need to work properly with variable size during model using/testing phase if the framework is to be competitive what related github issues or stackoverflow threads have you found by searching the web for your problem environment infooperating system ubuntu lts installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): cuda_config.txt error log:valueerror cannot feed value of shape for tensor uconvolutiond_input which has shape
174877251,4187,https://api.github.com/repos/tensorflow/tensorflow/issues/4187,davidzchen,2,0,0,0,0,0,trevorwelchfrom the discussion in and this is the tracking bug for the following error: error users/production/github/tensorflow/tensorflow/cc/build executing genrule tensorflow/cc:training_ops_genrule failed bash failed error executing command cd private/var/tmp/_bazel_production/edbbfbcdcfeebaaff/execroot/tensorflow exec env path=/usr/local/cuda/bin:/library/frameworks/python.framework/versions/./bin:/usr/local/bin:usr/local/sbin:/usr/local/mysql/bin:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin tmpdir=/var/folders/h/pnkxnqdjgksqbkpnlgn/t bin/bash c source external/bazel_tools/tools/genrule/genrule-setup.sh bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc com.google.devtools.build.lib.shell.abnormalterminationexception process terminated by signal dyld library not loaded rpath/libcudart...dylib referenced from private/var/tmp/_bazel_production/edbbfbcdcfeebaaff/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc reason image not found/bin/bash line trace/bpt trap bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc target tensorflow/cc:tutorials_example_trainer failed to buildinfo elapsed time s critical path sproduction@trevors-macbook-pro tensorflow
174493965,4151,https://api.github.com/repos/tensorflow/tensorflow/issues/4151,zhangcx93,2,0,0,0,0,0,"environment infooperating system:ubuntu installed version of cuda and cudnn rc on gtx if installed from source provide head aedeeeebeebbeeecbuild target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu jan build timestamp thu jan build timestamp as int problem:in my application i need to change value of some variable and run the minimize in a loop thus i have to run assign op continuously which may add new op to graph every time thus the program become very slow and memory explode. sess tf.session()a tf.variable(np.ones sess.run(tf.initialize_all_variables())t time.time()for i in range sess.run(tf.assign(a,np.ones t time.time print(t-t t t so is there a method to change the value of variables without adding an op to graph or a way to remove it after(i have a big graph defined before the loop i dont want to reset all of them and define again with reset_default_graph)?and since the value assigned to the variable is different all the time i cannot define the op before the for loop"
174407019,4138,https://api.github.com/repos/tensorflow/tensorflow/issues/4138,cloudygoose,1,0,0,0,0,0,hi my commit number isdeecdafdecffedabdbi want to try sampled softmax to speed up my lm training im changing slightly on the ptb training example code im working on a cpu machine.first on ptb(vocab k im using the small config i see some speed up gain:normal softmax wpssample_softmax hlba sample wpsthen i move to a larger set and using k vocab im still setting the sample number to i think the speed should be similar to the ptb case but i got the speed to be about wps even if i set the sample number as small as i dont understand why it should be so slower than the ptb case do you know how can i make it faster?heres some of the related code output tf.reshape(tf.concat outputs size softmax_w tf.get_variable(softmax_w size vocab_size softmax_w_t tf.transpose(softmax_w softmax_b tf.get_variable(softmax_b vocab_size if use_sample_softmax true loss tf.nn.sampled_softmax_loss(softmax_w_t softmax_b output tf.reshape(self._targets num_samples vocab_size todo self._cost cost tf.reduce_sum(loss batch_size else logits tf.matmul(output softmax_w softmax_b loss tf.nn.seqseq.sequence_loss_by_example logits tf.reshape(self._targets tf.ones( batch_size num_steps self._cost cost tf.reduce_sum(loss batch_size thanks!goose
174384041,4135,https://api.github.com/repos/tensorflow/tensorflow/issues/4135,kevin-keraudren,1,0,0,0,0,0,minimal example to reproduce the bug pythonimport numpy as npimport tensorflow as tfsess tf.session()with sess.as_default input batch height width depth input_channels x_shape filter kernel_height kernel_width depth input_channels output_channels f_shape x_shape output batch height width depth output_channels y_shape f_shape np.random.seed make it reproducible x_val np.random.random_sample(x_shape).astype(np.float f_val np.random.random_sample(f_shape).astype(np.float output_val np.random.random_sample(y_shape).astype(np.float x tf.constant(x_val name=x dtype=tf.float f tf.constant(f_val name=f dtype=tf.float output tf.nn.convd(x f strides padding=same r tf.gradients(output f print(r .eval()) this leads to a seg fault tested on a mac with cpu both with tensorflow binary release and when compiled from source last commit for certain values of x_shape f_shape for instance or the seg fault occurs but for other values or there is no seg fault.inserting some std::cout print statements in the code shows that the segmentation fault occurs during this function call in convdbackpropfilterop::compute
174330266,4130,https://api.github.com/repos/tensorflow/tensorflow/issues/4130,sonalgupta,1,0,0,0,0,0,there are examples in the repo for loading a pre-trained model in c can someone add an example to train a simple hidden layer fully connected neural net or even logistic regression using sgd in c i realize there is no publicly available support for auto differentiation in c are there any plans to do so
174107072,4106,https://api.github.com/repos/tensorflow/tensorflow/issues/4106,grenager,1,0,0,0,0,0,given some particular encoder input it would be great to be able to sample sequences from the embedding_tied_rnn_seqseq decoder to my understanding at present this is possible with the tied_rnn_seqwseq decoder by passing in a custom loop_function but it isnt possible in the embedding version it appears that only argmax is supported with the feed_previous parameter this would be useful in dialog/conversation applications and perhaps others
174105608,4105,https://api.github.com/repos/tensorflow/tensorflow/issues/4105,trevorwelch,0,0,0,0,1,0,environment infooperating system:os installed version of cuda and cudnn ls l usr/local/cuda/lib/libcud*-rwxr-xr-x root wheel apr usr/local/cuda/lib/libcuda.dyliblrwxr-xr-x root wheel apr usr/local/cuda/lib/libcudadevrt.a developer/nvidia/cuda-./lib/libcudadevrt.alrwxr-xr-x root wheel apr usr/local/cuda/lib/libcudart...dylib developer/nvidia/cuda-./lib/libcudart...dyliblrwxr-xr-x root wheel apr usr/local/cuda/lib/libcudart.dylib developer/nvidia/cuda-./lib/libcudart.dyliblrwxr-xr-x root wheel apr usr/local/cuda/lib/libcudart_static.a developer/nvidia/cuda-./lib/libcudart_static.a-rwxr-xr-x production staff feb usr/local/cuda/lib/libcudnn..dyliblrwxr-xr-x root admin aug usr/local/cuda/lib/libcudnn..dylib developer/nvidia/cuda-./lib/libcudnn..dyliblrwxr-xr-x root admin aug usr/local/cuda/lib/libcudnn.dylib developer/nvidia/cuda-./lib/libcudnn.dylib-rw-r--r production staff feb usr/local/cuda/lib/libcudnn_static.a the output from python c import tensorflow print(tensorflow.__version cant get that far but im using import tensorflowi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.dylib locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.dylib locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.dylib locallysegmentation fault if installed from source provide the commit hash git rev-parse head ) cdbebefcedafcdfe the output of bazel version bazel versionbuild label homebrewbuild target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time thu aug build timestamp build timestamp as int if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code bazel build c opt config=cuda tensorflow/cc:tutorials_example_trainererror no such package local_config_cuda//crosstool build file not found on package path.error no such package local_config_cuda//crosstool build file not found on package path.info elapsed time s what other attempted solutions have you tried downgrading to cudnn switching between and re-installing bazel modifying crosstool file according to various threads manually linking cuda libraries during configure to not use symlinked libraries various other hacks over the last week
174103566,4104,https://api.github.com/repos/tensorflow/tensorflow/issues/4104,fabioasdias,1,0,0,0,0,0,related to issue tensors can be used for a whole lot more than implementing a nn one of the useful operations in multidimensional arrays is its decomposition into smaller components tucker cp native implementations would allow the use of tensorflow in other ml contexts...refs
173910398,4100,https://api.github.com/repos/tensorflow/tensorflow/issues/4100,LinJM,1,0,0,0,0,0,about tf.contrib slim any plan to release pretrained checkpoint
173787623,4094,https://api.github.com/repos/tensorflow/tensorflow/issues/4094,matthiasreisser,1,0,0,0,0,0,"hey tensorflow community,a while ago i wrote some code for doing gradient descent in a distributed environment i adapted code from tensorflow/tensorflow/python/training/sync_replicas_optimizer.py and wrote my own apply_gradients method crucially i included a conditional op that either performed distributed synchronised gradient descent or returned the apply_gradients op from the original optimizer this code worked well when i used it with tensorflow but it broke with and is still broken in my current installation from source see below the queuerunner that is responsible for the synchronization op between the workers now fails with the error message operation has been marked as not fetchable.a bit of searching in the execution stack led me to the lines if self._control_flow_context is not none self._control_flow_context.addop(self) in tensorflow/tensorflow/python/framework/ops.py from which i take that the error comes from the fact that the queue operations are placed inside the conditional execution context out of curiosity what is the idea behind fetchable ops and why should enqueuing and dequeueing be dangerous to fetch any help would be appreciated!matthias environment infooperating system distributor id linuxmintdescription linux mint rafaelarelease codename rafaelainstalled version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):cuda/lib/libcudadevrt.acuda/lib/libcudart.so libcudart.so..cuda/lib/libcudart.so libcudart.so...cuda/lib/libcudart.so...cuda/lib/libcudart_static.acuda/lib/libcudnn.socuda/lib/libcudnn.so.cuda/lib/libcudnn.so...cuda/lib/libcudnn_static.aif installed from binary pip package provide:if installed from source provide the commit hash git rev-parse head ffdbeffaabaadefcafa if possible provide a minimal reproducible example we usually dont have time to read hundreds of lines of your code what other attempted solutions have you tried?removing the conditional execution clause removes the error logs or other output that would be helpful(if logs are large please upload as attachment or provide link"
173726036,4090,https://api.github.com/repos/tensorflow/tensorflow/issues/4090,akors,1,0,0,0,0,0,hi!is it possible to make the versions of the cuda libraries available as a python variable currently after importing there is the following output: i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally so apparently the versioning information is loaded i just couldnt find an api to access it.i would like to record the cuda as meta-information along with my checkpoint and event files to be able to reproduce any runs
173725450,4089,https://api.github.com/repos/tensorflow/tensorflow/issues/4089,akors,1,0,0,0,0,0,hi is it possible to record the git commit hash when building from source and making it available as a variable?currently there is only tensorflow.__version__ which is nice but with a fast-moving codebase is not quite granular enough something like tensorflow.__commit would make each build uniquely identifiable which is important for me to produce reproducible runs
173627763,4079,https://api.github.com/repos/tensorflow/tensorflow/issues/4079,chasep255,4,0,0,0,0,0,right now i am implementing leaky relus like this tf.maximum x x this works fine except when it comes to memory usage networks which will fit on my gpu when using tf.nn.relu or tf.nn.elu fail when i am using my leaky relu implementation i think this is because it needs to store both the intermediate x and x values of the activations to compute the gradients which essentially does the memory usage however i do not think this would be an issue if there were a dedicated tf.nn.leaky_relu can someone consider adding this to a future tensorflow release
173501255,4064,https://api.github.com/repos/tensorflow/tensorflow/issues/4064,mrry,1,0,0,0,0,0,cherry pick for r branch.)if the notification was set before waiting the waiter would never wake up.added a test to cover this case fixes change
173286340,4044,https://api.github.com/repos/tensorflow/tensorflow/issues/4044,cardshuffle,3,0,0,0,1,0,ive been trying to import a frozen graph into a new program and do a simple forward pass but tf.import_graph_def has been throwing a valueerror that i really cant make sense of environment infooperating system ubuntu lts bitinstalled version of cuda and cudnn noneif installed from source provide the commit hash git rev-parse head fcedafbccccfab the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri jun build timestamp build timestamp as int steps to reproduce copy the ipython notebook from here change sample_prediction tf.nn.softmax(tf.nn.xw_plus_b(sample_output w b to sample_prediction tf.nn.softmax(tf.nn.xw_plus_b(sample_output w b name=sample_prediction modify the code like so pythonwith tf.session(graph=graph as session tf.initialize_all_variables().run print(initialized mean_loss code omitted no changes new code below saver tf.train.saver(tf.all_variables saver.save(session home/me/documents/checkpoint.ckpt write_meta_graph=false tf.train.write_graph(graph.as_graph_def home/me/documents graph.pb run and verify that checkpoint.ckpt and graph.pb have been created run bazel build tensorflow/python/tools:freeze_graph bazel-bin/tensorflow/python/tools/freeze_graph input_graph=/home/me/documents/graph.pb input_checkpoint=/home/me/documents/checkpoint.ckpt output_graph=/home/me/documents/frozen_graph.pb output_node_names=sample_prediction verify that frozen_graph.pb has been created create a new ipython notebook with the following code pythonfrom future import print_functionimport osimport numpy as npimport randomimport stringimport tensorflow as tffrom tensorflow.python.platform import gfileimport zipfilefrom six.moves import rangefrom six.moves.urllib.request import urlretrievegraph tf.graph()with graph.as_default graph_def tf.graphdef with open(/home/me/documents/frozen_graph.pb rb as f graph_def.parsefromstring(f.read sample_prediction tf.import_graph_def(graph_def name return_elements= sample_prediction run what have you tried originally the graph also contained a node named saved_sample_output and when i tried importing that frozen graph the error complained about saved_sample_output i tried removing the name re-writing the checkpoint and graph files re-freezing and re-running the code it then complained about variable which after checking graph.pb was what had originally been named saved_sample_output other than that i havent been able to find anything else out checked out and looked at the solutions suggested for similar errors but my import_graph_def never had an input map to begin with removing the name parameter or the return_elements parameter or both hasnt made a difference logs or other output that would be helpful valueerror traceback most recent call last)
172893458,4009,https://api.github.com/repos/tensorflow/tensorflow/issues/4009,untom,1,0,0,0,0,0,tf.image.decode_jpeg crashes on a seemingly valid input image with an invalidargumenterror invalid jpeg data size ive used both the current nightly from tonight as well as tf using python on centos on two different machines.the image can be opened/displayed without problems in firefox gimp and other image viewers as well as with the pil image library within python the image is part of the ilsvrc dataset n/n_.jpeg ive uploaded it to for your reference i hope imgur doesnt recode the image let me know this is a minimal code example: import tensorflow as tffn imagenet/ilsvrc/data/cls-loc/train/n/n_.jpegwith tf.graph().as_default image_contents tf.read_file(fn image tf.image.decode_jpeg(image_contents channels init_op tf.initialize_all_tables with tf.session as sess sess.run(init_op tmp sess.run(image) which crashes with the following error: invalidargumenterror invalid jpeg data size node decodejpeg decodejpeg acceptable_fraction channels fancy_upscaling=true ratio try_recover_truncated=false device=/job:localhost/replica:/task:/cpu: (readfile) caused by op decodejpeg defined at: for reference here the full stack trace is here
172860276,4003,https://api.github.com/repos/tensorflow/tensorflow/issues/4003,cancan101,2,0,0,0,0,0,right now what you get is device info for example: i tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties:name grid kmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y theano prints: using gpu device grid k cnmem is enabled with initial size of memory cudnn giving you device info but also info on the version of cudnn
172655715,3980,https://api.github.com/repos/tensorflow/tensorflow/issues/3980,davidzchen,3,0,0,0,0,0,some improvements to the new cuda autoconf mechanism from the discussion in and autodetect nvcc version and set cuda_toolkit_path based on detected version set expt-relaxed-constexpr iff detected version is set cxx_builtin_include_directory for cuda headers based on detected include directories similar to get_cxx_inc_directories move remaining checks in configure script into cuda_configure so that the configure script only contains the user interface
172502283,3958,https://api.github.com/repos/tensorflow/tensorflow/issues/3958,jmugan,1,0,0,0,0,0,in chrome it says graph visualization failed typeerror connot read property length of undefined.in firefox it says graph visualization failed typeerror rawnodes is undefinedi have attached the events file appended txt so it github would accept it.) events.out.tfevents..jmugan.local.txt i run with inspect i get ======================================================================processing event files this can take a few minutes)======================================================================found event files in:/users/jmugan/tensorlogthese tags are in users/jmugan/tensorlog:audio histograms images scalars exp_cost perplexity train_cost======================================================================event statistics for users/jmugan/tensorlog:audio graph first_step last_step max_step min_step num_steps outoforder_steps histograms images scalars first_step last_step max_step min_step num_steps outoforder_steps sessionlog:checkpoint sessionlog:start sessionlog:stop
172479633,3953,https://api.github.com/repos/tensorflow/tensorflow/issues/3953,erickrf,1,0,0,0,0,0,operating system scientific linux release nitrogen)cuda im trying to install tensorflow from source with debug options in order to solve another issue from github branch r using bazel apparently related to after configuring tensorflow i ran bazel build c dbg config=cuda tensorflow/cc:tutorials_example_trainer and got the following errors: error hltsrv/rocha/deps/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by connection timed out github.com and referenced by tensorflow/cc:tutorials_example_trainer.error hltsrv/rocha/deps/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by connection timed out github.com and referenced by tensorflow/cc:tutorials_example_trainer.error hltsrv/rocha/deps/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by connection timed out github.com and referenced by tensorflow/cc:tutorials_example_trainer.error hltsrv/rocha/deps/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by connection timed out github.com and referenced by tensorflow/cc:tutorials_example_trainer.error hltsrv/rocha/deps/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by connection timed out github.com and referenced by tensorflow/cc:tutorials_example_trainer.error hltsrv/rocha/deps/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by connection timed out github.com and referenced by tensorflow/cc:tutorials_example_trainer.error analysis of target tensorflow/cc:tutorials_example_trainer failed build aborted. im using a server where i have no root privileges and theres no internet access i understand the building process is trying to clone a repository so would it be possible to clone it and download any other necessary files in another machine and then copy them to the server if so what exactly do i need to copy and where
172327068,3948,https://api.github.com/repos/tensorflow/tensorflow/issues/3948,ganggangsouth,0,1,0,0,0,0,github issues are for bugs installation problems feature requests for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice environment infooperating system:ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide which pip package you installed the output from python c import tensorflow print(tensorflow.__version__) .if installed from source provide the commit hash git rev-parse head the output of bazel version steps to reproduce what have you tried logs or other output that would be helpful(if logs are large please upload as attachment
172256972,3936,https://api.github.com/repos/tensorflow/tensorflow/issues/3936,act65,1,0,0,0,0,0,hey just wondering if there are any plans to add video summaries to tensorboard??alex
171792627,3889,https://api.github.com/repos/tensorflow/tensorflow/issues/3889,classner,1,0,0,0,0,0,environment infooperating system:linuxif installed from source provide the commit hash git rev-parse head cbebecfbef the output of bazel version build label build target bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/bazelserver_deploy.jarbuild time fri jul build timestamp build timestamp as int steps to reproduce minimal example to reproduce the error.import tensorflow as tffrom tensorflow.contrib.layers import convd batch_normsess tf.interactivesession()is_train_var tf.placeholder(tf.bool if tf.float is used here it works flawlessly alternatively if a constant instead of a variable is used for is_training everything works as expected.inp tf.placeholder(tf.float name=data)conv convd(inp padding=valid normalizer_fn=batch_norm normalizer_params={is_training is_train_var})optimizer tf.train.momentumoptimizer(learning_rate momentum=.)optimizer.compute_gradients(conv) this results in the error traceback most recent call last file repr.py line in module optimizer.compute_gradients(conv file lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/training/optimizer.py line in compute_gradients colocate_gradients_with_ops=colocate_gradients_with_ops file lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/gradients.py line in gradients in_grads aslist(grad_fn(op out_grads file lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/control_flow_grad.py line in switchgrad return merge( good_grad zero_grad name=cond_grad none file lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/control_flow_ops.py line in merge return gen_control_flow_ops._merge(inputs name file lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/gen_control_flow_ops.py line in merge result op_def_lib.apply_op(merge inputs=inputs name=name file lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py line in apply_op raise typeerror(%s that dont all match prefix)typeerror tensors in list passed to inputs of merge op have types float float that dont all match what have you tried?i am currently using constant variables as a workaround as described
171747128,3882,https://api.github.com/repos/tensorflow/tensorflow/issues/3882,hhexiy,1,0,0,0,0,0,tuple(array_ops.transpose(input for input in flat_input) here seems we can only transpose matrix of size batch_size time_size input_size it doesnt work if my input at each time step is a matrix.in torch you can just specify the two dimensions and here to be swapped instead of perm is there a similar functionality
171351219,3840,https://api.github.com/repos/tensorflow/tensorflow/issues/3840,hholst80,1,0,0,0,0,0,since tensorflow uses c i think it would be good style to fix the errors such as: framework/allocator.h warning missing return statement at end of non-void function tensorflow::allocator::requestedsize the code is not harmful in this instance but the compiler would have to parse and trust comments cpp check dies with a fatal error if condition is not true it is not controlled by ndebug so the check will be executed regardless of compilation mode. a simple patch for the problem would be cpp virtual size_t requestedsize(void ptr check(false allocator doesnt track sizes cpp virtual size_t requestedsize noreturn void ptr check(false allocator doesnt track sizes reference
171224933,3821,https://api.github.com/repos/tensorflow/tensorflow/issues/3821,ghost,0,0,0,0,1,0,environment infooperating system:mac osx el capitanif installed from binary pip package provide which pip package you installed.the latest version of tensorflow the output from python c import tensorflow print(tensorflow.__version__) ...rc steps to reproducei have trained a convolutional model through the main program and stored it into a checkpoint and ckpt file the problem lies in the evaluation program the ckpt file seems to not output anything but a bunch of errors the program does not complete either.the code for main is: import inputimport processimport timeimport numpy as npimport tensorflow as tffrom datetime import datetimeflags tf.app.flags.flagsdef train with tf.session as sess images labels process.inputs forward_propgation_results process.forward_propagation(images train_loss cost process.error(forward_propgation_results labels image_summary_t tf.image_summary(images.name images max_images summary_op tf.merge_all_summaries init tf.initialize_all_variables saver tf.train.saver sess.run(init saver tf.train.saver(tf.all_variables tf.train.start_queue_runners(sess sess train_dir users/zanhuang/desktop/nnp/model.ckpt summary_writer tf.train.summarywriter(train_dir sess.graph for step in range start_time time.time print(sess.run( train_loss cost duration time.time start_time if step num_examples_per_step flags.batch_size examples_per_sec num_examples_per_step duration sec_per_batch float(duration format_str s step d f examples/sec f sec/batch print format_str datetime.now step examples_per_sec sec_per_batch summary_str sess.run(summary_op summary_writer.add_summary(summary_str step if step checkpoint_path train_dir saver.save(sess checkpoint_path global_step step)def main(argv none train()if name main tf.app.run() and the eval is import tensorflow as tfimport mainimport processimport inputeval_dir users/zanhuang/desktop/nnp/model.ckpt-checkpoint_dir users/zanhuang/desktop/nnp/checkpointdef evaluate with tf.graph().as_default as g images labels process.eval_inputs forward_propgation_results process.forward_propagation(images init_op tf.initialize_all_variables saver tf.train.saver top_k_op tf.nn.in_top_k(forward_propgation_results labels with tf.session(graph g as sess tf.train.start_queue_runners(sess sess sess.run(init_op saver.restore(sess eval_dir for i in range print(sess.run(top_k_op))def main(argv none evaluate()if name main tf.app.run what have you tried i tried initializing the variables before running the queues but that only removed the errors the errors also do not point to any problem thats in the code logs or other output that would be helpful(if logs are large please upload as attachment).the output is: error.txt
171115563,3813,https://api.github.com/repos/tensorflow/tensorflow/issues/3813,guozhizou,1,0,0,0,0,0,wide_n_deep seems like unable to fit large dataset when i put million data i got: traceback most recent call last file search_click.py line in module tf.app.run file usr/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv file search_click.py line in main train_and_eval file search_click.py line in train_and_eval m.fit(input_fn=lambda input_fn(df_train steps=flags.train_steps file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in fit monitors=monitors file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in train_model summary_writer=graph_actions.get_summary_writer(self._model_dir file usr/lib/python./site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py line in get_summary_writer graph=ops.get_default_graph file usr/lib/python./site-packages/tensorflow/python/training/summary_io.py line in init self.add_graph(graph=graph graph_def=graph_def file usr/lib/python./site-packages/tensorflow/python/training/summary_io.py line in add_graph true_graph_def graph.as_graph_def(add_shapes=true file usr/lib/python./site-packages/tensorflow/python/framework/ops.py line in as_graph_def raise valueerror(graphdef cannot be larger than gb.)valueerror graphdef cannot be larger than gb. def input_fn(df input builder function creates a dictionary mapping from each continuous feature column name k to the values of that column stored in a constant tensor continuous_cols k tf.constant(df k .values for k in continuous_columns creates a dictionary mapping from each categorical feature column name k to the values of that column stored in a tf.sparsetensor categorical_cols k tf.sparsetensor indices= i for i in range(df k .size values=df k .values shape= df k .size for k in categorical_columns merges the two dictionaries into one feature_cols dict(continuous_cols feature_cols.update(categorical_cols converts the label column into a constant tensor label tf.constant(df label_column .values returns the feature columns and the label return feature_cols label is there a replacement of tf.constant and tf.sparsetensor so we can load a batch once a time?any help would be appreciated
171041952,3801,https://api.github.com/repos/tensorflow/tensorflow/issues/3801,lingz,11,0,0,0,0,0,"it looks like from the latest documentation that rnn performs early stopping for dynamic length sequences whereas dynamic_rnn does not this would seem to be the reverse of the intuition so it looks like in commit dbabecbcdf the definition of dynamic_rnn was changed from the parameter sequence_length is required and dynamic calculation is automatically performed.to the parameter sequence_length is optional and is used to copy-through state and zero-out outputs when past a batch elements sequence length so its more for correctness than performance unlike in rnn().whereas the rnn documentation states if the sequence_length vector is provided dynamic calculation is performed this method of calculation does not compute the rnn steps past the maximum sequence length of the minibatch thus saving computational time),for me this makes it very unclear in this context does rnn do more dynamic unrolling than dynamic_rnn because it actually uses sequence_length to stop early so if i want to do efficient variable length sequences dynamically i should use rnn instead of dynamic_rnn in this case what does dynamic_rnn actually do what is dynamic unrolling explicitly defined except for accept input in a different format"
170890988,3771,https://api.github.com/repos/tensorflow/tensorflow/issues/3771,Hvass-Labs,3,0,0,0,0,0,in my opinion a simple builder api is essential to tensorflow programming because the procedure of constructing neural networks is otherwise complicated repetitive and error-prone but there are at least different builder apis being developed in parallel tf.contrib.layers tf.contrib.slim tf.contrib.learn aka tf learn tf learn same name but apparently different from the above pretty tensor of these add-on packages seem to be more or less in the early stages of development and lacking good documentation but the apis appear to be very similar so why not consolidate them into a single builder api that can become an integrated part of tensorflow it is very confusing for new users the way it is done now and the development would get much further if people would work on a single builder api rather than almost identical ones the builder api is how most people will use tensorflow so it shouldnt be in limbo like this.furthermore im trying to put together some tutorials on youtube but im concerned that the api im using pretty tensor might become deprecated at some point.i hope that a consolidated and official builder api will receive more attention from the dev-team thanks
170692964,3752,https://api.github.com/repos/tensorflow/tensorflow/issues/3752,dead,2,0,0,0,0,0,bazel-bin/tensorflow/cc/tutorials_example_trainer use_gpui tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id f tensorflow/cc/tutorials/example_trainer.cc check failed tensorflow::status::ok root.tographdef(&def ok vs unimplemented explict cast of a non-empty tensor not implemented yet)f tensorflow/cc/tutorials/example_trainer.cc check failed tensorflow::status::ok root.tographdef(&def ok vs unimplemented explict cast of a non-empty tensor not implemented yet)abort core dumped environment infooperating system ubuntu cuda toolkit and cudnn driver version ls l usr/local/cuda/lib/libcud*-rw-r--r root root mai usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root mai usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root mai usr/local/cuda/lib/libcudart.so libcudart.so...-rw-r--r root root mai usr/local/cuda/lib/libcudart.so...-rw-r--r root root mai usr/local/cuda/lib/libcudart_static.a-rwxr-xr-x root root ago usr/local/cuda/lib/libcudnn.so-rwxr-xr-x root root ago usr/local/cuda/lib/libcudnn.so.-rwxr-xr-x root root ago usr/local/cuda/lib/libcudnn.so...-rw-r--r root root ago usr/local/cuda/lib/libcudnn_static.a
170638255,3750,https://api.github.com/repos/tensorflow/tensorflow/issues/3750,tomrunia,1,0,0,0,0,0,after a clean install of tensorflow v from master my tensorboard is suddenly broken the scalar event plots do not show upon clicking see screenshow below while the logs in the terminal do not show any errors the chrome developer console shows the following error upon opening a figure: tf-tensorboard.html uncaught error tf-chart-scaffolds content doesnt implement the required interfaceinsertbefore vm polymer-mini.html: i am running chrome version bit on linux mint tensorboard
170620881,3748,https://api.github.com/repos/tensorflow/tensorflow/issues/3748,florijanstamenkovic,1,0,0,0,0,0,we are getting unexpected names they lack outer name scope in namedoutputs tuples added to collections using tf.contrib.layers outputs_collections the following code demonstrates the issue: import tensorflow as tffrom tensorflow.contrib import slimwith tf.name_scope(train with slim.arg_scope( slim.fully_connected slim.flatten outputs_collections=tf.graphkeys.activations ph tf.placeholder(tf.float fc slim.fully_connected(ph flat slim.flatten(ph){print(name in tuple no.name tensor name no.outputs.name for no in tf.get_collection(tf.graphkeys.activations)} the output is: name in tuple fully_connected tensor name train/fully_connected/relu:name in tuple train/flatten tensor name train/flatten/reshape: we have tracked the cause of this down in to tf.contrib.layers for layers that use internal variables fully_connected convd final outputs are added to collections based on internal variable_scope name please see tensorflow/tensorflow/contrib/layers/python/layers/layers.py lines version commit dffbbaefeeaaa)it is our understanding that activation names generally fall under name_scopes which is consistent with actual op names in the output above.this issue makes it impossible to retrieve items from collections filtered down with a name scope an approach that we are trying to use for decoupling op creation and summarizing it seems a valid use-case
170578021,3745,https://api.github.com/repos/tensorflow/tensorflow/issues/3745,gustavla,2,0,0,0,0,0,when i use a thread-based data queue in tensorflow and also include import ipython i get various errors thrown at the very end of the tensorflow session since i am offering a solution do not import ipython i am fine closing this immediately and re-opening if necessary i am filing this mostly to have these errors and the cause on file in case it happens to someone else environment info-bit centos cuda cudnn bazel python ipython tensorflow installed from source feabebbcbbcfbaf steps to reproduce create file files.txt lines omitted file file file create file bug.py import tensorflow as tf import ipython remove and errors stop happening with tf.session as sess filename_queue tf.train.string_input_producer( files.txt shuffle=true reader tf.textlinereader key value reader.read(filename_queue batch_size min_after_dequeue capacity min_after_dequeue batch_size batch_fn tf.train.shuffle_batch value batch_size=batch_size capacity=capacity min_after_dequeue=min_after_dequeue sess.run(tf.initialize_all_variables coord tf.train.coordinator threads tf.train.start_queue_runners(sess=sess coord=coord try while not coord.should_stop print(sess.run(batch_fn break except tf.errors.outofrangeerror pass finally coord.request_stop coord.join(threads run python bug.py repeatedly if no error occurs what have you tried?removing import ipython fixes it i have only tested it on one platform so i do not know if this is a universal problem.of course this might also happen if packages that import ipython are imported it originally happened to me when i imported ipdb errorsin the tradition of thread-related bugs the error message is not deterministic and runs breaks down as follows i am being thorough here to make these searchable occurrences error no error error error error investigating these errors further leads nowhere since it is clear that python is behaving erratically and variables that are clearly set one line can be corrupted the next error exception ignored in bound method session.__del of tensorflow.python.client.session.session object at xfdd>>traceback most recent call last file python./site-packages/tensorflow/python/client/session.py line in del__attributeerror nonetype object has no attribute raise_exception_on_not_ok_status error exception ignored in bound method session.__del of tensorflow.python.client.session.session object at xfde>>traceback most recent call last file python./site-packages/tensorflow/python/client/session.py line in del file share/data/vision-greg/common/anaconda/lib/python./contextlib.py line in helpertypeerror nonetype object is not callable error exception ignored in bound method session.__del of tensorflow.python.client.session.session object at xfeecb>>traceback most recent call last file python./site-packages/tensorflow/python/client/session.py line in del file share/data/vision-greg/common/anaconda/lib/python./contextlib.py line in enter file python./site-packages/tensorflow/python/framework/errors.py line in raise_exception_on_not_ok_statusunboundlocalerror local variable status referenced before assignment
170445823,3731,https://api.github.com/repos/tensorflow/tensorflow/issues/3731,jlowin,1,0,0,0,0,0,when supplied a negative axis tf.pack actually operates on axis for example providing axis operates on the second-to-last axis not the last axis.this code illustrates the issue pythonimport tensorflow as tfimport numpy as np tensors with shape x tf.convert_to_tensor(np.zeros pack along last axis explicitlytf.pack(x get_shape().as_list pack along last axis with negative axis doesnt worktf.pack(x get_shape().as_list this doesnt match the behavior of unpack so packing/unpacking along the same negative axis failslen(tf.unpack(tf.pack(x axis axis false environment infooperating system macos installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud n/aif installed from binary pip package provide which pip package you installed the output from python c import tensorflow print(tensorflow.__version rc
170384701,3728,https://api.github.com/repos/tensorflow/tensorflow/issues/3728,jbwenisch,1,0,0,0,0,0,imho the new architecture of contrib.learn.dnnregressor compared to the old contrib.learn.tensorflowdnnregressor makes it impossible to properly use things like gridsearchcv from sklearn to use gridsearchcv i have to pass all parameters to be tuned to the constructor in the new architecture of dnnregressor some parameters are passed to the fit-method which is quite untypical for sklearns architecture usually expecting only x and y as parameters of the fit-method for example i can no longer tune steps and batch_size now part of the fit-method with gridsearchcv i can only pass them as fixed fit_params or am i getting something wrong
169764608,3680,https://api.github.com/repos/tensorflow/tensorflow/issues/3680,EderSantana,2,0,0,0,0,0,i was installing tf from source like i always did but im having the following issue this time when running bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkg i got bash~/python/tensorflow bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkgsat aug edt using tmpdir tmp/tmp.tnjrrcgpe/tmp/tmp.tnjrrcgpe python/tensorflowsat aug edt building wheelerror cant copy tensorflow/models/embedding/gen_wordvec.py doesnt exist or not a regular file plus i cant find that file anywhere did anything change or am i missing something
169576595,3663,https://api.github.com/repos/tensorflow/tensorflow/issues/3663,stilda,2,0,0,0,0,0,hi i use r version but still have an error when use nested map_fn and a variable involved in calculations of inner function.the error is tensorflow.python.framework.errors.invalidargumenterror input of node gradients/map/while/map/tensorarraypack_grad/tensorarraygrad/tensorarraygrad was passed string from gradients/map/while/map/tensorarraypack_grad/tensorarraygrad/tensorarraygrad/stackpop incompatible with expected string_ref.the code is import tensorflow as tfdef inner_loop(t def fn(n return n var using var here leads to the error return tf.map_fn(fn=fn elems=t parallel_iterations=)def outer_loop(input def fn(n return inner_loop(n if i would return inner_loop(n)+var here no errors appear return tf.map_fn(fn=fn elems=input parallel_iterations=)with tf.session as sess var tf.variable(tf.constant input tf.to_float(tf.convert_to_tensor res outer_loop(input optimizer tf.train.adamoptimizer(learning_rate trainoperation optimizer.minimize(tf.reduce_mean(tf.square(res sess.run(tf.initialize_all_variables sess.run(trainoperation
169465608,3649,https://api.github.com/repos/tensorflow/tensorflow/issues/3649,ppwwyyxx,3,0,0,0,0,0,archlinux cuda nightly tf built for python pythonimport tensorflow as tfimport timev tf.get_variable(test shape vb tf.get_variable(test shape dtype=tf.bool initializer=tf.constant_initializer(false))b tf.reduce_sum(v)b tf.reduce_all(vb)b tf.reduce_all(tf.cast(v tf.bool))sess tf.session()sess.run(tf.initialize_all_variables())with sess.as_default start time.time for k in range sess.run(b print time.time start s start time.time for k in range sess.run(b print time.time start s start time.time for k in range sess.run(b print time.time start s! cpu version of the same operation is also much faster than this
169195660,3628,https://api.github.com/repos/tensorflow/tensorflow/issues/3628,pavelgonchar,17,0,0,0,0,0,error when loading the frozen graph with tensorflow.contrib.layers.python.layers.batch_norm graph_def is invalid at node ubatchnorm/cond/assignmovingavg/switch input tensor batchnorm/moving_mean cannot convert a tensor of type float to an input of type float_ref freeze_graph.py doesnt seem to store moving_mean and moving_variance properly
168743959,3603,https://api.github.com/repos/tensorflow/tensorflow/issues/3603,rob-rowe,4,0,0,0,0,0,this issue is a follow-up to this posting on stack overflow as noted the version is x slower than version for a particular training script i am using the same slow-down is observed when using the pip version with cudnn or with a version compiled from source and cudnn im not sure what information about the training script would help it is a fairly complex script that is training a network for object detection using methods similar to yolo and ssd it wont be possible to post the entire set of code but if there is a way to isolate the source of the delay i may be able to modify it and post that portion.while trying to get insight into the source of the slow-down i also noted that i wasnt able to compile from source using cudnn as noted in the so posting this is a secondary issue but may also warrant investigation
168592359,3598,https://api.github.com/repos/tensorflow/tensorflow/issues/3598,arvindsg,1,0,0,0,0,0,"gather_nd doesnt work for batched slicingsteps to reproducerun the following code pythonimport tensorflow as tfcons_indices=tf.constant batch=tf.constant( a b c d )gathered=tf.gather_nd(batch,cons_indices)sess tf.session()result sess.run(gathered)print(result)sess.close() always gives an error regarding incompatible dimensions.either the documentation needs to be updated to indicate proper argument format or function implementation is buggy"
168434938,3579,https://api.github.com/repos/tensorflow/tensorflow/issues/3579,arjunsesh,2,0,0,0,0,0,github issues are for bugs installation problems feature requests for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice environment infooperating system linux steps to reproduce create any tensor tf.variable or tf.constant containing batches of self adjoint matrices call this b and note its shape run something like alpha_u tf.batch_self_adjoint_eig(b look at bs shape again the second to last dimension increments by what have you tried selecting all but the last element of the second to last dimension solves the problem but is hacky logs or other output that would be helpful(if logs are large please upload as attachment bsess.run(b)array dtype=float)alpha_u tf.batch_self_adjoint_eig(b b
168168251,3550,https://api.github.com/repos/tensorflow/tensorflow/issues/3550,tamimcsedu19,3,0,0,0,0,0,operating system ubuntu installed version of cuda and cudnn cuda rc cudnn for rc(please attach the output of ls l path/to/cuda/lib/libcud* ): -rw-r--r root root libcudadevrt.alrwxrwxrwx root root libcudart.so libcudart.so..lrwxrwxrwx root root libcudart.so libcudart.so...-rwxr-xr-x root root libcudart.so...-rw-r--r root root libcudart_static.a-rwxr-xr-x root root libcudnn.so-rwxr-xr-x root root libcudnn.so.-rwxr-xr-x root root libcudnn.so...-rw-r--r root root libcudnn_static.a ran the command with bazel build c opt verbose_failures config=cuda tensorflow/cc:tutorials_example_trainer what have you tried tried adding usr/lib/gcc/x_-linux-gnu to the path then it fails usr/include/stdlib.h fatal error stddef.h no such file or directory error message error home/tamim/.cache/bazel/_bazel_tamim/fccffbbcddafcbe/external/highwayhash/build c compilation of rule highwayhash//:sip_hash failed crosstool_wrapper_driver_is_not_gcc failed error executing command cd home/tamim/.cache/bazel/_bazel_tamim/fccffbbcddafcbe/execroot/tensorflow exec env path=/home/tamim/bin:/usr/local/cuda/bin:/home/tamim/anaconda/bin:/home/tamim/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc u_fortify_source d_fortify_source fstack-protector fpie wall wunused-but-set-parameter wno-free-nonheap-object fno-omit-frame-pointer g o dndebug ffunction-sections fdata-sections g std=c frandom-seed=bazel-out/host/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o iquote external/highwayhash iquote bazel-out/host/genfiles/external/highwayhash iquote external/bazel_tools iquote bazel-out/host/genfiles/external/bazel_tools isystem external/highwayhash isystem bazel-out/host/genfiles/external/highwayhash isystem external/bazel_tools/tools/cpp/gcc no-canonical-prefixes wno-builtin-macro-redefined d__date__=redacted d__timestamp__=redacted d__time__=redacted fno-canonical-system-headers md mf bazel-out/host/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d c external/highwayhash/highwayhash/sip_hash.cc o bazel-out/host/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o com.google.devtools.build.lib.shell.badexitstatusexception process exited with status gcc error trying to exec ccplus execvp no such file or directorytarget tensorflow/cc:tutorials_example_trainer failed to buildfeature request a prebuilt wheel for cuda release candidate
168099893,3548,https://api.github.com/repos/tensorflow/tensorflow/issues/3548,tmquan,2,0,0,0,0,0,dear community may i ask is there any option that enables us to change the orientation of the current layout of graphs in tensorboard by default it is vertically from bottom to top in the current form best
167943988,3536,https://api.github.com/repos/tensorflow/tensorflow/issues/3536,hillegass,2,0,0,0,0,0,thanks to the tensorflow:libtensorflow.so target i can now build a shared library that i can use in my c programs i want to install that library in usr/local/lib and put the headers for it in usr/local/include but there is no target to build and install those headers in particular building the library doesnt build the headers from the proto files so i have error_codes.proto but not error_codes.pb.h so i cant even use the source directory for the headers
167629897,3507,https://api.github.com/repos/tensorflow/tensorflow/issues/3507,akors,1,0,0,0,0,0,hi i have come across a very very strange issue namely training on an nvidia gtx does not work at all and judging from the error rate the predicted labels are completely random.i have almost identical systems see below and while on the system with the gtx the training runs perfectly fine on the system with the gtx the training simply doesnt work.to test this i ranthe following code python m tensorflow.models.image.mnist.convolutional on the system gtx i get to an error rate below within batches and at the end its below step epoch msminibatch loss learning rate minibatch error validation error step epoch msminibatch loss learning rate minibatch error validation error step epoch msminibatch loss learning rate minibatch error validation error step epoch msminibatch loss learning rate minibatch error validation error test error on the gtx system the performance simply never improves error rate is steady at around step epoch msminibatch loss learning rate minibatch error validation error step epoch msminibatch loss learning rate minibatch error validation error test error i tested this with tensorflow from the release pip package for python with gpu support.i also tested this with tensorflow master from a week ago fcedafbccccfab compiled to a pip package on one machine installed on both machines.here are the full system specs but the difference between them is only the gpu vs and the driver vs system os ubuntu lts kernel generic nvidia driver version cpu intel(r core(tm i cpu ghz ram gb ram gpu nvidia gtx gb vram edition msi gtx gaming g)system os ubuntu lts kernel generic nvidia driver version cpu intel(r core(tm i cpu ghz ram gb ram gpu nvidia gtx gb vrammy ld_library_path on both machines is: :/usr/local/cuda/lib:/usr/local/cuda-./extras/cupti/lib/ my cuda version is cudnn is on both machines output of ls l usr/local/cuda/lib on the machine with the gtx and gtx anyone know what could cause this and how to fix this
167357968,3492,https://api.github.com/repos/tensorflow/tensorflow/issues/3492,psicalculus,9,0,0,0,0,0,could you please implement d/d dilated convolution and d/d dilated pooling in tensorflow?please see for a reference on dilated convolution.dilated max-pooling is simply regular max-pooling but the pixels/voxels you use in each application of the max-pooling operation are exactly the same pixels/voxels you would select with dilated convolution.dilated convolution/pooling are useful for connectomics and d shape datasets d deep learning
167243286,3482,https://api.github.com/repos/tensorflow/tensorflow/issues/3482,drcrook1,1,0,0,0,0,0,"hey all,there are a variety of issues im running into with the docker image just basic stuff sudo apt-get update python support nano is not installed vim is the worst a i want this so i can easily edit the notebook config for setting passwords the jupyter notebook needs to be updated to the latest pip updates maybe ship with anaconda as well.its just a variety of things that arent working quite well im trying to put together a jupyter notebook our machine learning meetup group can use and share and its just a pain in the neck i tried to do just a fresh setup from a fresh linux vm but i was having issues getting jupyter to actually recognize the tensorflow install so now im trying the docker image again the biggest issue with the docker image in its current state is that after a while the auto-save starts breaking json formats for posting to save break and then it messes up the encoding of the notebook which then becomes more or less un-recoverable without extensive effort).i beleive not sure that updating everything in the supported image would fix at least some of these issues"
167147454,3472,https://api.github.com/repos/tensorflow/tensorflow/issues/3472,ngaloppo,2,0,0,0,0,0,this warning message in this quantized ops code snippet is output when input_offset zero is not representable in the quantized range used by the input this means quantizedconvd has to fall back to a slow implementation since the border of zero values cant be represented easily you should try to construct graphs that avoid this situation.is that really whats going on there what does that have to do with input_offset secondly there is a code comment about gemmlowp needing support for a specific code path perhaps its the input_offset path whats going on here adding petewarden as hes probably the most familiar with this code
167097831,3467,https://api.github.com/repos/tensorflow/tensorflow/issues/3467,drcrook1,15,0,0,0,0,0,this is more a feature request any chance we can get support for this
166809942,3441,https://api.github.com/repos/tensorflow/tensorflow/issues/3441,tvogels,1,0,0,0,0,0,this is a feature request currently i find the documentation for adding an op quite minimal there are a few important questions that remain unanswered how should one use multithreaded cpu code in an operation can we use openmp how many threads should an op use it is hard to find this in the implementations of existing ops because they are all based on eigen how should a gpu version of the op be written should it be a cuda kernel or the code spawning cuda kernels maybe there could be a simple example for both cpu and gpu code not using eigen
166482495,3401,https://api.github.com/repos/tensorflow/tensorflow/issues/3401,wfs,1,0,0,0,0,0,environment infooperating system:(tensorflow paddlescoot@paddlescoot-satellite-p cat proc/versionlinux version generic buildd@lgw gcc version ubuntu ubuntu ubuntu smp wed jul utc if installed from binary pip package provide which pip package you installed follow python virtualenv installtion notes the output from python c import tensorflow print(tensorflow.__version steps to reproduce follow test notes here when you get to the last step the following error appears tensorflow paddlescoot@paddlescoot-satellite-p python home/paddlescoot/tensorflow/local/lib/python./site-packages/tensorflow/models/image/mnist/convolutional.py extracting data/train-images-idx-ubyte.gz traceback most recent call last file home/paddlescoot/tensorflow/local/lib/python./site-packages/tensorflow/models/image/mnist/convolutional.py line in module tf.app.run file home/paddlescoot/tensorflow/local/lib/python./site-packages/tensorflow/python/platform/app.py line in run sys.exit(main(sys.argv file home/paddlescoot/tensorflow/local/lib/python./site-packages/tensorflow/models/image/mnist/convolutional.py line in main train_data extract_data(train_data_filename file home/paddlescoot/tensorflow/local/lib/python./site-packages/tensorflow/models/image/mnist/convolutional.py line in extract_data buf bytestream.read(image_size image_size num_images file usr/lib/python./gzip.py line in read self._read(readsize file usr/lib/python./gzip.py line in read self._read_eof file usr/lib/python./gzip.py line in read_eof hex(self.crc ioerror crc check failed xbedf xccbel what have you tried searching similar issues
166366767,3392,https://api.github.com/repos/tensorflow/tensorflow/issues/3392,rdipietro,1,0,0,0,0,0,right now there is conflicting behavior when padding=same if inputs have a defined height and width then convolutions require that filters be no larger than input images spatially if inputs do not have a defined height and width then its okay for filters to be larger than images.i think this conflicting behavior should be removed especially since padding=same is used for convenience and with the intention of allowing some border effects and because this way we can continue to use this convenience even when filter size input size.tensorflow example with defined height and width: inputs tf.placeholder(tf.float shape= none weights tf.get_variable(weights tf.float initializer=tf.random_normal_initializer())t tf.nn.convd(inputs weights same valueerror filter must not be larger than the input filter input tensorflow example without defined height and width: inputs tf.placeholder(tf.float shape= none none none weights tf.get_variable(weights tf.float initializer=tf.random_normal_initializer())t tf.nn.convd(inputs weights same)with tf.session as sess sess.run(tf.initialize_all_variables print(sess.run(t feed_dict={inputs np.random.rand shape shape is what we expect
166322179,3389,https://api.github.com/repos/tensorflow/tensorflow/issues/3389,wishforgood,13,0,0,0,0,0,what im trying to do i am trying to extract cnn features for my own images with residual-net based on i plan to input image data from jpg files before exploring how to convert the images into a single file. what i have done i have read and some related materials about how to input data like feeding and placeholder here is my code: import tensorflow as tffrom convert import print_prob checkpoint_fn meta_fnfrom image_processing import image_preprocessingtf.app.flags.define_integer(batch_size batch size)tf.app.flags.define_integer(input_size input image size)tf.app.flags.define_integer(min_after_dequeue min after dequeue)tf.app.flags.define_integer(layers the number of layers in the net)tf.app.flags.define_integer(image_number number of images)flags tf.app.flags.flagsdef placeholder_inputs images_placeholder tf.placeholder(tf.float shape=(flags.batch_size flags.input_size flags.input_size label_placeholder tf.placeholder(tf.int shape=flags.batch_size return images_placeholder label_placeholderdef fill_feed_dict(image_ba label_ba images_pl labels_pl feed_dict images_pl image_ba return feed_dictmin_fraction_of_examples_in_queue min_queue_examples int(flags.image_number min_fraction_of_examples_in_queue)dataset tf.train.string_input_producer( hollywood_test.txt )reader tf.textlinereader file_content reader.read(dataset)image_name label tf.decode_csv(file_content label tf.string_to_number(label)num_preprocess_threads images_and_labels with tf.session as sess for thread_id in range(num_preprocess_threads image_buffer tf.read_file(image_name bbox train false image image_preprocessing(image_buffer bbox train thread_id image image_buffer images_and_labels.append( image label image_batch label_batch tf.train.batch_join(images_and_labels batch_size=flags.batch_size capacity=min_queue_examples flags.batch_size images_placeholder labels_placeholder placeholder_inputs new_saver tf.train.import_meta_graph(meta_fn(flags.layers new_saver.restore(sess checkpoint_fn(flags.layers graph tf.get_default_graph prob_tensor graph.get_tensor_by_name(prob images graph.get_tensor_by_name(images feed_dict fill_feed_dict(image_batch label_batch images labels_placeholder coord tf.train.coordinator threads tf.train.start_queue_runners(coord=coord sess.run(tf.initialize_all_variables prob sess.run(prob_tensor feed_dict=feed_dict print_prob(prob coord.request_stop coord.join(threads) what my question is the code above got the error typeerror the value of a feed cannot be a tf.tensor object acceptable feed values include python scalars strings lists or numpy ndarrays you can see i am trying to do all the work in the context of tensorflow as far as i know tensorflow is a framework where all the inputs and outputs of the nodes are tensors however i am quite confused why feed_dict doesnt support tensor as an input but the batch_join function return a tensor and so do other ops i found that in the mnist example of tensorflow there is even another function to produce a batch when tensorflow is providing methods for batching so i wonder if there is an elegant way to do these things if this is because of my lack of searching and careful reading i really apologize
166303822,3388,https://api.github.com/repos/tensorflow/tensorflow/issues/3388,Hvass-Labs,2,0,0,0,0,0,"hello everyone,the following code produces an error in tensorflow: import tensorflow as tfa tf.constant()b tf.constant()c a bsession tf.session a slightly different error is produced if this is removed.session.run(tf.initialize_all_variables())result session.run(c)print(result)session.close the error is produced regardless of this.#quit this produces the error.import syssys.exit this also produces the error. this is the contents of sandbox.py which i run in pycharm.the output is: /home/magnus/anaconda/envs/tensorflow/bin/python home/magnus/development/tensorflow-tutorials/sandbox.pyexception ignored in bound method basesession.__del of tensorflow.python.client.session.session object at xfbecdd>>traceback most recent call last file home/magnus/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in del file home/magnus/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in closeattributeerror nonetype object has no attribute raise_exception_on_not_ok_statusprocess finished with exit code im using the following versions tensorflow cpu installed from binary python anaconda pycharm linux mint"
166230073,3382,https://api.github.com/repos/tensorflow/tensorflow/issues/3382,petewarden,2,0,0,0,0,0,this should help reduce the frequency of breakages since it uses a wildcard and exclusion list approach modeled on the bazel rule rather than a plain list of files
166213724,3378,https://api.github.com/repos/tensorflow/tensorflow/issues/3378,danijar,13,0,0,0,0,0,i couldnt find anything in the documentation on this is it possible to feed placeholders by name if not why i think this would be useful to feed graphs after loading them from disk pythonimport tensorflow as tfx tf.placeholder(tf.float none x)y tf.reduce_sum(x)sess tf.session()sess.run(y x sess.run(y x cannot interpret feed_dict key as tensor the name x refers to an operation not a tensor tensor names must be of the form op_name>:.sess.run(y tf.get_default_graph().get_operation_by_name(x).outputs
166212411,3377,https://api.github.com/repos/tensorflow/tensorflow/issues/3377,lizallendorf,5,0,0,0,0,0,i have noticed that when moving mnist data from the cpu to the gpu there is a significant time lag when using tensorflow in comparison to theano specifically we have noticed this problem in the context of feed_dict which moves information from the cpu to the gpu when running minibatches i am using python our current solution to this problem is to move all of the data directly to the gpu at the beginning of the program which is of course not sustainable unless one has a significant amount of space on their gpu when we time minibatches of size each tensorflow is approximately four times as slow as theano i have attached both files and the time difference is evident in the final output environment infooperating system ubuntu cpu information:architecture x_cpu op-mode(s bit bitbyte order little endiancpu(s on-line cpu(s list thread(s per core core(s per socket socket(s numa node(s vendor id genuineintelcpu family model stepping cpu mhz bogomips virtualization vt-xld cache kli cache kl cache kl cache knuma node cpu(s gpu nvidia geforce gtx titan x graphics card gb installed version of cuda and cudnn logs or other output that would be helpful(if logs are large please upload as attachment).relevant files i apologize for the poor naming conventions):. mnist_softmax.txt file contains the relevant program that uses tensorflow mnist.pkl.gz this file contains the dataset for the tensorflow file mnist_softmax_theano.txt this file contains the relevant program that uses theano tf_data.pkl.gz this file contains the dataset necessary for the theano file
166206768,3376,https://api.github.com/repos/tensorflow/tensorflow/issues/3376,stas-sl,2,0,0,0,0,0,"when i try the following code: import tensorflow as tfx tf.placeholder(float shape=(none none))tf.cond(x lambda lambda x) i get this error: /users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/ops/control_flow_ops.pyc in cond(pred fn fn name if isinstance(pred bool raise typeerror(pred must not be a python bool p p switch(pred pred pivot array_ops.identity(p name=switch_t pivot array_ops.identity(p name=switch_f)/users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/ops/control_flow_ops.pyc in switch(data pred dtype name pred ops.convert_to_tensor(pred name=pred if isinstance(data ops.tensor return gen_control_flow_ops._switch(data pred name=name else if not isinstance(data ops.indexedslices ops.sparsetensor)):/users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/ops/gen_control_flow_ops.pyc in switch(data pred name output_true a tensor has the same type as data if pred is true data will be forwarded to this output result op_def_lib.apply_op(switch data=data pred=pred name=name return switchoutput._make(result users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self op_type_name name keywords op g.create_op(op_type_name inputs output_types name=scope input_types=input_types attrs=attr_protos op_def=op_def outputs op.outputs return restructure(ops.convert_n_to_tensor(outputs),/users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/framework/ops.pyc in create_op(self op_type inputs dtypes input_types name attrs op_def compute_shapes compute_device original_op=self._default_original_op op_def=op_def if compute_shapes set_shapes_for_outputs(ret self._add_op(ret self._record_op_seen_by_control_dependencies(ret)/users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op raise runtimeerror(no shape function registered for standard op s op.type shapes shape_func(op if shapes is none raise runtimeerror(/users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/ops/control_flow_ops.pyc in switchshape(op def switchshape(op input_shape op.inputs .get_shape unused_pred_shape op.inputs .get_shape().merge_with(tensor_shape.scalar return input_shape users/stas/.pyenv/versions/anaconda-../lib/python./site-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self other except valueerror raise valueerror(shapes s and s are not compatible self other def concatenate(self other):valueerror shapes and are not compatible as i understand pred argument should be scalar but what i need is element-wise condition.though theano switch is working as expected: import theano.tensor as tx t.matrix()t.switch(x x).eval({x np.zeros float"
166137997,3368,https://api.github.com/repos/tensorflow/tensorflow/issues/3368,gideonite,38,0,0,2,0,1,has there been discussion of exporting tensorboard visualizations as images e.g png this would be helpful for rough drafts and work-in-progress type presentations
165914098,3340,https://api.github.com/repos/tensorflow/tensorflow/issues/3340,llealgt,1,0,0,0,0,0,hi i been struggling some days trying to save a contrib.learn.dnnclassifier and im getting desperate can you help me i tried everything it says in official documentation but it sees as if the documentation isnt coherent with the api things i have tried are rain.saver but got a no variables to save error tried this examples created my dnnclassifier but when trying to call function save on my classifier it says dnnclassifier object has no attribute save tried the deprecated class tensorflowdnnclassifier and it can be saved right but when you try to restore it it says that theres no model to restore tried to restore the saved tensorflowdnnclassifier with estimator.restore but it says that estimator has no attribute restore.is there a way to save and restore a dnnclassifier this question is asked multiple times in stackoverflow and in would be very thankfull if you can help me
165842496,3332,https://api.github.com/repos/tensorflow/tensorflow/issues/3332,fculinovic,7,0,0,0,0,0,"as much as i have managed to follow the api section of tensorflow the depthwise_convd doesnt fulfill my thoughts on what i would like to do with the input/filters. shape initializer tf.truncated_normal_initializer(stddev=e-)kernel tf.get_variable(name=weights shape=shape initializer=initializer) considering the input is of shape batch_size the function definition would be: def depthwise_group_convd(input filter groups strides padding name) which would be able to do conv tf.nn.depthwise_group_convd(input kernel strides padding name) that would split the input depthwise into depths from given groups parameter and perform the convolution with shared parameters i.e depths take weights and depths take weights inside a group then the outputs of convolutions by groups would be concatenated depthwise.this would make it easier to define groups such as one used in alexnet/caffenet architectures.hopefully i have missed a certain feature already existing for making this easier.best regards,filip"
165710866,3324,https://api.github.com/repos/tensorflow/tensorflow/issues/3324,wfs,1,0,0,0,0,0,environment infooperating system:(tf anaconda/bin cat proc/versionlinux version generic buildd@lgw gcc version ubuntu ubuntu ubuntu smp wed jul utc steps to reproduce follow anaconda/bin conda create name tf python fetching package metadata solving package specifications package plan for installation in environment home/paddlescoot/anaconda/envs/tf:the following packages will be downloaded package build python mb setuptools py kb wheel py kb pip py mb total mb the following new packages will be installed: openssl h pip py python readline setuptools py_sqlite tk wheel py_zlib proceed y /n yfetching packages python setuptools wheel-..-p pip-..-py extracting packages linking packages anaconda/bin source activate tf(tf anaconda/bin conda info envs conda environments tf home/paddlescoot/anaconda/envs/tfroot home/paddlescoot/anaconda(tf anaconda/bin python versionpython continuum analytics inc.(tf anaconda/bin export tf_binary_url anaconda/bin pip install upgrade tf_binary_urlcollecting tensorflow from downloading mb mb kb/s collecting numpy from tensorflow downloading numpy-..-cp-cpmu-manylinux_x_.whl mb mb kb/s collecting six from tensorflow using cached six-..-py.py-none-any.whlcollecting protobuf==..b from tensorflow using cached protobuf-..b-py.py-none-any.whlrequirement already up-to-date wheel in home/paddlescoot/anaconda/envs/tf/lib/python./site-packages from tensorflow==..)collecting setuptools from protobuf==..b->tensorflow downloading setuptools-..-py.py-none-any.whl kb kb mb/s installing collected packages numpy six setuptools protobuf tensorflow found existing installation setuptools cannot remove entries from nonexistent file home/paddlescoot/anaconda/envs/tf/lib/python./site-packages/easy-install.pth(tf anaconda/bin pythonpython continuum analytics inc default jul gcc red hat on linuxtype help copyright credits or license for more information.anaconda is brought to you by continuum analytics.please check out and import tensorflow as tf traceback most recent call last file stdin line in module importerror no module named tensorflow quit()(tf anaconda/bin ls ali home/paddlescoot/anaconda/envs/tf/lib/python./site-packages/numpy pip readme setuptools.pth six.py wheel/numpy-...dist-info pip-..-py..egg-info setuptools-..-py..egg six-...dist-info six.pyc wheel-..-py..egg-info/(tf anaconda/bin ls ali home/paddlescoot/anaconda/envs/tf/lib/python./site-packages/numpy pip readme setuptools.pth six.py wheel/numpy-...dist-info pip-..-py..egg-info setuptools-..-py..egg six-...dist-info six.pyc wheel-..-py..egg-info what have you tried searching for similar issues
165632976,3317,https://api.github.com/repos/tensorflow/tensorflow/issues/3317,mbz,1,0,0,0,0,0,in numerous locations in the documentation it has been mentioned that it is possible to handle multiple graph s but i couldnt find any example since session only support a single graph it seems the only way of doing it is to create multiple session instances is it safe to have multiple instances session at the same time if not what is the proper way of handling multiple graph s thanks
165148915,3285,https://api.github.com/repos/tensorflow/tensorflow/issues/3285,hzhangxyz,0,0,0,0,0,1,"i install it directly with condaand get version glibc not found,required by pywrap_tensorflow.soi use ld_preload to load glibc_.and get vdso_time invalid mode for dlopen invalid argumenti try to install from the source but bazel require glibc_.what should i do?--- uname alinux mu el.x smp tue jan est x x x gnu/linux and the version of glibc is"
164808267,3269,https://api.github.com/repos/tensorflow/tensorflow/issues/3269,davidzchen,1,0,0,0,0,0,fixes
164689854,3249,https://api.github.com/repos/tensorflow/tensorflow/issues/3249,vrv,0,0,0,0,0,1,cudnn versions with the same major version should be abicompatible which means that if we release a binarywith cudnn version it should be compatible witha system whose runtime loads version this checkwould trigger if the cudnn version loaded at runtime is but the compile-time version used was this change is based off of recommendations from
164686276,3247,https://api.github.com/repos/tensorflow/tensorflow/issues/3247,tanmayshankar,2,0,0,0,0,0,hello i recently started to use tensorflow to implement and idea using d convolutions it requires me to use different padding boundary conditions along spatial and temporal directions dimensions of the tensor i need padding=same for spatial and padding=valid for temporal directions tf.nn.convd and tf.nn.max_poold specify padding for the operation as a whole is there any way to specify padding type for each dimension individually like how the stride is specified i dont mind contributing for this if necessary if someone could direct me on how to add this feature or create a new op for it
164677030,3246,https://api.github.com/repos/tensorflow/tensorflow/issues/3246,davidbernat,1,0,0,0,0,0,tf.matmul(a b a_is_sparse=true fails on a ops.convert_to_tensor(a name=a with cryptic error.example code: import tensorflow as tfimport numpy as npx tf.sparse_placeholder(tf.float)y tf.variable(tf.random_uniform minval maxval dtype=tf.float))with tf.session as sess sess.run(tf.initialize_all_variables indices np.array dtype=np.int values np.array dtype=np.float shape np.array dtype=np.int sess.run(tf.matmul(x y feed_dict x tf.sparsetensorvalue(indices values shape)}) error line in module sess.run(tf.matmul(x y a_is_sparse=true feed_dict file usr/local/lib/python./site-packages/tensorflow/python/ops/math_ops.py line in matmul a ops.convert_to_tensor(a name=a file usr/local/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file usr/local/lib/python./site-packages/tensorflow/python/ops/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file usr/local/lib/python./site-packages/tensorflow/python/ops/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape file usr/local/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file usr/local/lib/python./site-packages/tensorflow/python/util/compat.py line in as_bytes raise typeerror(expected binary or unicode string got r bytes_or_text)typeerror expected binary or unicode string got tensorflow.python.framework.ops.sparsetensor object at xdd>is fixed by explicitly converting from sparse to dense tensor before multiplication: import tensorflow as tfimport numpy as npx tf.sparse_placeholder(tf.float)z tf.sparse_tensor_to_dense(x)y tf.variable(tf.random_uniform minval maxval dtype=tf.float))with tf.session as sess sess.run(tf.initialize_all_variables indices np.array dtype=np.int values np.array dtype=np.float shape np.array dtype=np.int sess.run(tf.matmul(z y feed_dict x tf.sparsetensorvalue(indices values shape
164469949,3233,https://api.github.com/repos/tensorflow/tensorflow/issues/3233,nikitakit,1,0,0,0,0,0,here is a section of code copied from the docstring of partial_run with required import statements added however note that this constructs an interactivesession instead of a session pythonimport tensorflow as tfimport numpy as npfrom tensorflow.python.ops import array_ops math_opsfrom tensorflow.python import dtypessess tf.interactivesession()a array_ops.placeholder(dtypes.float shape= )b array_ops.placeholder(dtypes.float shape= )c array_ops.placeholder(dtypes.float shape= )r math_ops.add(a b)r math_ops.mul(r c)h sess.partial_run_setup( r r a b c )res sess.partial_run(h r feed_dict={a b res sess.partial_run(h r feed_dict={c res}) the code fails with a notfounderror when the interactivesession is replaced by a normal session the code runs correctly the difference between the two is that the interactive session sets the place_pruned_graph option to true starting a plain session with place_pruned_graph=true reproduces the issue.)i do all of my work in an interactive terminal and i would find it useful to have partial_run work correctly in this setting. system info ubuntu tensorflow fbbfafaaeeeaaafecbff python cudnn cuda
164143144,3209,https://api.github.com/repos/tensorflow/tensorflow/issues/3209,zhaopku,0,0,0,0,0,1,im running code from on my own computer which is a sample of how to use tensorboard however i see nothing from the tensor board from my computer every tab of the tensorflow is empty saying no xx data is found.i tried the inspect option zhao@zhao-ubuntu:~/desktop/samples tensorboard logdir logs inspect processing event files this can take a few minutes no event files found within logdir and the debug option zhao@zhao-ubuntu:~/desktop/samples tensorboard logdir logs debug info:tensorflow:tensorboard is in debug mode info:tensorflow:starting tensorboard in directory home/zhao/desktop/samples info:tensorflow:tensorboard path_to_run is home/zhao/desktop/samples none info:tensorflow:multiplexer done loading load took secs info:tensorflow:tensorboard is tag:b starting tensorboard b on port you can navigate to im using python on my ubuntu machine
163741594,3188,https://api.github.com/repos/tensorflow/tensorflow/issues/3188,chrisgrimm,1,0,0,0,0,0,environment infooperating system osx yosemite if installed from binary pip package provide which pip package you installed cpu only mac-version the output from python c import tensorflow print(tensorflow. version rcive been getting a strange error when trying to use the basiclstm cell running the below code: import tensorflow as tfsess tf.session()init_state tf.zeros init_state tf.zeros input tf.placeholder(tf.float input tf.placeholder(tf.float print init_state.get_shape()output state tf.nn.rnn_cell.basiclstmcell()(input init_state)output state tf.nn.rnn_cell.basiclstmcell()(input init_state) results in a valueerror: valueerror variable basiclstmcell/linear/matrix already exists disallowed did you mean to set reuse=true in varscope originally defined at:output state tf.nn.rnn_cell.basiclstmcell()(input init_state) the error doesnt occur if the line generating output state is omitted
163642726,3181,https://api.github.com/repos/tensorflow/tensorflow/issues/3181,sjperkins,2,0,0,0,0,0,environment infooperating system ubuntu installed version of cuda and cudnn cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud bashls l usr/local/cuda-./lib/libcud*-rw-r--r root root aug usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root aug usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root aug usr/local/cuda-./lib/libcudart.so libcudart.so...-rwxr-xr-x root root aug usr/local/cuda-./lib/libcudart.so...-rw-r--r root root aug usr/local/cuda-./lib/libcudart_static.alrwxrwxrwx users feb usr/local/cuda-./lib/libcudnn.so libcudnn.so.lrwxrwxrwx users feb usr/local/cuda-./lib/libcudnn.so libcudnn.so...-rwxrwxr-x users feb usr/local/cuda-./lib/libcudnn.so...-rw-rw-r users feb usr/local/cuda-./lib/libcudnn_static.a if installed from binary pip package provide which pip package you installed nightly build ubuntu/linux bit gpu enabled python the output from python c import tensorflow print(tensorflow.__version bash python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally.. if installed from sources provide the commit hash steps to reproduce running the following python script pythonimport tensorflow as tfwith tf.session as s v tf.linspace s.run(tf.initialize_variables( v )) produces the following output pythoni tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx mmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx m pci bus id traceback most recent call last file test_init_fail.py line in module s.run(tf.initialize_variables( v file home/foobar/venv/mb/local/lib/python./site-packages/tensorflow/python/ops/variables.py line in initialize_variables v.initializer for v in var_list name=name)attributeerror tensor object has no attribute initializer what have you tried see above logs or other output that would be helpful(if logs are large please upload as attachment
163620784,3179,https://api.github.com/repos/tensorflow/tensorflow/issues/3179,wchuanghard,4,0,0,0,0,0,environment infooperating system mac os x steps to reproduce.%matplotlib inline%load_ext autoreload%autoreload import matplotlibimport matplotlib.pyplot as pltimport matplotlib.cm as cmfrom matplotlib import gridspec.import syssys.path.append(..).from tasks import input_dim=output_dim=sess tf.interactivesession()cell ntmcell(input_dim=input_dim output_dim=output_dim)ntm ntm(cell sess forward_only=true)ntm.load(../checkpoint copy)---importerror traceback most recent call last)
163587753,3175,https://api.github.com/repos/tensorflow/tensorflow/issues/3175,cardshuffle,1,0,0,0,0,0,environment infooperating system ubuntu lts bitinstalled version of cuda and cudnn none not using gpu steps to reproducenote tensorflow was installed previously install bazel as instructed here install android studio which includes the sdk install android ndk through the android studio sdk manager download and unzip the tensorflow graph as instructed here uncomment the android entries in the workspace file and add in paths to the sdk and ndk in my case these were home/me/android/sdk and home/me/android-studio/android-studio/plugins/android-ndk run bazel build tensorflow/examples/android:tensorflow_demo what have you tried ive looked around and my understanding is that the release.txt file is not included in the most recent version of the android ndk since the ndk installed via android studio is a jar file i wasnt sure what to do with that so i went to the path indicated by the terminal log and created a blank release.txt file this made no difference according to it can be resolved by downgrading to an earlier version of the ndk which contains release.txt i downloaded the version of bazel for linux from the links given but the downloaded file is a bin which is unusable to me as such i found this solution to be a dead end commenting out the ndk entry is said to resolve the issue but i havent tried this yet since i dont know if itll cause more complications down the road logs or other output that would be helpful error no such package androidndk could not read release.txt in android ndk home/me/.cache/bazel/_bazel_me/fbedebfaacedaa/external/androidndk/ndk/release.txt no such file or directory).error no such package androidndk could not read release.txt in android ndk home/me/.cache/bazel/_bazel_me/fbedebfaacedaa/external/androidndk/ndk/release.txt no such file or directory). is there another way to resolve this issue without downgrading or commenting out the ndk entry if not how can i install a previous version of android ndk thanks in advance
163567973,3172,https://api.github.com/repos/tensorflow/tensorflow/issues/3172,dtracers,0,0,0,0,0,1,it seems that if you only run training step at a time then you can come to a point where it is too fast traceback most recent call last file main.py line in module connection.start_socket callback=handler.message_processor file mnt/d/workspace/sketchrecognitionwithtensorflow/src/main/python/connection/python_socket_server.py line in start_socket process_message(connection callback=callback file mnt/d/workspace/sketchrecognitionwithtensorflow/src/main/python/connection/python_socket_server.py line in process_message result callback(general_proto file mnt/d/workspace/sketchrecognitionwithtensorflow/src/main/python/recognition/proto_handler.py line in message_processor return train_shape(general_proto.template file mnt/d/workspace/sketchrecognitionwithtensorflow/src/main/python/recognition/proto_handler.py line in train_shape rec.add_training_data(recognition_template.interpretation.label recognition_template.shape file mnt/d/workspace/sketchrecognitionwithtensorflow/src/main/python/recognition/recognition_manager.py line in add_training_data self.recognizers label .train(label points file mnt/d/workspace/sketchrecognitionwithtensorflow/src/main/python/recognition/simple/recognizer.py line in train self.classifier.fit(x=reshaped_points y=target steps file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in fit monitors=monitors file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py line in train_model monitors=monitors file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py line in train supervisor.stop(close_summary_writer=false file usr/local/lib/python./dist-packages/tensorflow/python/training/supervisor.py line in stop stop_grace_period_secs=self._stop_grace_secs file usr/local/lib/python./dist-packages/tensorflow/python/training/coordinator.py line in join six.reraise(*self._exc_info_to_raise file usr/local/lib/python./dist-packages/tensorflow/python/training/coordinator.py line in stop_on_exception yield file usr/local/lib/python./dist-packages/tensorflow/python/training/coordinator.py line in run self.run_loop file usr/local/lib/python./dist-packages/tensorflow/python/training/supervisor.py line in run_loop steps_per_sec added_steps elapsed_timezerodivisionerror float division by zero this happens because the time between the start of the step and the end is exactly the same because it is a single step.here is the code snippet current_time time.time elapsed_time current_time self._last_time self._last_time current_time reports the number of steps done per second steps_per_sec added_steps elapsed_time summary summary(value= summary.value(tag=self._summary_tag simple_value=steps_per_sec) ) maybe add an if statement to just say zero if elapsed_time is zero
163554669,3166,https://api.github.com/repos/tensorflow/tensorflow/issues/3166,fayeshine,0,0,0,0,0,1,"the previous classifier.fit(x_train y_train val_monitor has a problem,the learn.tensorflowestimator.fit(x y steps=none monitors=none logdir=none now has monitors as the fourth argument and should be a list i modified it to classifier.fit(x_train y_train monitors= val_monitor but this cause a new problem.is is this a bug from tensorflowestimators or validationmonitor typeerror traceback most recent call last)/home/wenjian/digits.py in module steps learning_rate batch_size classifier.fit(x_train y_train monitors= val_monitor score metrics.accuracy_score(y_test classifier.predict(x_test print(test accuracy f}.format(score))/home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py in fit(self x y steps monitors logdir feed_fn=self._data_feeder.get_feed_dict_fn steps=steps or self.steps monitors=monitors return self home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in train_model(self input_fn steps feed_fn init_op init_feed_fn init_fn device_fn monitors log_every_steps fail_on_nan_loss max_steps fail_on_nan_loss=fail_on_nan_loss monitors=monitors max_steps=max_steps def extract_metric_update_ops(self eval_dict):/home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py in train(graph output_dir train_op loss_op global_step_tensor init_op init_feed_dict init_fn log_every_steps supervisor_is_chief supervisor_master supervisor_save_model_secs keep_checkpoint_max supervisor_save_summaries_steps feed_fn steps fail_on_nan_loss monitors max_steps finally if excinfo reraise(*excinfo return loss_value home/wenjian/anaconda/lib/python./site-packages/six.py in reraise(tp value tb if value.__traceback is not tb raise value.with_traceback(tb raise value else:/home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py in train(graph output_dir train_op loss_op global_step_tensor init_op init_feed_dict init_fn log_every_steps supervisor_is_chief supervisor_master supervisor_save_model_secs keep_checkpoint_max supervisor_save_summaries_steps feed_fn steps fail_on_nan_loss monitors max_steps try outputs should_stop run_with_monitors session last_step train_op loss_op feed_dict monitors except errors.abortederror as e happens when ps restarts keep training./home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py in run_with_monitors(session step tensors feed_dict monitors should_stop false for monitor in monitors induce_stop monitor.step_end(step outputs should_stop should_stop or induce_stop return outputs should_stop/home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/monitors.py in step_end(self step output if self._active_step is not none and self._active_step step self._last_step step to_stop self.every_n_step_end(step output self._active_step none return to_stop/home/wenjian/anaconda/lib/python./site-packages/tensorflow/contrib/learn/python/learn/monitors.py in every_n_step_end(self step outputs outputs self._estimator.evaluate x=self.x y=self.y input_fn=self.input_fn batch_size=self.batch_size steps=self.eval_steps metrics=self.metrics name=self.name stats for name in outputs:typeerror evaluate got an unexpected keyword argument batch_size"
163265954,3130,https://api.github.com/repos/tensorflow/tensorflow/issues/3130,mrry,1,0,0,0,0,1,we no longer perhaps never did expose this function in our public api so it shouldnt be mentioned in our public api docs
163229319,3128,https://api.github.com/repos/tensorflow/tensorflow/issues/3128,KendallWeihe,0,0,0,0,0,4,im running into an issue with the recently added convd_transpose error statement in the title the network builds but upon first training session it outputs the above error runs for a second and then aborted core dumped i have verified all dimensions and data i wrote a simple convolutional-deconvolutional network to reproduce the bug this small program actually outputs a different error than the network i am trying to build but i believe it is pointing to the same issue.the error it is outputting is error in python double free or corruption out): i have tried running the program on two different linux ubuntu machines i had to install tensorflow from the nightly builds in order to include the convd_transpose source code i want to try and run the program on my mac but there isnt a build out yet that includes the function. #this is a small program to reproduce the bug error in python free invalid pointer and error in python double free or corruption out the network is designed for image segmentation input and outputs have the same dimensionsimport tensorflow as tfimport numpy as npimport pdblearning_rate n_depth n_input_x n_input_y n_classes n_examples x tf.placeholder(tf.float n_examples n_depth n_input_x n_input_y )y tf.placeholder(tf.float n_examples n_depth n_input_x n_input_y n_classes name=ground_truth)#generate random datainput_data np.random.rand(n_examples n_depth n_input_x n_input_y)label_data np.random.rand(n_examples n_depth n_input_x n_input_y n_classes)weights l tf.variable(tf.random_normal l tf.variable(tf.random_normal biases l tf.variable(tf.random_normal l tf.variable(tf.random_normal( ))}#build networkdef conv(x w b one convolutional layer followed by one deconvolutional layer x tf.reshape(x shape n_depth n_input_x n_input_y conv tf.nn.convd(x weights l strides padding=same conv tf.nn.bias_add(conv biases l conv tf.nn.relu(conv conv tf.nn.max_poold(conv ksize strides padding=same output_shape n_examples n_depth n_input_x n_input_y n_classes deconv tf.nn.convd_transpose(conv weights l output_shape=output_shape strides padding=valid deconv tf.nn.bias_add(deconv biases l return deconvpred conv(x weights biases)#reshape for classificationtemp_pred tf.reshape(pred n_classes )temp_y tf.reshape(y n_classes )#define loss optimizercost tf.nn.softmax_cross_entropy_with_logits(temp_pred temp_y)optimizer tf.train.adamoptimizer(learning_rate=learning_rate).minimize(cost)init tf.initialize_all_variables()#trainwith tf.session as sess sess.run(init sess.run(optimizer feed_dict={x input_data temp_y label_data
163220017,3127,https://api.github.com/repos/tensorflow/tensorflow/issues/3127,arashno,0,0,0,0,0,3,"hello,i tried to install tensorflow using pip.it has been installed without any error.when i am trying to use it:import tensorflow as tfit gives me the following error but the requested library exists at the lib/.what should i do?!thanks traceback most recent call last file string line in module file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/ init .py line in module from tensorflow.python import file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/ init .py line in module from tensorflow.python import pywrap_tensorflow file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow fp pathname description importerror lib/libc.so version glibc not found required by project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so environment infooperating system:linux mmmlog el.x smp tue apr edt x x x gnu/linux red hatinstalled version of cuda and cudnn rw-rw-r jbaker jbaker may apps/cuda/cuda-./lib/libcudadevrt.a lrwxrwxrwx jbaker jbaker may apps/cuda/cuda-./lib/libcudart.so libcudart.so lrwxrwxrwx jbaker jbaker may apps/cuda/cuda-./lib/libcudart.so libcudart.so rwxrwxr-x jbaker jbaker may apps/cuda/cuda-./lib/libcudart.so rw-rw-r jbaker jbaker may apps/cuda/cuda-./lib/libcudart_static.aif installed from binary pip package provide:i am using conda output from python c import tensorflow print(tensorflow.__version traceback most recent call last file string line in module file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/ init .py line in module from tensorflow.python import file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/ init .py line in module from tensorflow.python import pywrap_tensorflow file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in module pywrap_tensorflow swig_import_helper file project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/pywrap_tensorflow.py line in swig_import_helper mod imp.load_module(_pywrap_tensorflow fp pathname description importerror lib/libc.so version glibc not found required by project/evolvingai/mnorouzz/anaconda/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so"
162952191,3103,https://api.github.com/repos/tensorflow/tensorflow/issues/3103,eamartin,3,0,0,0,0,0,im running tensorflow installed from wheel on python on a k with cuda the following test case attempts to minimize the mean of a vector through gradient descent the script finds that the vectors are equal at all steps but the means are not i believe the vectors being equal at all steps is pure numerical luck since non-deterministic loss likely means non-deterministic gradient which means non-deterministic/reproducible iterative optimization ive observed cases where training results in different final losses where the only source of non-determinism is from reduce_mean pythonimport numpy as npimport tensorflow as tfn_dims n_steps np.random.seed()vec tf.variable(np.random.randn(n_dims).astype(np.float))mean tf.reduce_mean(vec)optimizer tf.train.gradientdescentoptimizer(.)train_step optimizer.minimize(mean)def generate data with tf.session as sess sess.run(tf.initialize_all_variables for in xrange(n_steps vec mean sess.run( vec mean train_step data.append((_vec mean return np.array( f i for f in data for i in xrange() first_vec first_mean generate()second_vec second_mean generate()print vecs equal np.all(first_vec second_vec)print mean equal np.all(first_mean second_mean)print means not equal at idxs np.nonzero(first_mean second_mean) example output: vecs equal truemean equal falsemeans not equal at idxs from looking through the code it appears the gpu mean reduction is implemented with gpu sum reduction confirmed my test case still triggers when i replace reduce_mean with reduce_sum.the gpu sum reduction appears to be implemented using cudas atomicadd floating point adds on gpu are the problem having floating point adds to the same address in an undefined order is inherently non-deterministic due to non-associativity of floating point arithmetic.this issue could be solved and reduction performance improved by using some sort of reduction tree to reduce within blocks and then launching a second kernel or doing some manual block synchronization tricks to reduce across blocks
162922033,3101,https://api.github.com/repos/tensorflow/tensorflow/issues/3101,RobRomijnders,3,0,0,0,0,0,attention models get more popular in many of my experiments with attention i end up using softmax over multiple dimensions tf.nn.softmax supports only one dimension it would be nice to expand the api with another argument softmax_dim for softmax over multiple dimension say over images or ct-scans videos)i;d like to here what you think do more people end up writing softmax code themselves
162808229,3091,https://api.github.com/repos/tensorflow/tensorflow/issues/3091,vrv,0,0,0,1,0,0,and build the binary with that soname which is supposed to beabi-compatible.when we build our pip packages we will now build with preciselythe version of the soname that we intend to support with thebinary packages this is the solution recommended by xx.tested by installing cudnn rc and seeing that the source codemodifications show the use of version the soname versionpointed to by libcudnn.so symlink).in addition installing the pip package shows import tensorflow as tf i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locally i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally
162556459,3066,https://api.github.com/repos/tensorflow/tensorflow/issues/3066,mmuneebs,1,0,0,0,0,0,tensorboard gui is breaking on the latest build after commit chrome only a blue header with buttons appears.on firefox large button icons appear unclickable).the nightly build from about a week ago works fine with the orange header and using same event files.operating system ubuntu x
162332522,3052,https://api.github.com/repos/tensorflow/tensorflow/issues/3052,yaroslavvb,3,0,0,0,0,0,current nightly linux wheel seems to be built with cuda going from to cuda gives me about speed in tensorflow up on x matmul on gtx t ops/sec t ops/sec) export url install upgrade url check that it has gpu support note i see performance gain of on matmul going from to ldd tensorflow/local/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so grep libcudart libcudart.so usr/local/cuda-./lib/libcudart.so xfbec
162293291,3047,https://api.github.com/repos/tensorflow/tensorflow/issues/3047,ilblackdragon,1,0,0,0,0,0,currently running examples leads to a void of empty output.both in console and in jupyter notebooks which for example of linearclassifier leads to a stuck model because steps by default is none
162290630,3046,https://api.github.com/repos/tensorflow/tensorflow/issues/3046,ShuaiW,0,0,0,0,0,1,i am building tensorflow from source and after running below from the root bazel build c opt config=cuda tensorflow/cc:tutorials_example_trainer i get error home/shuaiwang/downloads/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by github.com and referenced by tensorflow/cc:tutorials_example_trainer error home/shuaiwang/downloads/tensorflow/tensorflow/cc/build error loading package tensorflow/core encountered error while reading extension file protobuf.bzl no such package protobuf error cloning repository cannot open git-upload-pack caused by cannot open git-upload-pack caused by github.com and referenced by tensorflow/cc:tutorials_example_trainer error analysis of target tensorflow/cc:tutorials_example_trainer failed build aborted info elapsed time si guess this is because tensorflow doesnt support compute capability for which is the only option for gtx according to this
162284719,3044,https://api.github.com/repos/tensorflow/tensorflow/issues/3044,hnarayanan,2,0,0,0,0,0,the basic version of this tutorial has a nice diagram that makes it really clear what model the code is attempting to reproduce.the deep mnist for experts tutorial however doesnt have such an image especially for the multilayer convolutional network that is the heart of the article i had a hard time and still do a bit visualising what exactly the model code is attempting to do i think the tutorial would benefit from some diagrams.i am willing to contribute this if someone could handhold me a bit as to how the graphic above was generated and what the bar is for contributing to the docs
162202669,3029,https://api.github.com/repos/tensorflow/tensorflow/issues/3029,MadcowD,2,0,0,0,0,0,"hey all i am getting the following error building tensorflow from sources: error home/william/tensorflow/tensorflow/tools/proto_text/build linking of rule tensorflow/tools/proto_text:gen_proto_text_functions failed crosstool_wrapper_driver_is_not_gcc failed error executing command cd home/william/.cache/bazel/_bazel_william/cfbdbacea/execroot/tensorflow exec env third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc o bazel-out/host/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/host/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/tensorflow/tools/proto_text/gen_proto_text_functions.o bazel-out/host/bin/tensorflow/tools/proto_text/libgen_proto_text_functions_lib.a bazel-out/host/bin/tensorflow/core/liblib_internal.a bazel-out/host/bin/external/farmhash_archive/libfarmhash.a bazel-out/host/bin/external/jpeg_archive/libjpeg.a bazel-out/host/bin/external/png_archive/libpng.a bazel-out/host/bin/external/highwayhash/libsip_hash.a bazel-out/host/bin/external/re/libre.a bazel-out/host/bin/tensorflow/core/libprotos_all_cc.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a bazel-out/host/bin/external/zlib_archive/libzlib.a ldl lz pthread lpthread lstdc b/usr/bin pie wl,-z,relro,-z,now no-canonical-prefixes pass-exit-codes wl,--build-id=md wl,--hash-style=gnu wl,-s wl,--gc-sections com.google.devtools.build.lib.shell.badexitstatusexception process exited with status usr/bin/ld bazel-out/host/bin/tensorflow/core/liblib_internal.a(numbers.o undefined reference to symbol ceil@@glibc_..//lib/x_-linux-gnu/libm.so error adding symbols dso missing from command linecollect error ld returned exit statustarget tensorflow/cc:tutorials_example_trainer failed to build this occurs when i run bazel build c opt config=cuda tensorflow/cc:tutorials_example_trainer verbose_failures environment infooperating system ubuntu installed version of cuda and cudnn ls l usr/local/cuda-./lib/libcud*-rw-r--r root root may usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root may usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root may usr/local/cuda-./lib/libcudart.so libcudart.so...-rw-r--r root root may usr/local/cuda-./lib/libcudart.so...-rw-r--r root root may usr/local/cuda-./lib/libcudart_static.a-rwxr-xr-x root root jun usr/local/cuda-./lib/libcudnn.so-rwxr-xr-x root root jun usr/local/cuda-./lib/libcudnn.so.-rwxr-xr-x root root jun usr/local/cuda-./lib/libcudnn.so...-rw-r--r root root jun usr/local/cuda-./lib/libcudnn_static.a if installed from sources provide the commit hash abfeccfccbcb reproduce itfollow the documnentation exactly for building from sources install cuda and cudnn from nvidia install python/build dependencies install bazel from apt-get sudo apt-get install openjdk--jdk sudo apt-get install pkg-config zip g zlibg-dev unzip echo deb stable jdk sudo tee etc/apt/sources.list.d/bazel.list curl sudo apt-key add sudo apt-get update sudo apt-get install bazel then run the first install command using bazel as mentioned before"
162172718,3028,https://api.github.com/repos/tensorflow/tensorflow/issues/3028,amolchanov86,11,0,0,0,0,0,it would be very advantageous to have support tools for iterative pruning in tf as in deep compression paper in many cases it can give up to x speedup and weight reduction practically for free ps thanks for the excellent support of the framework
162001656,3012,https://api.github.com/repos/tensorflow/tensorflow/issues/3012,slundqui,1,0,0,0,0,0,feature request would it be possible to provide a convd_transpose analogous to convd_transpose
161866300,3006,https://api.github.com/repos/tensorflow/tensorflow/issues/3006,jd20,1,0,0,0,0,0,ive been attempting to view events in tensorboard using both the mnist_with_summaries.py tutorial and a much simpler example from the hello tensorflow article example code at the bottom of this page in both cases i can see the graph but nothing under events environment infooperating system osx running python installed from the dmg installer on python.org)installed tensorflow rc via pip steps to reproduce paste the following code into a file pythonimport tensorflow as tfx tf.constant name=input)w tf.variable name=weight)y tf.mul(w x name=output)y tf.constant name=correct_value)loss tf.pow(y y name=loss)train_step tf.train.gradientdescentoptimizer(.).minimize(loss)for value in x w y y loss tf.scalar_summary(value.op.name value)summaries tf.merge_all_summaries()sess tf.session()summary_writer tf.train.summarywriter(log_simple_stats sess.graph)sess.run(tf.initialize_all_variables())for i in range summary_writer.add_summary(sess.run(summaries i sess.run(train_step run the code then after it is complete run tensorboard logdir=log_simple_stats go to localhost under events there are no runs listed what have you tried adding a flush call to the summary writer copying the css and js directories under tensorboard/lib from source this had no effect so i reverted the change logs or other output that would be helpfulthe log output looks like:looking in log_simple_stats reveals bash-rw-r--r jason staff jun events.out.tfevents..hostname.localmd events.out.tfevents..hostname.local feebbacbacbdb and the logs from tensorboard bash tensorboard logdir=log_simple_stats/warning:tensorflow:found more than one graph event per run overwriting the graph with the newest event.warning:tensorflow:ioerror errno no such file or directory library/frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/tensorboard/tag on path library/frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/tensorboard/tagwarning:tensorflow:unable to read tensorboard tagstarting tensorboard on port you can navigate to jun get http jun get lib/css/global.css http jun get external/d/d.min.js http jun get external/lodash/lodash.min.js http
161830739,3004,https://api.github.com/repos/tensorflow/tensorflow/issues/3004,sitefeng,1,0,0,0,0,0,while following the ios readme installation instructions the framework didnt compile for ios below are the details.operating system os x el capitan pip package mac os x cpu only python tensorflow version rc steps bashsh tensorflow/contrib/makefile/download_dependencies.sh bashcd tensorflow/contrib/makefile/downloads/protobuf/./autogen.sh./configuremakesudo make installcd sh tensorflow/contrib/makefile/compile_ios_protobuf.sh sh tensorflow/contrib/makefile/compile_ios_tensorflow.sh log showing error undefined symbols for architecture armv tensorflow::shape_inference::unchangedshape(tensorflow::shape_inference::inferencecontext referenced from cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o cxx_global_var_init in libtensorflow-core-armv.a(math_ops.o ld symbol(s not found for architecture armvclang error linker command failed with exit code use v to see invocation)make users/main/documents/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark error armv compilation failed
161807205,3001,https://api.github.com/repos/tensorflow/tensorflow/issues/3001,cancan101,9,0,0,0,0,0,take advantage of the bnns library that apple announced for ios
161776500,2999,https://api.github.com/repos/tensorflow/tensorflow/issues/2999,MammadTavakoli,2,0,0,0,0,0,"i used tensorflow i want save my model to be reused with that i simply add tf.train.save to save and restore my training variables.this is my code: import tensorflow as tfimport input_dataimport oscheckpoint_dir=./ckpt_dir/mnist input_data.read_data_sets(mnist_data one_hot true)x tf.placeholder(tf.float shape none y tf.placeholder(tf.float none sess tf.interactivesession()def load_model(sess saver checkpoint_dir ckpt tf.train.get_checkpoint_state(checkpoint_dir)if ckpt and ckpt.model_checkpoint_path:print(ckpt.model_checkpoint_path)saver.restore(sess ckpt.model_checkpoint_path)else:if not os.path.exists(checkpoint_dir):os.makedirs(checkpoint_dir)sess.run(init)returndef weight_variable(shape):initial tf.truncated_normal(shape stddev return tf.variable(initial)def bias_variable(shape):initial tf.constant shape shape)return tf.variable(initial)def convd(x w):return tf.nn.convd(x w strides padding same)def max_pool_x(x):return tf.nn.max_pool(x ksize strides padding same)w_conv weight_variable b_conv bias_variable( )x_image tf.reshape(x h_conv tf.nn.relu(convd(x_image w_conv))h_pool max_pool_x(h_conv)#w_conv weight_variable b_conv bias_variable( )h_conv tf.nn.relu(convd(h_pool w_conv))h_pool max_pool_x(h_conv)w_fc weight_variable b_fc bias_variable( )h_pool_flat tf.reshape(h_pool h_fc tf.nn.relu(tf.matmul(h_pool_flat w_fc b_fc)#keep_prob tf.placeholder(tf.float)h_fc_drop tf.nn.dropout(h_fc keep_prob)#w_fc weight_variable b_fc bias_variable( )y_conv tf.nn.softmax(tf.matmul(h_fc_drop,w_fc b_fc)#cross_entropy tf.reduce_mean(-tf.reduce_sum(y tf.log(y_conv reduction_indices train_step tf.train.adamoptimizer(e-).minimize(cross_entropy)correct_prediction tf.equal(tf.argmax(y_conv tf.argmax(y accuracy tf.reduce_mean(tf.cast(correct_prediction tf.float))init tf.initialize_all_variables()saver tf.train.saver()load_model(sess saver checkpoint_dir)for i in range():batch mnist.train.next_batch()if i train_accuracy accuracy.eval(feed_dict x batch y batch keep_prob print(step d training accuracy g%(i train_accuracy))train_step.run(feed_dict x batch y batch keep_prob print(test accuracy g%accuracy.eval(feed_dict={x mnist.test.images y mnist.test.labels keep_prob tf.scalar_summary(accuracy accuracy)saver.save(sess,checkpoint_dir+model.ckpt) when i restore the checkpoint: saver.restore(sess ckpt.model_checkpoint_path) then arises this error: traceback most recent call last):...notfounderror tensor name global_step not found in checkpoint files ckpt_dir/model.ckpt- node save_/restore_slice restoreslicedt=dt_int preferred_shard device=/job:localhost/replica:/task:/cpu: caused by op save_/restore_slice defined at:file home/m/anaconda/lib/python./site-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py line inipythonkernel.start()...file home/m/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in initraise typeerror(control input must be an operation how can i solve this problem"
161581993,2987,https://api.github.com/repos/tensorflow/tensorflow/issues/2987,chasep255,2,0,0,0,0,0,usually the speed of each training batch is somewhat consistent for me when using tensorflow however i am currently training an rnn for the first epochs each batch of samples took about second now on the third epoch my times have become inconsistent the batches will randomly become very slow i can hear the fan on my gtx titian spin down indicating that during these slowdowns it is not using the compute resources on the gpu i am timing it like this so i dont think anything else in my code could be affecting it begin_time time.time loss ts sess.run( cost train_step feed_dict input_tensor x_train expected_output y_train keep_prob end_time time.time() here is the output with timing information. epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time here is what the output normally looks like most of the time when this issue is not occurring. epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time epoch batch loss last loss time after a while it will make it through the slow phase and return to being s per batch i am using ubuntu with cudnn and cuda this problem is intermittent and i have no real way to reproduce it
161502679,2980,https://api.github.com/repos/tensorflow/tensorflow/issues/2980,yaroslavvb,0,1,0,0,0,0,something changed in the last days so that setting cuda_visible_devices to makes tensorflow crash on mac with sigserv exception type exc_bad_access sigsegv)exception codes kern_invalid_address at xexception note exc_corpse_notifyvm regions near text d-da k r-x/rwx sm=cow system/library/frameworks/python.framework/versions/./resources/python.app/contents/macos/pythonthread crashed dispatch queue com.apple.main-thread libsystem_c.dylib xfffebc strlen pywrap_tensorflow.so xceac perftools::gputools::cuda::diagnostician::findkerneldriverversion pywrap_tensorflow.so xce perftools::gputools::cuda::diagnostician::logdriverversioninformation pywrap_tensorflow.so xceff perftools::gputools::cuda::diagnostician::logdiagnosticinformation pywrap_tensorflow.so xcfd perftools::gputools::cuda::cudadriver::init pywrap_tensorflow.so xcaef perftools::gputools::cuda::cudaplatform::visibledevicecount const pywrap_tensorflow.so xcdf tensorflow::gpumachinemanager pywrap_tensorflow.so xcace tensorflow::basegpudevicefactory::getvaliddeviceids(std::__::vector
161438100,2974,https://api.github.com/repos/tensorflow/tensorflow/issues/2974,naughtyfox,0,0,0,0,2,0,hi i link my library against tensorflow library when loading i get the following error f tensorflow/core/framework/op.cc check failed tensorflow::status::ok registeralreadylocked(deferred_ i ok vs invalid argument could not parse default value from attr(distortion float for op fixedunigramcandidatesampler)i tried to figure out what causes this problem and found that tensorflow cant parse op with float attrs for example have a look at core/ops/candidate_sampling_ops.cc attr(distortion float if i rewrite it as follows attr(distortion float it works fine until it finds another float attribute i tried to trace down this problem and stuck here tensorflow/core/lib/strings/numbers.cc safe_strtof(const char str float value char endptr value strtof(str endptr while isspace(*endptr endptr ignore range errors from strtod/strtof the values it returns on underflow and overflow are the right fallback in a robust setting return str endptr this function returns false for str because endptr points to please fix this problem or suggest me the to fix it and ill contribute or tell me how i can work around this thank you.operating system uname alinux user-desktop generic ubuntu smp wed jun utc x x x gnu/linux cat etc/issueubuntu lts n l g versiong ubuntu ubuntu copyright c free software foundation inc.this is free software see the source for copying conditions there is nowarranty not even for merchantability or fitness for a particular purpose.commit hash bfecefcdccbcaada upd solutioni found out the reason of this strange behavior if i run my program as follows lc_numeric=c my_program_with_tensorflow than it works properly my previous lc_numeric was ru_ru.utf where decimal separator is i find it weird when correctness of program depends on current user locale you always use the same decimal separator in op descriptions fix it please
161250919,2959,https://api.github.com/repos/tensorflow/tensorflow/issues/2959,prb12,3,0,0,0,0,0,using the gpu tracer e.g via session.run with runoptions full_trace results in warnings like the following: w tensorflow/core/common_runtime/gpu/gpu_tracer.cc unhandled api callback for w tensorflow/core/common_runtime/gpu/gpu_tracer.cc unhandled api callback for these are believed to be harmless but are obviously annoying a fix is in progress
161233481,2957,https://api.github.com/repos/tensorflow/tensorflow/issues/2957,mrry,2,0,0,0,0,0,from a question on stack overflow the following code fails pythonimport tensorflow as tfimport timewith tf.graph().as_default filename_list data_batch_{}.mat.format(i for i in range filename_queue tf.train.string_input_producer(filename_list with tf.session as sess coord tf.train.coordinator threads tf.train.start_queue_runners(sess=sess coord=coord time.sleep if i uncomment this it works for i in range print(sess.run(filename_queue.dequeue coord.request_stop coord.join(threads) ...with the following error: notfounderror fetchoutputs node input_producer_dequeue not found it turns out that my fix for was incomplete and there is still a race between concurrent graph modification and session.run calls im preparing a fix
161009527,2943,https://api.github.com/repos/tensorflow/tensorflow/issues/2943,kyrs,1,0,0,0,0,0,"hi,for my project i need to extract the features from the mixed layer in inception-v but for doing so i need the name of various tensor layer.as far as i know pool correspond to xx feature matrix pool correspond to feature matrix conv correspond to xx feature matrix conv correspond to xx feature matrix conv correspond to xx feature matrix conv correspond to xx feature matrix i am interested in final mixed layer xx mixed as mentioned in model.txt can you please tell me how to extract this layer"
160894230,2929,https://api.github.com/repos/tensorflow/tensorflow/issues/2929,morgangiraud,1,0,0,0,0,0,"hi everyone,we are trying to load a very simple graph inside ios that we generate and freeze with python.right now we have this following error: e running model failed invalid argument no opkernel was registered to support op convdbackpropinput with these attrs node convt convdbackpropinput t=dt_float data_format=nhwc padding=same strides use_cudnn_on_gpu=true (shape w/read x) here is the python script generating the model pythonfrom tools import freeze_graphimport tensorflow as tfprint(init session)sess tf.interactivesession()print(define vars and ops)x tf.placeholder(tf.float shape= none name=x)w tf.variable(tf.truncated_normal stddev name=w)convt tf.nn.convd_transpose(x w tf.shape(x strides padding=same name=convt)print(init vars)init tf.initialize_all_variables()sess.run(init)input_graph_name input_graph.pbsaver tf.train.saver(var_list=none)saver.save(sess data/conv global_step=none)graph_def sess.graph.as_graph_def()tf.train.write_graph(graph_def data input_graph_name)input_graph_path data input_graph_nameinput_saver_def_path input_binary falseinput_checkpoint_path data/convoutput_node_names convtrestore_op_name save/restore_allfilename_tensor_name save/const:output_graph_path data/frozen_convt.pbclear_devices truefreeze_graph.freeze_graph(input_graph_path input_saver_def_path input_binary input_checkpoint_path output_node_names restore_op_name filename_tensor_name output_graph_path clear_devices here is the objective-c interesting part c load the network nsstring network_path filepathforresourcename(@frozen_conv pb portablereadfiletoproto( network_path utfstring tensorflow_graph log(info creating session tensorflow::status s session->create(tensorflow_graph if s.ok log(error could not create tensorflow graph s run the network std::string input_layer x std::string output_layer convt std::vectorrun({{input_layer image_tensor output_layer outputs if run_status.ok log(error running model failed run_status we are using python for the script the python tensorflow build is downloaded from the compiled tensorflow for ios is from the master branch hour ago using the script build_all_ios.sh we have no problem running the ios example we have no problem running a one standard convolutional graph frozen with custom python script we checked that the file tensorflow/core/ops/nn_ops.cc is in the generated txt files and this is the file containing the registering of the op convdbackpropinput)i would be gratefull if anyone has an idea on why ios seems to not be able to find the convdbackpropinput op"
160840916,2924,https://api.github.com/repos/tensorflow/tensorflow/issues/2924,jinfengfeng,15,0,0,4,0,0,i encountered several problems in installing tensorflow on redhat system i solved them by googling solutions hope my experience may be helpful to some users.os version red hat enterprise linux server release santiago)linux kernel el.x_problems and solutions glibc version is too low install higer version glibc e.g glibc-..please use at least version you can configure the installation using prefix option to specify the path to install and then modify ld_library_path to include the new glibc library link problem after install higher version of glibc error info looks like:error while loading shared libraries vdso_time invalid mode for dlopen():invalid argumentyou need to use the following method to run a program eg python):/path/to/glibc-./lib/ld-linux-x-.so library-path path/to/glibc-./lib:$ld_library_path:/path/to/gcc-../lib:/usr/lib path/to/anaconda/bin/python numpy version problem install new version of numpy using pip protobuf not found and error in python after import tensorflow typeerror init got an unexpected keyword argument syntaxpip install protobuf>=..a failed call to cuinit cuda_error_no_device explicitly specify a cuda device in you environment variable:export cuda_visible_devices libcuda.so not found explicitly specify the path of this lib in your ld_library_path environment variable libcuda.so is in usually in usr/lib elfclass error elf is incorrect should use a bits so file usually in lib glibc cannot create regular file var/db/makefile permission denied use glibc or even higher version
160780586,2916,https://api.github.com/repos/tensorflow/tensorflow/issues/2916,anfeng,13,0,0,0,0,0,tensorflow paper states that tensorflow supports multiple communication protocols such as grpc over tcp and rdma over converged ethernet current repo on the other hand only has grpc implementations do you have any plan to introduce rdma support it should be beneficial especially for gpu-to-gpu communications across servers
160721044,2910,https://api.github.com/repos/tensorflow/tensorflow/issues/2910,yaroslavvb,7,0,1,0,0,0,commit broke existing gpu builds on macos with cuda it nows fails i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc failed to find libcuda.so on this system failed precondition could not dlopen dso libcuda..dylib dlerror dlopen(libcuda..dylib image not found a work-around is to create a symlink cd usr/local/cuda/lib/ln s libcuda.dylib libcuda..dylib
160635527,2902,https://api.github.com/repos/tensorflow/tensorflow/issues/2902,mhex,0,0,0,0,0,1,build_pip_package fails here with error path/to/src/tensorflow/tensorflow/contrib/session_bundle/example/build executing genrule tensorflow/contrib/session_bundle/example:half_plus_two failed bash failed error executing command caused by error while loading shared libraries libpython.m.so cannot open shared object file no such file or directory executing (cd path/to/execroot/tensorflow exec env path=somepath bin/bash c source external/bazel_tools/tools/genrule/genrule-setup.sh rm rf tmp/half_plus_two path_to/python bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two it looks like exec env unsets ld_library_pathand therefore python cannot find its libraries anymoreinstallation of tensorflow works
160615180,2901,https://api.github.com/repos/tensorflow/tensorflow/issues/2901,MammadTavakoli,3,0,0,0,0,0,i used tf when i restore the checkpoint i get the error:traceback most recent call last file ipython-input--fb line in module runfile(/media/m/e/deep/proj//t.py wdir=/media/m/e/deep/proj file home/m/anaconda/lib/python./site-packages/spyderlib/widgets/externalshell/sitecustomize.py line in runfile execfile(filename namespace file home/m/anaconda/lib/python./site-packages/spyderlib/widgets/externalshell/sitecustomize.py line in execfile exec(compile(f.read filename exec namespace file media/m/e/deep/proj//t.py line in module saver.restore(sess ckpt.model_checkpoint_path restore all variables file home/m/anaconda/lib/python./site-packages/tensorflow/python/training/saver.py line in restore self.saver_def.filename_tensor_name save_path file home/m/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in run the optional options argument expects a runoptions proto the options file home/m/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in run elif isinstance(fetches dict file home/m/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_run for fetch result in zip(unique_fetches results file home/m/anaconda/lib/python./site-packages/tensorflow/python/client/session.py line in do_call runs a step based on the given fetches and feeds.notfounderror tensor name global_step not found in checkpoint files ckpt_dir/model.ckpt node save_/restore_slice restoreslice dt=dt_int preferred_shard device=/job:localhost/replica:/task:/cpu: (_recv_save_/const save_/restore_slice_/tensor_name save_/restore_slice_/shape_and_slice) caused by op save_/restore_slice defined at file home/m/anaconda/lib/python./site-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py line in module ipythonkernel .start file home/m/anaconda/lib/python./site-packages/ipykernel/kernelapp.py line in start ioloop.ioloop.instance().start file home/m/anaconda/lib/python./site-packages/zmq/eventloop/ioloop.py line in start super(zmqioloop self).start file home/m/anaconda/lib/python./site-packages/tornado/ioloop.py line in start handler_func(fd_obj events file home/m/anaconda/lib/python./site-packages/tornado/stack_context.py line in null_wrapper return fn(_args kwargs file home/m/anaconda/lib/python./site-packages/zmq/eventloop/zmqstream.py line in handle_events self._handle_recv file home/m/anaconda/lib/python./site-packages/zmq/eventloop/zmqstream.py line in handle_recv self._run_callback(callback msg file home/m/anaconda/lib/python./site-packages/zmq/eventloop/zmqstream.py line in run_callback callback(_args kwargs file home/m/anaconda/lib/python./site-packages/tornado/stack_context.py line in null_wrapper return fn(_args kwargs file home/m/anaconda/lib/python./site-packages/ipykernel/kernelbase.py line in dispatcher return self.dispatch_shell(stream msg file home/m/anaconda/lib/python./site-packages/ipykernel/kernelbase.py line in dispatch_shell handler(stream idents msg file home/m/anaconda/lib/python./site-packages/ipykernel/kernelbase.py line in execute_request user_expressions allow_stdin file home/m/anaconda/lib/python./site-packages/ipykernel/ipkernel.py line in do_execute shell.run_cell(code store_history=store_history silent=silent file home/m/anaconda/lib/python./site-packages/ipython/core/interactiveshell.py line in run_cell interactivity=interactivity compiler=compiler result=result file home/m/anaconda/lib/python./site-packages/ipython/core/interactiveshell.py line in run_ast_nodes if self.run_code(code result file home/m/anaconda/lib/python./site-packages/ipython/core/interactiveshell.py line in run_code exec(code_obj self.user_global_ns self.user_ns file ipython-input--fb line in module runfile(/media/m/e/deep/proj//t.py wdir=/media/m/e/deep/proj file home/m/anaconda/lib/python./site-packages/spyderlib/widgets/externalshell/sitecustomize.py line in runfile execfile(filename namespace file home/m/anaconda/lib/python./site-packages/spyderlib/widgets/externalshell/sitecustomize.py line in execfile exec(compile(f.read filename exec namespace file media/m/e/deep/proj//t.py line in module saver tf.train.saver file home/m/anaconda/lib/python./site-packages/tensorflow/python/training/saver.py line in init restore_sequentially=restore_sequentially file home/m/anaconda/lib/python./site-packages/tensorflow/python/training/saver.py line in build filename_tensor vars_to_save restore_sequentially reshape file home/m/anaconda/lib/python./site-packages/tensorflow/python/training/saver.py line in addrestoreops values self.restore_op(filename_tensor vs preferred_shard file home/m/anaconda/lib/python./site-packages/tensorflow/python/training/saver.py line in restore_op preferred_shard=preferred_shard file home/m/anaconda/lib/python./site-packages/tensorflow/python/ops/io_ops.py line in restore_slice file_pattern tensor_name shape_and_slice base_type file home/m/anaconda/lib/python./site-packages/tensorflow/python/ops/gen_io_ops.py line in restore_slice def restore_slice(file_pattern tensor_name shape_and_slice dt file home/m/anaconda/lib/python./site-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file home/m/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op specified for this op_type file home/m/anaconda/lib/python./site-packages/tensorflow/python/framework/ops.py line in init raise typeerror(control input must be an operation how can i solve this problem
160420647,2882,https://api.github.com/repos/tensorflow/tensorflow/issues/2882,yyyreal,4,0,0,0,0,2,i have a ubuntu with titan x cuda and cudnn vcuda was installed by a run file just like i used to installed caffe.environment variables have been added to etc/profile and bashrc a file named cuda.conf have been added to etc/ld.so.conf.d i installed tensorflow via pip using the following command sudo pip install upgrade followed the install instruction carefully but i still met some issues import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally sess tf.session()e tensorflow/stream_executor/cuda/cuda_driver.cc failed call to cuinit cuda_error_unknowni tensorflow/stream_executor/cuda/cuda_diagnostics.cc retrieving cuda diagnostic information for host deeplearningi tensorflow/stream_executor/cuda/cuda_diagnostics.cc hostname deeplearningi tensorflow/stream_executor/cuda/cuda_diagnostics.cc libcuda reported version is i tensorflow/stream_executor/cuda/cuda_diagnostics.cc driver version file contents nvrm version nvidia unix x kernel module wed jan pst gcc version gcc version ubuntu ubuntu i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel reported version is i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel version seems to match dso i tensorflow/core/common_runtime/gpu/gpu_init.cc no gpu devices available on machine. it seems that tensorflow cannot find my gpu.i have no idea why these issues occur.please help thanks
160264751,2863,https://api.github.com/repos/tensorflow/tensorflow/issues/2863,gideonite,1,0,0,0,0,0,there was a bug which confused residual blocks with residual groups which is just a list of blocks the output of each block should be of the same dimension as the input so that a residual connection can be made upscaling only occurs between groups changed some of the variable names to clarify the distinction between groups blocks and layers
160246577,2858,https://api.github.com/repos/tensorflow/tensorflow/issues/2858,zhiqiangwan,1,0,0,0,0,0,on my ubuntu i have installed tensorflow from source as specified in the tensorflow installation instructions and it works well.then i install anaconda but it changes the path environment variable so i cannot import tensorflow.following the instruction in i created an environment and installed tensorflow in it as specified in i use this command sudo pip install upgrade tf_binary_url to install tensorflow but the tensorflow is not installed in this environment it seems that it just updates the tensorflow i have installed previously the terminal window is shown in the image:i have solved this problem use pip install upgrade tf_binary_url instead using sudo will install the package globally terminal
160023561,2838,https://api.github.com/repos/tensorflow/tensorflow/issues/2838,danijar,9,0,0,0,0,0,for rnn cells we get the initial state using cell.zero_state and the last state after processing a sequence using rnn.dynamic_rnn however to use the last state as the initial state for the next run one must create a tf.placeholder as far as i know currently there is no way to create and fill such a placeholder or nested tuple of placeholders automatically such a feature would be very useful so that we dont have to adjust the placeholder manually when changing the rnn cell
159953908,2830,https://api.github.com/repos/tensorflow/tensorflow/issues/2830,rushatrai,1,0,0,0,0,0,i ran bazel test syntaxnet util/utf and it gave a few errors it gave me this output: fail syntaxnet:parser_trainer_test see home/me/.cache/bazel/_bazel_rushat/ccdfbeadfdf/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log).info elapsed time s critical path s//syntaxnet:arc_standard_transitions_test passed in s//syntaxnet:beam_reader_ops_test passed in s//syntaxnet:graph_builder_test passed in s//syntaxnet:lexicon_builder_test passed in s//syntaxnet:parser_features_test passed in s//syntaxnet:reader_ops_test passed in s//syntaxnet:sentence_features_test passed in s//syntaxnet:shared_store_test passed in s//syntaxnet:tagger_transitions_test passed in s//syntaxnet:text_formats_test passed in s//util/utf:unicodetext_unittest passed in s//syntaxnet:parser_trainer_test failed in s home/me/.cache/bazel/_bazel_me/ccdfbeadfdf/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.logexecuted out of tests tests pass and fails locally.there were tests whose specified size is too big use the test_verbose_timeout_warnings command line option to see which ones these are. if you want the output of test_verbose_timeout_warnings then please aski have no idea what these mean...please help me pray:ps if this is the wrong place to ask then please direct me where to post this(but kindly suggest an answer
159883875,2826,https://api.github.com/repos/tensorflow/tensorflow/issues/2826,RobRomijnders,4,0,0,0,0,0,"feature request:mixture density networks attention models and variational inference getting more popular we are defining more and more gaussians in our model my feature request would be to have a native gaussian implementation in tensorflow.up to now i re-use this code: def tf_d_normal(x x mu mu s s rho eq and of norm tf.sub(x mu norm tf.sub(x mu ss tf.mul(s s z tf.square(tf.div(norm s))+tf.square(tf.div(norm s))-*tf.div(tf.mul(rho tf.mul(norm norm ss negrho tf.square(rho result tf.exp(tf.div(-z,*negrho denom np.pi*tf.mul(ss tf.sqrt(negrho result tf.div(result denom return result however it might be nice for tensorflow to implement it in the library they might be able to apply it more computationally efficient also they might go for n-dimensional gaussians not only two.what do you think is this feasible"
159882452,2825,https://api.github.com/repos/tensorflow/tensorflow/issues/2825,sdemyanov,5,0,1,0,0,0,currently hidden moving averages are initialized by if the decay is large it takes a lot of time for the moving average variable to forget about initial zero value it would be much more reasonable to initialize by the first obtained value of the observed variable and then start to apply decay in this case the initial initialization is not required anymore so the variable can be removed from the assert_variables_initialized list
159782222,2807,https://api.github.com/repos/tensorflow/tensorflow/issues/2807,changyun79,12,0,0,0,0,0,on ubuntu with cuda cudnn tensorflow-..rc ran tensorflow/examples/label_image application by taking inception-v graph and roughly measure the elapsed time then take tensorflow/contrib/quantization/tools:quantize_graph to quant inception-v rebuilt application by giving tensorflow/contrib/quantization:cc_ops tensorflow/contrib/quantization/kernels:quantized_ops into tensorflow/examples/label_image/build and redo the same classification and measure the time before/after quantization elapsed time were seconds vs seconds i.e quantization doubled the inference time the results looks ok as below so i think i was running it correctly before military uniform suit academic gown bow tie bolo tie after military uniform suit bow tie bolo tie academic gown my tensor flow was built as cpu only have also tried to enable gpu while the timing didnt change do we know what the expected performance would be
159780825,2806,https://api.github.com/repos/tensorflow/tensorflow/issues/2806,akors,6,0,0,0,0,0,hi i am using fedora to build tensorflow with gpu support.gpu support requires a specific gcc version for successful compilation since this version is not available from the fedora package repositories it has to be compiled from source unfortunately there are a few obstacles using this version with bazel.one of those is the following:after configuring tensorflow with the self-compiled gcc and running bazel the compilation stops with the following message: gcc error trying to exec as execvp no such file or directory or gcc error trying to exec nm execvp no such file or directory or gcc error trying to exec ld execvp no such file or directory to work around this one can compile gcc by hardcoding the paths to those tools by adding this to the configuration line of gcc: --with-ld=/bin/ld with-nm=/bin/nm with-as=/usr/bin/as this does seem rather strange however because those programs are on the path and the custom gcc without bazel is capable of producing working binaries just fine i feel that in a well-working build system this kind of hacks should not be necessary.this is probably a bazel bug but since i am compiling tensorflow with it and i cannot be sure that its not a tf issue i am creating this here.please ping bazel devs as required
159770584,2803,https://api.github.com/repos/tensorflow/tensorflow/issues/2803,3rd3,3,0,0,0,0,0,i am wondering whether it is possible to feed say sequences of length while letting the gradients only flow back over a maximum of time steps this seem to me a pretty common thing to do in sequential learning tasks so i am wondering whether there is support for that in dynamic_rnn and if not whether it is planned
159752061,2797,https://api.github.com/repos/tensorflow/tensorflow/issues/2797,guohengkai,1,0,0,0,0,0,when i try to use batch_normalize as bn batch_normalize(conv convnet=true the error is as follows file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/ops/batch_norm_ops.py line in batch_normalize lambda ema_mean ema_var file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in cond p p switch(pred pred file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in switch return gen_control_flow_ops._switch(data pred name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_control_flow_ops.py line in switch result op_def_lib.apply_op(switch data=data pred=pred name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op prefix dtypes.as_dtype(input_arg.type).name typeerror input pred of switch op has type float that does not match expected type of bool.operating system ubuntu bitspip package output from python c import tensorflow print(tensorflow.__version rc
159730846,2793,https://api.github.com/repos/tensorflow/tensorflow/issues/2793,asheshjain399,4,0,0,0,0,0,new feature request:while training lstms is it often useful to clip the derivates w.r.t the inputs into the lstm at each time step alex graves sec this is different from clipping the overall gradient theano supports this feature with theano.gradient.grad_clip(tensor_var can we have a similar feature in tensorflow
159621507,2781,https://api.github.com/repos/tensorflow/tensorflow/issues/2781,connormclaud,1,0,0,0,0,0,environment infooperating system uname a linux vvinahradski generic ubuntu smp mon may utc x x x gnu/linux ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):not installedif installed from binary pip package provide which pip package you installed the output from python c import tensorflow print(tensorflow.__version rc steps to reproduce copy/past tutorial run code observe that accuracy is around instead of expected what have you tried increased test dataset x times accuracy is around logs or other output that would be helpful(if logs are large please upload as attachment).source code: tut.zip output: log.txt
159166603,2730,https://api.github.com/repos/tensorflow/tensorflow/issues/2730,kuza55,3,0,0,0,0,0,i have been trying to retrain the inception v network and tensorboard has been the only real way to introspect the network and its implementation so far.it would be very useful to be able to find nodes in the graph by name or other properties e.g in my case i am interested in seeing the total_loss node that is being defined here to get a greater idea of what it is linked to
159094408,2723,https://api.github.com/repos/tensorflow/tensorflow/issues/2723,MarvinTeichmann,0,0,0,0,0,1,the function tf.image.per_image_whitening can now be applied to a whole batch mean and adjusted_stddev is then computed over all batch dimensions.i find it very useful to be able to ably tf.image.whitening to an entire batch in some situations a batch is given due to previous computation in the graph splitting the batch into individual images in order to be able to apply whitening seams to be a bit redundant applying whitening to a batch is statistically superior and more elegant when used in combination with batch normalization).it might be considered to rename the function in per_batch_whitening as this would now be more accurate
159029016,2716,https://api.github.com/repos/tensorflow/tensorflow/issues/2716,StephenOman,3,0,0,0,0,0,the tensorflow ios libraries created by the makefile have a very large footprint mb to help reduce the size of apps there should also be a lite version of the libraries possibly based on the bazel android_tensorflow_lib_lite target.note that apple prevents apps larger than mb from being downloaded over the air
159001875,2714,https://api.github.com/repos/tensorflow/tensorflow/issues/2714,zhengwy888,3,0,0,0,0,0,"hello,i am trying to add tf.historgram_summary in my lstm cells in order to visualize the output but currentlyneither rnn or dynamic_rnn supports writing out summaries for rnn the log indicates the summary tensor are invalid which seems to due to the dynamic unrolling of the rnn loop. traceback most recent call last file test_dynamic_rnn.py line in module summarys sess.run(thing feed_dict=feeds file home/ezheng/tf_head/local/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/ezheng/tf_head/local/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file home/ezheng/tf_head/local/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file home/ezheng/tf_head/local/lib/python./site-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors.invalidargumenterror the tensor returned for dynamic_scope/rnn/cond_/basiclstmcell/histogramsummary was not valid. i am currently constructing rnn like this python outputs state tf.nn.rnn cell inputs dtype=tf.float sequence_length=sequence_length) for dynamic_rnn it just hangs when i am trying to obtain summaries with the session.run()and this is how i am constructing dynamic_rnn python outputs state tf.nn.dynamic_rnn cell inputs=concat_inputs time_major=true dtype=tf.float) wondering if there is plan to support histogram summary with either of these two rnns thanks"
158976882,2711,https://api.github.com/repos/tensorflow/tensorflow/issues/2711,ibab,1,0,0,0,0,0,this implements tf.scan_sum cumulative sum and tf.scan_prod cumulative product ops.unfortunately this wont compile with the current version of eigen as i have to make some small changes to tensorscanop.h but this should be useful to discuss how the api should look and whether we want to have inclusive/exclusive ops and so on.ill also add tests and gradients to the two ops.this relates to
158623282,2682,https://api.github.com/repos/tensorflow/tensorflow/issues/2682,rajarsheem,12,0,0,0,0,0,hessian free optimizers are successfully applied on neural networks especially rnns currently need one in tf
158525431,2661,https://api.github.com/repos/tensorflow/tensorflow/issues/2661,KnHuq,0,0,0,0,1,0,"import tensorflow as tfa=tf.constant( ,, , ,, , ,, , ,, , ,, , ,, , ,, , ,, )sess=tf.interactivesession()print sess.run(tf.map_fn(lambda x tf.scan(lambda y,z:tf.add(y,z),x),a))error:alreadyexistserror resource tensor_arrays/map_/while/scan/tensorarray/ntensorflowtensorarraye node map_/while/scan/tensorarray tensorarray clear_after_read=true dtype=dt_int dynamic_size=false tensor_array_name device=/job:localhost/replica:/task:/cpu: (map_/while/scan/squeeze cloopmap_/while/scan/tensorarraywrite/index node map_/while/scan/while/identity hostsend t=dt_int client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__map_/while/scan/while/identity device=/job:localhost/replica:/task:/gpu: (map_/while/scan/while/identity) caused by op umap_/while/scan/tensorarray defined at file usr/lib/python./runpy.py line in run_module_as_main main fname loader pkg_name file usr/lib/python./runpy.py line in run_code exec code in run_globals file usr/local/lib/python./dist-packages/ipykernel/__main__.py line in module app.launch_new_instance file usr/local/lib/python./dist-packages/traitlets/config/application.py line in launch_instance app.start file usr/local/lib/python./dist-packages/ipykernel/kernelapp.py line in start ioloop.ioloop.instance().start file usr/local/lib/python./dist-packages/zmq/eventloop/ioloop.py line in start super(zmqioloop self).start file usr/local/lib/python./dist-packages/tornado/ioloop.py line in start handler_func(fd_obj events file usr/local/lib/python./dist-packages/tornado/stack_context.py line in null_wrapper return fn(_args kwargs file usr/local/lib/python./dist-packages/zmq/eventloop/zmqstream.py line in handle_events self._handle_recv file usr/local/lib/python./dist-packages/zmq/eventloop/zmqstream.py line in handle_recv self._run_callback(callback msg file usr/local/lib/python./dist-packages/zmq/eventloop/zmqstream.py line in run_callback callback(_args kwargs file usr/local/lib/python./dist-packages/tornado/stack_context.py line in null_wrapper return fn(_args kwargs file usr/local/lib/python./dist-packages/ipykernel/kernelbase.py line in dispatcher return self.dispatch_shell(stream msg file usr/local/lib/python./dist-packages/ipykernel/kernelbase.py line in dispatch_shell handler(stream idents msg file usr/local/lib/python./dist-packages/ipykernel/kernelbase.py line in execute_request user_expressions allow_stdin file usr/local/lib/python./dist-packages/ipykernel/ipkernel.py line in do_execute shell.run_cell(code store_history=store_history silent=silent file usr/local/lib/python./dist-packages/ipython/core/interactiveshell.py line in run_cell interactivity=interactivity compiler=compiler result=result file usr/local/lib/python./dist-packages/ipython/core/interactiveshell.py line in run_ast_nodes if self.run_code(code result file usr/local/lib/python./dist-packages/ipython/core/interactiveshell.py line in run_code exec(code_obj self.user_global_ns self.user_ns file ipython-input--aecbe line in module print sess.run(tf.map_fn(lambda x tf.scan(lambda y,z:tf.add(y,z),x),a file usr/local/lib/python./dist-packages/tensorflow/python/ops/functional_ops.py line in map_fn swap_memory=swap_memory file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in while_loop result context.buildloop(cond body loop_vars file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in buildloop body_result body(*vars_for_body_with_tensor_arrays file usr/local/lib/python./dist-packages/tensorflow/python/ops/functional_ops.py line in compute ta ta.write(i fn(elems_ta.read(i file ipython-input--aecbe line in lambda print sess.run(tf.map_fn(lambda x tf.scan(lambda y,z:tf.add(y,z),x),a file usr/local/lib/python./dist-packages/tensorflow/python/ops/functional_ops.py line in scan infer_shape=true file usr/local/lib/python./dist-packages/tensorflow/python/ops/tensor_array_ops.py line in init tensor_array_name=tensor_array_name name=scope file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_data_flow_ops.py line in tensor_array tensor_array_name=tensor_array_name name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack"
158512033,2655,https://api.github.com/repos/tensorflow/tensorflow/issues/2655,j9ac9k,21,0,0,0,0,0,the distributed tensorflow documentation said to open an issue to request support for a specific cluster manager support for yarn would be beneficial thanks
158487397,2652,https://api.github.com/repos/tensorflow/tensorflow/issues/2652,ghost,3,0,0,0,0,0,"pythonimport tensorflow as tfdef run(on_gpu tf.reset_default_graph tf.set_random_seed with tf.device(/gpu if on_gpu else cpu a tf.random_normal b tf.get_variable(b shape initializer tf.constant_initializer(value c a*b grad tf.gradients(c b gate_gradients=tf.train.optimizer.gate_graph sess tf.session sess.run(tf.initialize_all_variables grad_val sess.run(grad return grad_valfor i in xrange print repr(run(on_gpu=true)),print for i in xrange print repr(run(on_gpu=false)), result as you can see consistent result across cpu runs but inconsistent result across gpu runs.no doubt a cuda reduction order issue but itd be really nice if we can have deterministic reduction i am using tf self-compiled against cudnn v cudnn version is not rc"
158230114,2626,https://api.github.com/repos/tensorflow/tensorflow/issues/2626,wookayin,1,0,0,0,0,0,environment infooperating system linux ubuntu lts bit)installed version of cuda and cudnn cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): ls l usr/local/cuda-./lib/libcud*-rw----r root root usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root usr/local/cuda-./lib/libcudart.so libcudart.so...-rwx---r-x root root usr/local/cuda-./lib/libcudart.so...-rw----r root root usr/local/cuda-./lib/libcudart_static.a ls l usr/local/cuda-./lib/libcud*-rw----r root root usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root usr/local/cuda-./lib/libcudart.so libcudart.so...-rwx---r-x root root usr/local/cuda-./lib/libcudart.so...-rw----r root root usr/local/cuda-./lib/libcudart_static.alrwxrwxrwx root root usr/local/cuda-./lib/libcudnn.so libcudnn.so.lrwxrwxrwx root root usr/local/cuda-./lib/libcudnn.so libcudnn.so...-rwxr-xr-x root root usr/local/cuda-./lib/libcudnn.so... if installed from binary pip package provide which pip package you installed tensorflow nightly python linux gpu e.g build the output from python c import tensorflow print(tensorflow. version if installed from sources provide the commit hash steps to reproducealthough it is experimental i am using the gpu profiling functionality with cupti run any tensorflow code that uses cupti or tf.runoptions.full_trace the following error segfault occurs. i tensorflow/stream_executor/dso_loader.cc couldnt open cuda library libcupti.so ld_library_path:f tensorflow/core/common_runtime/gpu/cupti_wrapper.cc check failed f nullptr could not find cuptiactivityregistercallbacksin libcupti dso dlerror home/wookayin/.virtualenvs/tfdebug/local/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so undefined symbol cuptiactivityregistercallbacks abort core dumped python mnist-profiling.py any tf code that invokes loading of libcupti.so will face the same error but for convenience i will share a code that can run standalone mnist-profiling.py what have you tried?the problem is that the shared library libcupti.so cannot be loaded however in some older nightly version such as build it worked.upd i binary-searched to find the changeset to be blamed build works but build failed and build does not work.i highly suspect that this regression is since commit bdc but not sure the path to libcupti.so would be usr/local/cuda/extras/cupti/lib/libcupti.so after this commit it seems that path to libcupti.so goes wrong but why?)a strange thing to me is that tensorflow already has a unit test for cupti and gpu tracing functionalities so ci must have run this test as well this bug might be happening in some environments only like nightly build i installed via pip or it can be a a bazel-related problem when generating packages).i have not investigated into this problem in detail it looks that after some troubleshooting i can figure out what the cause is.thanks logs or other output that would be helpfuln/a
158211176,2625,https://api.github.com/repos/tensorflow/tensorflow/issues/2625,ibab,2,0,0,0,0,0,it would be nice to be able to use numerically stable summation methods like pairwise summation kahan summationfor applications where numerical accuracy is important.this could work with an optional stable keyword argument to tf.reduce_sum or via a tf.stable_reduce_sum op.i think implementing kahan summation in the eigen tensor library wouldnt be difficult at all we would simply have to add a stateful kahansumreducer .it should also be possible to provide a vectorized version.pairwise summation might be more difficult as the reduction code would have to be touched
157844864,2603,https://api.github.com/repos/tensorflow/tensorflow/issues/2603,SpencerC,12,0,0,0,0,0,the new tensorboard refresh button looks great and will certainly be a bit more civilized than rapidly swapping the horizontal axis view to update the data but what if tensorboard could be optionally updated in real time whenever a summary writer was flushed ive written systems before where past data is loaded along with the page and new data is added continuously via a web socket would this work in tensorboards current architecture?if my understanding is correct tensorboard currently updates itself every seconds by reading the event files in the log directory could tensorboard also open a local socket and bounce summary updates from a connected tensorflow process to websocket clients?i was thinking of hacking a streaming system together on my branch but i figured i should reach out first to see if this is already in the roadmap if it isnt is this feature something you might like to merge if i can get it to work robustly
157587859,2591,https://api.github.com/repos/tensorflow/tensorflow/issues/2591,zplizzi,3,0,0,0,0,0,i just built tensorflow from the most recent source db and attempted to set it up for a development install using python setup.py develop after following the instructions in the documentation after doing this running import tensorflow in python results in a package not found error i have found that changing the line in the documentation ln s bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow to ln s bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow solves the problem the difference is an extra backtick near the end of the line perhaps this was a typo
157563185,2588,https://api.github.com/repos/tensorflow/tensorflow/issues/2588,ry,2,0,0,0,0,0,the name comes from french trous so it seems there should be an extra underscore in there at least but this is still unintelligible to non-french speakers better would be to use the english word dilated
157422551,2575,https://api.github.com/repos/tensorflow/tensorflow/issues/2575,HanyuGuo,1,0,0,0,0,0,"environment infooperating system centos installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):cpu versionif installed from binary pip package provide which pip package you installed the output from python c import tensorflow print(tensorflow. version steps to reproducei am using tensorflow to implement a network that needs to use tf.while_loop() import tensorflow as tfimport numpy as npclass model(object def init__(self self.argmax_ep_gate_array tf.placeholder(tf.int none for in range argmax_ep_gate_array_concat tf.concat self.argmax_ep_gate_array story_len tf.constant starter tf.constant z def body(hops hops tf.add(hops z.append(hops return hops def condition(hops return tf.logical_and(tf.less(tf.gather(argmax_ep_gate_array_concat hops),story_len),tf.less(hops,tf.constant self.gate_index tf.while_loop(condition,body, starter self.z=tf.concat(,z def step(self sess feed for i in range feed self.argmax_ep_gate_array i .name = i print sess.run( self.gate_index,self.z ,feed))with tf.session as sess while_loop model sess.run(tf.initialize_all_variables while_loop.step(sess what have you tried?i find that if i want to sess.run any variable in the body that is not returned tensorflow would stuck into endless loop the above example is trivial but it reveals something in the real case i am using tf.while_loop running a rnn which includes y wx+b something like that but the w and b are not returned after while loop in the forward network it works fine however if i run the back propagation the program would stuck into endless loop i suppose the code above reproducing my issue because back propagation do need to modify w and b or is there any way to handle this issue logs or other output that would be helpful(if logs are large please upload as attachment).no logs just stuck into endless loop"
157280822,2540,https://api.github.com/repos/tensorflow/tensorflow/issues/2540,TimSalimans,1,0,0,0,0,0,"tf.select will give nan gradients even if the source of the nans is not selected see example script below.import tensorflow as tfx tf.placeholder(tf.float)y tf.select(x tf.exp(x))z tf.gradients(y,x) with tf.session as sess yv,zv sess.run( y,z ,{x e print(yv correctly outputs print(zv this is nan but should be"
157033373,2514,https://api.github.com/repos/tensorflow/tensorflow/issues/2514,markpwoodward,11,0,0,0,0,0,examples on the web demonstrate signaling the end of queue data by calling queue.close in the data producer and then catching the tf.errors.outofrangeerror exception in the data consumer.this works fine for a single epoch but i do multiple epochs alternating between training data and testing data and i cant reuse the queue after calling queue.close().the two solutions that i have thought of using the existing code are enqueue some sentinel at the end of an epoch in the data producer and then tf.assert against the sentinel and catch the tf.errors.invalidargumenterror in the data consumer know the number of enqueues for the epoch and only dequeue that number.both seem a little hacky.multi-epoch use of queues might be simplified by adding one of the following a queue.reset that throws one tf.errors.outofrangeerror on dequeue or some other exception a queue.close(reset=true that only throws one tf.errors.outofrangeerror on dequeue or some other exception.example usage of q tf.fifoqueue(...)placeholder enqueue_op q.enqueue(placeholder)....def producer(data_dir sess q enqueue_op placeholder for sess.run(enqueue_op placeholder sess.run(q.reset())def do_epoch(data_dir learn threading.thread(target=producer args=(data_dir sess q enqueue_op placeholder)).start while true try sess.run exception tf.errors.outofrangeerror breakfor epoch in range(num_epochs do_epoch(train_dir learn=true do_epoch(test_dir learn=false
156880157,2510,https://api.github.com/repos/tensorflow/tensorflow/issues/2510,cgorman,2,0,0,0,0,0,would it be reasonable to add a basic text summary feature to tensorboard personally ive run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where i could put some arbitrary text where i just wrote the key differences in my runs.for example on the events page or somewhere else there would be a dropdown similar to the summaries on the events and histograms page with text i added either hardcoded or as a script argument that says what i did differently this run maybe i would print out the argument values for each run as well that would be pretty useful but basically something where i can say what did i do with this run again why was it different than the one before oh yeah i changed the batch size or oh yeah i used my other dataset instead.obviously if its arbitrary text you could maybe use it to write up a description of the network or whatever you want
156711649,2502,https://api.github.com/repos/tensorflow/tensorflow/issues/2502,Remper,2,0,0,0,0,0,so given the basic wordvec example being bound to cpu we can see that tf.nn.embedding_lookup doesnt work on gpu therefore ops that use embedding_lookup internally doesnt support gpu either for example nce_loss ).can we have explicit info in the documentation on which operations are currently gpu-capable and which are not?for example are tf.gather or loguniformcandidatesampler gpu-capable
156600635,2494,https://api.github.com/repos/tensorflow/tensorflow/issues/2494,tensorfiend,1,0,0,0,0,0,"hi,first off thanks for releasing tensorflow it is immensely helpful in many ways e.g getting it working on my gpu was a breeze however because of its novelty im running into a problem i cannot just google.after adding batch normalization to the code for a variational autoencoder found here starts crashing inconsistently sometimes it will save sometimes it will crash the error is unhelpful so im not sure what is wrong see error logs below).i call the saver as follows saver tf.train.saver with tf.session(graph=vae.graph as sess tf.train.summarywriter(/tmp/vae_logs sess.graph init tf.initialize_all_variables environment infooperating system:ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ): -rw-r--r root root aug usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root aug usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root aug usr/local/cuda-./lib/libcudart.so libcudart.so...-rwxr-xr-x root root aug usr/local/cuda-./lib/libcudart.so...-rw-r--r root root aug usr/local/cuda-./lib/libcudart_static.a-rwxr-xr-x root root apr usr/local/cuda-./lib/libcudnn.so-rwxr-xr-x root root apr usr/local/cuda-./lib/libcudnn.so.-rwxr-xr-x root root apr usr/local/cuda-./lib/libcudnn.so...-rw-r--r root root apr usr/local/cuda-./lib/libcudnn_static.a if installed from binary pip package provide which pip package you installed. pip freeze tensorflow the output from python c import tensorflow print(tensorflow. version print(tensorflow.__version__).. if installed from sources provide the commit hash steps to reproduce adapted the code from for a variational autoencoder for gaussian decoders this works fine added batch normalization added the following to variational_autoencoder.py in def init : if self.batch_normalized self.scale_decoder tf.variable(tf.ones( hidden_dim name=decoder_scale self.beta_decoder tf.variable(tf.zeros( hidden_dim name=decoder_beta self.scale_encoder tf.variable(tf.ones( hidden_dim name=encoder_scale self.beta_encoder tf.variable(tf.zeros( hidden_dim name=encoder_beta added the following to the generate and encode functions changing the different variables of course): if self.batch_normalized layer_output tf.matmul(z self._decoder_w self._decoder_bias batch_mean batch_var tf.nn.moments(layer_output axes epsilon e batch_normalized tf.nn.batch_normalization(layer_output batch_mean batch_var self.beta_decoder self.scale_decoder epsilon name=normalization_decoder h self.activation_func(batch_normalized now sometimes the saver throws a very cryptic invalidargumenterror but it is unclear to me why what have you tried changed the names that i store the models in does not help turned off batch normalization helps went through source code trying to track the error but got stuck not sure what protobuf does tried using less or more epochs does not help)is this a known bug?best regards logs or other output that would be helpful(if logs are large please upload as attachment). traceback most recent call last file train_mnist_vae.py line in module batch_normalized=args.bn file train_mnist_vae.py line in train_test_mnist_vae save_path saver.save(sess os.path.join(saved_models model_path ckpt file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in save self.saver_def.filename_tensor_name checkpoint_file file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call e.code)tensorflow.python.framework.errors.invalidargumenterror saved_models/bern=false_bn=true_bs=_ds=frey_epochs=_ff=true_hd=_iw=false_ld=_lr=e-_mm=__nt=_opt=rmsprop_wd=_.ckpt.tempstate node save/save saveslices t= dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float dt_float device=/job:localhost/replica:/task:/cpu: (_recv_save/const save/save/tensor_names save/save/shapes_and_slices variable variable/rmsprop variable/rmsprop variable variable_/rmsprop variable_/rmsprop variable variable_/rmsprop variable_/rmsprop variable variable_/rmsprop variable_/rmsprop decoderb decoderb/rmsprop decoderb/rmsprop decoderw decoderw/rmsprop decoderw/rmsprop encoderb encoderb/rmsprop encoderb/rmsprop encoderw encoderw/rmsprop encoderw/rmsprop log_var_decoder log_var_decoder/rmsprop log_var_decoder/rmsprop log_var_decoder_b log_var_decoder_b/rmsprop log_var_decoder_b/rmsprop log_var_encoder log_var_encoder/rmsprop log_var_encoder/rmsprop log_var_encoder_b log_var_encoder_b/rmsprop log_var_encoder_b/rmsprop mean_decoder mean_decoder/rmsprop mean_decoder/rmsprop mean_decoderb mean_decoderb/rmsprop mean_decoderb/rmsprop mean_encoder mean_encoder/rmsprop mean_encoder/rmsprop mean_encoderb mean_encoderb/rmsprop mean_encoderb/rmsprop_/_) caused by op usave/save defined at file train_mnist_vae.py line in module batch_normalized=args.bn file train_mnist_vae.py line in train_test_mnist_vae saver tf.train.saver file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in init restore_sequentially=restore_sequentially file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in build save_tensor self._addsaveops(filename_tensor vars_to_save file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in addsaveops save self.save_op(filename_tensor vars_to_save file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in save_op tensor_slices= vs.slice_spec for vs in vars_to_save file usr/local/lib/python./dist-packages/tensorflow/python/ops/io_ops.py line in save tensors name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_io_ops.py line in save_slices name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack"
156449669,2487,https://api.github.com/repos/tensorflow/tensorflow/issues/2487,laouer,1,0,0,0,0,0,using the version pip_gpu version i noted that a corrected issue by ilblackdragon re-occurred and a pr from him self ref has disappeared in the master code.could the issue be corrected and the enhancement reintegrated cordially
156204393,2471,https://api.github.com/repos/tensorflow/tensorflow/issues/2471,KyungJunAn,4,0,0,0,0,0,per_process_gpu_memory_fracion option didnt work in follow code.gpu_options tf.gpuoptions(per_process_gpu_memory_fraction=.)sess sv.prepare_or_wait_for_session(server.target config=tf.configproto(gpu_options=gpu_options
156168471,2465,https://api.github.com/repos/tensorflow/tensorflow/issues/2465,arita297,0,0,1,0,0,0,"i have a series of multdimensionnal time series as follow:input xi i=..n samplesxi= y,..yk yt k=..t yk is a vector t sequence length with yk yk ykm m with ykj float or intusing lstm tensor flow,would like to predict the next step t given the training of samples like xi= y yt the current code gives input form for m unidimensionnal y),input_data tf.placeholder(tf.float batchsize numsteps numinputs )targets tf.placeholder(tf.float batchsize numsteps numinputs )wondering if there is a way to input data if m in the lstm tensor flow if not is there a workaround to input multdimensionnal time series or new devs needs to be done in rnn"
156144133,2462,https://api.github.com/repos/tensorflow/tensorflow/issues/2462,ushnish,17,0,0,0,0,0,i see that we have methods for computing softmax and sigmoid cross entropy which involve taking the softmax or sigmoid of the logit vector and then computing cross entropy with the target and the weighted and sparse implementations of these but what if i simply want to compute the cross entropy between vectors
156103332,2452,https://api.github.com/repos/tensorflow/tensorflow/issues/2452,MichaelLi0913,1,0,0,0,0,0,i downloaded tensorflow using anacaonda environment installation i use python when i enter import tensorflow as tf it always shows the message importerror no module named tensorflow can someone please help me resolve this issue
156034307,2444,https://api.github.com/repos/tensorflow/tensorflow/issues/2444,scroyston,2,0,0,0,0,1,i was looking at applying deepminds ddpg algorithm to the new openai gym problem set focusing on continuous control problems like the pendulum noticed that a tensorflow version of the algorithm was running about x slower than a theano version of the same algorithm. tensorflow implementation but with a couple bugs fixed) theano implementation i started profiling both implementations in detail i noticed there were huge performance difference even when just comparing the forward pass of the actor neural network.i built a quick script to just compare a forward pass of a layered net varying the layer depth and layer breadth code and results below and using a mini batch size of for reference in deepminds ddpg paper they used a network of hidden layers sized and mini batch size of using the handy tf.runoptions.full_trace it showed of the cost was in running the ops themselves vs op scheduling startup time etc).i then stumbled on a couple of settings that seemed to help config_proto.intra_op_parallelism_threads config_proto.inter_op_parallelism_threads that seemed to improve things a bunch to where tensorflow was as fast in a couple instances or only x slower.before finding the threading flags i started looking at speeding up the individual ops in matmulop i replaced the eigen matrix multiply call with a blas call that also seemed to help quite a bit.below are some chart of the performance delta vs theano.eigenmultithread is just vanilla tensorflow eigensinglethread is tensorflow with intra/inter op_parallelism blassinglethread is the same as eigensinglethread with the eigen matrix multiply replaced by blas.the data that makes up these charts can be found here primarily ran these tests on a mac pro with avx i did compile tensorflow with the avx flag i also spot checked these results on a linux box and got similar results blas implementation was openblasi guess my issue/question is is there any ongoing work to improve performance for reinforcement learning scenarios like these especially on cpus i have heard deepmind is moving completely to tensorflow and they used/are using the new tpu chips however they also got a huge performance boost in runtime and error rate over gpus by using normal cores asynchronously which btw i dont think i can set the threading options to and have an implementation still work environment infooperating system os x ghz core intel xeon eandfedora physical cores xeon x ghzinstalled version of cuda and cudnn noneif installed from binary pip package provide:tried both a release version:pip install upgrade from source commit hash:dcefdbbbcbaeccompile command:bazel build c opt copt=-mavx tensorflow/tools/pip_package:build_pip_package(for the xeon e machine steps to reproduce i used the script below to generate the numbers tf_perf_test.py.zip
155794449,2433,https://api.github.com/repos/tensorflow/tensorflow/issues/2433,thuzhf,1,0,0,0,0,0,is there any configuration file which can be used to configure the default behavior such as which gpu device to use and so on just like the theanorc file in package theano if there isnt i hope some one can add this feature in the future code
155763055,2431,https://api.github.com/repos/tensorflow/tensorflow/issues/2431,orome,1,0,0,0,0,0,unless im missing something the ability to automatically break up calculations that dont fit on a device does not seem to be part of the tensorflow api this surprises me since one of the very first things users at least naive ones like me encounter in readily accessible gpu machines e.g aws ec instances are memory crashes on gpus to avoid these errors users have to either give up on gpus for large parts of their calculations again unless im missing something or hand code some form of batching to avoid the crashes if they can figure out where theyre happening which isnt always easy with such errors).shouldnt tensorflow do this automatically under the hood breaking up too-large calculations and merging them to avoid code like that below?---_from a related question asked on so:_im puzzled about how to efficiently assign my tensorflow operations and variables to devices its clear that at least for my implementation of a basic convolutional neural network placing as many operations as possible on a gpu is desirable but the gpu i currently have access to has limited memory and results in many warnings of the form ran out of memory trying to allocate gib the caller indicates that this is not a failure but may mean that there could be performance gains if more memory is available. and occasional crashes for certain specific operations like ran out of memory trying to allocate mib see logs for memory state resource exhausted oom when allocating tensor with shape these can be avoided by placing variables on cpus but in my implementation this results in training epochs taking times as long to compute clearly the ideal policy is to identify specific chunks of code that generate errors and attempt to place only those on cpus but it is unclear to me how to do this because those calculations cant be isolated from others that require gpu placement to achieve efficiencies.for example simply generating predictions on a test set with something like evals sess.run(tf.argmax(y feed_dict={x use_x_all}) where x is a tf.placeholder of inputs to my model and y are the output activations of my network produces the above error when use_x_all is a large array here with examples attempting to put this calculation alone on a cpu fails presumably because the network evaluation producing y is on the gpu.because of this i seem to need to resort to approaches like use_x_all data_loader.stack_data(use_data as_cols=false)use_x_split np.split(use_x_all splits)for use_x in use_x_split evals_part sess.run(tf.argmax(y feed_dict={x use_x accumulate evals which clearly doesnt scale.is there a better way specifically is there a way to place calculations like the one above on a cpu and still have those calculations for the same graph e.g training run on a gpu?or alternatively is there an idiom like batching that can be more easily applied in such situations to reduce the memory demands of such calculations
155574945,2420,https://api.github.com/repos/tensorflow/tensorflow/issues/2420,servomac,8,0,0,0,0,0,using a nightly build im unable to execute the text classification examples in i have tried text_classification.py text_classification_cnn.py and text_classification_builtin_rnn_model.py sudo pip install upgrade python c import tensorflow print(tensorflow. version python rnn.pytotal words traceback most recent call last file rnn.py line in module classifier.fit(x_train y_train logdir=/tmp/tf_examples/word_rnn file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py line in fit feed_params_fn=self._data_feeder.get_feed_params file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/trainer.py line in train feed_dict feed_dict_fn file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py line in feed_dict_fn out.itemset((i self.y sample typeerror an integer is required
155404750,2412,https://api.github.com/repos/tensorflow/tensorflow/issues/2412,vladfi1,34,0,0,0,0,0,currently building a c application in tensorflow requires creating a project in the tensorflow source tree and compiling with bazel in my case i would like to use tensorflow in a fairly large existing application with an existing build system that would be difficult to port to bazel the solution to me seems to be exposing tensorflow as a library that can be linked with
155202124,2399,https://api.github.com/repos/tensorflow/tensorflow/issues/2399,drcege,3,0,0,0,0,0,when i continue to train my model from last interrupted training tf.train.summarywriter create a new event file.could i add new summaries to my existing event file so i can visualize the whole learning run.p.s whats the right way to suspend the training binary on gpu we share one gpu and i always use ctrl+c
155086739,2391,https://api.github.com/repos/tensorflow/tensorflow/issues/2391,amjadsaadeh,1,0,0,0,0,0,i would like to implement an fft op kernel for cpu since eigen got an fft implementation this shouldnt be too hard its compiling without problems but i am always running into an invalidargumenterror when trying to use it from python see below).code of kernel for fft op on cpu modification of this file cpu implementationtemplate bool forward>class fftcpud public opkernel public explicit fftcpud(opkernelconstruction ctx opkernel(ctx void compute(opkernelcontext ctx override const tensor in ctx->input const tensorshape shape in.shape op_requires(ctx shape.dims errors::invalidargument(input must have rank but got shape.debugstring tensor out op_requires_ok(ctx ctx->allocate_output shape out if forward out.vec(fft).vec()(fft).vec());register_kernel_builder(name(ifft).device(device_cpu fftcpud);... python-script for testing: import numpy as npimport tensorflow as tfsess tf.session()data np.complex(np.random.normal(size=))input tf.constant(data)fftop tf.fft(input)print(sess.run(fftop)) error-message python test.pytraceback most recent call last file root/test.py line in module print(sess.run(fftop file tensorflow/_python_build/tensorflow/python/client/session.py line in run run_metadata_ptr file tensorflow/_python_build/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file tensorflow/_python_build/tensorflow/python/client/session.py line in do_run target_list options run_metadata file tensorflow/_python_build/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors.invalidargumenterror no opkernel was registered to support op fft with these attrs node fft fft (const) caused by op ufft defined at file root/test.py line in module fftop tf.fft(input file tensorflow/_python_build/tensorflow/python/ops/gen_math_ops.py line in fft return op_def_lib.apply_op(fft input=input name=name file tensorflow/_python_build/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file tensorflow/_python_build/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file tensorflow/_python_build/tensorflow/python/framework/ops.py line in init self._traceback extract_stack() ive got the gcr.io/tensorflow/tensorflow:latest-devel docker image and commit f and built tensorflow according to the instructions not sure whether its a bug or im doing something wrong ive already opened a stackoverflow question
155055543,2387,https://api.github.com/repos/tensorflow/tensorflow/issues/2387,alkhaled,1,0,0,0,0,0,"hi,i know the white paper describes how tensorflow handles failures in distributed execution but the docs for open source tensorflow specifically dont really explain what to expect if a machine goes down in a cluster it would be nice to know what the expected behavior is if a machine fails if its possible to add a new machine to the cluster if one fails.thanks!sulaiman"
155022605,2386,https://api.github.com/repos/tensorflow/tensorflow/issues/2386,smartcat2010,0,0,0,0,0,2,i follow distributed tensorflow to port our rnn to multi-gpu cards i start processes and each process run on a different gpu card.after solving the issue i could run it on my gpu cards server in asynchronized mode then i change my code to synchronized mode by adding only the following code optimizer tf.train.syncreplicasoptimizer optimizer replicas_to_aggregate replica_id flags.task_index total_num_replicas variable_averages exp_moving_averager remove this for simple variables_to_average variables_to_average remove this for simple then the job will hang in session.run(...)is there any other thing needed to change when switch from async mode to sync mode?thanks a lot in advance
154945094,2381,https://api.github.com/repos/tensorflow/tensorflow/issues/2381,22csnyder,1,0,0,0,0,0,bear with me as i am not an expert in the practical or theoretical details perhaps because tensorflow has developed so rapidly i find it a little difficult to find the recommended procedure for rnns with varying length at least from my lay end-user perspective it is unclear to me on may what tool within tensorflow is best suited to this task.it seems that originally rnns of varying sequence length were handled by bucketing this means building a separate model for each of several bucket lengths so that inputs could be padded to the same length without wasting too much gpu memory there is not presently a simple example with bucketing but to those unfamiliar i would look to the seqseq model for clarity also the model_with_buckets function between march and may a number of maybe different tools were added that could possibly be used to avoid this bucketing trick without wasting memory rnn.py/dynamic_rnn functional_ops though within this code its not clear without investigation if map_fn or tf.scan would be better suited control_flow_ops spite of this the seqseq model still uses bucketing and i struggle to find any examples using these newer methods.creating detailed documentation is ultimately part of the endgoal but for now i thought it would be useful to at least make a mention of what the recommended approach is.thanks. relevant issue reddit with broken link to example
154926050,2376,https://api.github.com/repos/tensorflow/tensorflow/issues/2376,zxzhijia,1,0,0,0,0,0,"i tried to learn tensorflow and ran the py file i created by copying the example code on the tensorflow website it runs well initially and printing its training epochs but finally it gave some error showing as below:(tensorflow)xu@xu-thinkcentre-me python buildconvnet.pyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyextracting mnist_data/train-images-idx-ubyte.gzextracting mnist_data/train-labels-idx-ubyte.gzextracting mnist_data/tk-images-idx-ubyte.gzextracting mnist_data/tk-labels-idx-ubyte.gzi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties:name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy step training accuracy i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks kib client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks kib client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use kib allocated for chunks mib client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use mib allocated for chunks kib client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin total chunks chunks in use b allocated for chunks b client-requested for chunks b in use in bin b client-requested in use in bin.i tensorflow/core/common_runtime/bfc_allocator.cc bin for mib was mib chunk state:i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xab of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xac of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xaf of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xaf of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xaf of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xafa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xaa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xaa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xab of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xac of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xad of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xaa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xab of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xabb of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xabc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xcbc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xccc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xdc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xdd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xde of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xdf of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xdb of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xdc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xe of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xf of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfb of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xb of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfac of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfad of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfaaa of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfab of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfab of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfab of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xfdd of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xf of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at x of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xc of size i tensorflow/core/common_runtime/bfc_allocator.cc chunk at xad of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xd of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xeb of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xf of size i tensorflow/core/common_runtime/bfc_allocator.cc free at xa of size i tensorflow/core/common_runtime/bfc_allocator.cc summary of in-use chunks by size:i tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling kibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling mibi tensorflow/core/common_runtime/bfc_allocator.cc chunks of size totalling gibi tensorflow/core/common_runtime/bfc_allocator.cc sum total of in-use chunks gibi tensorflow/core/common_runtime/bfc_allocator.cc stats:limit inuse maxinuse numallocs maxallocsize w tensorflow/core/common_runtime/bfc_allocator.cc xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxw tensorflow/core/common_runtime/bfc_allocator.cc ran out of memory trying to allocate mib see logs for memory state.w tensorflow/core/framework/op_kernel.cc resource exhausted oom when allocating tensor with shape ,,, traceback most recent call last file buildconvnet.py line in module x mnist.test.images y mnist.test.labels keep_prob file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in eval return eval_using_default_session(self feed_dict self.graph session file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in eval_using_default_session return session.run(tensors feed_dict file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/client/session.py line in do_call e.code)tensorflow.python.framework.errors.resourceexhaustederror oom when allocating tensor with shape node convd convd t=dt_float data_format=nhwc padding=same strides use_cudnn_on_gpu=true device=/job:localhost/replica:/task:/gpu: (reshape variable_/read node mean recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__mean tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () caused by op uconvd defined at file buildconvnet.py line in module h_conv tf.nn.relu(convd(x_image w_conv b_conv file buildconvnet.py line in convd return tf.nn.convd(x w strides padding=same file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/gen_nn_ops.py line in convd data_format=data_format name=name file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file home/xu/anaconda/envs/tensorflow/lib/python./site-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack()(tensorflow)xu@xu-thinkcentre-me"
154925107,2375,https://api.github.com/repos/tensorflow/tensorflow/issues/2375,chrishenx,3,0,0,0,0,0,runtimeerror when importing matplotlib.pyplot in a jupyter notebook from a virtual environment: runtimeerror python is not installed as a framework the mac os x backend will not be able to function correctly if python is not installed as a framework see the python documentation for more information on installing python as a framework on mac os x please either reinstall python as a framework or try one of the other backends if you are working with matplotlib in a virtual enviroment see working with matplotlib in virtual environments in the matplotlib faq i have solved the problem using one of the solutions mentioned at matplotlib virtualenv faq and that is ok when using python from the command line or a script file but not with the kernel used in jupyter.so the question is how to create a jupyter kernel that uses a specific bash which properly setups python to use matplotlib within a virtual environment.another solution is: import matplotlib matplotlib.use(tkagg import matplotlib.pyplot as plt but maybe that is not the best thing to do so any suggestions?.operating system os x el capitantensorflow version
154811961,2358,https://api.github.com/repos/tensorflow/tensorflow/issues/2358,altaetran,4,0,0,0,0,0,"hi,i am interested in using scatter_add when the tensor being update is not a variable is this possible?i am looking to do something like this: x_ph tf.placeholder(tf.float shape=(none ind_ph tf.placeholder(tf.int shape=(none)) #z tf.variable(tf.zeros z tf.zeros n_feat ) x np.array( ,., , ,., , ,., , ,.,. , ,.,. ) ind z tf.scatter_add(z ind_ph x) if i declare z as a tf.variable i can do this but i need to call this operation hundreds of thousands of times and do not want to store any copies of z once i am done with them if i were to declare z as a variable would there be any way to destroy z once i am done with it maybe with a garbage collector or something similar thank you so much for your help"
154788647,2354,https://api.github.com/repos/tensorflow/tensorflow/issues/2354,jinghuangzhu,3,0,0,0,0,0,tf.train.batch is the only one can handle variable-length sequence but it does not have shuffling option.for rnn training shuffling is needed
154749456,2351,https://api.github.com/repos/tensorflow/tensorflow/issues/2351,vladfi1,2,0,0,0,0,0,it appears that the log_softmax doesnt have a gradient import tensorflow as tf var tf.variable softmax tf.nn.softmax(var log_softmax tf.nn.log_softmax(var entropy tf.reduce_sum(softmax log_softmax trainer tf.train.gradientdescentoptimizer(.).minimize(-entropy)lookuperror gradient registry has no entry for logsoftmax
154619200,2348,https://api.github.com/repos/tensorflow/tensorflow/issues/2348,rzabcik,1,0,0,0,0,0,just downloaded and built on archlinux and it failed with: external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/mathfunctions.h error more than one instance of overloaded function fmin matches the argument list function std::fmin(float float function fmin(float float argument types are const float const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/mathfunctions.h error more than one instance of overloaded function fmax matches the argument list function std::fmax(float float function fmax(float float argument types are const float const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erf matches the argument list function std::erf(float function erf(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erf matches the argument list function std::erf(float function erf(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erf matches the argument list function std::erf(float function erf(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erf matches the argument list function std::erf(float function erf(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erfc matches the argument list function std::erfc(float function erfc(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erfc matches the argument list function std::erfc(float function erfc(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erfc matches the argument list function std::erfc(float function erfc(float argument types are const float)external/eigen_archive/eigen-eigen-bbc/unsupported/eigen/cxx/../../../eigen/src/core/arch/cuda/mathfunctions.h error more than one instance of overloaded function erfc matches the argument list function std::erfc(float function erfc(float argument types are const float errors detected in the compilation of tmp/tmpxft_f_-_batchtospace_op_gpu.cu.compute_.cpp.ii.error tensorflow/tensorflow/core/kernels/build output tensorflow/core/kernels/_objs/batchtospace_op_gpu/tensorflow/core/kernels/batchtospace_op_gpu.cu.o was not created.error tensorflow/tensorflow/core/kernels/build not all outputs were created. gcc version gcc cuda compilation tools release v..cudnn version ecdbecadeaebcfadba
154293668,2327,https://api.github.com/repos/tensorflow/tensorflow/issues/2327,MarvinTeichmann,2,0,0,0,0,0,the function tf.nn.softmax_cross_entropy_with_logits(logits labels is numerical unstable when used in weak labelling scenarios i.e there are no labels for some rows of the labels the instability does not occure when using tf.nn.softmax followed by a simple cross_entropy implementation i.e.: epsilon tf.constant(value shape=shape)logits logits epsilonsoftmax tf.nn.softmax(logits)cross_entropy tf.reduce_sum(labels tf.log(softmax reduction_indices= ) i therefore conclude that this is a bug occurring in the softmax layer i can not give a a minimal example as it only occurs in bigger models also i did only observe the bug when using weakly labeled data that is when cases without labels occur this case however is explicitly mentioned in the documentation labels each row labels i must be a valid probability distribution or all zeros if all zeros the corresponding loss will be regardless of the contents of logits i . if the unstability occurs tf.nn.softmax_cross_entropy_with_logits produces high gradients causing the training process to diverge this usually happens after about iterations changing the learning rate will not avoid this issue environment infooperating system linuxinstalled version of cuda and cudnn cuda cudnn installed from pip which pip package you installed the output from python c import tensorflow print(tensorflow. version steps to reproduce use loss function provided below use data containing about unlabeled entries what have you tried implementing tf.nn.softmax_cross_entropy_with_logits myselfe it works fine logs or other output that would be helpful(if logs are large please upload as attachment).the error occurs when estimating the loss like below. def loss(hypes logits labels calculates the loss from the logits and the labels args logits logits tensor float batch_size labels labels tensor int batch_size returns loss loss tensor of type float with tf.name_scope(loss logits tf.reshape(logits labels tf.to_float(tf.reshape(labels shape logits.get_shape epsilon tf.constant(value=hypes solver epsilon shape=shape logits logits epsilon cross_entropy tf.nn.softmax_cross_entropy_with_logits logits labels name=xentropy cross_entropy_mean tf.reduce_mean cross_entropy name=xentropy_mean tf.add_to_collection(losses cross_entropy_mean loss tf.add_n(tf.get_collection(losses name=total_loss return loss when the error occurs my training output looks like this root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info doing evaluate with training data root info data train num examples num correct precision root info doing evaluation with testing data root info data val num examples num correct precision root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info doing evaluate with training data root info data train num examples num correct precision root info doing evaluation with testing data root info data val num examples num correct precision root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info doing evaluate with training data root info data train num examples num correct precision root info doing evaluation with testing data root info data val num examples num correct precision root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info step loss sec per batch examples/sec root info doing evaluate with training data. when training longer the loss will eventually become nan the error does not occure when using the following loss code: def loss(hypes logits labels calculates the loss from the logits and the labels args logits logits tensor float batch_size labels labels tensor int batch_size returns loss loss tensor of type float with tf.name_scope(loss logits tf.reshape(logits shape logits.get_shape epsilon tf.constant(value=hypes solver epsilon shape=shape logits logits epsilon labels tf.to_float(tf.reshape(labels softmax tf.nn.softmax(logits cross_entropy tf.reduce_sum(labels tf.log(softmax reduction_indices cross_entropy_mean tf.reduce_mean(cross_entropy name=xentropy_mean tf.add_to_collection(losses cross_entropy_mean loss tf.add_n(tf.get_collection(losses name=total_loss return loss
154166354,2318,https://api.github.com/repos/tensorflow/tensorflow/issues/2318,shoyer,5,1,0,1,0,0,currently it does not.i would suggest implementing this by checking for a array method which is the standard api used for indicating that an object is coercible into numpy arrays and is implemented by nearly every library that implements array like objects in the python ecosystem this includes pandas as well as many other widely used libraries including xarray and dask.array
154104037,2315,https://api.github.com/repos/tensorflow/tensorflow/issues/2315,avati,1,0,0,0,0,0,environment infooperating system ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud* ):ls l usr/local/cuda/lib_/libcud_-rw-r--r root root apr usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root apr usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root apr usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root apr usr/local/cuda/lib/libcudart.so...-rw-r--r root root apr usr/local/cuda/lib/libcudart_static.alrwxrwxrwx users feb usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx users feb usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxrwxr-x users feb usr/local/cuda/lib/libcudnn.so...-rw-rw-r users feb usr/local/cuda/lib/libcudnn_static.a-rw-r--r root root apr usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root apr usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root apr usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root apr usr/local/cuda/lib/libcudart.so...-rw-r--r root root apr usr/local/cuda/lib/libcudart_static.aif installed from binary pip package provide which pip package you installed the output from python c import tensorflow print(tensorflow. version )...rcif installed from sources provide the commit hash steps to reproduceit appears that tf.nn.softmax is outputing negative values after a lot of debugging i added the following tf.print line right after the call to tf.nn.softmax the output does indeed have negative values strangely enough the values match the output of tf.nn.log_softmax issue could not be reproduced with simple test cases tf.nn.softmax behaves well in simple test cases what have you tried logs or other output that would be helpful(if logs are large please upload as attachment).i tensorflow/core/kernels/logging_ops.cc checking for negative -. i tensorflow/core/kernels/logging_ops.cc checking for negative -. i tensorflow/core/kernels/logging_ops.cc checking for negative -. i tensorflow/core/kernels/logging_ops.cc checking for negative -. i tensorflow/core/kernels/logging_ops.cc checking for negative
154095822,2314,https://api.github.com/repos/tensorflow/tensorflow/issues/2314,stephenroller,2,0,0,0,0,0,at the moment it appears that only gradientdescentoptimizer supports running on the gpu when there is a sparsetensor update this is particularly relevant for any rnns which train their embeddings including the wordvec example yes wv isnt an rnn so far the common workaround everyone seems to use is to force the embedding variables to be on the cpu but there can be substantial speed improvements by allowing them to be stored on the gpu for one there is no need to transfer the vectors of embeddings to/from the gpu and instead one can just transfer the embedding indexes and then the gradients also dont need to be transferred backwards in one test ive run where i implemented a version on the gpu the difference in one epoch was roughly s vs s basically anyone who doesnt freeze their embeddings can substantially benefit from this.this is related to and in there is a bug where variables are placed on the gpu even though an op that appears later isnt available on the gpu my understanding is a fix for this is under development but it will only make sure variables are placed on devices which can perform the necessary ops it will not actually include sparseapplys this feature request would resolve the ops reported bug in but not solve the variable-operator placement issue.in the issue is that no one has implemented any sparseapplyrmsprop along with another bug that has since been resolved the issue used to contain all optimizers except gdo but since then sparseapply ops have been added for adagrad yet theyre currently only implemented for the cpu not the gpu this feature request is a rough superset of one thing that is presently unclear to me is whether these really need to be implemented as c operators with all the associated book keeping currently that is how the cpu version of the sparseapply*s are updated see example except for gradientdescent its sparse_apply is implemented in python and uses scatter_sub .i already have a python implementation of sparse_apply for momentumoptimizer which uses tf.gather tf.scatter_update and tf.scatter_sub it passes unit tests and performs on both cpu and the gpu though it does take about a performance hit compared to the current cpu c sparseapplymomentum would this be of interest or is there a motivation behind implementing this in c
154079139,2311,https://api.github.com/repos/tensorflow/tensorflow/issues/2311,jhollowayj,2,0,0,0,0,0,"background im working on a set of networks that only share some layers so i have a parameter server that sends new weights for the different clients to use these clients accept the new weights and bias for the layers they are using and assign the values to the tf.variables via sess.run(self.w.assign(new_weights however when i start it up and let it run it crashes saying w tensorflow/core/common_runtime/bfc_allocator.cc ran out of memory trying to allocate b see logs for memory state. (sometimes its allocating b other times its kib)to give you an idea of the size of the weights i have three layers of layer w,b layer w,b layer w,b im running on a titan x with g memory.with per_process_gpu_memory_fraction the program dies at assign commands.with per_process_gpu_memory_fraction the program dies at assign commands.with per_process_gpu_memory_fraction the program dies at assign commands.with per_process_gpu_memory_fraction the program dies at assign commands.with per_process_gpu_memory_fraction the program dies at assign commands.with per_process_gpu_memory_fraction the program dies at assign commands.ive tried to set allow_growth=true and deferred_deletion_bytes in the sessions gpuoptions after reading issue but that didnt get me much further i have no idea what deferred_deletion_bytes does looking at the numbers just above gpu%vsassignmentcommands it seems to be fairly linear so it seems to me that the assign operation takes some of the gpu ram and its never freed up is there any sense of gc on the gpu memory allocated durring the var.assign op? it seems that i could delete and create a new session but that sounds expensive to me and id have to maintain the weights outside of session to be able to restore them correctly the second idea i had would to use placeholders and ship the weights in every time with the feed_dict but again that seems less that ideal and i think it would struggle in the optimizer on knowing what to optimize if they are just placeholders.let me know if you would like any other logs or reports from me i figure this is the first time someone has tried to use assign operations like this so i want to be helpful in fixing it if its a bug.thanks environment info operating system ubuntu installed version of cuda and cudnn: /usr/local/cuda/lib/libcudart.so libcudart.so../usr/local/cuda/lib/libcudart.so libcudart.so.../usr/local/cuda/lib/libcudart.so.../usr/local/cuda/lib/libcudart_static.a built from source commit hash cdaabdabaddafed"
154025828,2305,https://api.github.com/repos/tensorflow/tensorflow/issues/2305,boosh,4,0,0,0,0,0,not sure if youve seen tpot introductory blog post at but it would be awesome if you collaborated with them to add something like this to tensorflow
153826732,2294,https://api.github.com/repos/tensorflow/tensorflow/issues/2294,rdipietro,2,0,0,0,0,0,right now tf.scan splits tensors along dimension and to process multiple state/input tensors with tf.scan we need to concatenate/split for example to create an lstm block using tf.scan we might have something like def lstm_block(c_prev_and_m_prev x c_prev m_prev tf.split c_prev_and_m_prev compute new c m c_and_m tf.concat c m return c_and_m this has been asked about by others in and if we hope to process multiple tensors with significantly different shapes then the current solution will become even less concise as wed probably have to do something like flatten maintain shape info unflatten etc.i can probably make this change if its something thatd be accepted
153736298,2285,https://api.github.com/repos/tensorflow/tensorflow/issues/2285,myme5261314,3,0,0,0,0,0,environment infooperating system ubuntu installed version of cuda and cudnn and please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from sources provide the commit hash afddeecebeac steps to reproducerun the following code pythonimport tensorflow as tfdef main a tf.variable init_a tf.initialize_all_variables with tf.session as sess sess.run(init_a with tf.device(/gpu b tf.constant init_b tf.initialize_all_variables with tf.session as sess sess.run(init_b with tf.device(/cpu c tf.variable init_c tf.initialize_all_variables with tf.session as sess sess.run(init_c with tf.device(/gpu d tf.variable init_d tf.initialize_all_variables with tf.session as sess sess.run(init_d)if name main main logs or other output that would be helpful(if logs are large please upload as attachment). i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx titan xmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc cannot enable peer access from device ordinal to device ordinal i tensorflow/core/common_runtime/gpu/gpu_init.cc cannot enable peer access from device ordinal to device ordinal i tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y n i tensorflow/core/common_runtime/gpu/gpu_init.cc n y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id traceback most recent call last file test_multi_gpu.py line in module main file test_multi_gpu.py line in main sess.run(init_d file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_call raise type(e)(node_def op message)tensorflow.python.framework.errors.invalidargumenterror cannot assign a device to node variable could not satisfy explicit device specification device:gpu because no supported kernel for gpu devices is available node variable variable container dtype=dt_int shape shared_name device=/device:gpu: () caused by op uvariable defined at file test_multi_gpu.py line in module main file test_multi_gpu.py line in main d tf.variable file usr/local/lib/python./dist-packages/tensorflow/python/ops/variables.py line in init dtype=dtype file usr/local/lib/python./dist-packages/tensorflow/python/ops/variables.py line in init_from_args name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/state_ops.py line in variable_op container=container shared_name=shared_name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_state_ops.py line in variable name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack() i also noticed that the documentation for using gpus doesnt mentioned about tf.variable it only involves the tf.constant and tf.matmul. ok i found the documentation from convolutional neural networks variables are pinned to the cpu and accessed via tf.get_variable in order to share them in a multi-gpu version see how-to on sharing variables. i want ask that since tf.variables is pinned to cpu by tensorflow could we fix this error do we need to looking very carefully to exclude the tf.variable declaration outside the with tf.device(/gpu:xx scope or use netsted with tf.device(none to handle it
153256856,2237,https://api.github.com/repos/tensorflow/tensorflow/issues/2237,ibab,4,0,0,0,0,0,id like to implement a recursive neural network as in socher et al in tensorflow.note that this is different from recurrent neural networks which are nicely supported by tensorflow the difference is that the network is not replicated into a linear sequence of operations but into a tree structure.maybe a tree traversal can already be implemented using while ops but im not sure how.(a bottom-up traversal of a tree would have to be performed for each entry in the dataset applying the network at each node)instinctively i have the impression that this would need a new c op that generalizes while
153124390,2223,https://api.github.com/repos/tensorflow/tensorflow/issues/2223,ghyun,1,0,0,0,0,0,feature request can the basic_rnn_seqseq function in the seqseq module implement a feed_previous argument just like the embedding_rnn_seqseq function
152893542,2211,https://api.github.com/repos/tensorflow/tensorflow/issues/2211,hexahedria,2,0,0,0,0,0,"when i construct a graph that contains a tf.while_loop and then try to call tf.add_check_numerics_ops i get an invalidargumenterror.given the code pythonimport tensorflow as tfdef test i tf.constant tf.float c lambda i tf.less(i b lambda i tf.add(i r tf.while_loop(c b i check tf.add_check_numerics_ops with tf.session as sess print sess.run( r,check ) i get the error pythontraceback most recent call last file stdin line in module file minimal_bug_example.py line in test print sess.run( r,check file library/python/./site-packages/tensorflow/python/client/session.py line in run run_metadata_ptr file library/python/./site-packages/tensorflow/python/client/session.py line in run feed_dict_string options run_metadata file library/python/./site-packages/tensorflow/python/client/session.py line in do_run target_list options run_metadata file library/python/./site-packages/tensorflow/python/client/session.py line in do_call e.code)tensorflow.python.framework.errors.invalidargumenterror all inputs to node checknumerics must be from the same frame. im running tensorflow installed from pip version as far as i can tell this is caused by the while_loop creating a new control flow context but the check numerics op living outside that context"
152864995,2207,https://api.github.com/repos/tensorflow/tensorflow/issues/2207,quilby,1,0,0,0,0,0,feature request implement svdlike for example in torch
152857932,2206,https://api.github.com/repos/tensorflow/tensorflow/issues/2206,peterbraden,3,0,0,0,0,0,retrieves the tensorflow versions string.test in the contrib/nodejs directory with: npm run generate npm install npm test
152695532,2200,https://api.github.com/repos/tensorflow/tensorflow/issues/2200,mrphoenix13,1,0,0,0,0,0,this change is for making the memory allocation from the whole memory not only the remaining memory to achieve better configuration for multi processes
152644291,2198,https://api.github.com/repos/tensorflow/tensorflow/issues/2198,mirikle,1,0,0,0,0,0,for tensorflowestimator models currently theres no way to change learning rate once the model is saved and restored.referencing tensorflow/tensorflow/contrib/learn/python/learn/estimators/base.py
152543146,2192,https://api.github.com/repos/tensorflow/tensorflow/issues/2192,RobRomijnders,1,0,0,0,0,0,"i am wondering if tensorflow has some op for fast calculation of distances for example like matlabs pdist or pdist do you know of any way to optimize this piece of code pythonwith tf.name_scope(hidden_layer as scope centroids tf.variable(tf.random_uniform( num_centr,d ,dtype=tf.float),name=centroids var tf.variable(tf.truncated_normal( num_centr ,mean=,stddev=,dtype=tf.float exp_list for i in xrange(num_centr exp_list.append(tf.exp((-*tf.reduce_sum(tf.square(tf.sub(x,centroids i,: )),))/(*var i phi tf.transpose(tf.pack(exp_list)) for now my code works and the results beautifully surpass my expectations my prof deemed this impossible with tensorflow i was able to prototype within two hours for future i am curious if tensorflow can improve this for-loop for calculating distances with a function like matlabs pdist()thanks for your help!rob"
152378921,2188,https://api.github.com/repos/tensorflow/tensorflow/issues/2188,fcole90,4,0,0,0,0,0,environment infooperating system:ubuntu xenial xerusinstalled version of cuda and cudnn none using cpu version)if installed from binary pip package provide what have you tried?i tried to install it failed with the following message: tensorflow-..-cp-cpm-linux_x_.whl is not a supported wheel on this platform. this is probably because the python version in xenial is python xi managed to install tensorflow using instead: linux/cpu/tensorflow-..-py-none-linux_x_.whl but theres not anything like that neither for the protobuf nor for the tensorflow packages
152005330,2173,https://api.github.com/repos/tensorflow/tensorflow/issues/2173,stephenroller,2,0,0,0,0,0,must import numpy before a call to sys.setdlopenflags or there may be a segfault in certain situations fixes the situation occurs when users have compiled tensorflow but use binary distributions of scipy alternatives to this patch would be documentation stating that users who compile tensorflow from source should also recompile scipy if they need it
151878335,2169,https://api.github.com/repos/tensorflow/tensorflow/issues/2169,ziky90,29,0,0,0,0,0,it would be nice to have in tensorflow also the unpooling layer as it is described in the paper on deconvolution networks was googling a bit and i found that the added unpooling layer would be handful also for others
151852141,2167,https://api.github.com/repos/tensorflow/tensorflow/issues/2167,imcomking,1,0,0,0,0,0,operating system ubunutu lts installed version of cuda and cudnn cuda cudnn v)nvidia-smi driver version geforce gtx titan x i installed tensorflow with next command line and that is a link of tensorflow github linux gpu python whl file.sudo pip install upgrade tensorflow version outputs of python c import tensorflow print(tensorflow. version steps to reproduce save and restore a tfmodel modifying tensorflow/tensorflow/examples/skflow/mnist.py image and see the accuracy is not deterministic but stochastic after restored image what have you tried?when i try to predict mnist data after save and restore the tfmodel with skflow function the dropout shows stochastic outcomes and when i remove the dropout there is no stochasticity i guess when the model has been restored the collections of a graph about dropout are may not properly loaded so some codes on! image feed_dict prob for prob in dropouts}the function of predict at tensorflow/contrib/learn/python/learn/estimators/base.py may dont work if you run following code after restore tfmodel then you will see that there is no droputs key print classifier._graph.get_all_collection_keys
151798322,2164,https://api.github.com/repos/tensorflow/tensorflow/issues/2164,suryabhupa,4,0,0,0,0,0,when im running the inception examples from the image recognition tutorial im getting a lot of warnings in the following form:w tensorflow/core/kernels/batch_norm_op.cc op is deprecated it will cease to work in graphdef version use tf.nn.batch_normalization().i recently upgraded tensorflow to version and im working on mac os x while this isnt harming the performance im wondering if theres a fix or an update that im missing
151672068,2153,https://api.github.com/repos/tensorflow/tensorflow/issues/2153,Dapid,1,0,0,0,0,0,when building tensorflow with bazel master following the instructions in i get many warnings like: in file included from third_party/gpus/cuda/include/host_config.h from third_party/gpus/cuda/include/cuda_runtime.h from command-line>::/usr/include/features.h warning warning fortify_source requires compiling with optimization o wcpp warning fortify_source requires compiling with optimization o in fact running a simple network on tf built by me and linked against cudnn is slower than the wheel built by google against the default cudnn
151589518,2146,https://api.github.com/repos/tensorflow/tensorflow/issues/2146,Hibbert-pku,4,0,0,0,0,0,very happy to see ctc in tensorflow my question is:is someone working on ctc gpu implement internal or is it contribution welcomed?baidu has a ctc gpu implement
151509765,2140,https://api.github.com/repos/tensorflow/tensorflow/issues/2140,Russell91,3,0,0,0,0,0,it would be nice if one could use symbolic links to organize groups of training files tensorboard does not search over symbolic links for event files because the following line of python/summary/event_multiplexer.py pythonsubdirs subdir for subdir files in io_wrapper.listrecursively(path if list(filter(event_accumulator.istensorfloweventsfile files)) does not walk across symbolic links this would be easy to fix by adding a symlink option to listrecursively().current difficulties over time the number of distinct training runs in a directory can grow to over at which point tensorboard becomes slow and too information rich reorganizing a subset with symbolic links would be quite nice sometimes it would be nice to compare a small subset of training runs that exist in different directories copying these files with cp r is quite slow
151445081,2135,https://api.github.com/repos/tensorflow/tensorflow/issues/2135,jerabaul29,2,0,0,0,0,0,it would be convenient to have some tools out of the box for assessing networks complexity computational cost resources use etc without need for the user to do it by hand do you think the following would be possible to implement in a general way getting a command to know how many parameters will be optimized by a tf.train node and how many floating operations are required more generally getting a command to know how many operations are required to execute a node with sess.run getting a set of commands to evaluate memory cuda cores and bandwith use both absolute value and proportion of the device by a node on gpu and cpu use a little bit as mentioned here
151443542,2134,https://api.github.com/repos/tensorflow/tensorflow/issues/2134,beopst,1,0,0,0,0,0,"tf.image.extract_glimpse function does not work as expected in some cases the extracted glimpses are the same regardless of offset parameters please see the below code to reproduce environment infooperating system ubuntu installed version of cuda and cudnn cuda cudnn rif installed from sources provide the commit hash cfd steps to reproducetry to run the code import tensorflow as tfimport numpy as npinput_img np.arange().reshape((,,,))first_glimpse tf.image.extract_glimpse(input_img centered=false normalized=false)second_glimpse tf.image.extract_glimpse(input_img centered=false normalized=false)sess tf.session()print first_glimpse.eval(session=sess) ,:,:, print second_glimpse.eval(session=sess) ,:,:, results are first_glimpse second_glimpse is this a bug or am i missing something"
151321522,2125,https://api.github.com/repos/tensorflow/tensorflow/issues/2125,rshin,3,0,0,0,0,0,environment infooperating system ubuntu installed version of cuda and cudnn rw-r--r root root aug usr/local/cuda/lib/libcudadevrt.alrwxrwxrwx root root aug usr/local/cuda/lib/libcudart.so libcudart.so..lrwxrwxrwx root root aug usr/local/cuda/lib/libcudart.so libcudart.so...-rwxr-xr-x root root aug usr/local/cuda/lib/libcudart.so...-rw-r--r root root aug usr/local/cuda/lib/libcudart_static.alrwxrwxrwx users feb usr/local/cuda/lib/libcudnn.so libcudnn.so.lrwxrwxrwx users feb usr/local/cuda/lib/libcudnn.so libcudnn.so...-rwxrwxr-x users feb usr/local/cuda/lib/libcudnn.so...-rw-rw-r users feb usr/local/cuda/lib/libcudnn_static.a commit hash dbfcffcddddfbfbcfeeba steps to reproducerun the following test case: import tensorflow as tfout tf.cond(tf.constant(false lambda tf.nn.dynamic_rnn(tf.nn.rnn_cell.basicrnncell tf.zeros dtype=tf.float lambda tf.zeros tvars tf.trainable_variables()grads tf.gradients(tf.reduce_sum(out tvars) the above gives the following traceback: traceback most recent call last file cond_rnn_bug.py line in module grads tf.gradients(tf.reduce_sum(out tvars file usr/local/lib/python./dist-packages/tensorflow/python/ops/gradients.py line in gradients in_grads aslist(grad_fn(op out_grads file usr/local/lib/python./dist-packages/tensorflow/python/ops/math_grad.py line in tanhgrad return grad math_ops.square(y file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_math_ops.py line in square return op_def_lib.apply_op(square x=x name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._control_flow_context.addop(self file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in addop self._addopinternal(op file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in addopinternal self.addvalue(x file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in addvalue real_val grad_ctxt.grad_state.getrealvalue(val file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in getrealvalue real_value self.addbackpropaccumulatedvalue(h_value value file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in addbackpropaccumulatedvalue history_value switchrefortensor(history_value pred) branch file usr/local/lib/python./dist-packages/tensorflow/python/ops/control_flow_ops.py line in switchrefortensor return ref_switch(data pred name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_control_flow_ops.py line in ref_switch return op_def_lib.apply_op(refswitch data=data pred=pred name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op as_ref=input_arg.is_ref).dtype.name file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file usr/local/lib/python./dist-packages/tensorflow/python/ops/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape file usr/local/lib/python./dist-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto raise valueerror(none values not supported.)valueerror none values not supported what have you tried?running tf.nn.dynamic_rnn outside the lambda works but my understanding is that the two are not semantically equivalent although im not sure exactly in which ways).that is the following works: import tensorflow as tfrnn_out tf.nn.dynamic_rnn(tf.nn.rnn_cell.basicrnncell tf.zeros dtype=tf.float) out tf.cond(tf.constant(false lambda rnn_out lambda tf.zeros tvars tf.trainable_variables()grads tf.gradients(tf.reduce_sum(out tvars
151235937,2118,https://api.github.com/repos/tensorflow/tensorflow/issues/2118,ziky90,28,0,0,0,0,0,i am currently working with tf.nn.convd_transpose from the past i am used to caffe deconvolution layer and tf.nn.convd_transpose is kind of tensorflow equivalent to it.my question here is if someone could point me to detailed behaviour/documenatation of tf.nn.convd_transpose .particularly i am confused by output_shape parameter my question is why there is needed the output_shape parameter isnt the output shape directly come from the convd_transpose operation?based on how i understand it from caffe and from here computed as: h len(value stride_h kernel_h pad_hw len(value stride_w kernel_w pad_w what happens when i set output_shape smaller than h and w is the new layer being cropped?what happens when i set it higher is there being created padding with only zeros
151203428,2115,https://api.github.com/repos/tensorflow/tensorflow/issues/2115,drscotthawley,1,0,0,0,0,0,github issues are for bugs installation problems feature requests for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice environment infooperating system mac os x yosemite macportspython installed version of cuda and cudnn none(please attach the output of ls l path/to/cuda/lib/libcud* ):if installed from binary pip package provide which pip package you installed sudo pip install upgrade collecting tensorflow from downloading mb mb kb/s requirement already up-to-date six in opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages from tensorflow collecting protobuf==..b from tensorflow downloading protobuf-..b-py.py-none-any.whl kb kb mb/s collecting wheel from tensorflow downloading wheel-..-py.py-none-any.whl kb kb mb/s collecting numpy from tensorflow downloading numpy-..-cp-cpm-macosx___intel.macosx___intel.macosx___x_.macosx___intel.macosx___x_.whl mb mb kb/s collecting setuptools from protobuf==..b->tensorflow downloading setuptools-..-py.py-none-any.whl kb kb mb/s installing collected packages setuptools protobuf wheel numpy tensorflow found existing installation setuptools uninstalling setuptools successfully uninstalled setuptools found existing installation numpy deprecation uninstalling a distutils installed project numpy has been deprecated and will be removed in a future version this is due to the fact that uninstalling a distutils project will only partially uninstall the project uninstalling numpy successfully uninstalled numpy successfully installed numpy protobuf-..b setuptools tensorflow wheel the output from python c import tensorflow print(tensorflow. version python c import tensorflow print(tensorflow. version if installed from sources provide the commit hash steps to reproduce read the documentation at tensorboardto run tensorboard use the commandtensorboard logdir=path/to/log-directory do what it says tensorboard logdir=path/to/log-directorytensorboard command not found what have you tried try rehash rehash tensorboard logdir=path/to/log-directorytensorboard command not found locate tensorboard which tensorboardtensorboard command not found. :check google find stackexchange post how to install tensorboard suggest that if one installs via pip as i did then tensorboard is available on the command-line but it is not.alternate invocation of tensorboard via python is not the same as in the tutorial but does work python opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/tensorboard/tensorboard.py logdir=/tmp/mnist_logsstarting tensorboard on port you can navigate to everything works....conclusion is that either online tutorial documentation is incorrect and needs revision or pip installation does not perform as advertised or...user error.workarounds alias tensorboard python opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/tensorboard/tensorboard.py tensorboard logdir=/tmp/mnist_logs . echo usr/bin/env python tmp.txt cat tmp.txt opt/local/library/frameworks/python.framework/versions/./lib/python./site-packages/tensorflow/tensorboard/tensorboard.py tensorboard chmod u+x tensorboard rm f tmp.txt sudo mv tensorboard opt/local/bin logs or other output that would be helpful(if logs are large please upload as attachment
150742785,2089,https://api.github.com/repos/tensorflow/tensorflow/issues/2089,ilblackdragon,13,0,0,0,0,7,hdf is a popular format to store complicated datasets alternative to proto files).also has different random seeking functionality and is widely used in scientific community.this is fr for adding hdf reader support of hdf in tf.learn.(originally from
150580044,2075,https://api.github.com/repos/tensorflow/tensorflow/issues/2075,educob,1,0,0,0,0,0,"hi,tf.argmin only works in one dimension.lets say i have a picture which is a x array of pixels each pixels is a value array and i want to know which pixel is closest to a certain color in that case i have to reshape the x array to a d array then get the min index then find out that index to what x array position corresponds.couldnt i just as tensorflow to give me the index in the x array for instance i really dont understand the reason for having to translate between lineal index and array location all the time.thanks"
150568631,2073,https://api.github.com/repos/tensorflow/tensorflow/issues/2073,fayeshine,2,0,0,0,0,0,"ubuntu lts has update default gcc to and string.h as well even we install a gcc-. ,we will get a bug like usr/include/string.h in function void mempcpy_inline(void const void size_t):/usr/include/string.h error memcpy was not declared in this scope return char memcpy dest src n n; this bus is described in can be solved by insert cxx_flag d_force_inlines into tensorflow/third_party/gpus/crosstool/crosstool"
150494015,2069,https://api.github.com/repos/tensorflow/tensorflow/issues/2069,wchan,1,0,0,0,0,0,new fifobucketedqueue this queue is similar to the existing fifoqueue but w buckets this allows you to use the same graph for all your buckets rather than creating multiple graphs for each bucket i.e as shown in the tf examples for seqseq the main advantage is having a single graph makes creating a graph much faster and simplier python code finally for those unfamiliar w bucketing see sutskever et al sequence to sequence learning with neural networks it is a strategy to sample by say sequence length to reduce the variance of each minibatch and consequently be much more efficient in computation at the cost of losing uniform sampling however this doesnt seem to be a big problem w large datasets).fifobucketedqueuetest also included
150176807,2053,https://api.github.com/repos/tensorflow/tensorflow/issues/2053,akors,3,0,0,0,0,0,"hi i tried to compile the tutorials_example_trainer file and i have quite a journey behind me i recompiled gcc several times i recompiled bazel dozens of times and did a fair share of crosstools editing.at this point i am stuck the compliation fails with the message: bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc error while loading shared libraries libcudart.so cannot open shared object file no such file or directorytarget tensorflow/cc:tutorials_example_trainer failed to build i am using tensorflow head currently bcdcfdbfdfbcafb i have cuda installed in usr/local/cuda my ld_library_path is set to usr/local/cuda/lib:/usr/local/cuda/lib and the files exist there.i tried to point bazel to the library directory by adding linker_flag l/usr/local/cuda/lib environment infooperating system fedora installed version of cuda and cudnn and ls usr/local/cuda-./lib/libcud*/usr/local/cuda-./lib/libcudadevrt.a usr/local/cuda-./lib/libcudart.so usr/local/cuda-./lib/libcudnn.so./usr/local/cuda-./lib/libcudart.so usr/local/cuda-./lib/libcudart_static.a usr/local/cuda-./lib/libcudnn.so.../usr/local/cuda-./lib/libcudart.so usr/local/cuda-./lib/libcudnn.so usr/local/cuda-./lib/libcudnn_static.a if installed from sources provide the commit hash bcdcfdbfdfbcafb steps to reproduce recompiled bazel version with the following patch applied the following patch to tensorflow ran configure with opt/gcc-./bin/gcc as compiler but default otherwise ran bazel build c opt config=cuda local_resources j tensorflow/cc:tutorials_example_trainer verbose_failures what have you tried i cried a lot add linker_flag l/usr/local/cuda/lib to third_party/gpus/crosstool/crosstool add linker_flag wl,-r/usr/local/cuda/lib to third_party/gpus/crosstool/crosstool recompile bazel with added linker_flag wl,-r/usr/local/cuda/lib in tools/cpp/crosstool create a file etc/ld.so.conf.d/local_cuda-lib.conf with the contents usr/local/cuda/lib and ran ldconfig logs or other output that would be helpfulheres the full output of the last operation: error home/alexander/.local/src/tensorflow/tensorflow/cc/build executing genrule tensorflow/cc:random_ops_genrule failed namespace-sandbox failed error executing command cd home/alexander/.cache/bazel/_bazel_alexander/fcadcfedec/tensorflow exec env path=/usr/lib/ccache:/usr/lib/ccache:/usr/lib/qt-./bin:/usr/lib/ccache:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/home/alexander/.local/bin:/home/alexander/bin:/home/alexander/.local/bin:/home/alexander/bin home/alexander/.cache/bazel/_bazel_alexander/fcadcfedec/tensorflow/_bin/namespace-sandbox home/alexander/.cache/bazel/_bazel_alexander/fcadcfedec/tensorflow/bazel-sandbox/dc--a-adc-bfafcc-.params bin/bash c source external/bazel_tools/tools/genrule/genrule-setup.sh bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc bazel-out/local_linux-opt/genfiles/tensorflow/cc/ops/random_ops.h bazel-out/local_linux-opt/genfiles/tensorflow/cc/ops/random_ops.cc bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc error while loading shared libraries libcudart.so cannot open shared object file no such file or directorytarget tensorflow/cc:tutorials_example_trainer failed to buildinfo elapsed time s critical path s ps out of curiosity what are you tensorflow devs using for a development machine has any of you actually tried using a modern linux distribution that comes with gcc newer than for compilation you really should its quite the experience"
150049767,2050,https://api.github.com/repos/tensorflow/tensorflow/issues/2050,s-gv,8,0,0,0,0,0,tensorboard seems to scan the log infrequently right now i have to kill and re-start the server to make it re-scan the log could we have tensorboard re-scan the log when the tensorboard webpage is refreshed
149940788,2045,https://api.github.com/repos/tensorflow/tensorflow/issues/2045,meanmee,1,0,0,0,0,0,when i import tensorflow it raise an error like this(the newest version of tensorflow via pip installtaion) /usr/bin/python home/dell/wxm/code/imagecaption-tensorflow/test.pytraceback most recent call last file home/dell/wxm/code/imagecaption-tensorflow/test.py line in module import tensorflow as tf file usr/local/lib/python./dist-packages/tensorflow/ init .py line in module from tensorflow.python import file usr/local/lib/python./dist-packages/tensorflow/python/ init .py line in module import tensorflow.contrib as contrib file usr/local/lib/python./dist-packages/tensorflow/contrib/ init .py line in module from tensorflow.contrib import learn file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/ init .py line in module from tensorflow.contrib.learn.python.learn import file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/ init .py line in module from tensorflow.contrib.learn.python.learn import file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/ init .py line in module from tensorflow.contrib.learn.python.learn.io import file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/io/ init .py line in module from tensorflow.contrib.learn.python.learn.io.dask_io import file usr/local/lib/python./dist-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py line in module import dask.dataframe as dd file usr/local/lib/python./dist-packages/dask/dataframe/ init .py line in module from core import dataframe series index frame map_partitions file usr/local/lib/python./dist-packages/dask/dataframe/core.py line in module class index(series file usr/local/lib/python./dist-packages/dask/dataframe/core.py line in index derived_from(pd.index file usr/local/lib/python./dist-packages/dask/utils.py line in wrapper original_args getargspec(original_method).args file usr/local/lib/python./dist-packages/dask/compatibility.py line in getargspec return getargspec(func file usr/local/lib/python./dist-packages/dask/compatibility.py line in getargspec return inspect.getargspec(func file usr/lib/python./inspect.py line in getargspec raise typeerror({!r is not a python function.format(func))typeerror method max of numpy.ndarray objects is not a python functionprocess finished with exit code
149667632,2033,https://api.github.com/repos/tensorflow/tensorflow/issues/2033,hephaex,4,0,0,0,0,0,github issues are for bugs installation problems feature requests for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice environment infooperating system ubuntu installed version of cuda and cudnn please attach the output of ls l path/to/cuda/lib/libcud ls l usr/local/cuda-./lib/libcud*-rw-r--r root root aug usr/local/cuda-./lib/libcudadevrt.alrwxrwxrwx root root aug usr/local/cuda-./lib/libcudart.so libcudart.so..lrwxrwxrwx root root aug usr/local/cuda-./lib/libcudart.so libcudart.so...-rwxr-xr-x root root aug usr/local/cuda-./lib/libcudart.so...-rw-r--r root root aug usr/local/cuda-./lib/libcudart_static.a-rwxr-xr-x root root apr usr/local/cuda-./lib/libcudnn.so-rwxr-xr-x root root apr usr/local/cuda-./lib/libcudnn.so.-rwxr-xr-x root root apr usr/local/cuda-./lib/libcudnn.so...-rw-r--r root root apr usr/local/cuda-./lib/libcudnn_static.a if installed from binary pip package provide which pip package you installed the output from python c import tensorflow print(tensorflow. version python c import tensorflow print(tensorflow.__version__)i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally..rc if installed from sources provide the commit hash steps to reproduce what have you tried when i run the deep convolutional nn tutorial from tensorflows website i get the following error: i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow dev\ice gpu device name geforce gtx pci bus id successfully loaded saved_networks/dqn-f tensorflow/stream_executor/cuda/cuda_dnn.cc could not set cudnn filter d\escriptor cudnn_status_bad_param logs or other output that would be helpful(if logs are large please upload as attachment
149642082,2030,https://api.github.com/repos/tensorflow/tensorflow/issues/2030,waleedka,2,0,0,0,0,0,gridsearchcv is a great way to test and optimize hyper-parameters automatically i use it with tensorflowestimator to optimize learning_rate batch_size etc it would be a great addition if i can also use it to customize other parameters in my custom model for example say i have a custom model with a convnet and i want to optimize the stride value this pseudo code explains what im trying to achieve i used a custom params input to the model function just as an example not to imply that this is necessarily the right way to implement this feature my custom model feature request new params dict with values filled by gridsearchcvdef cnn_model(x y params stride params stride custom model definition here create the convnet classifiercnn_classifier learn.tensorflowestimator(model_fn=cnn_model grid search on different stride values.parameters stride grid_searcher gridsearchcv(cnn_classifier parameters)grid_searcher.fit(x y
148907648,1992,https://api.github.com/repos/tensorflow/tensorflow/issues/1992,niskander,1,0,0,0,0,0,environment infooperating system ubuntu ltsinstalled version of cuda and cudnn cuda-.cudnn steps to reproduce import tensorflow import scipy.misc scipy.misc.imread fails with ioerror broken data stream when reading image fileimporting scipy before tensorflow solves the issue
148902982,1990,https://api.github.com/repos/tensorflow/tensorflow/issues/1990,dlmacedo,2,0,0,0,0,0,"dear friends,i have tried to install tensorflow in an anaconda python environment but it didnt worked i got a message saying something like not a supported wheel on this platform environment infooperating system ubuntu lts steps to reproduce dlm@pc-aero dlm@pc-aero conda create n tensorflow python=.(...)dlm@pc-aero dlm@pc-aero source activate tensorflowdiscarding home/dlm/anaconda/bin from pathprepending home/dlm/anaconda/envs/tensorflow/bin to path(tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero python vpython continuum analytics inc.(tensorflow)dlm@pc-aero pip vpip from home/dlm/anaconda/envs/tensorflow/lib/python./site-packages python tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero pip install ignore-installed upgrade is not a supported wheel on this platform. (tensorflow)dlm@pc-aero what have you tried?i have tried to use the pip for python instead of it appears to install but didnt worked after all i think the the problem now is that conda is using the pip of the system environment since this version isnt installed in the tensorflow environment see below: (tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero pip vpip from usr/local/lib/python./dist-packages python tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero sudo h pip install ignore-installed upgrade password for dlm collecting tensorflow==..rc from using cached numpy from tensorflow==..rc using cached numpy-..-cp-cpm-manylinux_x_.whlcollecting wheel from tensorflow==..rc using cached wheel-..-py.py-none-any.whlcollecting six from tensorflow==..rc using cached six-..-py.py-none-any.whlcollecting protobuf==..b from tensorflow==..rc using cached protobuf-..b-py.py-none-any.whlcollecting setuptools from protobuf==..b->tensorflow==..rc using cached setuptools-..-py.py-none-any.whlinstalling collected packages numpy wheel six setuptools protobuf tensorflowsuccessfully installed numpy protobuf-..b setuptools six tensorflow-..rc wheel-..(tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero pythonpython continuum analytics inc default dec gcc red hat on linuxtype help copyright credits or license for more information import sys sys.path home/dlm/anaconda/envs/tensorflow/lib/python.zip home/dlm/anaconda/envs/tensorflow/lib/python home/dlm/anaconda/envs/tensorflow/lib/python./plat-linux home/dlm/anaconda/envs/tensorflow/lib/python./lib-dynload home/dlm/anaconda/envs/tensorflow/lib/python./site-packages home/dlm/anaconda/envs/tensorflow/lib/python./site-packages/setuptools-..-py..egg import tensorflow tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero python c import os import inspect import tensorflow print(os.path.dirname(inspect.getfile(tensorflow)))traceback most recent call last file string line in module file home/dlm/anaconda/envs/tensorflow/lib/python./inspect.py line in getfile raise typeerror({!r is a built-in module.format(object))typeerror module tensorflow namespace is a built-in module(tensorflow)dlm@pc-aero python c import os import inspect import tensorflow print(os.path.dirname(inspect.getfile(tensorflow)))traceback most recent call last file string line in module file home/dlm/anaconda/envs/tensorflow/lib/python./inspect.py line in getfile raise typeerror({!r is a built-in module.format(object))typeerror module tensorflow namespace is a built-in module(tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero python m tensorflow.models.image.mnist.convolutional/home/dlm/anaconda/envs/tensorflow/bin/python error while finding spec for tensorflow.models.image.mnist.convolutional class importerror no module named tensorflow.models)(tensorflow)dlm@pc-aero tensorflow)dlm@pc-aero python c import tensorflow print(tensorflow.__version__)traceback most recent call last file string line in module>attributeerror module tensorflow has no attribute version__(tensorflow)dlm@pc-aero but if you try to create a conda environment using python it works therefore one possible way to fix it is change the installation procedure in order to create a conda environment using python and not python dlm@pc-aero conda create n tensorflowx python=.using anaconda cloud api site package metadata solving package specifications dlm@pc-aero source activate tensorflowxdiscarding home/dlm/anaconda/bin from pathprepending home/dlm/anaconda/envs/tensorflowx/bin to path(tensorflowx)dlm@pc-aero tensorflowx)dlm@pc-aero pip install ignore-installed upgrade tensorflow==..rc from installed numpy protobuf-..b setuptools six tensorflow-..rc wheel-..(tensorflowx)dlm@pc-aero python m tensorflow.models.image.mnist.convolutionali tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyextracting data/train-images-idx-ubyte.gzextracting data/train-labels-idx-ubyte.gzextracting data/tk-images-idx-ubyte.gzextracting data/tk-labels-idx-ubyte.gzi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx timajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx ti pci bus id initialized!step epoch msminibatch loss learning rate minibatch error validation error step epoch msminibatch loss learning rate minibatch error validation error step epoch ms"
148899838,1989,https://api.github.com/repos/tensorflow/tensorflow/issues/1989,ilya-edrenkin,1,0,0,0,0,0,minor docstring fix time_major true avoids extra transposes in dynamic_rnn
148886235,1988,https://api.github.com/repos/tensorflow/tensorflow/issues/1988,Cactucs,1,0,0,0,0,0,environment infooperating system ubuntu lsttensorflow v no cuda)python steps to reproduce pythonimport tensorflow as tfsess tf.interactivesession()x tf.placeholder(tf.float shape= none x tf.print(x x message=p)i tf.reshape(x i tf.print(i i message=p)i.eval(feed_dict={x prints i tensorflow/core/kernels/logging_ops.cc p array dtype=float) where is the first print with p message
148870678,1984,https://api.github.com/repos/tensorflow/tensorflow/issues/1984,lahwran,6,0,0,0,0,0,its unclear from both the code and documentation why scan defaults to parallel iterations doesnt that cause it to sometimes run the function before the accumulated value is available i also poked around and read the documentation for while and that didnt clear things up things it looks like it might do run steps in parallel if and only if there are no graph dependencies on the last step run steps in parallel but provide nonsense values for graph dependencies on the last step ignore it because the parameter doesnt do anything yet do magic beyond my ken to skip ahead in serial computations if so can i tell it to only run the last one thereby creating a halting oracle steps to reproduce read the docs in master for scan and while read the code for functional_ops.scan compare to the code for functional_ops.map_fn be confused that scan appears to get the right answer when you use it
148823848,1978,https://api.github.com/repos/tensorflow/tensorflow/issues/1978,ultrons,1,0,0,0,0,0,for study and debug purposes it could be very useful to view computation network graph in tensor flow without having to dump any histogram/learning data dsmilkov i learnt from danmane that such a feature already exists for debug purposes in tensor flow not yet available for users what do think about possibility of making it accessible to users
148706125,1968,https://api.github.com/repos/tensorflow/tensorflow/issues/1968,concretevitamin,1,0,0,0,0,0,it should be easy to extend/modify tf.rank tf.shape and tf.size to have them work on sparsetensor s.happy to review prs
148656754,1965,https://api.github.com/repos/tensorflow/tensorflow/issues/1965,karesh,2,0,0,0,0,0,"hi,i have tried to install tensorflow as in have cuda installed under usr/local/cuda after i install gpu version using pip it gives the error importerror libcudart.so cannot open shared object file no such file or directory but i have libcudart.so file why this is referring by default issues for version and then i tried to install cpu only version and giving me the errorattributeerror type object newbase has no attribute is_abstract only to version how can i solve those issues thanks"
148414558,1950,https://api.github.com/repos/tensorflow/tensorflow/issues/1950,Remper,5,0,0,0,0,0,as the code in tf.nn.embedding_lookup suggests there were attempts in to make it accept sparsetensors but then everything funnels into tf.gather that tries to convert sparsetensor to tensor and fails.is it a correct behaviour or is it a bug if not is there any workaround to achieve the same effect get some of the rows of a sparsetensor based on a index list otherwise sparsetensors are close to being completely unusable environment infooperating system os x tensorflow version rc log file usr/local/lib/python./site-packages/tensorflow/python/ops/gen_array_ops.py line in gather validate_indices=validate_indices name=name file usr/local/lib/python./site-packages/tensorflow/python/ops/op_def_library.py line in apply_op as_ref=input_arg.is_ref file usr/local/lib/python./site-packages/tensorflow/python/framework/ops.py line in convert_to_tensor ret conversion_func(value dtype=dtype name=name as_ref=as_ref file usr/local/lib/python./site-packages/tensorflow/python/ops/constant_op.py line in constant_tensor_conversion_function return constant(v dtype=dtype name=name file usr/local/lib/python./site-packages/tensorflow/python/ops/constant_op.py line in constant tensor_util.make_tensor_proto(value dtype=dtype shape=shape file usr/local/lib/python./site-packages/tensorflow/python/framework/tensor_util.py line in make_tensor_proto tensor_proto.string_val.extend( compat.as_bytes(x for x in proto_values file usr/local/lib/python./site-packages/tensorflow/python/util/compat.py line in as_bytes raise typeerror(expected binary or unicode string got r bytes_or_text)typeerror expected binary or unicode string got tensorflow.python.framework.ops.sparsetensor object at xdbc
148359785,1941,https://api.github.com/repos/tensorflow/tensorflow/issues/1941,Styrke,13,0,0,0,0,0,the ideathe tf.session.run method currently supports taking either a single graph element or a list of graph elements as input for the fetches argument and will return either a single value or a list of values correspondingly. i propose adding support for using a dictionary as input for the fetch argument and receiving a corresponding dictionary as the return value.using dictionaries has at least two advantages compared to lists it will make the code more readable and keys are easier to remember than indices in the following example the return value of the loss can be accessed as result loss instead of result python fetches train_op train_op accuracy accuracy loss loss result sess.run(fetches feed_dict print(result loss result accuracy it will be much easier to only fetch certain elements on flexible schedules for example i might want to fetch summaries for tensorboard every iterations while only fetching the loss printing to the console every iterations that means that the return value would be a list that sometimes have none of the extra elements sometimes one of them and sometimes both as a consequence the index in the returned list that contains the loss might vary from one run to the next and i would have to build extra logic to handle this with dictionaries the key doesnt change depending on other elements in the dict so it would be possible to do something like saving summaries without even knowing the schedule python if summaries in result writer.add_summary(result summaries i implementationi have recently been emulating the proposed behavior in a project using using the following wrapper function to translate from dictionary to list and back.i suspect that it wouldnt be hard to do something similar in tensorflow/python/client/session.py perhaps as part of process_fetches but i didnt want to spend a lot of time on it before asking pythondef run(session fetches feed_dict wrapper for making session.run more user friendly with this function fetches can be either a list or a dictionary if fetches is a list this function will behave like tf.session.run and return a list in the same order as well if fetches is a dict then this function will also return a dict where the returned values are associated with the corresponding keys from the fetches dict keyword arguments session an open tensorflow session fetches a list or dict of ops to fetch feed_dict the dict of values to feed to the computation graph if isinstance(fetches dict keys values fetches.keys list(fetches.values res session.run(values feed_dict return key value for key value in zip(keys res else return session.run(fetches feed_dict questions is this a bad idea for some reason i dont realize or maybe i just overlooked some disadvantages that are worth considering why stop at dictionaries what about tuples nested dictionaries or dictionaries containing lists of elements general comments and thoughts
148131185,1924,https://api.github.com/repos/tensorflow/tensorflow/issues/1924,ppwwyyxx,0,0,0,4,0,0,environment infooperating system ubuntu if installed from sources provide the commit hash fb steps to reproduce bash python c import tensorflow import cv print cv.imread(cat.jpg)none what have you tried?import cv before tensorflow works.this problem also exists on centos it doesnt happen on a latest archlinux.its likely due to that tensorflow has its own libjpeg that conflicts with opencv on some platforms
148129731,1922,https://api.github.com/repos/tensorflow/tensorflow/issues/1922,LucaRonin,14,0,8,0,0,0,in tensorflow logo the left arm of the t looks like units long while the right arm is unit long so if i am not mistaken the shadow should look different asymmetrical).! tensorflow
148001935,1912,https://api.github.com/repos/tensorflow/tensorflow/issues/1912,fayeshine,1,0,0,0,0,0,add a reference for logistic regression in wordvec tutorial
147874225,1894,https://api.github.com/repos/tensorflow/tensorflow/issues/1894,vladfi1,1,0,0,0,0,0,it looks like there are some missing types in tensorflow/core/framework/types.h such as uint and uint this means i cant make tensors of these types in c
147871334,1893,https://api.github.com/repos/tensorflow/tensorflow/issues/1893,alexggmatthews,1,0,0,0,0,0,"running from master the following small code snippet gives an error. import tensorflow as tfimport numpy as npw tf.constant tf.float)x tf.placeholder(tf.float y tf.scalar_mul w tf.ones(tf.pack( tf.shape(x tf.shape(x tf.float z tf.diag_part(y)init tf.initialize_all_variables()sess tf.session()sess.run(init)print(sess.run(z feed_dict={x np.ones((,))})) the relevant part of the error is tensorflow/python/ops/array_ops.py line in diagpartshape do not match valueerror invalid shape shape :mid and shape mid do not match i think it is reasonable to be able to get the diagonal of this matrix therefore i think this is a bug with the shape inference thanks,alex"
147492193,1852,https://api.github.com/repos/tensorflow/tensorflow/issues/1852,nemo,1,0,0,0,0,0,environment inforunning on latest nightly build steps to reproduce save an estimator to a folder lets call it classifier.model attempt to restore on another machine in different machinethis breaks since the checkpoint file is pointing to the place in the new machine what have you tried manually changing the path in the checkpoint file fixes the issue
147462736,1848,https://api.github.com/repos/tensorflow/tensorflow/issues/1848,VigneshSrinivasan10,1,0,0,0,0,0,"hi,i recently updated to tensorflow version using the command: pip install upgrade i run pip show tensorflow then it gives me that version of tensorflow as however tf.__version in python gives me i am using pip to install a few other dependencies(like prettytensor it looks for the latest version of tensorflow since it fails to find the latest version using pip it isnt updating my other dependencies.please let me know if there is a workaround for this thanks in advance"
147284390,1843,https://api.github.com/repos/tensorflow/tensorflow/issues/1843,fritzo,2,0,0,0,0,0,tensorflow currently lacks efficient in-place matrix updates such as rank update a u v rmlarsen recommends supporting these in-place updates through a single new assignmatmul method that wraps the underlying gemm kernels in eigen or cublas etc
146967926,1825,https://api.github.com/repos/tensorflow/tensorflow/issues/1825,alexggmatthews,2,0,0,0,0,0,a feature request.at the moment as far as im aware tensorflow doesnt have equivalents of the following numpy functionality np.tril and similarlynp.triu np.tril_indeces and the upper triangular equivalent and the ability to assign to a tensor using them the ability to convert a vector to a lower or upper triangular square matrix which blas has this sort of thing is useful for instance in linear algebra usages of tensorflow i am unaware of an easy work around for these but i would be happy to be wrong
146958443,1824,https://api.github.com/repos/tensorflow/tensorflow/issues/1824,alexggmatthews,34,0,0,0,0,4,it would be very useful to understand which parts of the computational graph are causing computational bottlenecks.this question has been asked on stack overflow here here it seems more appropriate as a feature request as this would allow systematic discussion the tensorflow white paper talks about very advanced internal tools that google have along these lines something like that would make open source tensorflow even better i did it look at the road map but im not sure it explicitly mentioned this topic my apologies if i have missed a post about this
146865416,1820,https://api.github.com/repos/tensorflow/tensorflow/issues/1820,JosuGoiko,1,0,0,0,0,0,"hi all,i have the pip wheel distribution of tensorflow r in red hat in our cluster.my issue is that i wanted to extract the embedding from a huge corpora tokens but the bit corpus_size counter in wordvec_kernel.cc is not enough it seems that it is not so straightforward to fix it changing the counter to int i have seen that this very error was reported last th of dec in issue i also receive the following error:w tensorflow/models/embedding/wordvec_kernels.cc invalid argument the text file wbu.txt contains too little data wordse tensorflow/core/framework/op_segment.cc create kernel failed invalid argument the text file wbu.txt contains too little data wordse tensorflow/core/common_runtime/executor.cc executor failed to create kernel invalid argument the text file wbu.txt contains too little data words node skipgram skipgram batch_size filename=wbu.txt min_count subsample window_size device=/job:localhost/replica:/task:/cpu: () traceback most recent call last file wordvec_minibatch.py line in module tf.app.run file opt/python-./lib/python./site-packages/tensorflow/python/platform/default/_app.py line in run sys.exit(main(sys.argv file wordvec_minibatch.py line in main model wordvec(opts session file wordvec_minibatch.py line in init self.build_graph file wordvec_minibatch.py line in build_graph opts.words_per_epoch self._session.run( words counts words_per_epoch file opt/python-./lib/python./site-packages/tensorflow/python/client/session.py line in run return self._run(none fetches feed_dict file opt/python-./lib/python./site-packages/tensorflow/python/client/session.py line in run feed_dict_string file opt/python-./lib/python./site-packages/tensorflow/python/client/session.py line in do_run target_list file opt/python-./lib/python./site-packages/tensorflow/python/client/session.py line in do_call e.code)tensorflow.python.framework.errors.invalidargumenterror the text file wbu.txt contains too little data words node skipgram skipgram batch_size filename=wbu.txt min_count subsample window_size device=/job:localhost/replica:/task:/cpu: () caused by op uskipgram defined at file wordvec_minibatch.py line in module tf.app.run file opt/python-./lib/python./site-packages/tensorflow/python/platform/default/_app.py line in run sys.exit(main(sys.argv file wordvec_minibatch.py line in main model wordvec(opts session)thanx"
146461403,1803,https://api.github.com/repos/tensorflow/tensorflow/issues/1803,anuragkr90,1,0,0,0,0,0,i am getting core dump errors on running the example codes e.g convolutional.py on mnist installed it through pip linux bit gpu enabled the output is below doing simple graph construction here to check gpu device works fines i already checked but in my case the compute mode for gpu is set to default so i dont think that is the problem what could be the problem then i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyextracting data/train-images-idx-ubyte.gzextracting data/train-labels-idx-ubyte.gzextracting data/tk-images-idx-ubyte.gzextracting data/tk-labels-idx-ubyte.gzi tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties:name tesla kcmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla kc pci bus id i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc allocating gib bytes.i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc gpu memory begins at xfa extends to xaadefloating point exception core dumped
146383577,1793,https://api.github.com/repos/tensorflow/tensorflow/issues/1793,Thrasi,5,0,0,0,0,0,i swapped out the first max_pool operation in convolutional.py tutorial for a max_pool_with_argmax operation and there doesnt seem to exist a gradient for that op: (virtenv python convolutional.py extracting data/train-images-idx-ubyte.gz extracting data/train-labels-idx-ubyte.gz extracting data/tk-images-idx-ubyte.gz extracting data/tk-labels-idx-ubyte.gz traceback most recent call last): file convolutional.py line in module> tf.app.run() file home/name/virtenv/local/lib/python./site-packages/tensorflow/python/platform/default/_app.py line in run sys.exit(main(sys.argv)) file convolutional.py line in main global_step=batch) file home/name/virtenv/local/lib/python./site-packages/tensorflow/python/training/optimizer.py line in minimize colocate_gradients_with_ops=colocate_gradients_with_ops) file home/name/virtenv/local/lib/python./site-packages/tensorflow/python/training/optimizer.py line in compute_gradients colocate_gradients_with_ops=colocate_gradients_with_ops) file home/name/virtenv/local/lib/python./site-packages/tensorflow/python/ops/gradients.py line in gradients (op.name op.type)) lookuperror no gradient defined for operation maxpoolwithargmax op type maxpoolwithargmax environment infooperating system ubuntu ltsif installed from binary pip package provide:(virtenv pip install upgrade python c import tensorflow print(tensorflow. version steps to reproduce change line of convolutional.py from pool tf.nn.max_pool(relu ksize strides padding=same to pool pos tf.nn.max_pool_with_argmax(relu ksize strides padding=same run virtenv python convolutional.py what have you tried logs or other output that would be helpful(if logs are large please upload as attachment
146297943,1790,https://api.github.com/repos/tensorflow/tensorflow/issues/1790,neotheicebird,6,0,0,0,0,0,when i try: tensorboard logdir=/path/to/eventlogs i get importerror no module named tensorflow.tensorboard.tensorboard packagesi use ubuntu python and i installed tensorflow using: pip install upgrade
146173811,1786,https://api.github.com/repos/tensorflow/tensorflow/issues/1786,fayeshine,6,0,0,0,0,0,anyone can help this
145998256,1779,https://api.github.com/repos/tensorflow/tensorflow/issues/1779,zhangjiajie,4,0,0,0,0,0,the bidirectional_rnn is not taking the advantages of dynamic_rnn it runs into memory issues for even moderate length of time steps i guess this should be really easy to implement but a really big help for people using the bidirectional version.thanks!jiajie
145733131,1771,https://api.github.com/repos/tensorflow/tensorflow/issues/1771,alonsovidales,7,0,0,0,0,0,"hello,during the last weeks i have been working on this implementation of the go api this is still a work in progress im doing this pr just to show what im working on and to get feedback.i forked from where the bindings was implemented by travis cline this go library allows to interact with the tensors in an easy way from go being able to create populate and retrieve the content from the tensors this library also provides the api for graph generation from go create sessions run this graphs and so on the goal is to achieve the same functionality provided by the c api.in order to show how the api works i updated the label_image example with an implementation using the go api and i also updated the tutorial trying to keep all the code covered with tests i still have to cover some corner cases and functionality like the data type recognition for the graph etc.by the moment the supported tensor data types are tf_float tf_double tf_int tf_uint tf_uint tf_int tf_int tf_string tf_int tf_bool im planing to add support for tf_qint tf_bfloat and tf_complex soon.i have some doubts would it be better to add the api docu as md in tensorflow/gdoc or just with godoc it would be enough having the docu on the markdown files could be better for the people without too much experience on go but having the docu on godocs is going to be updated in an easiest way and the go developers are more used to read the documentation on godoc format we can also keep it in both places or just add a link from the markdown docu to godoc in order to get the definition for all the available operations im loading this info from tensorflow/core/ops/ops.pbtxt the problem is that im doing this cp of the file from go gen i think that this problem could be approached on three different ways converting ops.pbtxt into a go file converting the content into a constant string we dont need to move this file this option will remove this dependency but if the file changes it will requiere to regenerate the libs that shouldnt be too much problem we could even generate go code for each operation so we dont need to parse the file and it would improve the performance using something like the tf_getoplist function used by python in order to retrieve this info from the c api this would add more dependencies from the go to the c api that is not a mayor issue but will requiere to prepare a new method or modify the original since tf_getoplist depends on the python libs as im doing right now just copying the file at least for a first iteration this could be a good option since it makes more easy to work with the code regarding to the installation im not sure if keep using go generate or bazel go generate is more or less the standar for go projects but bazel is what tensorflow is using i think that im going to change to bazel so i can avoid things like bazel from go but im not sure if this is the best approach.i would be really grateful if you could please take a look at the code and let me know your thoughts what im missing how can i improve it etc.thanks!related ticket"
145547764,1758,https://api.github.com/repos/tensorflow/tensorflow/issues/1758,kmader,2,0,0,0,0,0,i am trying to modify an existing graph inception loaded using the standard approach with gfile.fastgfile(os.path.join model_dir classify_image_graph_def.pb r as f graph_def tf.graphdef graph_def.parsefromstring(f.read tf.import_graph_def(graph_def name=) the nodes are accessible using the get_tensor_by_name method here i try to replace the input to cast which is decodejpeg with a tf.variable with the same contents allowing it to be used for optimization) old_cast_node sess.graph.get_tensor_by_name(cast:)old_image_input old_cast_node.op.inputs tf_new_image tf.variable(old_image_input.eval())old_cast_node.op.inputs tf_new_image results in this error typeerror inputlist object does not support item assignment can the inputlist of an op be modified in any other way
145451160,1753,https://api.github.com/repos/tensorflow/tensorflow/issues/1753,daviddao,1,0,0,0,0,0,as described in he et al the first x convolution inside the bottleneck architecture reduces the dimension to save flops hence the correct number of filters is block.bottleneck_size instead of block.num_filters .
145400633,1749,https://api.github.com/repos/tensorflow/tensorflow/issues/1749,robwell,46,0,0,6,0,3,i am using tfrecords to use the sharding/queuing machinery for simple problems for which the examples can be cloned it is easy to use but recently i tried to do something more complex train examples where there are multiple labels i posted a question on stackoverflow about the specific issue i have would be nice to have clearer guidance on the use of tfrecords ive looked through the code have a pretty good understanding of protocol buffers etc in spite of this it is difficult to understand how the machinery works
144958009,1727,https://api.github.com/repos/tensorflow/tensorflow/issues/1727,gustavla,8,0,0,0,0,0,as i understand from the documentation running sess.close is supposed to release the resources but it doesnt i have been running the following test pythonwith tf.session as sess with tf.device(/gpu matrix tf.constant matrix tf.constant product tf.matmul(matrix matrix result sess.run(product print(result) this allocates all the free memory of gpu but it is not released when sess is closed both using a context manager as in the code above but also when calling sess.close manually the memory usage persists until that python process is terminated the way i have been checking memory usage is through nvidia-smi but i have also confirmed that other processes cant allocate that gpu memory until the process terminates not the session closes i would like to be able to free the resources and still keep the python process running environment infoi am running a bit linux centos with a computer that has two tesla kc driver cuda i installed the tensorflow for linux and python through pip the output of tf.__version is steps to reproducesimply running the code above should according to the document allocate and then release the memory however the gpu memory is still allocated and thus unusable by other processes however it can be re-used by the same python process meaning that i can re-run the snippet over and over as long as i do it from the same python process logs or other output that would be helpfulhere is a log of the session at the end the memory is still allocated note that another user is connected to both gpus through torch and is actively using gpu pythonin import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyin with tf.session as sess with tf.device(/gpu matrix tf.constant matrix tf.constant product tf.matmul(matrix matrix result sess.run(product print(result i tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties:name tesla kcmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties:name tesla kcmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc cannot enable peer access from device ordinal to device ordinal i tensorflow/core/common_runtime/gpu/gpu_init.cc cannot enable peer access from device ordinal to device ordinal i tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y ni tensorflow/core/common_runtime/gpu/gpu_init.cc n yi tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla kc pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name tesla kc pci bus id i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc allocating gib bytes.i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc gpu memory begins at xf extends to xabcd in
144453507,1704,https://api.github.com/repos/tensorflow/tensorflow/issues/1704,rdadolf,6,0,0,0,0,0,environment infooperating system os x ubuntu source build commit hash edfeebbacddfadebba steps to reproducei have tf as a submodule and build it local to the project i use more or less the exact same commands in the development setup the only minor change is that i dont run python setup.py develop under os x this breaks my anaconda python installation completely via screwing with the site-packages so the steps are in makefile syntax bashgit submodule update init recursivecd tf_dir configurecd tf_dir bazel build c opt tensorflow/tools/pip_package:build_pip_packagecd tf_install_dir ln s addprefix wildcard tf_dir)/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles tf_install_dir ln s addprefix wildcard tf_dir)/tensorflow/tools/pip_package the prefix commands are for making globs work in make i have confirmed they expand to the same symlink commands issueimporting tensorflow no longer works using development guidelines specifically assuming tensorflow is cloned into tf_dir symlinked into tf_install_dir and built by bazel in bazel_tmp this no longer works output sanitized manually pythonpath=$tf_install_dir pythonpython default oct gcc on linuxtype help copyright credits or license for more information import tensorflow as tftraceback most recent call last file stdin line in module file tf_install_dir/tensorflow/__init__.py line in module from tensorflow.python import file tf_install_dir/tensorflow/python/__init__.py line in module from tensorflow.core.framework.graph_pb import file tf_install_dir/tensorflow/core/framework/graph_pb.py line in module from google.protobuf import descriptor as descriptor file bazel_tmp/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google/protobuf/descriptor.py line in module from google.protobuf.pyext import message file bazel_tmp/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google/protobuf/pyext/__init__.py line in module import__(pkg_resources).declare_namespace(__name file usr/local/lib/python./dist-packages/pkg_resources/__init__.py line in declare_namespace handle_ns(packagename path_item file usr/local/lib/python./dist-packages/pkg_resources/__init__.py line in handle_ns path.sort(key=sort_key file usr/local/lib/python./dist-packages/pkg_resources/__init__.py line in sort_key return sys_path.index(_normalize_cached(os.sep.join(parts)))valueerror bazel_tmp/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles is not in list linux output shown but os x also fails with a similar exception what have you tried?i believe the issue comes down to the same one as i am creating this issue separately so that if it turns out to be a different problem it isnt muddled if its the same we can close it.the exception is a result of namespace package resolution in google.protobuf trying to find bazel_tmp/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google/protobuf/pyext it is trying to find it in a list of system paths which includes tf_install_dir recall that due to the development setup rules the files in tf_install_dir are symlinks to files in bazel_tmp specifically tf_install_dir/google/protobuf is the same directory as bazel_tmp/google/protobuf i have verified this with pwd p .what seems to be going on is that python is attempting to unify the protobuf namespace package but its filename bazel_tmp/google/protobuf/pyext does not match the sys.path entry tf_install_dir ). note this only appears to be a problem when trying to use the development install rules and it only appears to be a problem when theres another tensorflow or protobuf installed in the system somewhere else at least on os x the other version of tensorflow on my linux machine is in use by other users so i cant uninstall it safely i have not attempted to create a clean virtual environment just for my development directory for one because virtual environments are a pain in the neck and two thats part of the whole point of having a local build of tensorflow).workaround replacing the symlinks with a recursive copy from bazel_tmp to tf_install_dir is apparently sufficient to convince python that its unified the namespace packages correctly and it chooses the tf_install_dir specified in pythonpath .so i dont know what the appropriate solution here is but the problem seems to be the result of a convergence of semi-hacky solutions any one of which alone might not be an issue but work together to cause problems theres no clean way to extract the necessary files from a built tensorflow without going through pip install perhaps theres a method using pip im not aware of maybe something with target python namespace packages have poor path resolution semantics it looks like this ship has sailed the symlink approach is kind of coarse and tricky to script around
144416734,1702,https://api.github.com/repos/tensorflow/tensorflow/issues/1702,pronobis,1,0,0,0,0,0,environment infooperating system linuxusing pip for python steps to reproducerun: a=tf.constant b=tf.print(a a output i tensorflow/core/kernels/logging_ops.cc while the tensor evalutes to: array dtype=int expected output i tensorflow/core/kernels/logging_ops.cc
144412823,1701,https://api.github.com/repos/tensorflow/tensorflow/issues/1701,chemelnucfin,2,0,0,0,0,0,environment infooperating system fedora if installed from sources provide the commit hash:eea steps to reproduce bazel/output/bazel output_base=/projects/.cache build c opt config=cuda tensorflow/tools/pip_package:build_pip_package verbose_failures what have you tried configure on root directory cuda_config.sh script building without config=cuda is successful logs or other output that would be helpful(if logs are large please upload as attachment).output from build noticed that path in the log doesnt have the directory to libcudart.so which is here:/usr/local/cuda/lib/echo ld_library_path::/usr/local/cuda/lib:/usr/local/cuda/lib:/usr/lib/nvidia:/usr/local/cuda/lib:/usr/local/cuda/lib:/usr/lib/nvidia:/usr/local/cuda/lib:/usr/local/cuda/lib:/usr/lib/nvidiapointers will be welcome
144271231,1690,https://api.github.com/repos/tensorflow/tensorflow/issues/1690,NicholasYuan,4,0,0,0,0,0,environment infooperating system:mac os i install the tensorflow with sudo pip install upgrade run sudo python cifar_train.py i got an error error in shell sudo python cifar_train.py filling queue with cifar images before starting to train this will take a few minutes.traceback most recent call last file cifar_train.py line in module tf.app.run file usr/local/lib/python./site-packages/tensorflow/python/platform/default/_app.py line in run sys.exit(main(sys.argv file cifar_train.py line in main train file cifar_train.py line in train summary_writer tf.train.summarywriter(flags.train_dir sess.graph file usr/local/lib/python./site-packages/tensorflow/python/training/summary_io.py line in init self.add_graph(graph_def file usr/local/lib/python./site-packages/tensorflow/python/training/summary_io.py line in add_graph graph_bytes graph_def.serializetostring()attributeerror graph object has no attribute serializetostring please help
144133005,1686,https://api.github.com/repos/tensorflow/tensorflow/issues/1686,cgorman,3,0,0,0,0,0,per the comment on this introduction i.e n.b manually specifying these cluster specifications can be tedious especially for large clusters we are working on tools for launching tasks programmatically e.g using a cluster manager like kubernetes if there are particular cluster managers for which youd like to see support please raise a github issue.is there is any possibility of supporting slurm forgive my ignorance but ive really only played around with tensorflow and ive only used slurm for fairly simple mpi projects but i recently got access to a cluster with some gpu nodes and id like to incorporate tf in my research project it would be great if i was able to use all the resources i could to speed things along.if it helps specific info about the setup can be found here
143749420,1661,https://api.github.com/repos/tensorflow/tensorflow/issues/1661,jerabaul29,7,0,0,0,0,0,are there some plans to include a d and possibly higher dimension generalization convolution op would be nice for doing d time processing for example for the d version).also should something be done to offer the user the possibility to choose the shape of the kernel for example allowing both the natural hyper cube kernel nodes location or a hyper thetrahedron location could have consequences on the computing power requested
143674280,1652,https://api.github.com/repos/tensorflow/tensorflow/issues/1652,seominjoon,1,0,0,0,0,0,i want to use rnn cell on multiple gpus for maximum performance i want to place rnn cell variables on cpu and operations matmul tanh etc on gpu just like in cifar multi gpu example however the current implementation of rnn cell seems to only allow same device placement for both variables and ops could you enable different device placement
143073558,1607,https://api.github.com/repos/tensorflow/tensorflow/issues/1607,Muaazbinsaeed,1,0,0,0,0,0,why android ndk and other versions are not being supported by bazel android build?it only supports android-ndk-re
143072157,1606,https://api.github.com/repos/tensorflow/tensorflow/issues/1606,Muaazbinsaeed,1,0,0,0,0,0,is there any operation which generates confusion matrix in tensorflow?cant find it on
143070148,1604,https://api.github.com/repos/tensorflow/tensorflow/issues/1604,jstaker7,17,0,0,0,0,0,i am inspired by dr ben grahams recent work regarding spatially-sparse convolutional neural networks particularly section has graciously open-sourced his neural network library but id like to utilize these same ideas in my models which are implemented in tensorflow.unfortunately im finding his implementation and citations a little hard to follow but his description of a feature matrix and a pointer matrix sounds a little like something that can be implemented using sparse variable updates in tensorflow but im afraid that it might not be so simple and would require building custom kernels to support a new-ish type of convolution but i dont know enough to say for certain which direction to take.any thoughts on how we can bring spatially-sparse convolutions to tensorflow anyone interested in collaborating on implementing this
142902605,1592,https://api.github.com/repos/tensorflow/tensorflow/issues/1592,bhack,17,0,0,0,0,0,is there already a plan to add binary ops like bitcount for xnor-net
142820048,1587,https://api.github.com/repos/tensorflow/tensorflow/issues/1587,decentralion,4,0,0,0,0,0,right now tensorboard does not properly evaluate a path beginning with at least on mac example: (tensorflow space pwd/users/danmane/space(tensorflow space tensorboard logdir=~/foo/zoid debug host=localhostinfo:tensorflow:tensorboard is in debug mode.info:tensorflow:starting tensorboard in directory users/danmane/spaceinfo:tensorflow:tensorboard path_to_run is users/danmane/space/~/foo/zoid none
142455804,1573,https://api.github.com/repos/tensorflow/tensorflow/issues/1573,Mistobaan,1,0,0,0,0,0,this branch was previously in but for some reason most likely a bad force push on my repo was permanently closed
142034714,1551,https://api.github.com/repos/tensorflow/tensorflow/issues/1551,zkl99999,0,0,0,0,1,0,sudo pip install upgrade is not a supported wheel on this platform.storing debug log for failure in home/xxx/.pip/pip.log
142008884,1548,https://api.github.com/repos/tensorflow/tensorflow/issues/1548,zhang8473,1,0,0,0,0,0,on my tensorboard the run name always shows as the board put all of my runs under the same log path into one plot how do i separate them by setting a run name i did not find it out from your documentary.! image did you achieve this in your documentary?! image tried your run name is also just a
140406334,1475,https://api.github.com/repos/tensorflow/tensorflow/issues/1475,artgillespie,5,0,0,0,1,0,environment infooperating system mac os x steps to reproduce virtualenv venv source venv/bin/activate pip install ipython pip install upgrade git clone cd tensorflow/tensorflow/examples/udacity ipython notebook in the browser chrome load notmnist.ipynb run cell imports run cell cell fails with exception failed to verify notmnist_large.tar.gz can you get to it with a browser checked url with browser downloads fine.checked working directory found byte notmnist_large.tar.gz opened and found this; forbiddenyou dont have permission to access upload/notmnist/notmnist_large.tar.gzon this server.additionally a forbiddenerror was encountered while trying to use an errordocument to handle the request.apache server at yaroslavvb.com port address> small thing and easy to work around but would improve our students experience if they didnt have to debug this
140340471,1468,https://api.github.com/repos/tensorflow/tensorflow/issues/1468,ghost,1,0,0,0,0,0,github issues are for bugs installation problems feature requests for general support from the community see stackoverflow make bugs and feature requests more easy to find and organize we close issues that are deemedout of scope for github issues and point people to stackoverflow.for bugs or installation issues please provide the following information.the more information you provide the more easily we will be able to offerhelp and advice environment infooperating system:os x ei capitan version if installed from sources provide the commit hash:eacabaaecabfaccbcbaafa steps to reproduce build tensorflow without uncommenting the android sdk and ndk parts successfully we can import tensorflow in python environment after step and following installation steps download sdk and ndk from what suggested uncomment the android sdk and ndk repository in workspace and change the path rebuild using bazel build c opt tensorflow/tools/pip_package:build_pip_package the console returns the error shellerror tensorflow/workspace no such package androidndk could not read release.txt in android ndk private/var/tmp/.../fff.../external/androidndk/ndk/release.txt no such file or directory and referenced by external:android/crosstool what have you tried commenting ndk repository and leave the sdk in tensorflow workspace build using bazel build c opt tensorflow/tools/pip_package:build_pip_package passed and without error..finding that downloaded ndk and ndk in private/var/tmp/.../fff.../external/androidndk/ndk are the same and there are no release.txt in in directory logs or other output that would be helpful(if logs are large please upload as attachment shellerror tensorflow/workspace no such package androidndk could not read release.txt in android ndk private/var/tmp/.../fff.../external/androidndk/ndk/release.txt no such file or directory and referenced by external:android/crosstool. is this error due to the codes or my building processes
140239945,1467,https://api.github.com/repos/tensorflow/tensorflow/issues/1467,yaroslavvb,1,0,0,0,0,0,two recent requests recently to suppress logging on console tensorflow uses google logging library standard flags should work ie export glog_logtostderr should turn off logging to console however that doesnt work i suspect these flags are explicitly overriden in code somewhere
140202542,1465,https://api.github.com/repos/tensorflow/tensorflow/issues/1465,alexggmatthews,1,0,0,0,0,2,this implements a gradient op for the cholesky op as requested and discussed in the code was written by myself and dr james henman user name jameshensman the implementation is a blocked cholesky backpropagation as advocated by dr iain murray in this recent paper unit tests are included a lot of work has gone into the implementation of the algorithm and i therefore believe the code has value to the community less work so far has gone in to tightly integrating it with tensorflow code base and i am looking for help and advice from the tensorflow team as such it should be regarded as a work in progress martinwicke suggested the best path forward was to submit this pull request
139812560,1450,https://api.github.com/repos/tensorflow/tensorflow/issues/1450,Duum,1,0,0,0,0,0,"i tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size bi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size bi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc allocating gib bytes.i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc gpu memory begins at xdc extends to xeee tensorflow/stream_executor/cuda/cuda_driver.cc could not synchronize on cuda context cuda_error_illegal_address no stack trace availablef tensorflow/core/common_runtime/gpu/gpu_util.cc gpu sync failed hi!whats wrong with this how i can solve this.im using cuda cudnn and all they are ok running on cpu but when run gpu it occur wrong.and i can local the operation which cant run on gpu with tf.device(/cpu optimizer tf.train.adamoptimizer(learning_rate=learning_rate).minimize(cost) when i remvoe tf.device(/cpu:),it ocure the bug reported above"
139812281,1449,https://api.github.com/repos/tensorflow/tensorflow/issues/1449,lfwin,3,0,0,0,0,0,"hi,although stack overflow have a question about this but right now the answers are not so clear and the doc also not tell the difference too but i believe understanding the difference between name scope and variable scope and context manager is beneficial to programming for newbie using tensorflow"
139309710,1433,https://api.github.com/repos/tensorflow/tensorflow/issues/1433,raingo,0,0,0,0,0,2,the reduce_mean gpu implementations are not registered here so that the reduce_mean is still placed on cpu.wondering if its a negligence or intended if its intended please close this
139162404,1426,https://api.github.com/repos/tensorflow/tensorflow/issues/1426,yhuanghamu,3,0,0,0,0,0,i have try each method in installation guide to install gpu-tensorflow in my centos server?however the error below is still exists. importerror lib/libc.so version glibc not found required by usr/local/lib/python./site-packages/tensorflow/python/_pywrap_tensorflow.so) is there anyone whove successfully installed tensor flow in centos
139111655,1424,https://api.github.com/repos/tensorflow/tensorflow/issues/1424,kashif,1,0,0,0,0,0,for issue
138924372,1415,https://api.github.com/repos/tensorflow/tensorflow/issues/1415,jplu,1,0,0,0,0,0,"hello,i have an issue with google protobuf even after the install python does not find the module google.protobuf environment infooperating system ubuntu python bazel cuda cudnn if installed from sources provide the commit hash steps to reproduce bazel build c opt config=cuda tensorflow/cc:tutorials_example_trainer bazel build c opt config=cuda tensorflow/tools/pip_package:build_pip_package bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkg pythonuserbase=/usr/local/python pip install user tmp/tensorflow_pkg/tensorflow-..-py-none-any.whloutput for pip install: unpacking tmp/tensorflow_pkg/tensorflow-..-py-none-any.whlrequirement already satisfied use upgrade to upgrade six in usr/local/python/lib/python./site-packages from tensorflow==..)downloading/unpacking protobuf==..b from tensorflow downloading protobuf-..b-py.py-none-any.whl kb kb downloadedrequirement already satisfied use upgrade to upgrade wheel in usr/local/python/lib/python./site-packages from tensorflow==..)requirement already satisfied use upgrade to upgrade numpy in usr/local/python/lib/python./site-packages from tensorflow==..)requirement already satisfied use upgrade to upgrade setuptools in usr/local/python/lib/python./site-packages from protobuf==..b->tensorflow==..)installing collected packages tensorflow protobufsuccessfully installed tensorflow protobufcleaning up... error in python interpreter: python default jun gcc on linuxtype help copyright credits or license for more information import google.protobuftraceback most recent call last file stdin line in module>importerror no module named google.protobuf>>> in the python prompt when i run help(modules i can see the module tensorflow but not the module google .the first thing i tried is pip uninstall protobuf pythonuserbase=/usr/local/python pip install user upgrade tmp/tensorflow_pkg/tensorflow-..-cp-none-any.whloutput: installing collected packages tensorflow protobufsuccessfully installed tensorflow protobufcleaning up... same result no protobuf module found.the second thing i tried is pip uninstall protobuf pip uninstall tensorflow pythonuserbase=/usr/local/python pip install user tmp/tensorflow_pkg/tensorflow-..-py-none-any.whloutput: installing collected packages tensorflow protobufsuccessfully installed tensorflow protobufcleaning up... same result again not protobuf module found.any idea of why protobuf is properly installed but not recognized by python whereas tensorflow is propertly recognized by python?thanks for any help that you can provide"
138840775,1408,https://api.github.com/repos/tensorflow/tensorflow/issues/1408,markusdr,0,0,0,0,0,1,"would it be possible to support generalized matrix multiplication with non-standard semirings in matrix multiplication we have a times b c where c(i,j sum_k a(i,k times b(k,j) using a different semiring practically means redefining the plus and times operations for example the log semiring assumes that all numbers in the matrices are log numbers and redefines plus as logplus aka logaddexp or logsumexp and times as plus this is useful for computing the log denominator in log-linear models e.g log z log sum_y exp(f(y times exp(f(y)) .the max-plus semiring aka viterbi or tropical semiring redefines plus as max and times as plus this is useful for finding the best path assuming all matrix entries are log numbers"
138822468,1405,https://api.github.com/repos/tensorflow/tensorflow/issues/1405,bernardopires,1,0,0,0,0,0,"hi,since i started using tensorboard i noticed some odd behavior that sometimes id open it and it seemed like a lot of the summary values were missing i.e even though its already at k steps the summary only shows k steps or so today i actually finally noticed that it simply meant that it was just still being loaded i refreshed the page after a minute or so and all data was there my pc is not bad i-k so this was quite surprising any reason why the summaries sometimes take so long to load also can we get some sort of warning that not all summaries have been completely loaded yet"
138787778,1402,https://api.github.com/repos/tensorflow/tensorflow/issues/1402,ushnish,17,0,0,0,0,0,"these are the sequence of steps i followed to install the distributed version of tf git clone recurse-submodules configure default with gpu bazel build c opt config=cuda tensorflow/cc:tutorials_example_trainer bazel-bin/tensorflow/cc/tutorials_example_trainer use_gpu at this point saw the expected output and gpu being used bazel build c opt config=cuda define=use_fast_cpp_protos=true tensorflow/tools/pip_package:build_pip_package bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkg sudo pip install tmp/tensorflow_pkg/tensorflow-..-py-none-any.whl processing tmp/tensorflow_pkg/tensorflow-..-py-none-any.whl installing collected packages tensorflow pip show tensorflow metadata-version name tensorflow version summary tensorflow helps the tensors flow home-page author google inc author-email opensource@google.com license apache location usr/local/lib/python./dist-packages requires six protobuf wheel numpy classifiers development status beta intended audience developers intended audience education intended audience science/research license osi approved apache software license programming language python topic scientific/engineering mathematics topic software development libraries python modules topic software development libraries entry-points console_scripts tensorboard tensorflow.tensorboard.tensorboard:mainnow for the error python python default apr gcc on linux type help copyright credits or license for more information import tensorflow traceback most recent call last file stdin line in module file usr/local/lib/python./dist-packages/tensorflow/ init .py line in module from tensorflow.python import file usr/local/lib/python./dist-packages/tensorflow/python/ init .py line in module raise importerror(msg importerror traceback most recent call last file usr/local/lib/python./dist-packages/tensorflow/python/ init .py line in module from tensorflow.core.framework.graph_pb import file usr/local/lib/python./dist-packages/tensorflow/core/framework/graph_pb.py line in module from google.protobuf import descriptor as descriptor importerror no module named protobuferror importing tensorflow unless you are using bazel,you should not try to import tensorflow from its source directory;please exit the tensorflow source tree and relaunch your python interpreterfrom there.can someone help me understand what might cause this thank you"
138762254,1398,https://api.github.com/repos/tensorflow/tensorflow/issues/1398,mxrguspxrt,1,0,0,0,0,0,is a really cool course in the context of machine learning and tensorflow after completing coursera andrew ng course they give valuable reasoning why and how something should be done in tensorflow.but there is an issue.theory part in videos and implementing it in tensorflow do not click together forum is full of questions in the format how should it be done am i doing it correctly?.yes i can spend days on a one assignment search internet and take parts from manuals that are on the tensorflow.org page but still i am not convinced that i am doing it correctly please improve so that best practice how previous task would have been correctly solved is included in the next task
138309534,1377,https://api.github.com/repos/tensorflow/tensorflow/issues/1377,viksit,3,0,0,0,0,0,the website tries to load fails someone needs to fix the path to get the right version of mathjax and have all the latex symbols show up.thanks
137821571,1355,https://api.github.com/repos/tensorflow/tensorflow/issues/1355,omarcr,0,7,0,0,0,0,this error happens when training several models in a for loop: w tensorflow/core/common_runtime/executor.cc xefbb w tensorflow/core/common_runtime/executor.cc xefbb compute status resource exhausted oom when allocating tensor with shape node sub sub t=dt_float device=/job:localhost/replica:/task:/gpu: (sub_/x variable_/read) traceback most recent call last file media/konet/ddc/pycharm_ubuntu/deep_neural_network_binary/dnn__models_binary.py line in module fited model.fit(xtrain ytrain nb_epoch=n_epochs batch_size=batch_size show_accuracy=true shuffle=true validation_split file usr/local/lib/python./dist-packages/keras/models.py line in fit shuffle=shuffle metrics=metrics file usr/local/lib/python./dist-packages/keras/models.py line in fit outs f(ins_batch file usr/local/lib/python./dist-packages/keras/backend/tensorflow_backend.py line in call updated session.run(self.outputs self.updates feed_dict=feed_dict file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run results self._do_run(target_list unique_fetch_targets feed_dict_string file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run e.code)tensorflow.python.framework.errors.resourceexhaustederror oom when allocating tensor with shapedim size dim size node mul mul t=dt_float device=/job:localhost/replica:/task:/gpu: (variable_/read variable_/read node add recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__add tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () caused by op umul defined at file media/konet/ddc/pycharm_ubuntu/deep_neural_network_binary/dnn__models_binary.py line in module model.compile(loss=binary_crossentropy optimizer=adam class_mode=binary file usr/local/lib/python./dist-packages/keras/models.py line in compile train_loss file usr/local/lib/python./dist-packages/keras/optimizers.py line in get_updates m_t self.beta m self.beta g file usr/local/lib/python./dist-packages/tensorflow/python/ops/variables.py line in lambda setattr(variable operator lambda a b variable._runop(operator a b file usr/local/lib/python./dist-packages/tensorflow/python/ops/variables.py line in runop return getattr(ops.tensor operator)(a._astensor b file usr/local/lib/python./dist-packages/tensorflow/python/ops/math_ops.py line in binary_op_wrapper return func(x y name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_math_ops.py line in mul return op_def_lib.apply_op(mul x=x y=y name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack oom when allocating tensor with shape node sub sub t=dt_float device=/job:localhost/replica:/task:/gpu: (sub_/x variable_/read) traceback most recent call last file media/konet/ddc/pycharm_ubuntu/deep_neural_network_binary/dnn__models_binary.py line in module fited model.fit(xtrain ytrain nb_epoch=n_epochs batch_size=batch_size show_accuracy=true shuffle=true validation_split file usr/local/lib/python./dist-packages/keras/models.py line in fit shuffle=shuffle metrics=metrics file usr/local/lib/python./dist-packages/keras/models.py line in fit outs f(ins_batch file usr/local/lib/python./dist-packages/keras/backend/tensorflow_backend.py line in call updated session.run(self.outputs self.updates feed_dict=feed_dict file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run results self._do_run(target_list unique_fetch_targets feed_dict_string file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run e.code)tensorflow.python.framework.errors.resourceexhaustederror oom when allocating tensor with shapedim size dim size node mul mul t=dt_float device=/job:localhost/replica:/task:/gpu: (variable_/read variable_/read node add recv client_terminated=false recv_device=/job:localhost/replica:/task:/cpu send_device=/job:localhost/replica:/task:/gpu send_device_incarnation tensor_name=edge__add tensor_type=dt_float device=/job:localhost/replica:/task:/cpu: () caused by op umul defined at file media/konet/ddc/pycharm_ubuntu/deep_neural_network_binary/dnn__models_binary.py line in module model.compile(loss=binary_crossentropy optimizer=adam class_mode=binary file usr/local/lib/python./dist-packages/keras/models.py line in compile train_loss file usr/local/lib/python./dist-packages/keras/optimizers.py line in get_updates m_t self.beta m self.beta g file usr/local/lib/python./dist-packages/tensorflow/python/ops/variables.py line in lambda setattr(variable operator lambda a b variable._runop(operator a b file usr/local/lib/python./dist-packages/tensorflow/python/ops/variables.py line in runop return getattr(ops.tensor operator)(a._astensor b file usr/local/lib/python./dist-packages/tensorflow/python/ops/math_ops.py line in binary_op_wrapper return func(x y name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/gen_math_ops.py line in mul return op_def_lib.apply_op(mul x=x y=y name=name file usr/local/lib/python./dist-packages/tensorflow/python/ops/op_def_library.py line in apply_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in create_op original_op=self._default_original_op op_def=op_def file usr/local/lib/python./dist-packages/tensorflow/python/framework/ops.py line in init self._traceback extract_stack() any advice on how to solve this?thank you
137031858,1325,https://api.github.com/repos/tensorflow/tensorflow/issues/1325,mightyroy,16,0,0,0,0,0,"with tf.variable_scope(conv reuse=true x tf.get_variable(w, ) the above code cannot recognize an existing variable but clearly the existing variable was created before as print out of the variable name shows conv/w: i get an error when using tf.get_variable: valueerror under-sharing variable conv/w does not exist disallowed did you mean to set reuse=none in varscope? if set to reuse=none... with tf.variable_scope(conv reuse=none x tf.get_variable(w, ) then it creates another variable with name conv/w_: its a bug now i have variables with names conv/w and conv/w"
136722135,1300,https://api.github.com/repos/tensorflow/tensorflow/issues/1300,sesse,16,0,0,0,0,0,this is a tracking bug for adding support for the half type aka float or fp in tensorflow half computation is supported by gpus only although newer intel cpus haswell and newer have support for converting back and forth between fp and fp in hardware fc cuda has some support for half since although its a bit cumbersome its not a first-class type but relies on macros containing asm statements effectively intrinsics).fp is interesting for two primary reasons it would allow us to fit twice as large models in available gpu ram and it reduces memory bandwidth use a precious resource on the gpu the next generation of nvidia gpus pascal will also be able to do computation directly on two half-floats in a simd-like structure as fast as on a single float although that would be somewhat more intrusive in code.it is not clear exactly how much of tensorflow we need fp support for interested parties are asked to comment cublas and cudnn already has some support for half in their latest versions so it would be natural to provide those interfaces and eigen also has beginning support
136115650,1276,https://api.github.com/repos/tensorflow/tensorflow/issues/1276,peterbraden,1,0,0,0,0,0,i wanted to make a tracking issue for the deep dream tutorial as mentioned as coming soon on this page very interested to see this
135856604,1258,https://api.github.com/repos/tensorflow/tensorflow/issues/1258,rekhajoshm,21,0,0,0,0,0,create a way to control log message output most options should be passed via configproto or another proto maybe add a logoptions structure to hold options related to logging environment infooperating system mac feature request have a way to control logs that you record.as in gflags/re flags.refer what have you tried
134883730,1197,https://api.github.com/repos/tensorflow/tensorflow/issues/1197,tomrunia,1,0,0,0,0,0,for some reason the variable initialization of tensorflow takes a very long time when opencv is imported import cv running the operation tf.init_all_variables takes less than second without opencv but as soon as i import cv this takes seconds see attachment for the simple hello world code that i run hello_world.txt this problem occurs in tensorflow and during the initialization the cpu load is gpu is idle but all available memory is initialized by tensorflow. i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally.. tensorflow is installed in an virtualenv using pip after installing cuda toolkit and cudnn v my opencv installation is from sources github master so this is version some possibly relevant installation parameters are with_cuda=on with_cublas=off .i am using a powerful machine running linux mint cpu cores and an titan x
134187927,1136,https://api.github.com/repos/tensorflow/tensorflow/issues/1136,ghost,9,0,0,0,0,0,there is no convd method and it would be very welcome.or is convd implemented in such a way that h w channels would work with no issues and no compute penalties?it might even be a good idea to implement an arbitrary convxd function there are instances where high dimensional inputs have spatial relevance
133810341,1108,https://api.github.com/repos/tensorflow/tensorflow/issues/1108,fxsuper,3,0,0,0,0,0,id like to request support for basic trigonometric ops nothing fancy but just simple stuff like tan and the arc complement arcsin arctan etc all thats there right now is sin and cos
133547223,1095,https://api.github.com/repos/tensorflow/tensorflow/issues/1095,carpedm20,3,0,0,0,0,0,i know about tf.py_func which i can use a python code as an op but how gradient is calculated with it?also is there anyway i can set the gradient only with python i think one way might be adding a custom op that calculate gradients into an array and use the array as an argument of apply_gradients .for example: x tf.variable this is an custom opy tf.variable(.)loss x yopt tf.train.gradientdescentoptimizer(.) (y_grad y_val opt.compute_gradients(loss y )clipped_grads_and_vars x y x y_grad y_val custom gradient is x yoptim opt.apply_gradients(clipped_grads_and_vars) is this safe way to apply custom gradients only with python
133111577,1066,https://api.github.com/repos/tensorflow/tensorflow/issues/1066,nlgranger,2,0,0,0,0,0,using gcc with cuda support results in a compilation error: info from compiling tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc omitting warnings/usr/lib/gcc/x_-unknown-linux-gnu/../include/mwaitxintrin.h error identifier builtin_ia_monitorx is undefined/usr/lib/gcc/x_-unknown-linux-gnu/../include/mwaitxintrin.h error identifier builtin_ia_mwaitx is undefined errors detected in the compilation of tmp/tmpxft_a_-_adjust_contrast_op_gpu.cu.cpp.ii. build process is as follows tf_unofficial_setting configure eof/usr/bin/pythony./opt/cuda../opt/cuda.eof bazel build jobs config=cuda c opt verbose_failures tensorflow/tools/pip_package:build_pip_package the environnement is bazel cuda cudnn gcc boost is it even relevant python usr/bin/python is python)this issue might be related to boost or gcc as mentioned here or there suggested fix is to restrict gcc to ansi c which can be achieved like so diffdiff git a/third_party/gpus/crosstool/crosstool b/third_party/gpus/crosstool/crosstoolindex dfdecd..f a/third_party/gpus/crosstool/crosstool b/third_party/gpus/crosstool/crosstool toolchain use std=c for nvcc for consistency force both the host compiler and the device compiler to use std=c cxx_flag std=c cxx_flag d_mwaitxintrin_h_included cxx_flag d__strict_ansi linker_flag lstdc linker_flag b/usr/bin/ (btw thank you vrv for telling which file to change).could anyone please kindly review this and integrate it or not note that i am not quite sure wether both flags are actually required and what the side effects might be
133047835,1062,https://api.github.com/repos/tensorflow/tensorflow/issues/1062,DSLituiev,1,0,0,0,0,0,are there any plans to include python support incl binaries and dedicated matmul operator
132415754,1029,https://api.github.com/repos/tensorflow/tensorflow/issues/1029,mackcmillion,0,0,2,0,0,0,it looks like the op tf.image.resize_images has no effect on the image shape when the parameters new_height and new_width are dynamically computed this is possible since the fix of issue for example this piece of code shape tf.shape(image)height shape width shape new_shorter_edge tf.constant dtype=tf.int)height_smaller_than_width tf.less_equal(height width)new_height_and_width tf.cond height_smaller_than_width lambda new_shorter_edge compute_longer_edge(height width new_shorter_edge lambda compute_longer_edge(width height new_shorter_edge new_shorter_edge))image tf.image.resize_images(image new_height_and_width new_height_and_width )image tf.print(image tf.shape(image height width new_height_and_width new_height_and_width )return tf.image.random_crop(image throws the following exception: i tensorflow/core/kernels/logging_ops.cc w tensorflow/core/common_runtime/executor.cc xfc compute status failed precondition width must be target_width width target_width so as you can see in the first line of the printout the shape of the image is not affected by tf.image.resize_images is this a bug or am i doing something wrong?---i also made up a workaround for this: image tf.expand_dims(image image tf.image.resize_bilinear(image tf.pack(new_height_and_width))image tf.squeeze(image image tf.print(image tf.shape(image height width new_height_and_width new_height_and_width )return tf.image.random_crop(image for example prints i tensorflow/core/kernels/logging_ops.cc
130556593,956,https://api.github.com/repos/tensorflow/tensorflow/issues/956,cesarsalgado,10,0,0,0,0,0,i would like to have other options of padding for tf.pad and convolution ops.some types that come to my mind right now reflect constant value other than zero maybe implement other options of numpy.pad
130514566,953,https://api.github.com/repos/tensorflow/tensorflow/issues/953,caisq,1,0,0,0,0,0,on a linux machine with a gpu with the gpu configuration i ran the following command bazel test c opt config=cuda tensorflow/python:framework_function_test then i got the following error which seems to be similar to the closed issues im using the latest code of the master branch tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc couldnt open cuda library libcurand.so ld_library_path i tensorflow/stream_executor/cuda/cuda_rng.cc unable to load curand dso.i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc successful numa node read from sysfs had negative value but there must be at least one numa node so returning numa node zeroi tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx major minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size kibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size mibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc creating bin of max chunk size gibi tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc allocating gib bytes.i tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc gpu memory begins at xa extends to xec...i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx pci bus id e tensorflow/stream_executor/cuda/cuda_driver.cc failed to enqueue async memcpy from device to host cuda_error_invalid_value host dst xfec gpu src xd size x f tensorflow/core/common_runtime/gpu/gpu_util.cc gpu->cpu memcpy failedexternal/bazel_tools/tools/test/test-setup.sh line aborted core dumped
129663421,923,https://api.github.com/repos/tensorflow/tensorflow/issues/923,auroua,5,0,0,0,0,0,when i test cifar_input_test.py (tensorflow/tensorflow/models/image/cifar/cifar_input_test.py).i got the following information how could i solve this error. w tensorflow/core/common_runtime/executor.cc xfeea compute status out of range fifoqueue fifo_queue is closed and has insufficient elements requested current size node readerread readerread _device=/job:localhost/replica:/task:/cpu: (fixedlengthrecordreader fifo_queue
129448113,916,https://api.github.com/repos/tensorflow/tensorflow/issues/916,xxxzhi,2,0,0,0,0,0,i want use my pylearn dataset code with tensorflow but when i add from pylearn.datasets.dense_design_matrix import defaultviewconverter into mnist convolutional.py and run it in my server i get this errors: f tensorflow/stream_executor/cuda/cuda_driver.cc current context was not created by the streamexecutor cuda_driver api xe a cuda runtime call was likely performed without using a streamexecutor contextaborted core dumped) my server is gtx with core cpu i will get that errors in my server but i can run it in my desktop pc with gtx/gtx and core cpu and i dont get any errors all the cuda version is same in my machices.does anyone can help me the full traceback is: i tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locallyusing gpu device geforce gtx cnmem is disabled)/home/b/sharecache/houzhi/datatrain-images-idx-ubyte.gz/home/b/sharecache/houzhi/datatrain-labels-idx-ubyte.gz/home/b/sharecache/houzhi/datatk-images-idx-ubyte.gz/home/b/sharecache/houzhi/datatk-labels-idx-ubyte.gzextracting home/b/sharecache/houzhi/data/train-images-idx-ubyte.gzextracting home/b/sharecache/houzhi/data/train-labels-idx-ubyte.gzextracting home/b/sharecache/houzhi/data/tk-images-idx-ubyte.gzextracting home/b/sharecache/houzhi/data/tk-labels-idx-ubyte.gzi tensorflow/core/common_runtime/local_device.cc local device intra op parallelism threads f tensorflow/stream_executor/cuda/cuda_driver.cc current context was not created by the streamexecutor cuda_driver api xe a cuda runtime call was likely performed without using a streamexecutor contextaborted core dumped
128372209,859,https://api.github.com/repos/tensorflow/tensorflow/issues/859,Russell91,14,0,0,0,0,0,when using ipython notebooks one frequently runs the same cell without restarting the kernel if the cell contains code of the form: x tf.get_variable(x the cell will run correctly the first time but throw an error the second obviously this error is appropriate in python files but really just gets in the way for notebooks it would be great if something like tf.clear_all_variables were implemented such that i could have a cell with contents: tf.clear_all_variables()x tf.get_variable(x that i could run over and over again on a larger scale tensorflow seems to be rather unfriendly to notebooks in general frequently requiring kernel restarts for small changes but this issue for definitely contributes to the greatest number of forced kernel restarts for me
128253380,843,https://api.github.com/repos/tensorflow/tensorflow/issues/843,thompsonb,1,0,0,0,0,0,as of dbdaefcffbc im getting the error below when i try to build from source is my bazel out of date or something error home/bjt/git_repos/tensorflow/tensorflow/cc/build first argument of load is a path not a label it should start with a single slash if it is an absolute path..error home/bjt/git_repos/tensorflow/tensorflow/cc/build first argument of load is a path not a label it should start with a single slash if it is an absolute path..error home/bjt/git_repos/tensorflow/tensorflow/cc/build file tensorflow:tensorflow.bzl.bzl was not correctly loaded make sure the load statement appears in the global scope in your file.error home/bjt/git_repos/tensorflow/tensorflow/cc/build file tensorflow:tensorflow.bzl.bzl was not correctly loaded make sure the load statement appears in the global scope in your file.error home/bjt/git_repos/tensorflow/tensorflow/cc/build traceback most recent call last file home/bjt/git_repos/tensorflow/tensorflow/cc/build line cc_library(name cc_op_gen_main srcs ops/cc_op_gen.cc ops/cc_op_gen_main.cc hdrs ops/cc_op_gen.h copts tf_copts deps tensorflow/core:framework file home/bjt/git_repos/tensorflow/tensorflow/cc/build line in cc_library tf_coptsname tf_copts is not defined.error home/bjt/git_repos/tensorflow/tensorflow/cc/build name tf_gen_op_wrappers_cc is not defined.error home/bjt/git_repos/tensorflow/tensorflow/cc/build traceback most recent call last file home/bjt/git_repos/tensorflow/tensorflow/cc/build line cc_binary(name tutorials_example_trainer srcs tutorials/example_trainer.cc copts tf_copts linkopts lpthread lm deps cc_ops tensorflow/core:kernels tensorflow/core:tensorflow file home/bjt/git_repos/tensorflow/tensorflow/cc/build line in cc_binary tf_coptsname tf_copts is not defined.error no such target tensorflow/cc:tutorials_example_trainer target tutorials_example_trainer not declared in package tensorflow/cc defined by home/bjt/git_repos/tensorflow/tensorflow/cc/build.info elapsed time s bazel versionbuild label build target bazel-out/local_linux-fastbuild/bin/src/main/java/bazel-main_deploy.jarbuild time thu oct build timestamp build timestamp as int
128243150,842,https://api.github.com/repos/tensorflow/tensorflow/issues/842,cesarsalgado,47,0,0,0,0,0,i would like tensorflow to have an official tool similar to this would be helpful to check if our cnns were learning useful features to debug and to have better insights.check this video
127390667,808,https://api.github.com/repos/tensorflow/tensorflow/issues/808,mcuadros,1,0,0,0,0,0,running b.gcr.io/tensorflow/tensorflow:latest-gpu having cuda installed on the host when i try to create the session it returns cuda_error_no_device and was unable to find libcuda.so dso loaded into this program but when the strange thing is that when the module is imported all the libraries are loaded correctly log: root@beb pythonpython default jun gcc on linuxtype help copyright credits or license for more information import tensorflow as tfi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcublas.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcudnn.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcufft.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcuda.so locallyi tensorflow/stream_executor/dso_loader.cc successfully opened cuda library libcurand.so locally with tf.session as sess with tf.device(/gpu matrix tf.constant matrix tf.constant product tf.matmul(matrix matrix sess.run(product i tensorflow/core/common_runtime/local_device.cc local device intra op parallelism threads e tensorflow/stream_executor/cuda/cuda_driver.cc failed call to cuinit cuda_error_no_devicei tensorflow/stream_executor/cuda/cuda_diagnostics.cc retrieving cuda diagnostic information for host bebi tensorflow/stream_executor/cuda/cuda_diagnostics.cc hostname bebi tensorflow/stream_executor/cuda/cuda_diagnostics.cc libcuda reported version is not found was unable to find libcuda.so dso loaded into this programi tensorflow/stream_executor/cuda/cuda_diagnostics.cc driver version file contents nvrm version nvidia unix x kernel module mon nov pst gcc version gcc version red hat gcc i tensorflow/stream_executor/cuda/cuda_diagnostics.cc kernel reported version is i tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/direct_session.cc direct session inter op parallelism threads traceback most recent call last file stdin line in module file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run results self._do_run(target_list unique_fetch_targets feed_dict_string file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run e.code)tensorflow.python.framework.errors.invalidargumenterror cannot assign a device to node const could not satisfy explicit device specification gpu node const const dtype=dt_float value=tensor
126842425,783,https://api.github.com/repos/tensorflow/tensorflow/issues/783,joshburkart,7,0,0,0,0,0,pythonin tf.gradients(tf.constant tf.variable())out none the derivative of with respect to x should be
126817716,781,https://api.github.com/repos/tensorflow/tensorflow/issues/781,mostafachatillon,23,0,0,0,0,0,i would like to rotate an image from a random angle for data augmentation but i dont find this transformation in the tf.image module.see
126573831,769,https://api.github.com/repos/tensorflow/tensorflow/issues/769,NHDaly,1,0,0,0,0,0,the end of the first tutorial mnist for ml beginners has this paragraph what matters is that we learned from this model still if youre feeling a bit down about these results check out the next tutorial where we do a lot better and learn how to build more sophisticated models using tensorflow!but the next tutorial is deep mnist for experts and it does not explain things nearly as well as the first tutorial for example the second tutorial starts differing at the build a multilayer convolutional network section but the very first paragraph under weight initialization does not explain the concepts it is using to create this model were going to need to create a lot of weights and biases one should generally initialize weights with a small amount of noise for symmetry breaking and to prevent gradients since were using relu neurons it is also good practice to initialize them with a slightly positive initial bias to avoid dead neurons instead of doing this repeatedly while we build the model lets create two handy functions to do it for us.what are relu neurons why are we using them what is a convolutional network even?i would say either this tutorial should be expanded to clarify/explain things better for beginners or this tutorial should be moved to later in the list of tutorials and the first tutorial mnist for beginners should not say the next tutorial but instead say a later tutorial on mnist for experts .thanks
125890996,739,https://api.github.com/repos/tensorflow/tensorflow/issues/739,ExonRen,9,0,0,0,0,0,ive been working on transplanting fast r-cnn to a tensorflow version and i came across problems as below in roi_pooling_layer.cpp the c implementation of roi pooling layer a dynamic variable num_rois is used to control a loop to process each roi of each picture its value is stored in the original dataset and will only be valid during the graph is running however since the graph in tensorflow has to be settled before it being run i dont know what to put in the loop control as the iteration limit after the roi_pooling_layer there follows a innerproduct layer in caffe the shape of whose input depends on the output shape of the roi_pooling_layer which ultimately depends on num_rois so the shape of the corresponding weight_variable and bias_variable for the following operation matmul which is going to act as the innerproduct layer cannot be settled before the graph is running.in addition in the roi pooling layer there are some other loop control variables based on the former calculation like hstart hend wstart and wend they are even harder to get valid values before the graph is built because the num_rois is part of the input after all.so may i ask if tensorflow supports fast r-cnn currently or are there any possible specific tips to solve these problems?thank you
125463134,720,https://api.github.com/repos/tensorflow/tensorflow/issues/720,ghost,3,0,0,0,0,0,will you make it possible to install and update it by distributions package systems deb rpm you can make an official repository.will you make it possible to install a tensorflow-devel package which gives acces to c headers to make programs which link to tensorflow
124740366,675,https://api.github.com/repos/tensorflow/tensorflow/issues/675,zackchase,32,0,0,0,0,7,currently if you call gradients(ys xs it will return the sum of dy/dx over all ys for each x in xs i believe this doesnt accord with an a priori mathematical notion of the derivative of a vector id like the way to take the derivative of ys wrt xs where both are vectors and have a jacobian matrix returned by extension id like to take the derivative of a vector wrt a matrix and get back a tensor there doesnt seem to be a convenient tensorflow function to compute the jacobian or higher order derivatives am i missing something or is this functionality that we could add
124656693,672,https://api.github.com/repos/tensorflow/tensorflow/issues/672,Jabberwockyll,12,0,0,0,0,0,motivated by this stackoverflow question to george dahls answer there is currently no way to run part of a graph then later run the entire graph without recomputing that part of the graph couldnt you always just run the whole thing in the first place you ask consider this scenario context/exampleyoure using tensorflow to implement a reinforcement learning agent with value function approximation trained using stochastic gradient descent youre agent contains a method that is called once every iteration/timestep in the experiment the method takes an obervation and reward as input and outputs an action consider this sequence of events say youre doing q-learning at the end of an iteration you compute your state-action values to choose an action and then return control to the calling program to simulate a step in the environment at the beginning of the next iteration its time to update the parameters you want to use tensorflows optimizer class to automatically calculate the gradients here you have three options just recompute the state-action values so you can use the optimizer this option leaves you inefficiently calculating part of the graph twice this also might not even be viable if you were using something like recurrent neural networks where calculating the activations changes the state of the nodes hardcode the gradients you could just write a function to calculate the gradients yourself and not even use optimizer however if you were experimenting with different network architectures and activation functions in a big convolutional network this could get pretty cumbersome call everything else from within your agent this sacrifices a lot of modularity and neatness in any framework bigger than a small experiment.so in this case the problem is that your graph learning with sgd depends on a value observation/reward calculated externally that depends on the result of part of the graph state-action values and we cant save the state-action values as variables to feed later as placeholders because then they wouldnt be associated with the operations that produced them which are required for automatic differentiation solutionim not yet very familiar with the inner workings of tensorflow so im not sure what the best solution would be it would need to be able to give optimizer the tensors already calculated and the graph with the operations that produced them maybe some kind of session.partial_run that saves the computed tensors associated with their nodes until a normal run is done maybe a new method under optimizer that returns an operation to calculate gradients depending on a placeholder what do you think
124531615,664,https://api.github.com/repos/tensorflow/tensorflow/issues/664,ville-k,16,0,0,0,0,0,building with cuda support on os x requires gnu coreutils due to the os x native readlink command behaving differently from the gnu version you can install it using homebrew brew install coreutils os x cuda builds against cuda toolkit v to overcome host compiler incompatibility the toolkit versions cuda cudnn are now controlled by variables set in the configure script cuda/platform.bzl file is dynamically generated by the configure script to overcome bazel limitations for using select to set the platform specific names and paths of cuda libraries se_static_thread_local_pod now uses thread instead of thread_local which is not yet supported by the version of clang shipped by apple thread supports only primitive types but is more performant updates eigen to a newer version that fixes a clang compilation error
124367200,654,https://api.github.com/repos/tensorflow/tensorflow/issues/654,mfederico,9,0,0,0,0,0,hi im wondering which steps are necessary to move from the greedy decoder currently implemented to an actual beam search decoder is this enhancement already in someones roadmap if not could anyone tell me which is the right point in the code where to add this functionality thanks a lot marcello
124288801,650,https://api.github.com/repos/tensorflow/tensorflow/issues/650,MarkDaoust,2,0,0,0,0,0,tensorboards graph tab works fine in chrome and safari but does not render in firefox. a few other people have reported this issue
124267636,647,https://api.github.com/repos/tensorflow/tensorflow/issues/647,sunformoon,4,0,0,0,0,0,hi all i first successfully installed the tensorflow following the instructions of pip installation however i couldnt import tensorflow in python thanks a lot!!the error comes like this:zhuotun@sunformoon pythonpython anaconda bit default dec gcc red hat on linuxtype help copyright credits or license for more information.anaconda is brought to you by continuum analytics.please check out and import tensorflow as tf traceback most recent call last file stdin line in module importerror no module named tensorflowi double check that i have installed successfully the tensorflow by re-installing the tensorflow as following:zhuotun@sunformoon sudo pip install upgrade password for zhuotun downloading/unpacking downloading tensorflow-..-cp-none-linux_x_.whl mb mb downloadedrequirement already up-to-date six in usr/local/lib/python./dist-packages from tensorflow==..)requirement already up-to-date protobuf==..a in usr/local/lib/python./dist-packages from tensorflow==..)requirement already up-to-date wheel in usr/local/lib/python./dist-packages from tensorflow==..)requirement already up-to-date numpy in usr/local/lib/python./dist-packages from tensorflow==..)requirement already up-to-date setuptools in usr/local/lib/python./dist-packages from protobuf==..a->tensorflow==..)installing collected packages tensorflowsuccessfully installed tensorflow
124017056,634,https://api.github.com/repos/tensorflow/tensorflow/issues/634,wchan,7,0,0,0,0,0,when using the adamoptimizer does the checkpoint save the state of the adamoptimizer if yes how do you clear the state i.e restart the adamoptimizer but keep the model weights from the checkpoint if no how do you maintain the adam state question can be applied to adagrad as well
123918277,622,https://api.github.com/repos/tensorflow/tensorflow/issues/622,vinhqdang,5,0,0,0,0,0,hello everyonei am using anaconda on mac os i created a new environment with anaconda bashconda create n tensorflow python then tried to install tensorflow bashpip install upgrade i faced an error bashinstalling collected packages six setuptools protobuf numpy tensorflow found existing installation setuptools cannot remove entries from nonexistent file users/qdang/anaconda/envs/tensorflow/lib/python./site-packages/easy-install.pth how could i install tensorflow with anaconda python btw i have cuda nvidia m with my macbook how could i utilize its power other than running only on cpu mode?thanks
123856832,615,https://api.github.com/repos/tensorflow/tensorflow/issues/615,leftstone2015,6,0,0,0,0,0,hi tfscurrent im trying prediction in c following the tutorial in tensorflow/tensorflow/gdoc/tutorials/image_recognition/index.md.i want to train the mode in python using interface tf.train.write_graph to write the graph to a file and then loading it in the c for predition just like the above tutorial because there is no python codes in the tutorial and no guides to create the file of tensorflow_inception_graph.pb.ive tried several times but failed could you share a total example in both sides in pyhton and c
123691291,600,https://api.github.com/repos/tensorflow/tensorflow/issues/600,marcotrombetti,5,0,0,0,0,0,the output i am getting from the translate.py tutorial looks corrupted.hello gi followed the translate tutorial here wanted to create a small test model limiting to m recordspython translate.py data_dir data train_dir train en_vocab_size fr_vocab_size size num_layers steps_per_checkpoint max_train_data_size=no errors during the training.after hours i stopped the training when the perplexity was below points.global step learning rate step-time perplexity eval bucket perplexity eval bucket perplexity eval bucket perplexity eval bucket perplexity after stopping the training i deleted the last corrupted training file.rm train/translate.ckpt-if i try to translate simple words i get junk.python translate.py decode data_dir data train_dir traincreated model with fresh parameters hellog processing processing processing processing processing processing processing processing processing houseg g pturage dinfrastructures dinfrastructures twin twin twin twin twin who is the president of the united states?expdies expdies expdies m m m m m m m m m m m mother translation technologies es moses will provide a decent translation with that training data.if there a glitch somewhere or did i do something wrong
123427730,586,https://api.github.com/repos/tensorflow/tensorflow/issues/586,DSLituiev,2,0,0,0,0,0,id like to suggest numpy-like shape read-only property for tf.tensor something like: @propertydef shape(self return tuple(self.get_shape().as_list
123338794,582,https://api.github.com/repos/tensorflow/tensorflow/issues/582,ry,2,0,0,0,0,0,i just upgraded tf to bddbbbafddeedbcbdfce and im now getting an error importing the inception v model provided at didnt have this problem before the upgrade here is the error traceback most recent call last file test.py line in module graph_def.parsefromstring(filecontent)google.protobuf.message.decodeerror error parsing message and this is the code im using pythonimport tensorflow as tfwith open(tensorflow_inception_graph.pb mode=rb as f filecontent f.read()graph_def tf.graphdef()graph_def.parsefromstring(filecontent)input_layer tf.placeholder(float tf.import_graph_def(graph_def input_map mul input_layer
123139020,566,https://api.github.com/repos/tensorflow/tensorflow/issues/566,davefairtex,9,0,0,0,0,0,i am using tf with python on both ubuntu linux and osx tf base functionality is working fine the issue is logging i want to suppress the following messages that are emitted whenever i construct the first tensorflow session:i tensorflow/core/common_runtime/local_device.cc local device intra op parallelism threads i tensorflow/core/common_runtime/direct_session.cc direct session inter op parallelism threads they do not appear to be transmitted through the normal python logging mechanism any ideas did i just miss something?i dont want to turn off all log messages i just want tf to stop generating these two specific messages and perhaps if you can tell me how to turn off all info messages from this particular seemingly non-pythonic source that would be fine too
122681901,530,https://api.github.com/repos/tensorflow/tensorflow/issues/530,kentonl,1,0,0,0,0,0,tensorboard tries to load a file here tensorboard/tag is missing from the pip installation so it produces the following message: warning:tensorflow:ioerror errno no such file or directory usr/local/lib/python./dist-packages/tensorflow/tensorboard/tag on path usr/local/lib/python./dist-packages/tensorflow/tensorboard/tagwarning:tensorflow:unable to read tensorboard tagstarting tensorboard on port you can navigate to does not render any logged events
121854458,491,https://api.github.com/repos/tensorflow/tensorflow/issues/491,hardmaru,60,0,0,0,0,0,hi teamcurrently ive been doing some work with tensorflow some of my work on and have been developing everything a macbook pro running the ipython stack my mac is fairly fast for regular sized tasks and even has an nvida chip nvidia geforce gt m mb although the current build doesnt support gpus for macs it seems to me from reading earlier threads that it is not possible to custom build for gpu support on the mac despite having cuda libraries installed.this would really help out my work flow especially i can get other frameworks to utilize my gpu cuda on the mac the macbook pro is a great machine to develop use and develop stuff on and lots of people i know also use mbps to develop ml algorithms and then send jobs off to aws for the heavy-lifting after it works locally.i think having gpu support for mac would really help in not just the performance front but allows us to debug and check any gpu specific issues when running some script locally and be able to fix them quickly before sending them to a ec or some remote gpu server it would make the workflow a lot smoother for developers who use macbook pros with the nvidia chip.thanks in advance
121630790,476,https://api.github.com/repos/tensorflow/tensorflow/issues/476,sirinath,4,0,0,0,0,0,please make sure all the functionality is defined in the c core
121047783,446,https://api.github.com/repos/tensorflow/tensorflow/issues/446,zfrenchee,4,0,0,0,0,0,this would be a great addition to tensorflow and is conspicuously missing is there some specific reason its missing or is it in the works
120778530,428,https://api.github.com/repos/tensorflow/tensorflow/issues/428,vade,1,0,0,0,0,0,helloforgive me if ive missed something obvious new to tensorflow machine learning.i notice there are image creation functions which deal with jpg and png image creation via parsing those formats.is it planned to support uncompressed images client applications may want to decompress video frames and use them for analysis.perhaps i am missing a way to construct a tensor from an uncompressed byte array how would one handle some video optimization for byte alignment stride etc if there is a more appropriate place to ask beginner questions i am happy to take the conversation there thank you
120316357,406,https://api.github.com/repos/tensorflow/tensorflow/issues/406,zfrenchee,2,0,0,0,0,0,seems like there are only two multiplication operations available tf.mul which is element-wise and tf.matmul which performs regular d matrix multiplication am i missing something
119767099,390,https://api.github.com/repos/tensorflow/tensorflow/issues/390,skearnes,2,0,0,0,0,0,fyi i am working with example protos for model input and i am learning that use tf.parse_example to parse a shuffled batch of serialized examples is much faster than using tf.parse_single_example prior to batching for my particular dataset using parse_single_example allows me to create feed_dict s with batch size at about min batching the serialized example protos and then using parse_example is running at around min.you may want to update the documentation to suggest using tf.parse_example everywhere as is suggested when using sparse input data
119750224,388,https://api.github.com/repos/tensorflow/tensorflow/issues/388,adamcrume,3,0,0,0,0,0,tensorflow should have a rust interface.original e-mail:id like to write rust bindings for tensorflow and i had a few questions first of all is anyone already working on this and if so can i lend a hand if not is this something the tensorflow team would be interested in i assume that the tensorflow team would not be willing to commit right now to supporting rust so i thought a separate open source project with the option to fold into the main project later would be the way to go
119674651,383,https://api.github.com/repos/tensorflow/tensorflow/issues/383,Honghe,5,0,0,0,0,1,cause training a model is time consuming so save a checkpoint on training but error occurred when to restore.the saver.restore says as follow: signature saver.restore(sess save_path)docstring:restores previously saved variables.this method runs the ops added by the constructor for restoring variables.it requires a session in which the graph was launched the variables torestore do not have to have been initialized as restoring is itself a wayto initialize variables.the save_path argument is typically a value previously returned from a save call or a call to latest_checkpoint() .args sess a session to use to restore the parameters save_path path where parameters were previously saved. so i used it as following: with tf.graph().as_default saver tf.train.saver sess tf.session saver.restore(sess mnist_data/-) but got the following err: valueerror traceback most recent call last)
119625302,380,https://api.github.com/repos/tensorflow/tensorflow/issues/380,cdluminate,3,0,0,0,0,0,the memory consumption for bootstraping bazel is very huge according to my observation.and i really have trouble building bazel on my machine because lacking of enough memory.so could you please provide an alternative traditional build system e.g make cmake or setup.py for users who insist on building software from source by themselves.thanks
119314029,372,https://api.github.com/repos/tensorflow/tensorflow/issues/372,hartsantler,2,0,0,0,0,0,of all why only one c tutorial?next it would be nice to have a readme in that folder that gives at least the bazel command to compile it or at least a note at the top of the source code how to compile it. example_trainer.cc shows off how to use multiple threads but i would rather start with a single threaded version for a simple helloworld tutorial.lines ugly macros tf_define_int when i first saw this it is not clear why this is even required or how it is used later on.lines the ugly macro injects flags prefix opts.num_concurrent_steps tensorflow::example::flags_num_concurrent_steps; the option settings could be just simple hard coded literals there is no need to complicate the hello world with macro magic
119036653,362,https://api.github.com/repos/tensorflow/tensorflow/issues/362,FangLinHe,4,0,0,0,0,0,on your get started website found a small piece of wrong code:in the sample code of feeds section: input tf.placeholder(tf.types.float)input tf.placeholder(tf.types.float) i tried them and python gave the attributeerror module object has no attribute typesafter reading the document think it should be tf.float instead of tf.types.float probably its a version issue
118893064,351,https://api.github.com/repos/tensorflow/tensorflow/issues/351,tp6vup54,1,0,0,0,0,0,after cloning tensorflow on my osx i tried to run label_image example in tensorflow/examples and followed the instructions.currently seems fine after built even some warnings were occured but no error.but when i typed bazel-bin/tensorflow/examples/label_image/label_imageit showed:e tensorflow/examples/label_image/main.cc not found failed to load compute graph at tensorflow/examples/label_image/data/googlenet_graph.pbi saw a file named tensorflow_inception_graph.pb in that folder and i tried to rename it to googlenet_graph.pb and executed again and it showed:i tensorflow/core/common_runtime/local_device.cc local device intra op parallelism threads i tensorflow/core/common_runtime/direct_session.cc direct session inter op parallelism threads segmentation fault then i dont know what i am supposed to try please show me a way to solve it thanks
118779289,348,https://api.github.com/repos/tensorflow/tensorflow/issues/348,andyyuan78,2,0,0,0,0,0,ub@ub-a:~/github/tensorflow bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp/tensorflow_pkgwed nov est using tmpdir tmp/tmp.lvoypriq/tmp/tmp.lvoypriq github/tensorflowwed nov est building wheelusage setup.py global_opts cmd cmd_opts cmd cmd_opts or setup.py help cmd cmd or setup.py help-commands or setup.py cmd helperror invalid command bdist_wheel
118501615,338,https://api.github.com/repos/tensorflow/tensorflow/issues/338,skearnes,2,0,1,0,0,0,is there any support for reading/writing compressed input files e.g with gzip i would like to use the standard file format but the uncompressed tfrecords files i am generating are huge for my datasets
118389643,335,https://api.github.com/repos/tensorflow/tensorflow/issues/335,panmari,8,0,0,0,0,0,it would be really neat if the tensorboard could additionally show the predicted and/or given label for an image in the images tab
118326365,329,https://api.github.com/repos/tensorflow/tensorflow/issues/329,shawnLeeZX,11,0,0,0,0,0,hi i have spent some time writing scripts to do stat in caffe and this let me know tensorboard is so amazing it is actually one of the major reasons that makes me switch to tensorflow quickly i just wonder could an option be added to plot more than two scalars in the same plot more specifically it could allow training loss and test loss being plotted together regularization loss and top level loss to be plotted together maybe adding accuracy on the right vertical axisjust name a few this would help a lot
118271007,324,https://api.github.com/repos/tensorflow/tensorflow/issues/324,mchirico,1,0,0,0,0,0,"it would be nice to have a complete end-to-end working example using tensorboard in addition some of the documentation appears to be incorrect.possible incorrect documentation width alt=screenshot src concise completely coded example may help note more detail is shown here but its still have hard to grasp the full picture a complete coded example may help python...sess.run(init build the summary operation based on the tf collection of summaries.tf.train.write_graph(sess.graph_def tenirissave/logsd,graph.pbtxt)tf.scalar_summary(accuracy tf_accuracy)tf.histogram_summary(weights tf_weight)tf.histogram_summary(bias tf_bias)tf.histogram_summary(softmax tf_softmax)tf.histogram_summary(accuracy tf_accuracy)summary_op tf.merge_all_summaries()summary_writer tf.train.summarywriter(./tenirissave/logs,sess.graph_def this will not work you need the full path tensorboard logdir=./tenirissave bad tensorboard logdir=$(pwd)/tenirissave good!...for i in range summary_str sess.run(summary_op,feed_dict={tf_in x_test tf_softmax_correct y_test_onehot summary_writer.add_summary(summary_str i) the complete example can be seen here the example needs to be cleaned up the point is to give the user a complete working example that can be run and verified"
118231597,322,https://api.github.com/repos/tensorflow/tensorflow/issues/322,pirate,1,0,0,0,0,0,generating the graph from python is not an expensive process it would greatly lower the barrier to entry if new users data scientists who are more comfortable with guis could modify graph parameters directly from tensorboard.if i understand correctly per a short tutorial from rsepassi you currently cant delete nodes from the graph without regenerating it however regenerating the graph takes seconds so it would be nice to allow creation of nodes and edges via the gui like cafe).i know this is a really high-level feature request and probably requires a fair amount of work feel free to close it if this would be better discussed on a mailing list instead of github issues
118186019,315,https://api.github.com/repos/tensorflow/tensorflow/issues/315,robert-zaremba,2,0,0,0,0,0,cffi is a big step towards standard practices for making and distributing python packages with c extension modules.with cffi the maintenance is much easier and the code is still fast.there are lot of benefits abstract the python version cpython cpython pypy better control over when and why the c compilation occurs and more standard ways to write distutils or setuptools-based setup.py files keep all the python-related logic in python so that you dont need to write much c code.im really looking forward to use pypy with tensorflow
118110091,312,https://api.github.com/repos/tensorflow/tensorflow/issues/312,alexatknit,26,0,0,0,0,0,im working on testing a few modifications to an existing network including attempting to use it as a teacher in a student/teacher network and swapping its optimizer but i get the following error: traceback most recent call last file knit_net_train_tf.py line in module saver.restore(sesh checkpoint_file file usr/local/lib/python./dist-packages/tensorflow/python/training/saver.py line in restore sess.run( self._restore_op_name self._filename_tensor_name save_path file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in run results self._do_run(target_list unique_fetch_targets feed_dict_string file usr/local/lib/python./dist-packages/tensorflow/python/client/session.py line in do_run e.code)tensorflow.python.framework.errors.notfounderror tensor name conv/biases/adagrad not found in checkpoint files knitnet..ckpt node save/restore_slice restoreslice dt=dt_float preferred_shard device=/job:localhost/replica:/task:/cpu: (_recv_save/const save/restore_slice_/tensor_name save/restore_slice_/shape_and_slice) it appears that the saver requires that every variable that exists in the current graph also be present in the checkpoint and that every variable that that exists in the checkpoint also be in the current graph i can work around this by making major modifications to the way that my network is constructed but i would be nice to have a feature that says dont error out if you dont find this variable its ok alternatively it appears that i can specify which variables i want to restore which could also solve the problem but i have a lot of variable that i havent explicitly tracked many of which are created by the optimizer it would make this feature a lot more useful if there were a function that did return all variables that this op depends on which i could call on the training variable or the prediction variable and only save those tensors
118037013,309,https://api.github.com/repos/tensorflow/tensorflow/issues/309,andrewcz,5,0,0,0,0,0,"is there a multivariate numerical time series example in tensorflow.many thanks,andrew"
117748892,288,https://api.github.com/repos/tensorflow/tensorflow/issues/288,vnkmr7620,11,0,0,0,0,0,how to sort a tensor argsort
117304361,252,https://api.github.com/repos/tensorflow/tensorflow/issues/252,FabHan,21,0,0,0,0,0,i know its not a priority and will be a long way to get there but making tf compatible with pypy woud be super cool.thoughts
116957577,225,https://api.github.com/repos/tensorflow/tensorflow/issues/225,FabHan,1,0,0,0,0,0,heres my code: merged_summary_op tf.merge_all_summaries()summary_writer tf.train.summarywriter(/tmp/mnist_logs sess.graph_def)for i in range batch mnist.train.next_batch sess.run(train_step feed_dict={x batch y batch if i train_accuracy sess.run(train_step feed_dict={x batch y batch summary_str sess.run(merged_summary_op feed_dict={x batch y batch summary_writer.add_summary(summary_str i) but got this error at line sess.run(merged_summary_op invalidargumenterror traceback most recent call last)
116896403,216,https://api.github.com/repos/tensorflow/tensorflow/issues/216,allentran,21,0,0,0,0,0,it would really help if matmul and element-wise mul were broadcastable like in numpy otherwise youre writing a bunch of boilerplate reshaping code.for example suppose i have a t x n x k and want to multiply it by a k x k and then to a max pool over t and then a mean pool over n to do this now i think you need to reshape do the matmul and then undo the reshape and then do the pooling
116869296,212,https://api.github.com/repos/tensorflow/tensorflow/issues/212,cinjon,2,0,0,0,0,0,is this possible im doing something where i need the outputs of multiple distinct models to be compared to do that im batching up inputs and running them over each model its unclear to me how i can do this in one session
116838032,208,https://api.github.com/repos/tensorflow/tensorflow/issues/208,sisp,2,0,0,0,0,0,"ive been wondering if there are plans to add symbolic loops to tensorflow because i feel like this is a major feature when it comes to variable length sequences finite unfolding with bucketing seems like a dirty hack to me and since as far as i understand tensorflow is meant for deployment too how do you envision using it for seqseq translation given that you dont know ahead of time how long the generated sequence will be?thanks,sigurd"
116827263,206,https://api.github.com/repos/tensorflow/tensorflow/issues/206,girving,49,0,0,0,0,9,we should make our slicing and assignment ops more general to capture more of the functionality of numpy slicing and add getitem sugar for all of it specifically we should have a dimensional set of ops with dimensions get vs set slice type and for the assignment ops the update op currently we have slice assign_update assign_add assign_sub gather scatter_update scatter_add scatter_sub we should also have assign_slice_update assign_slice_add assign_slice_sub both slicing and slice assignment should support strides with no performance cost if strides arent used ideally the slice ops should support negative indexing a la python since the slice parameters are already cpu this is implementable with near zero cost the unfortunate bit is that since we picked the wrong format for specifying ranges start length instead of start end negative indexing might be awkward thus it might be best left to a separate bug support numpy-like boolean indexing generalize gather and scatter to take an array of input index tensors efficiently broadcast them and do multidimensional indexing similar to numpy make getitem provide sugar for all of the above ideally wed have something idiomatically similar at least to setitem but this is problematic since the returned assignment op is important to have setitem does not return a value and the nice range sugar is available only inside indexing assignment calls.@ebrevdo im assigning this to you for now since you might get to it first but feel free to grab only the piece of it that you need for now
116554958,175,https://api.github.com/repos/tensorflow/tensorflow/issues/175,alquraishi,9,0,0,0,0,5,numpy has einsum which is very useful for formulating and computing tensor operations efficiently any plans to include an equivalent in tensorflow
116500690,170,https://api.github.com/repos/tensorflow/tensorflow/issues/170,lydhr,1,0,0,0,0,0,when i get started with train your first tensorflow neural net model python tensorflow/models/image/mnist/convolutional.py there is an error: file tensorflow/models/image/mnist/convolutional.py line in extract_data bytestream.read file library/frameworks/python.framework/versions/./lib/python./gzip.py line in read self._read(readsize file library/frameworks/python.framework/versions/./lib/python./gzip.py line in read self._read_gzip_header file library/frameworks/python.framework/versions/./lib/python./gzip.py line in read_gzip_header raise ioerror not a gzipped file
116428689,152,https://api.github.com/repos/tensorflow/tensorflow/issues/152,nouiz,4,0,0,0,0,0,when i set the nvidia driver in exclusive mode and one of the gpu is already used by another process i get a segmentation fault python c import tensorflow as tf;tf.interactivesession()i tensorflow/core/common_runtime/local_device.cc local device intra op parallelism threads segmentation fault core dumped) if i limit the visible gpus to only gpus that have nothing running on them it dont segfault: $cuda_visible_devices python c import tensorflow as tf;tf.interactivesession()i tensorflow/core/common_runtime/local_device.cc local device intra op parallelism threads i tensorflow/core/common_runtime/gpu/gpu_init.cc found device with properties name geforce gtx titan xmajor minor memoryclockrate ghz pcibusid total memory gibfree memory gibi tensorflow/core/common_runtime/gpu/gpu_init.cc dma i tensorflow/core/common_runtime/gpu/gpu_init.cc y i tensorflow/core/common_runtime/gpu/gpu_device.cc creating tensorflow device gpu device name geforce gtx titan x pci bus id i tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc setting region size to i tensorflow/core/common_runtime/local_session.cc local session inter op parallelism threads here is my output of nvidia-smi nvidia-smi wed nov nvidia-smi driver version gpu name persistence-m bus-id disp.a volatile uncorr ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m geforce gtx off on n/a n/a c p w w mib mib default geforce gtx tit off off n/a c p w w mib mib e process geforce gtx tit off off n/a c p w w mib mib e process geforce gtx tit off a off n/a c p w w mib mib e process processes gpu memory gpu pid type process name usage g usr/bin/x mib c python mib c python mib i suppose that in the code that check the available gpus it dont handle correctly a case when one of the gpus cant be used
116408949,150,https://api.github.com/repos/tensorflow/tensorflow/issues/150,sasadep,17,0,0,0,0,0,is there or will there be a d convnets support in tensorflow which helps for spatiotemporal features
116320676,136,https://api.github.com/repos/tensorflow/tensorflow/issues/136,ywatanabex,57,0,0,0,0,0,im using gpu(gtx with cuda-.&cudnnv on ubuntu i have gone through mnist tutorial was going well except for the last two lines pythonprint test accuracy g%accuracy.eval(feed_dict x mnist.test.images y mnist.test.labels keep_prob executing these lines i got an error pythonresourceexhaustederror oom when allocating tensor with shapedim size dim size dim size dim size i think basic reason for this error is that test data can not be allocated to gpu device.is this a bug or not are there good way to avoid this issue
116236573,110,https://api.github.com/repos/tensorflow/tensorflow/issues/110,trungnt13,10,0,0,0,0,0,many clusters system using module with redhat or centos which is glibc since bazel requires glibc and the prebuilt version for linux requires glibc it is hopeless to make tensorflow run on clusters.referred to this issue reported on bazel
116139917,97,https://api.github.com/repos/tensorflow/tensorflow/issues/97,vkuznet,41,0,0,0,0,0,hi can someone either point to code example or documentation how to extract final predictions after the training the model for example it would be nice to complement existing tutorials e.g mnist and show additional final step to get prediction out of the trained model
116127740,94,https://api.github.com/repos/tensorflow/tensorflow/issues/94,resouer,0,2,0,0,0,0,gerrit review workflow is very hard to maintain and follow a lot of project abandoned it and turns to github finally.really hope tensorflow will use github for all the cooperation workflow google kubernetes and docker are all excellent examples for how to use github maintain a huge and global scale open source project successfully.github travis ci slack is awesome just make everything be a pr
116097497,92,https://api.github.com/repos/tensorflow/tensorflow/issues/92,ghost,4,0,0,0,0,0,i can see there is only python and c api do we have any plans for java api or workarounds
116063868,86,https://api.github.com/repos/tensorflow/tensorflow/issues/86,abhishekpatnia,1,0,0,0,0,0,loads of operations in the embedding example are not supported on gpu some documentation detailing this would be good i tried to get around the problem by using tf.configproto(allow_soft_placement=true however now i get executor failed to create kernel invalid argument attrvalue must not have reference type value of float_ref for attr tensor_typethanksabhishek
116028395,75,https://api.github.com/repos/tensorflow/tensorflow/issues/75,udaysinghcode,0,1,0,0,0,0,currently this is my output after installing tensorflow i thought it installed six as well but was wondering why copyreg doesnt exist in six anyone know what to do?tensorflow python python default sep gcc compatible apple llvm clang on darwintype help copyright credits or license for more information.input:import tensorflow as tfresult:traceback most recent call last file stdin line in module file library/python/./site-packages/tensorflow/ init .py line in module from tensorflow.python import file library/python/./site-packages/tensorflow/python/ init .py line in module from tensorflow.core.framework.graph_pb import file library/python/./site-packages/tensorflow/core/framework/graph_pb.py line in module from google.protobuf import reflection as reflection file library/python/./site-packages/google/protobuf/reflection.py line in module from google.protobuf.internal import python_message as message_impl file library/python/./site-packages/google/protobuf/internal/python_message.py line in module import six.moves.copyreg as copyregimporterror no module named copyreg
115996914,50,https://api.github.com/repos/tensorflow/tensorflow/issues/50,FabianShallari,64,0,0,0,0,34,were hoping to entice you to contribute swig interfaces to your favorite language be it go java lua javascript or r no ruby really cmon ruby is awesome
115956927,37,https://api.github.com/repos/tensorflow/tensorflow/issues/37,keon,634,1,70,97,14,149,because javascript is awesome
115955547,36,https://api.github.com/repos/tensorflow/tensorflow/issues/36,wangdelp,0,0,0,2,0,0,when running python tensorflow/gdoc/tutorials/mnist/fully_connected_feed.py get the following error traceback most recent call last file tensorflow/gdoc/tutorials/mnist/fully_connected_feed.py line in module import tensorflow.python.platformimporterror no module named tensorflow.python.platform
115944861,31,https://api.github.com/repos/tensorflow/tensorflow/issues/31,eleijonmarck,5,0,0,0,0,0,is there any slack channel connected to tensorflow that you could access to get help
115938248,25,https://api.github.com/repos/tensorflow/tensorflow/issues/25,infojunkie,12,0,0,0,0,0,are there plans to support cuda compute capability
115928097,22,https://api.github.com/repos/tensorflow/tensorflow/issues/22,outlace,661,0,20,51,0,60,i understand tensorflow only supports cuda what would need to be done to add in opencl support
115926362,19,https://api.github.com/repos/tensorflow/tensorflow/issues/19,janerivi,71,0,0,0,0,0,swift is a very popular and expressive language with a large community of pro developers
115926208,18,https://api.github.com/repos/tensorflow/tensorflow/issues/18,janerivi,137,0,0,0,0,39,c is very popular and expressive language with a large community of pro developers
115925964,17,https://api.github.com/repos/tensorflow/tensorflow/issues/17,mohamedmansour,93,0,0,4,2,0,i was excited to see tensorflow but as many other users we are on windows would be nice to see this support happen will you accept windows port contributions?in the meantime microsoft recently released their deep learning toolkit which scales on multiple machines with gpus for both linux and windows
115914766,10,https://api.github.com/repos/tensorflow/tensorflow/issues/10,alonsovidales,55,0,0,0,0,11,"hi!,im really interested on contribute in my spare time on this project it would be great for me port the python libraries to go or help with this task the problem is that after read the contributing guidelines i still dont know how to do it.i tried to sing the individual cla but i get an error that says you must be an owner of the contributors group in order to submit this cla..btw are you working or do you plan to implement soon the go libs?some related projects that i implemented in go this is a distributed recommender system based in the adaptive bootstrapping of recommender systems using decision trees paper by yahoo the site of the project the go ml some machine learning algorithms in go i also implemented neural networks with cuda"
115910900,5,https://api.github.com/repos/tensorflow/tensorflow/issues/5,ravwojdyla,88,0,0,10,0,0,issue to trace effort of swig interface for java started implementation will update with progress if anyone has any comments/tips please feel welcome to join the discussion
115896656,3,https://api.github.com/repos/tensorflow/tensorflow/issues/3,sirinath,13,0,0,0,0,0,your website says contribute swig interfaces to your favorite language it will be good if yo have pure implementations in other languages like jvm and net without having to use swig
115886302,1,https://api.github.com/repos/tensorflow/tensorflow/issues/1,mrry,2,0,0,0,0,0,currently we only support python but we should support python
